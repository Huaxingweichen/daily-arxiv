title$summary$tag$affiliation$summary_zh$url
Self-supervised Pre-training with Masked Shape Prediction for 3D Scene  Understanding$  Masked signal modeling has greatly advanced self-supervised pre-training forlanguage and 2D images. However, it is still not fully explored in 3D sceneunderstanding. Thus, this paper introduces Masked Shape Prediction (MSP), a newframework to conduct masked signal modeling in 3D scenes. MSP uses theessential 3D semantic cue, i.e., geometric shape, as the prediction target formasked points. The context-enhanced shape target consisting of explicit shapecontext and implicit deep shape feature is proposed to facilitate exploitingcontextual cues in shape prediction. Meanwhile, the pre-training architecturein MSP is carefully designed to alleviate the masked shape leakage from pointcoordinates. Experiments on multiple 3D understanding tasks on both indoor andoutdoor datasets demonstrate the effectiveness of MSP in learning good featurerepresentations to consistently boost downstream performance.$Keywords: self-supervised pre-training, 3D scene understanding, masked signal modeling, geometric shape, context-enhanced shape target, deep shape feature, network design.$作者：Li Jiang（Max Planck Institute for Informatics），Zetong Yang（CUHK），Shaoshuai Shi、Vladislav Golyanik、Dengxin Dai、Bernt Schiele（Max Planck Institute for Informatics）$本文提出了一种新的自监督预训练框架——Masked Shape Prediction (MSP)，以掩码信号建模为核心，着重于三维场景下的预训练。该框架使用几何形状作为预测目标来进行掩码点的预测，并提出了上下文增强形状目标来促进利用形状预测的上下文线索。为了避免点坐标泄露掩码形状，本文设计了多种MSP网络模型。实验结果表明，该方法在室内和室外数据集上的多个三维场景理解任务中具有良好的表现。$http://arxiv.org/pdf/2305.05026v1
Crack Detection of Asphalt Concrete Using Combined Fracture Mechanics  and Digital Image Correlation$"  Cracking is a common failure mode in asphalt concrete (AC) pavements. Manytests have been developed to characterize the fracture behavior of AC. Accuratecrack detection during testing is crucial to describe AC fracture behavior.This paper proposed a framework to detect surface cracks in AC specimens usingtwo-dimensional digital image correlation (DIC). Two significant drawbacks inprevious research in this field were addressed. First, a multi-seed incrementalreliability-guided DIC was proposed to solve the decorrelation issue due tolarge deformation and discontinuities. The method was validated using syntheticdeformed images. A correctly implemented analysis could accurately measurestrains up to 450\\%, even with significant discontinuities (cracks) present inthe deformed image. Second, a robust method was developed to detect cracksbased on displacement fields. The proposed method uses critical crack tipopening displacement ($\\delta_c$) to define the onset of cleavage fracture. Theproposed method relies on well-developed fracture mechanics theory. Theproposed threshold $\\delta_c$ has a physical meaning and can be easilydetermined from DIC measurement. The method was validated using an extendedfinite element model. The framework was implemented to measure the crackpropagation rate while conducting the Illinois-flexibility index test on two ACmixes. The calculated rates could distinguish mixes based on their crackingpotential. The proposed framework could be applied to characterize AC crackingphenomenon, evaluate its fracture properties, assess asphalt mixture testingprotocols, and develop theoretical models."$Asphalt Concrete, Crack Detection, Fracture Mechanics, Digital Image Correlation, Pavement Cracking, Fracture Properties, Testing Protocols.$作者名（机构）：Zehui Zhu, Ph.D.1*和Imad L. Al-Qadi, Ph.D.2（1伊利诺伊大学厄巴纳-香槟分校土木与环境工程系；2伊利诺伊大学厄巴纳-香槟分校土木与环境工程系）$本文提出了一种新的方法，使用二维数字图像相关法（DIC）检测沥青混凝土样品表面的裂缝。通过提出可靠性导向DIC方法解决由于大变形和不连续性导致的相关性问题，并基于位移场开发了一种鲁棒的检测裂缝的方法，利用临界裂缝尖端张开位移（𝛿𝑐）来定义裂纹扩展。此方法基于破裂力学理论并使用扩展有限元模型进行验证，经验证可用于评估AC材料的裂纹性能和材料测试协议。在进行伊利诺伊-柔度指数测试时计算裂纹扩展速率，可区分不同混凝土材料的开裂潜力。该框架可用于表征AC开裂现象，并为其提高理论模型提供基础。$http://arxiv.org/pdf/2305.05057v1
Wood-sleeper Decayed Detection for Rural Railway Prognostics Using  Unsupervised Deeper FCDDs$  It is critical for railway managers to maintain a high standard to ensureuser safety during daily operations. Top-view or side-view cameras and GPSpositioning system have enabled progress toward automating the periodicinspection of defective features and assessing the deteriorated status of therailway components. Frequently, collecting deteriorated status data constraintstime consuming and repeated data acquisition, because the temporal occurrenceis extremely imbalanced. Supervised learning approach requires thousands ofpaired dataset of defective raw images and annotated labels. However, one-classclassification approach has a merit that fewer images enables us to optimizethe parameters for training normal and anomalous feature. Simultaneously, thevisual heat map explanation enables us to discriminate the localized damagefeature. In this paper, we propose a prognostic discriminator pipeline toautomate one-class damage classification towards defective railway components.We also sensitivity analyze toward the backbone and the receptive field basedon convolutional neural networks (CNNs) using pretrained networks: baselineCNN27, VGG16, ResNet101, and Inception Networks. We also visualize theexplanation of the defective railway feature using a transposed Gaussianupsampling. We demonstrate our application for railway inspection in anopen-accessed dataset of defective railway components, and wood-sleeperdeterioration in rural railway. The heatmap is so important that thehazard-marks could cause an operational delay, an urgent inspection, andunexpected accident to passenger impact in railway inspection. Furthermore, wemention its usability for prognostic monitoring and future works for railwaycomponents inspection in the predictive maintenance of railway systems.$Keywords: Wood-sleeper, Railway Prognostics, Unsupervised Learning, FCDDs, Deteriorated Status, One-class Classification, Convolutional Neural Networks, Heat Map Explanation, Rural Railway, Derailment Risk, Predictive Maintenance.$"作者：Takato Yasuno、Masahiro Okano和Junichiro Fujii（机构：Yachiyo Engineering株式会社，日本东京台东区111-8648）
文章题目：Wood-sleeper Decayed Detection for Rural Railway Prognostics Using Unsupervised Deeper FCDDs（利用无监督深层FCDD检测农村铁路木枕腐朽）
摘要：本篇论文提出了一种预后鉴别器管道，以自动化一类损伤分类，用于检测铁路组件的损伤。同时，本文还对基于卷积神经网络（CNN）的骨干和接受域的敏感性进行了分析，并对四种CNN进行了预训练网络的实验比较。作者们还使用转置高斯上采样可视化了损坏铁路组件的解释。最后，作者们在开放式与损坏铁路组件和农村铁路木枕退化数据集上对算法进行了验证，本篇文章为今后的预测维护铁路系统提供了有益的启示。"$本文研究了基于无监督深度FCDDs的木枕腐烂检测方法，该方法可用于预测农村铁路组件的损坏状态。作者提出了一种预测鉴别器流水线，用于自动分类损坏铁路组件的一类损坏。此外，针对卷积神经网络（CNN）的主干和感受野，作者进行了敏感性分析，并使用预训练网络（基线CNN27、VGG16、ResNet101和Inception Networks）可视化了损坏铁路组件的说明。作者在公开数据集中对铁路检查进行了应用，并针对农村铁路的木枕损坏进行了演示。本文认为可视化热图对于检查铁路安全至关重要，因为危险标志可能会导致运营延误，紧急检查和意外影响乘客在铁路检查。此外，作者还探讨了该方法在铁路系统预测性维护中用于预测监测和未来工作的可用性。$http://arxiv.org/pdf/2305.05103v1
Dual flow fusion model for concrete surface crack segmentation$  Cracks and other diseases are important factors that threaten the safeoperation of transportation infrastructure. Traditional manual detection andultrasonic instrument detection consume a lot of time and resource costs. Withthe development of deep learning technology, many deep learning models arewidely used in actual visual segmentation tasks. The detection method based onthe deep learning model has the advantages of high detection accuracy, fastdetection speed and simple operation. However, the crack segmentation based ondeep learning has problems such as sensitivity to background noise, roughedges, and lack of robustness. Therefore, this paper proposes a fissuresegmentation model based on two-stream fusion, which simultaneously inputsimages into two designed processing streams to independently extractlong-distance dependent and local detail features, and realizes adaptiveprediction through a dual-head mechanism. At the same time, a new interactivefusion mechanism is proposed to guide the complementarity of different levelsof features to realize the location and identification of cracks in complexbackgrounds. Finally, we propose an edge optimization method to improvesegmentation accuracy. Experiments have proved that the F1 value of thesegmentation results on the DeepCrack[1] public dataset reached 93.7%, and theIOU value reached 86.6%; the F1 value of the segmentation results on theCRACK500[2] dataset reached 78.1%, and the IOU value reached 66.0%.$can be seen that concrete surface crack segmentation is a key area in transportation infrastructure safety. The proposed dual flow fusion model utilizes deep learning technology, with a focus on sensitivity to background noise and robustness, to improve the accuracy of crack segmentation. Key terms include deep learning, segmentation, feature interaction, and transformer.$"is necessary to develop efficient and accurate methods for concrete surface crack segmentation.

本论文的作者为段雨巍、林迅、唐文忠、曲小磊，所属机构为北京航空航天大学。论文标题为《Dual flow fusion model for concrete surface crack segmentation》。文章提出了一种基于双流融合的裂缝分割模型，通过将图像同时输入两个设计的处理流以独立提取远距离依赖和局部细节特征，并通过双头机制实现自适应预测。同时，提出一种新的交互式融合机制，以指导不同级别特征的互补性，实现在复杂背景中对裂缝的定位和识别。最后，提出了一种边缘优化方法以提高分割精度。实验表明，在DeepCrack公共数据集上，分割结果的F1值达到93.7％，IOU值达到86.6％；在CRACK500数据集上，分割结果的F1值达到78.1％，IOU值达到66.0％。 本文的关键词为变压器、裂缝分割、特征交互。"$is essential to accurately detect and segment concrete cracks in transportation infrastructure. Traditional detection methods are time-consuming and costly, while deep learning-based segmentation methods have high accuracy but are sensitive to noise and lack robustness. In this paper, a dual flow fusion model is proposed for concrete surface crack segmentation, which inputs images into two processing streams to extract long-distance dependent and local detail features and uses an interactive fusion mechanism to achieve adaptive prediction and complementarity of features. An edge optimization method is also proposed to improve segmentation accuracy. Experimental results on public datasets show high F1 and IOU values for segmentation accuracy. This model can be used for efficient and accurate concrete crack detection in transportation infrastructure maintenance.$http://arxiv.org/pdf/2305.05132v1
Linguistic More: Taking a Further Step toward Efficient and Accurate  Scene Text Recognition$"  Vision model have gained increasing attention due to their simplicity andefficiency in Scene Text Recognition (STR) task. However, due to lacking theperception of linguistic knowledge and information, recent vision models sufferfrom two problems: (1) the pure vision-based query results in attention drift,which usually causes poor recognition and is summarized as linguisticinsensitive drift (LID) problem in this paper. (2) the visual feature issuboptimal for the recognition in some vision-missing cases (e.g. occlusion,etc.). To address these issues, we propose a $\\textbf{L}$inguistic$\\textbf{P}$erception $\\textbf{V}$ision model (LPV), which explores thelinguistic capability of vision model for accurate text recognition. Toalleviate the LID problem, we introduce a Cascade Position Attention (CPA)mechanism that obtains high-quality and accurate attention maps throughstep-wise optimization and linguistic information mining. Furthermore, a GlobalLinguistic Reconstruction Module (GLRM) is proposed to improve therepresentation of visual features by perceiving the linguistic information inthe visual space, which gradually converts visual features into semanticallyrich ones during the cascade process. Different from previous methods, ourmethod obtains SOTA results while keeping low complexity (92.4% accuracy withonly 8.11M parameters). Code is available at$\\href{https://github.com/CyrilSterling/LPV}{https://github.com/CyrilSterling/LPV}$."$Keywords: Scene Text Recognition, linguistic perception, attention drift, Cascade Position Attention, Global Linguistic Reconstruction Module.$"作者：Boqiang Zhang, Hongtao Xie, Yuxin Wang, Jianjun Xu, Yongdong Zhang
机构：中国科学技术大学（University of Science and Technology of China, Hefei, China）"$这篇论文提出了一种解决场景文本识别中注意力漂移和视觉特征子优化问题的新方法——语言感知视觉模型（LPV）。通过级联位置注意力机制和全局语言重构模块，该模型能够获取高质量和准确的注意力图以及语义丰富的视觉特征，并在保持低复杂性的同时实现了SOTA结果（92.4%准确率，仅有8.11M个参数）。该论文分析了场景文本识别中注意力漂移的问题及其原因，并提出了一种有效的解决方案。此外，该模型还通过感知视觉空间中的语言信息，将视觉特征逐步转化为语义丰富的特征。论文对该模型的性能进行了充分的实验验证。$http://arxiv.org/pdf/2305.05140v1
A Mountain-Shaped Single-Stage Network for Accurate Image Restoration$  Image restoration is the task of aiming to obtain a high-quality image from acorrupt input image, such as deblurring and deraining. In image restoration, itis typically necessary to maintain a complex balance between spatial detailsand contextual information. Although a multi-stage network can optimallybalance these competing goals and achieve significant performance, this alsoincreases the system\'s complexity. In this paper, we propose a mountain-shapedsingle-stage design base on a simple U-Net architecture, which removes orreplaces unnecessary nonlinear activation functions to achieve the abovebalance with low system complexity. Specifically, we propose a feature fusionmiddleware (FFM) mechanism as an information exchange component between theencoder-decoder architectural levels. It seamlessly integrates upper-layerinformation into the adjacent lower layer, sequentially down to the lowestlayer. Finally, all information is fused into the original image resolutionmanipulation level. This preserves spatial details and integrates contextualinformation, ensuring high-quality image restoration. In addition, we propose amulti-head attention middle block (MHAMB) as a bridge between the encoder anddecoder to capture more global information and surpass the limitations of thereceptive field of CNNs. Extensive experiments demonstrate that our approach,named as M3SNet, outperforms previous state-of-the-art models while using lessthan half the computational costs, for several image restoration tasks, such asimage deraining and deblurring.$Keywords: image restoration, deblurring, deraining, U-Net architecture, feature fusion middleware, multi-head attention middle block, M3SNet, computational costs.$作者名（机构）：Hu Gao, Jing Yang, Ying Zhang, Ning Wang, Jingfan Yang, Depeng Dang（北京师范大学人工智能学院）$本文提出了一种基于单阶段网络的山形设计，可以在低系统复杂度下实现高质量图像恢复。在图像恢复中，需要保持空间细节和上下文信息之间的复杂平衡。尽管多阶段网络可以优化平衡这些竞争目标并实现显著性能，但也增加了系统的复杂性。该模型通过使用特征融合中间件和多头注意力中间块机制，无缝地将上层信息集成到相邻的下层，以保留空间细节和集成上下文信息，从而实现高质量图像恢复，并在几个图像恢复任务中优于以前的最新模型，同时使用不到一半的计算成本。$http://arxiv.org/pdf/2305.05146v1
Multi-Granularity Denoising and Bidirectional Alignment for Weakly  Supervised Semantic Segmentation$  Weakly supervised semantic segmentation (WSSS) models relying on classactivation maps (CAMs) have achieved desirable performance comparing to thenon-CAMs-based counterparts. However, to guarantee WSSS task feasible, we needto generate pseudo labels by expanding the seeds from CAMs which is complex andtime-consuming, thus hindering the design of efficient end-to-end(single-stage) WSSS approaches. To tackle the above dilemma, we resort to theoff-the-shelf and readily accessible saliency maps for directly obtainingpseudo labels given the image-level class labels. Nevertheless, the salientregions may contain noisy labels and cannot seamlessly fit the target objects,and saliency maps can only be approximated as pseudo labels for simple imagescontaining single-class objects. As such, the achieved segmentation model withthese simple images cannot generalize well to the complex images containingmulti-class objects. To this end, we propose an end-to-end multi-granularitydenoising and bidirectional alignment (MDBA) model, to alleviate the noisylabel and multi-class generalization issues. Specifically, we propose theonline noise filtering and progressive noise detection modules to tackleimage-level and pixel-level noise, respectively. Moreover, a bidirectionalalignment mechanism is proposed to reduce the data distribution gap at bothinput and output space with simple-to-complex image synthesis andcomplex-to-simple adversarial learning. MDBA can reach the mIoU of 69.5\\% and70.2\\% on validation and test sets for the PASCAL VOC 2012 dataset. The sourcecodes and models have been made available at\\url{https://github.com/NUST-Machine-Intelligence-Laboratory/MDBA}.$Field keywords: Weakly supervised semantic segmentation, class activation maps, saliency maps, multi-granularity denoising, bidirectional alignment, image-level label, noisy label.$作者名（机构）：Tao Chen, Yazhou Yao 和 Jinhui Tang（南京理工大学计算机科学与工程学院，中国）$本文提出了一种基于弱监督的语义分割模型，使用了多粒度去噪和双向对齐的技术来解决众所周知的问题，即基于类激活图（CAM）的模型需要扩展种子以生成伪标签，这一过程较为复杂和耗时，并且限制了设计高效单阶段的端到端模型。作者提出使用易得到的显著性图来替代CAM生成伪标签，并使用在线去噪和渐进噪声检测模块来处理噪声标签，以及双向对齐机制来减少输入和输出空间上的数据分布差异，从而提高模型的泛化能力。在测试中，该模型在PASCAL VOC 2012数据集上达到了69.5%和70.2%的mIoU。$http://arxiv.org/pdf/2305.05154v1
Child Palm-ID: Contactless Palmprint Recognition for Children$  Effective distribution of nutritional and healthcare aid for children,particularly infants and toddlers, in some of the least developed and mostimpoverished countries of the world, is a major problem due to the lack ofreliable identification documents. Biometric authentication technology has beeninvestigated to address child recognition in the absence of reliable IDdocuments. We present a mobile-based contactless palmprint recognition system,called Child Palm-ID, which meets the requirements of usability, hygiene, cost,and accuracy for child recognition. Using a contactless child palmprintdatabase, Child-PalmDB1, consisting of 19,158 images from 1,020 unique palms(in the age range of 6 mos. to 48 mos.), we report a TAR=94.11% @ FAR=0.1%. Theproposed Child Palm-ID system is also able to recognize adults, achieving aTAR=99.4% on the CASIA contactless palmprint database and a TAR=100% on theCOEP contactless adult palmprint database, both @ FAR=0.1%. These accuraciesare competitive with the SOTA provided by COTS systems. Despite these highaccuracies, we show that the TAR for time-separated child-palmprints is only78.1% @ FAR=0.1%.$Keywords: child recognition, contactless palmprint recognition, biometric authentication, healthcare aid, malnourishment, identification documents.$"作者：Akash Godbole, Steven A. Grosz 和 Anil K. Jain（密歇根州立大学）
机构：密歇根州立大学（Michigan State University）"$本文介绍了一种针对婴幼儿等无可靠身份证明的儿童的非接触式手掌识别系统——Child Palm-ID。随着移动设备的应用，该技术可在必要时用于分发营养和医疗援助。文章详细介绍了该系统在可用性、卫生、成本和准确性方面的要求，使用1,020个儿童手掌（年龄在6个月至48个月之间）的19,158张图像构建了儿童手掌数据库，并报告了94.11％@ FAR = 0.1％的真正识别率。实验证明，该系统也能够识别成人，CASIA无接触式手掌数据库的真正识别率为99.4％，COEP无接触式成人手掌数据库的真正识别率为100％。但是，对于时间分隔的儿童手掌图像，真正识别率仅为78.1％@ FAR = 0.1％。文章强调，儿童隐私保护是使用该技术的关键问题之一。$http://arxiv.org/pdf/2305.05161v1
SRIL: Selective Regularization for Class-Incremental Learning$  Human intelligence gradually accepts new information and accumulatesknowledge throughout the lifespan. However, deep learning models suffer from acatastrophic forgetting phenomenon, where they forget previous knowledge whenacquiring new information. Class-Incremental Learning aims to create anintegrated model that balances plasticity and stability to overcome thischallenge. In this paper, we propose a selective regularization method thataccepts new knowledge while maintaining previous knowledge. We first introducean asymmetric feature distillation method for old and new classes inspired bycognitive science, using the gradient of classification and knowledgedistillation losses to determine whether to perform pattern completion orpattern separation. We also propose a method to selectively interpolate theweight of the previous model for a balance between stability and plasticity,and we adjust whether to transfer through model confidence to ensure theperformance of the previous class and enable exploratory learning. We validatethe effectiveness of the proposed method, which surpasses the performance ofexisting methods through extensive experimental protocols using CIFAR-100,ImageNet-Subset, and ImageNet-Full.$Specific domain keywords: Class-Incremental Learning, Selective Regularization, Catastrophic forgetting, Continual learning, Knowledge distillation, Response-based methods, Relation-based methods, Feature-based methods, Cognitive science, Plasticity, Stability, Pattern completion, Pattern separation, Model confidence, Exploratory learning, CIFAR-100, ImageNet-Subset, ImageNet-Full.$"作者：Jisu Han, Jaemin Na, Wonjun Hwang
机构：Ajou University"$本文提出了一种选择性正则化的方法，以实现类增量学习。随着深度学习模型的学习，它们往往会忘记之前学到的知识，这就是灾难性遗忘现象。而类增量学习的目标是创造一个平衡可塑性和稳定性的综合模型来克服这个挑战。作者提出了一种非对称特征蒸馏方法，使用分类和知识蒸馏损失的梯度来确定是执行模式补齐还是模式分离，以维护旧的和新的类的知识。此外，还提出了一种方法来有选择性地插值前一模型的权重，以实现稳定性和可塑性之间的平衡，并根据模型置信度调整是否进行转移以确保以前类的性能并启用探索性学习。通过CIFAR-100，ImageNet-Subset和ImageNet-Full的广泛实验说明了所提出方法的有效性，超过了现有方法的性能。$http://arxiv.org/pdf/2305.05175v1
Hybrid Transformer and CNN Attention Network for Stereo Image  Super-resolution$  Multi-stage strategies are frequently employed in image restoration tasks.While transformer-based methods have exhibited high efficiency in single-imagesuper-resolution tasks, they have not yet shown significant advantages overCNN-based methods in stereo super-resolution tasks. This can be attributed totwo key factors: first, current single-image super-resolution transformers areunable to leverage the complementary stereo information during the process;second, the performance of transformers is typically reliant on sufficientdata, which is absent in common stereo-image super-resolution algorithms. Toaddress these issues, we propose a Hybrid Transformer and CNN Attention Network(HTCAN), which utilizes a transformer-based network for single-imageenhancement and a CNN-based network for stereo information fusion. Furthermore,we employ a multi-patch training strategy and larger window sizes to activatemore input pixels for super-resolution. We also revisit other advancedtechniques, such as data augmentation, data ensemble, and model ensemble toreduce overfitting and data bias. Finally, our approach achieved a score of23.90dB and emerged as the winner in Track 1 of the NTIRE 2023 Stereo ImageSuper-Resolution Challenge.$Hybrid Transformer, CNN Attention Network, Stereo Image Super-resolution, Transformer, CNN, multi-patch training, data augmentation, data ensemble, model ensemble.$作者名（机构）：Ming Cheng, Haoyu Ma, Qiufang Ma, Xiaopeng Sun, Weiqi Li, Zhenyu Zhang, Xuhan Sheng, Shijie Zhao, Junlin Li, Li Zhang (ByteDance Inc, Peking University Shenzhen Graduate School)$本论文提出了一个混合Transformer和CNN注意力网络 (HTCAN) 来解决立体图像超分辨率中的问题。传统的Transformer方法通常只考虑单独图像，而HTCAN将Transformer和CNN结合起来，利用立体图像中的互补信息，提高特征提取和超分辨率效果。此外，作者还采用了多个补丁训练策略和更大的窗口尺寸来增加输入像素的激活数量，减少数据偏差。最终，该方法在 NTIRE 2023 立体图像超分辨率挑战赛中获得了最高分数并获胜。$http://arxiv.org/pdf/2305.05177v1
Boosting Visual-Language Models by Exploiting Hard Samples$  Large vision and language models, such as Contrastive Language-ImagePre-training (CLIP), are rapidly becoming the industry norm for matching imagesand texts. In order to improve its zero-shot recognition performance, currentresearch either adds additional web-crawled image-text pairs or designs newtraining losses. However, the additional costs associated with training fromscratch and data collection substantially hinder their deployment. In thispaper, we present HELIP, a low-cost strategy for boosting the performance ofwell-trained CLIP models by finetuning them with hard samples over originaltraining data. Mixing hard examples into each batch, the well-trained CLIPmodel is then fine-tuned using the conventional contrastive alignment objectiveand a margin loss to distinguish between normal and hard negative data. HELIPis deployed in a plug-and-play fashion to existing models. On a comprehensivezero-shot and retrieval benchmark, without training the model from scratch orutilizing additional data, HELIP consistently boosts existing models to achieveleading performance. In particular, HELIP boosts ImageNet zero-shot accuracy ofSLIP by 3.05 and 4.47 when pretrained on CC3M and CC12M respectively. Inaddition, a systematic evaluation of zero-shot and linear probing experimentsacross fine-grained classification datasets demonstrates a consistentperformance improvement and validates the efficacy of HELIP . When pretrainingon CC3M, HELIP boosts zero-shot performance of CLIP and SLIP by 8.4\\% and18.6\\% on average respectively, and linear probe performance by 9.5\\% and 3.0\\%on average respectively.$Computer vision, natural language processing, contrastive language-image pretraining, zero-shot recognition, hard samples, margin loss.$文章的作者是 Haonan Wang、Minbin Huang、Runhui Huang、Lanqing Hong、Hang Xu、Tianyang Hu、Xiaodan Liang 和 Zhenguo Li，机构分别为 National University of Singapore、Sun Yat-sen University 和 Huawei Noah’s Ark Lab。$该论文研究了利用难样本来提高已经训练好的CLIP模型性能的方法。这种名为HELIP的低成本策略是将难例子混合到每个批次中，并通过一种边缘损失来区分普通和难以负样本，从而微调好的CLIP模型，以提高其零样本识别性能。本文系统评估了零样本和线性探针实验在细粒度分类数据集上的表现，验证了HELIP策略的有效性。实验表明，在不使用额外数据和从头开始训练模型的情况下，HELIP策略始终提高现有模型的性能，并使SLIP模型在预训练CC3M和CC12M的情况下ImageNet的零样本分类精度提高了3.05和4.47。同时，在CC3M预训练的情况下，HELIP策略将CLIP和SLIP的零样本表现分别提高8.4%和18.6%的平均值，并将线性探测表现分别提高9.5%和3.0%的平均值。本文在CLIP模型的零样本和检索基准测试上展现出卓越性能，表明优化现有视觉-语言模型的方法可以有效地提高其性能，而不需要额外的开销来收集数据和重新训练模型。$http://arxiv.org/pdf/2305.05208v1
Novel Synthetic Data Tool for Data-Driven Cardboard Box Localization$  Application of neural networks in industrial settings, such as automatedfactories with bin-picking solutions requires costly production of largelabeled data-sets. This paper presents an automatic data generation tool with aprocedural model of a cardboard box. We briefly demonstrate the capabilities ofthe system, its various parameters and empirically prove the usefulness of thegenerated synthetic data by training a simple neural network. We make samplesynthetic data generated by the tool publicly available.$Keywords: synthetic data, neural applications, intelligent robotics, cardboard box localization, data-driven.$作者：Peter Kravár（捷克马萨里克大学信息学院）和Lukáš Gajdošech（斯洛伐克康米乌斯大学数学、物理和信息学院，Skeletex Research）。$本文提出了一种用于数据驱动的纸箱定位的新型合成数据工具。在自动化工厂中应用神经网络，例如具有拾取装置的自动化工厂，需要昂贵的大规模标记数据集的生产。本文提出了一种自动数据生成工具，其中包含纸板箱的过程模型。我们简要展示了该系统的功能、各种参数，并通过训练一个简单的神经网络经验性地证明了生成的合成数据的有用性。我们公开提供了由该工具生成的样本合成数据。关键词：合成数据；神经应用；智能机器人。$http://arxiv.org/pdf/2305.05215v1
DynamicKD: An Effective Knowledge Distillation via Dynamic Entropy  Correction-Based Distillation for Gap Optimizing$  The knowledge distillation uses a high-performance teacher network to guidethe student network. However, the performance gap between the teacher andstudent networks can affect the student\'s training. This paper proposes a novelknowledge distillation algorithm based on dynamic entropy correction to reducethe gap by adjusting the student instead of the teacher. Firstly, the effect ofchanging the output entropy (short for output information entropy) in thestudent on the distillation loss is analyzed in theory. This paper shows thatcorrecting the output entropy can reduce the gap. Then, a knowledgedistillation algorithm based on dynamic entropy correction is created, whichcan correct the output entropy in real-time with an entropy controller updateddynamically by the distillation loss. The proposed algorithm is validated onthe CIFAR100 and ImageNet. The comparison with various state-of-the-artdistillation algorithms shows impressive results, especially in the experimenton the CIFAR100 regarding teacher-student pair resnet32x4-resnet8x4. Theproposed algorithm raises 2.64 points over the traditional distillationalgorithm and 0.87 points over the state-of-the-art algorithm CRD inclassification accuracy, demonstrating its effectiveness and efficiency.$Key Domain Keywords: Knowledge Distillation, Deep Neural Networks, Convolutional Neural Networks, CNN Compression, CNN Acceleration.$作者名（机构）：朱松岭（西安电子科技大学智能感知与图像理解教育部重点实验室），尚荣华（通讯作者，西安电子科技大学智能感知与图像理解教育部重点实验室），袁波（南方科技大学脑科学与智能技术广东省实验室），张伟通（西安电子科技大学智能感知与图像理解教育部重点实验室），李洋洋（西安电子科技大学智能感知与图像理解教育部重点实验室），焦利成（西安电子科技大学智能感知与图像理解教育部重点实验室）。$本文提出了一种动态熵校正的知识蒸馏方法，通过调整学生网络而不是教师网络，来减少教师网络和学生网络的性能差距，从而优化学生网络的训练。文章首先在理论上分析了改变学生输出熵对蒸馏损失的影响，证明了熵校正可以减少性能差距。然后，文章提出了一种基于动态熵校正的知识蒸馏算法，该算法可以通过在蒸馏损失中动态更新的熵控制器实时校正学生网络的输出熵。该算法在CIFAR100和ImageNet数据集上进行了验证，并与各种最先进的蒸馏算法进行了比较，表现出令人印象深刻的结果。特别是在教师-学生对resnet32x4-resnet8x4的实验中，该算法的分类精度比传统蒸馏算法提高了2.64点，比最先进的算法CRD提高了0.87点，证明了其有效性和效率。文章关键词包括：卷积神经网络、知识蒸馏、CNN压缩、CNN加速。$http://arxiv.org/pdf/2305.05233v1
Patch-DrosoNet: Classifying Image Partitions With Fly-Inspired Models  For Lightweight Visual Place Recognition$  Visual place recognition (VPR) enables autonomous systems to localizethemselves within an environment using image information. While ConvolutionNeural Networks (CNNs) currently dominate state-of-the-art VPR performance,their high computational requirements make them unsuitable for platforms withbudget or size constraints. This has spurred the development of lightweightalgorithms, such as DrosoNet, which employs a voting system based on multiplebio-inspired units. In this paper, we present a novel training approach forDrosoNet, wherein separate models are trained on distinct regions of areference image, allowing them to specialize in the visual features of thatspecific section. Additionally, we introduce a convolutional-like predictionmethod, in which each DrosoNet unit generates a set of place predictions foreach portion of the query image. These predictions are then combined using thepreviously introduced voting system. Our approach significantly improves uponthe VPR performance of previous work while maintaining an extremely compact andlightweight algorithm, making it suitable for resource-constrained platforms.$Visual place recognition, lightweight algorithms, DrosoNet, bio-inspired models, image partitions, convolutional-like prediction, resource-constrained platforms.$作者名（机构）：Bruno Arcanjo, Bruno Ferrarini, Michael Milford, Klaus D. McDonald-Maier and Shoaib Ehsan (University of Essex and Queensland University of Technology)$本文介绍了一种轻量级的视觉地点识别（VPR）算法Patch-DrosoNet, 该算法使用基于多个生物启发单元的投票系统来进行识别。传统的VPR算法使用卷积神经网络（CNN）具有高计算需求，不适合大多数嵌入式设备。本文介绍了一种新的训练方法，通过在参考图像的不同区域上训练单独的模型，使它们专门针对该特定部分的视觉特征进行训练，并提出了一种类卷积的预测方法，使每个DrosoNet单元为查询图像的每个部分生成一组位置预测。这些预测然后使用投票系统进行组合。实验结果表明，该方法在VPR性能方面明显优于以往的工作，并保持极其紧凑和轻量级的算法，适用于资源受限的平台。$http://arxiv.org/pdf/2305.05256v1
Guided Focal Stack Refinement Network for Light Field Salient Object  Detection$  Light field salient object detection (SOD) is an emerging research directionattributed to the richness of light field data. However, most existing methodslack effective handling of focal stacks, therefore making the latter involvedin a lot of interfering information and degrade the performance of SOD. Toaddress this limitation, we propose to utilize multi-modal features to refinefocal stacks in a guided manner, resulting in a novel guided focal stackrefinement network called GFRNet. To this end, we propose a guided refinementand fusion module (GRFM) to refine focal stacks and aggregate multi-modalfeatures. In GRFM, all-in-focus (AiF) and depth modalities are utilized torefine focal stacks separately, leading to two novel sub-modules for differentmodalities, namely AiF-based refinement module (ARM) and depth-based refinementmodule (DRM). Such refinement modules enhance structural and positionalinformation of salient objects in focal stacks, and are able to improve SODaccuracy. Experimental results on four benchmark datasets demonstrate thesuperiority of our GFRNet model against 12 state-of-the-art models.$"Keywords: Light field, salient object detection, focal stack, refinement, multi-modal fusion. 

This paper proposes a novel framework called guided focal stack refinement network (GFRNet) for light field salient object detection (SOD). Existing methods lack effective handling of focal stacks, leading to reduced SOD performance with interfering information. GFRNet integrates a guided refinement and fusion module (GRFM) that reﬁnes focal stacks and aggregates multi-modal features. Two sub-modules, AiF-based refinement module (ARM) and depth-based refinement module (DRM), are proposed for guided refinement based on modality-specific properties. Experimental results demonstrate the superiority of GFRNet compared to 12 state-of-the-art models on four benchmark datasets."$"作者名（机构）：袁波，江尧，傅可仁，赵琦骏，四川大学计算机科学学院，中国成都610065。

标题：GUIDED FOCAL STACK REFINEMENT NETWORK FOR LIGHT FIELD SALIENT OBJECT DETECTION"$本文提出了一种新颖的引导式焦点栈细化网络，用于处理光场鲜明目标检测。由于焦点栈中的干扰信息缺乏有效处理，导致现有方法的性能下降。为了解决这个问题，本文提出了一种利用多模态特征以引导方式细化焦点栈的新方法，命名为GFRNet。通过引导细化和融合模块（GRFM），本文提出了两个子模块来进行引导式细化，分别是基于AiF（全聚焦）和深度的细化模块（ARM和DRM）。实验结果表明，与12种最先进的模型相比，本文的方法在四个基准数据集上表现出显著优越性。$http://arxiv.org/pdf/2305.05260v1
Fooling State-of-the-Art Deepfake Detection with High-Quality Deepfakes$  Due to the rising threat of deepfakes to security and privacy, it is mostimportant to develop robust and reliable detectors. In this paper, we examinethe need for high-quality samples in the training datasets of such detectors.Accordingly, we show that deepfake detectors proven to generalize well onmultiple research datasets still struggle in real-world scenarios withwell-crafted fakes. First, we propose a novel autoencoder for face swappingalongside an advanced face blending technique, which we utilize to generate 90high-quality deepfakes. Second, we feed those fakes to a state-of-the-artdetector, causing its performance to decrease drastically. Moreover, wefine-tune the detector on our fakes and demonstrate that they contain usefulclues for the detection of manipulations. Overall, our results provide insightsinto the generalization of deepfake detectors and suggest that their trainingdatasets should be complemented by high-quality fakes since training on mereresearch data is insufficient.$Deepfake detection, face swapping, forgery, dataset, high-quality samples, deep-learning frameworks, security, privacy, image and video manipulation, convolutional neural networks (CNNs), target identity, driving identity, appearance information, encoder, decoder, visual quality, realism, deepfake databases, variability in generation methods, visual artifacts, poorly trained models, realism in deepfake datasets.$"作者：Arian Beckmann, Anna Hilsmann, Peter Eisert

机构：Fraunhofer Heinrich-Hertz-Institute, Humboldt University of Berlin, Berlin, Germany"$本文探讨了由于Deepfake对安全和隐私的威胁日益增加，开发鲁棒可靠的Deepfake检测器的重要性。文章指出，训练集中需要高质量的样本。为此，作者提出了一种新颖的自编码器用于面部交换，以及高级的面部融合技术，生成了90个高质量的Deepfake样本。作者将这些样本输入先进的检测器中，检测器的表现显著下降。作者说明，这些高质量Deepfake样本可以作为检测伪造的有用线索。本文研究结果表明，训练集应该补充高质量的Deepfake样本，因为仅仅使用现有数据集的训练是不够的。$http://arxiv.org/pdf/2305.05282v1
CAMIL: Context-Aware Multiple Instance Learning for Whole Slide Image  Classification$  Cancer diagnoses typically involve human pathologists examining whole slideimages (WSIs) of tissue section biopsies to identify tumor cells and theirsubtypes. However, artificial intelligence (AI)-based models, particularlyweakly supervised approaches, have recently emerged as viable alternatives.Weakly supervised approaches often use image subsections or tiles as input,with the overall classification of the WSI based on attention scores assignedto each tile. However, this method overlooks the potential for falsepositives/negatives because tumors can be heterogeneous, with cancer and normalcells growing in patterns larger than a single tile. Such errors at the tilelevel could lead to misclassification at the tumor level. To address thislimitation, we developed a novel deep learning pooling operator called CHARM(Contrastive Histopathology Attention Resolved Models). CHARM leverages thedependencies among single tiles within a WSI and imposes contextual constraintsas prior knowledge to multiple instance learning models. We tested CHARM on thesubtyping of non-small cell lung cancer (NSLC) and lymph node (LN) metastasis,and the results demonstrated its superiority over other state-of-the-art weaklysupervised classification algorithms. Furthermore, CHARM facilitatesinterpretability by visualizing regions of attention.$Context-aware multiple instance learning for whole slide image classification, cancer, histopathology, multiple instance learning, weakly-supervised learning, attention model.$"作者：Olga Fourkioti, Avi Arampatzis, Chen Jin, Mat De Vries, Daniel Alexander, Chris Bakal
机构：The Institute of Cancer Research、Democritus University of Thrace、University College London
论文题目：CAMIL: Context-Aware Multiple Instance Learning for Whole Slide Image Classification"$本文介绍了一种用于整张切片图像分类的上下文感知多实例学习方法。传统的弱监督方法使用图像子区域或瓦片作为输入，通过对每个瓦片分配注意力分数来进行整张图像的分类。然而，该方法忽略了肿瘤可以是异质性的，并且癌细胞和正常细胞在单个瓦片之外的区域生长的可能性。因此，本文提出了一种称为CHARM的新型深度学习池化运算符，它利用WSI内单个瓦片之间的依赖关系，并将上下文约束作为先验知识加入多实例学习模型中。在非小细胞肺癌（NSLC）和淋巴结（LN）转移亚型分类方面，CHARM在弱监督分类算法中展示了其优越性。此外，CHARM通过可视化关注区域来便于解释。关键词：癌症、组织病理学、多实例学习、弱监督学习、关注模型。$http://arxiv.org/pdf/2305.05314v1
TPS++: Attention-Enhanced Thin-Plate Spline for Scene Text Recognition$  Text irregularities pose significant challenges to scene text recognizers.Thin-Plate Spline (TPS)-based rectification is widely regarded as an effectivemeans to deal with them. Currently, the calculation of TPS transformationparameters purely depends on the quality of regressed text borders. It ignoresthe text content and often leads to unsatisfactory rectified results forseverely distorted text. In this work, we introduce TPS++, anattention-enhanced TPS transformation that incorporates the attention mechanismto text rectification for the first time. TPS++ formulates the parametercalculation as a joint process of foreground control point regression andcontent-based attention score estimation, which is computed by a dedicateddesigned gated-attention block. TPS++ builds a more flexible content-awarerectifier, generating a natural text correction that is easier to read by thesubsequent recognizer. Moreover, TPS++ shares the feature backbone with therecognizer in part and implements the rectification at feature-level ratherthan image-level, incurring only a small overhead in terms of parameters andinference time. Experiments on public benchmarks show that TPS++ consistentlyimproves the recognition and achieves state-of-the-art accuracy. Meanwhile, itgeneralizes well on different backbones and recognizers. Code is athttps://github.com/simplify23/TPS_PP.$Keywords of this article: scene text recognition, Thin-Plate Spline, attention mechanism, text rectification.$作者名（机构）：Tianlun Zheng、Zhineng Chen、Jinfeng Bai、Hongtao Xie、Yu-Gang Jiang（上海智能视觉计算协同创新中心，复旦大学计算机科学学院，中国；明日先生生活，中国；中国科学技术大学，中国）$本文提出了一种新的场景文本识别方法，称作TPS++，它是Thin-Plate Spline（TPS）变换的增强版本，首次将注意力机制引入到文本校正中。TPS++将参数计算形式化为前景控制点回归和基于内容的注意力得分估计的联合过程，由专门设计的门控注意力块进行计算。TPS++建立了一个更灵活的内容感知修正器，生成自然的文本校正，更容易被后续的识别器识别。另外，TPS++与识别器共享特征骨干部分，并在特征级别而不是图像级别上实现校正，因此不会带来太多的参数和推理时间开销。实验表明，TPS ++在公共基准上始终提高了识别率，并实现了最先进的精度。与此同时，它在不同的骨干网络和识别器上具有很好的泛化性能。$http://arxiv.org/pdf/2305.05322v1
GPT-NAS: Neural Architecture Search with the Generative Pre-Trained  Model$  Neural Architecture Search (NAS) has emerged as one of the effective methodsto design the optimal neural network architecture automatically. Althoughneural architectures have achieved human-level performances in several tasks,few of them are obtained from the NAS method. The main reason is the hugesearch space of neural architectures, making NAS algorithms inefficient. Thiswork presents a novel architecture search algorithm, called GPT-NAS, thatoptimizes neural architectures by Generative Pre-Trained (GPT) model. InGPT-NAS, we assume that a generative model pre-trained on a large-scale corpuscould learn the fundamental law of building neural architectures. Therefore,GPT-NAS leverages the generative pre-trained (GPT) model to propose reasonablearchitecture components given the basic one. Such an approach can largelyreduce the search space by introducing prior knowledge in the search process.Extensive experimental results show that our GPT-NAS method significantlyoutperforms seven manually designed neural architectures and thirteenarchitectures provided by competing NAS methods. In addition, our ablationstudy indicates that the proposed algorithm improves the performance of finelytuned neural architectures by up to about 12% compared to those without GPT,further demonstrating its effectiveness in searching neural architectures.$Keywords: Neural architecture search, Generative Pre-Trained model, search space, image classification, deep learning. The article proposes a novel NAS algorithm called GPT-NAS, which optimizes neural architectures using a Generative Pre-Trained (GPT) model to reduce the search space by introducing prior knowledge in the search process. The algorithm's effectiveness is demonstrated through extensive experimental results, outperforming manually designed and competing NAS methods. The proposed algorithm improves the performance of finely-tuned neural architectures by up to about 12% compared to those without GPT. The article addresses the main challenge of NAS: the vast search space of possible architectures.$"作者：Caiyang Yu, Xianggen Liu, Chenwei Tang, Wentao Feng 和 Jiancheng Lv

机构：四川大学计算机科学学院"$本文介绍了一种新的神经架构搜索算法，称为GPT-NAS，它利用预先训练的生成模型来优化神经网络架构。GPT-NAS假设预训练的生成模型可以学习构建神经网络架构的基本法则，通过提出合理的架构组件来大大减少搜索空间。实验结果表明，GPT-NAS方法显著优于七个手动设计的神经架构和竞争NAS方法提供的十三个架构。此外，消融研究表明，与不使用GPT相比，所提出的算法可以将精调神经架构的性能提高约12％，进一步证明了其在搜索神经架构方面的有效性。$http://arxiv.org/pdf/2305.05351v1
Unsupervised Writer Retrieval using NetRVLAD and Graph Similarity  Reranking$  This paper presents an unsupervised approach for writer retrieval based onclustering SIFT descriptors detected at keypoint locations resulting inpseudo-cluster labels. With those cluster labels, a residual network followedby our proposed NetRVLAD, an encoding layer with reduced complexity compared toNetVLAD, is trained on 32x32 patches at keypoint locations. Additionally, wesuggest a graph-based reranking algorithm called SGR to exploit similarities ofthe page embeddings to boost the retrieval performance. Our approach isevaluated on two historical datasets (Historical-WI and HisIR19). We include anevaluation of different backbones and NetRVLAD. It competes with related workon historical datasets without using explicit encodings. We set a newState-of-the-art on both datasets by applying our reranking scheme and showthat our approach achieves comparable performance on a modern dataset as well.$Computer Vision, Writer Retrieval, NetRVLAD, Graph Similarity Reranking, Document Analysis.$"作者：Marco Peer、Florian Kleber、Robert Sablatnig
机构：维也纳工业大学计算机视觉实验室（Computer Vision Lab, TU Wien）"$这篇论文提出了一种无监督的写作者检索方法，基于在关键点位置检测到的SIFT描述符进行聚类，并使用伪聚类标签训练一个残差网络，后接一个编码层（NetRVLAD）对关键点位置的32×32补丁进行编码。此外，作者提出了一种基于图形相似性重新排名的SGR算法，以此提高检索性能。该方法在两个历史数据集（Historical-WI和HisIR19）上进行了评估，并使用不同的骨干网络和NetRVLAD进行了比较。结果表明，该方法在历史数据集上表现良好，且无需使用显式编码。通过应用重排算法，该方法在两个数据集上均取得了最新排名，并在现代数据集上达到了可比较的性能。$http://arxiv.org/pdf/2305.05358v1
MSVQ: Self-Supervised Learning with Multiple Sample Views and Queues$"  Self-supervised methods based on contrastive learning have achieved greatsuccess in unsupervised visual representation learning. However, most methodsunder this framework suffer from the problem of false negative samples.Inspired by mean shift for self-supervised learning, we propose a new simpleframework, namely Multiple Sample Views and Queues (MSVQ). We jointly constructa soft label on-the-fly by introducing two complementary and symmetric ways:multiple augmented positive views and two momentum encoders forming varioussemantic features of negative samples. Two teacher networks perform similarityrelationship calculations with negative samples and then transfer thisknowledge to the student. Let the student mimic the similar relationshipbetween the samples, thus giving the student a more flexible ability toidentify false negative samples in the dataset. The classification results onfour benchmark image datasets demonstrate the high effectiveness and efficiencyof our approach compared to some classical methods. Source code and pretrainedmodels are available at $\\href{https://github.com/pc-cp/MSVQ}{this~http~URL}$."$"Keywords: Self-supervised learning, Contrastive learning, Knowledge distillation, Data augmentation, Momentum encoder. 

Summary: This paper proposes a new self-supervised learning framework, Multiple Sample Views and Queues (MSVQ), which addresses the problem of false negative samples in contrastive learning. MSVQ uses two complementary ways to construct a soft label on-the-fly, including multiple augmented positive views and two momentum encoders forming various semantic features of negative samples. The similarity relationship calculations with negative samples are performed by two teacher networks and then transferred to the student network. This improves the student's ability to identify false negative samples in the dataset. MSVQ achieves high effectiveness and efficiency compared to classical methods on four benchmark image datasets."$"文章作者：Chen Peng, Xianzhong Long, Yun Li
作者机构：南京邮电大学计算机科学学院，中国南京210023
文章标题：MSVQ：使用多个样本视图和队列的自监督学习
摘要：本文提出了一种新的自监督学习框架，名为“多个样本视图和队列（MSVQ）”。该方法通过引入多个增强过的正样本视图和两个动量编码器来形成各种负样本的语义特征。通过两个教师网络对负样本进行相似度关系计算，然后将这些知识传递给学生网络，使学生网络能够更灵活地识别数据集中的错误负样本。在四个基准图像数据集上对分类结果进行了实验验证，证明了我们的方法相对于其他经典方法的高效性和有效性。"$这篇论文提出了一种基于自监督学习的新框架——多样本观点与队列（MSVQ），用于解决对比学习中假阴性样本的问题。该方法通过引入多个增强的正样本视图和两个动量编码器形成负样本的各种语义特征来实现实时软标签的联合构建。两个教师网络执行与负样本的相似度关系计算，然后将这个知识传递给学生。在四个基准图像数据集上的分类结果表明，与一些经典方法相比，我们的方法具有高效性和高效性的优势。$http://arxiv.org/pdf/2305.05370v1
"Restormer-Plus for Real World Image Deraining: One State-of-the-Art  Solution to the GT-RAIN Challenge (CVPR 2023 UG$^2$+ Track 3)"$"  This technical report presents our Restormer-Plus approach, which wassubmitted to the GT-RAIN Challenge (CVPR 2023 UG$^2$+ Track 3). Detailsregarding the challenge are available athttp://cvpr2023.ug2challenge.org/track3.html. Our Restormer-Plus outperformedall other submitted solutions in terms of peak signal-to-noise ratio (PSNR). Itconsists mainly of four modules: the single image de-raining module, the medianfiltering module, the weighted averaging module, and the post-processingmodule. We named the single-image de-raining module Restormer-X, which is builton Restormer and performed on each rainy image. The median filtering module isemployed as a median operator for the 300 rainy images associated with eachscene. The weighted averaging module combines the median filtering results withthat of Restormer-X to alleviate overfitting if we only use Restormer-X.Finally, the post-processing module is used to improve the brightnessrestoration. Together, these modules render Restormer-Plus to be onestate-of-the-art solution to the GT-RAIN Challenge. Our code is available athttps://github.com/ZJLAB-AMMI/Restormer-Plus."$Keywords: image deraining, Restormer-Plus, de-raining module, median filtering, weighted averaging, post-processing.$"作者名（机构）: 郑超超（浙江实验室）、王鲁平（浙江实验室）、刘斌（浙江实验室）
本篇论文题为Restormer-Plus for Real World Image Deraining: One State-of-the-Art Solution to the GT-RAIN Challenge (CVPR 2023 UG2+ Track 3)，提交给了CVPR 2023 UG2+ Track 3的GT-RAIN Challenge。机构为浙江实验室应用数学与机器智能研究中心。本文提出了名为Restormer-Plus的解决方案，它包含四个模块：单图像去雨模块、中值滤波模块、加权平均模块和后处理模块。此外，文中还介绍了Restormer-X的两种版本，即基本版本Restormer和改进版本Restormery。该文认为Restormer-Plus是GT-RAIN Challenge的一种最先进的解决方案。"$这篇论文中介绍了一种名为Restormer-Plus的图像去雨技术，是针对CVPR 2023 UG2+ Track 3 GT-RAIN Challenge提交的申请的一种解决方案。其核心模块包括单个图像去雨模块，中值滤波模块，加权平均模块和后处理模块。通过使用这些模块，Restormer-Plus在峰值信噪比方面表现出色，被认为是该挑战的最佳解决方案之一。这篇论文介绍了Restormer-Plus的每个模块，并详细介绍了该方法的实验设置。该方法在去除雨天图像方面取得了优异成果，其代码也可以通过公开网站进行访问。$http://arxiv.org/pdf/2305.05454v1
Real-time instance segmentation with polygons using an  Intersection-over-Union loss$"  Predicting a binary mask for an object is more accurate but also morecomputationally expensive than a bounding box. Polygonal masks as developed inCenterPoly can be a good compromise. In this paper, we improve over CenterPolyby enhancing the classical regression L1 loss with a novel region-based lossand a novel order loss, as well as with a new training process for the verticesprediction head. Moreover, the previous methods that predict polygonal masksuse different coordinate systems, but it is not clear if one is better thananother, if we abstract the architecture requirement. We therefore investigatetheir impact on the prediction. We also use a new evaluation protocol withoracle predictions for the detection head, to further isolate the segmentationprocess and better compare the polygonal masks with binary masks. Our instancesegmentation method is trained and tested with challenging datasets containingurban scenes, with a high density of road users. Experiments show, inparticular, that using a combination of a regression loss and a region-basedloss allows significant improvements on the Cityscapes and IDD test setcompared to CenterPoly. Moreover the inference stage remains fast enough toreach real-time performance with an average of 0.045 s per frame for2048$\\times$1024 images on a single RTX 2070 GPU. The code is available$\\href{https://github.com/KatiaJDL/CenterPoly-v2}{\\text{here}}$."$Computer vision, real-time instance segmentation, polygons, Intersection-over-Union loss, urban scenes, mask approximation.$"作者：Katia Jodogne-del Litto, Guillaume-Alexandre Bilodeau（Polytechnique Montréal）
机构：LITIV Lab., Polytechnique Montréal
摘要：本文提出了一种使用多边形和IoU损失实现实时目标实例分割的方法。与Bounding Box相比，多边形掩码可以提供更准确的预测，但计算代价也更高。为了处理这个问题，本文将回归L1损失加强，以包括一种新的区域损失、一种新的排序损失和一种适用于顶点预测头的新的训练过程。我们还研究了不同坐标系对多边形掩码预测的影响。我们运用新的评估协议并在城市场景下进行实验。实验结果表明，与CenterPoly相比，本文提出的组合回归损失和区域损失的方法对Cityscapes和IDD测试集有显着的改进。此外，基于实例分割的推理阶段可在单个RTX 2070 GPU上以平均每帧0.045秒的速度实现实时性能。代码可在https://github.com/KatiaJDL/CenterPoly-v2中获得。"$本论文提出了一种实时实例分割方法，该方法使用多边形来代替传统的二进制掩模，从而在准确性和计算效率之间取得了平衡。该方法通过强化经典的回归L1损失，并引入一种新的面积损失和一种新的顺序损失，同时使用新的训练流程来改善CenterPoly方法。此外，文章还研究了在不同坐标系统下进行预测的影响，并使用了一种新的评估协议来比较多边形掩模和二进制掩模的性能。实验结果表明，在Cityscapes和IDD数据集上使用该方法相对于CenterPoly具有显著的改进，而且在单个RTX 2070 GPU上实时处理效率达到了0.045秒每帧。$http://arxiv.org/pdf/2305.05490v1
Self-supervised dense representation learning for live-cell microscopy  with time arrow prediction$  State-of-the-art object detection and segmentation methods for microscopyimages rely on supervised machine learning, which requires laborious manualannotation of training data. Here we present a self-supervised method based ontime arrow prediction pre-training that learns dense image representations fromraw, unlabeled live-cell microscopy videos. Our method builds upon the task ofpredicting the correct order of time-flipped image regions via a single-imagefeature extractor and a subsequent time arrow prediction head. We show that theresulting dense representations capture inherently time-asymmetric biologicalprocesses such as cell divisions on a pixel-level. We furthermore demonstratethe utility of these representations on several live-cell microscopy datasetsfor detection and segmentation of dividing cells, as well as for cell stateclassification. Our method outperforms supervised methods, particularly whenonly limited ground truth annotations are available as is commonly the case inpractice. We provide code at https://github.com/weigertlab/tarrow.$Keywords: Self-supervised learning, Live-cell microscopy, Dense representation learning, Time arrow prediction, Object detection, Cell segmentation, Cell state classification.$"作者：Benjamin Gallusser, Max Stieber, Martin Weigert（École polytechnique fédérale de Lausanne (EPFL)）
机构：École polytechnique fédérale de Lausanne (EPFL)"$这篇论文提出了一种基于自监督学习的方法，通过时间箭头预测来学习从原始、未标记的活细胞显微镜视频中的密集图像表示。该方法利用预测时间翻转图像区域的正确顺序的任务，通过单图像特征提取器和随后的时间箭头预测头来构建。结果表明，所得到的密集表示捕捉了具有内在时间不对称性的生物过程，例如像素级的细胞分裂。该方法在多个活细胞显微镜数据集上用于分裂细胞的检测和分割，以及用于细胞状态分类，其性能优于传统的基于监督学习的方法，特别是在实践中只有有限的地面真相标注可用的情况下。$http://arxiv.org/pdf/2305.05511v1
RMES: Real-Time Micro-Expression Spotting Using Phase From Riesz Pyramid$  Micro-expressions (MEs) are involuntary and subtle facial expressions thatare thought to reveal feelings people are trying to hide. ME spotting detectsthe temporal intervals containing MEs in videos. Detecting such quick andsubtle motions from long videos is difficult. Recent works leverage detailedfacial motion representations, such as the optical flow, and deep learningmodels, leading to high computational complexity. To reduce computationalcomplexity and achieve real-time operation, we propose RMES, a real-time MEspotting framework. We represent motion using phase computed by Riesz Pyramid,and feed this motion representation into a three-stream shallow CNN, whichpredicts the likelihood of each frame belonging to an ME. In comparison tooptical flow, phase provides more localized motion estimates, which areessential for ME spotting, resulting in higher performance. Using phase alsoreduces the required computation of the ME spotting pipeline by 77.8%. Despiteits relative simplicity and low computational complexity, our frameworkachieves state-of-the-art performance on two public datasets: CAS(ME)2 and SAMMLong Videos.$"Keywords: micro-expressions, real-time spotting, Riesz Pyramid, phase, CNN. 

Summary: This paper proposes a real-time micro-expression spotting framework called RMES that reduces computational complexity by using phase features extracted by a Riesz Pyramid in the front end with a lightweight three-stream shallow CNN in the back end. The proposed approach achieves state-of-the-art performance on two public datasets while reducing inference time by 77.8% compared to optical flow."$"作者名（机构）：Yini Fang, Didan Deng, Liang Wu, Frederic Jumellexy, and Bertram Shi（香港科技大学，Ydentity Organization，Bright Nation Limited） 

论文标题：RMES: Real-Time Micro-Expression Spotting Using Phase From Riesz Pyramid"$该论文介绍了一种用于实时微表情检测的RMES框架。微表情是无意识、细微的面部表情，揭示人们试图隐藏的情感。该论文提出了一种实时微表情识别框架，使用Riesz金字塔计算的相位来表示运动，并将这种运动表示为一个三流的浅层CNN，以预测每一帧属于微表情的可能性。与光流相比，相位提供更定位的运动估计，对于微表情识别至关重要，从而产生更高的性能。使用相位还可以将微表情检测管道的计算需求降低77.8%。尽管其相对简单和低计算复杂度，但此方法在两个公共数据集上都取得了最新的性能。$http://arxiv.org/pdf/2305.05523v1
EFE: End-to-end Frame-to-Gaze Estimation$  Despite the recent development of learning-based gaze estimation methods,most methods require one or more eye or face region crops as inputs and producea gaze direction vector as output. Cropping results in a higher resolution inthe eye regions and having fewer confounding factors (such as clothing andhair) is believed to benefit the final model performance. However, thiseye/face patch cropping process is expensive, erroneous, andimplementation-specific for different methods. In this paper, we propose aframe-to-gaze network that directly predicts both 3D gaze origin and 3D gazedirection from the raw frame out of the camera without any face or eyecropping. Our method demonstrates that direct gaze regression from the rawdownscaled frame, from FHD/HD to VGA/HVGA resolution, is possible despite thechallenges of having very few pixels in the eye region. The proposed methodachieves comparable results to state-of-the-art methods in Point-of-Gaze (PoG)estimation on three public gaze datasets: GazeCapture, MPIIFaceGaze, and EVE,and generalizes well to extreme camera view changes.$Keywords: gaze estimation, CNN, remote gaze estimation, frame-to-gaze network, eye tracking, data normalization, facial landmark detection.$"作者名（机构）：Haldun Balim（ETH Zürich）、Seonwook Park（Lunit Inc.）、Xi Wang（ETH Zürich）、Xucong Zhang（Delft University of Technology）、Otmar Hilliges（ETH Zürich）。

标题：EFE: End-to-end Frame-to-Gaze Estimation"$本文介绍了一种新的端到端的框架到凝视估计方法（EFE），可以直接从摄像头捕获的原始帧预测目光方向和起点，无需预处理模块。与传统的注视估计方法相比，EFE可以跳过数据规范化和注视估计前的人脸和面部标志检测步骤，并实现可比较的性能。该方法在GazeCapture、MPI-IFaceGaze和EVE三个公共数据集上实现了可靠的Point-of-Gaze（PoG）估计，并能够适应极端的相机视角变化。虽然早期的方法需要一个或多个眼睛或面部区域裁剪作为输入，并输出注视方向向量，但是这种眼/面补丁裁剪过程是昂贵且容易出错的。$http://arxiv.org/pdf/2305.05526v1
ColonMapper: topological mapping and localization for colonoscopy$  Mapping and localization in endoluminal cavities from colonoscopies orgastroscopies has to overcome the challenge of significant shape andillumination changes between reobservations of the same endoluminal location.Instead of geometrical maps that strongly rely on a fixed scene geometry,topological maps are more adequate because they focus on visual placerecognition, i.e. the capability to determine if two video shots are imagingthe same location. We propose a topological mapping and localization systemable to operate on real human colonoscopies. The map is a graph where each nodecodes a colon location by a set of real images of that location. The edgesrepresent traversability between two nodes. For close-in-time images, wherescene changes are minor, place recognition can be successfully managed with therecent transformers-based image-matching algorithms. However, under long-termchanges -- such as different colonoscopies of the same patient -- feature-basedmatching fails. To address this, we propose a GeM global descriptor able toachieve high recall with significant changes in the scene. The addition of aBayesian filter processing the map graph boosts the accuracy of the long-termplace recognition, enabling relocalization in a previously built map. In theexperiments, we construct a map during the withdrawal phase of a firstcolonoscopy. Subsequently, we prove the ability to relocalize within this mapduring a second colonoscopy of the same patient two weeks later. Code andmodels will be available upon acceptance.$Key words: topological mapping, localization, colonoscopy, place recognition, feature-based matching, GeM global descriptor, Bayesian filter.$作者名（机构）：Javier Morlana, Juan D. Tardos和J.M.M. Montiel（西班牙萨拉戈萨大学）：ColonMapper: topological mapping and localization for colonoscopy$本文提出了一种适用于人体结肠镜检测的拓扑建图和定位系统。该系统将结肠位置编码为一组该位置的实际图像，并建立了一个表示结肠状态的图形，并考虑到了图形变化。文章提出了一种全局描述方法，并通过贝叶斯滤波器处理地图图形，实现了长时间定位。实验表明该系统可以在同一患者的两次结肠镜检测中实现重定位。该系统提供了一种有效的结肠镜检测方法，有助于提高临床治疗的准确性。$http://arxiv.org/pdf/2305.05546v1
Group Activity Recognition via Dynamic Composition and Interaction$  Previous group activity recognition approaches were limited to reasoningusing human relations or finding important subgroups and tended to ignoreindispensable group composition and human-object interactions. This absencemakes a partial interpretation of the scene and increases the interference ofirrelevant actions on the results. Therefore, we propose our DynamicFormer withDynamic composition Module (DcM) and Dynamic interaction Module (DiM) to modelrelations and locations of persons and discriminate the contribution ofparticipants, respectively. Our findings on group composition and human-objectinteraction inspire our core idea. Group composition tells us the location ofpeople and their relations inside the group, while interaction reflects therelation between humans and objects outside the group. We utilize spatial andtemporal encoders in DcM to model our dynamic composition and build DiM toexplore interaction with a novel GCN, which has a transformer inside toconsider the temporal neighbors of human/object. Also, a Multi-level DynamicIntegration is employed to integrate features from different levels. We conductextensive experiments on two public datasets and show that our method achievesstate-of-the-art.$Field: Group Activity Recognition, Dynamic Composition, Dynamic Interaction, Human-Object Interaction, Spatial and Temporal Encoders, Graph Convolutional Network.$"作者：Youliang Zhang, Zhuo Zhou, Wenxuan Liu, Danni Xu, Zheng Wang
机构：武汉大学，武汉理工大学，新加坡国立大学"$本文提出了一种基于动态组合和交互的团体活动识别方法。之前的团体活动识别方法仅限于使用人际关系进行推理或找到重要子组，而往往忽略了不可或缺的团队组成和人物对象交互，缺乏全面的场景解释，增加了无关动作对结果的干扰。为此，本文提出了动态组合模块(DcM)和动态交互模块(DiM)来分别模拟人物之间的关系和位置，以及参与者的贡献。动态组成和人物对象交互启示了我们的核心思想。团队组成告诉我们人员的位置和他们在团队内的关系，而交互反映了人类与团队外的物体之间的关系。我们利用DcM中的空间和时间编码器来模拟动态组合，并建立DiM来探索与新型GCN的交互，其中包含一个转换器来考虑人/对象的时间邻居。同时，本文采用多级动态集成来集成不同级别的特征。本文在两个公共数据集上进行了广泛实验，并表明其方法达到了最先进水平。$http://arxiv.org/pdf/2305.05583v1
Region-based Contrastive Pretraining for Medical Image Retrieval with  Anatomic Query$  We introduce a novel Region-based contrastive pretraining for Medical ImageRetrieval (RegionMIR) that demonstrates the feasibility of medical imageretrieval with similar anatomical regions. RegionMIR addresses two majorchallenges for medical image retrieval i) standardization of clinicallyrelevant searching criteria (e.g., anatomical, pathology-based), and ii)localization of anatomical area of interests that are semantically meaningful.In this work, we propose an ROI image retrieval image network that retrievesimages with similar anatomy by extracting anatomical features (via boundingboxes) and evaluate similarity between pairwise anatomy-categorized featuresbetween the query and the database of images using contrastive learning. ROIqueries are encoded using a contrastive-pretrained encoder that was fine-tunedfor anatomy classification, which generates an anatomical-specific latent spacefor region-correlated image retrieval. During retrieval, we compare theanatomically encoded query to find similar features within a feature databasegenerated from training samples, and retrieve images with similar regions fromtraining samples. We evaluate our approach on both anatomy classification andimage retrieval tasks using the Chest ImaGenome Dataset. Our proposed strategyyields an improvement over state-of-the-art pretraining and co-trainingstrategies, from 92.24 to 94.12 (2.03%) classification accuracy in anatomies.We qualitatively evaluate the image retrieval performance demonstratinggeneralizability across multiple anatomies with different morphology.$Medical image retrieval, Region-based contrastive pretraining, Anatomic query, Region-Of-Interest (ROI), contrastive learning, anatomy classification, Chest ImaGenome Dataset.$"作者：Ho Hin Lee（1），Alberto Santamaria-Pang（2），Jameson Merkow（2），Ozan Oktay（3），Fernando Pérez-García（3），Javier Alvarez-Valle（3），Ivan Tarapov（2）
机构：1 Vanderbilt University，2 Microsoft Health AI，3 Microsoft Health Futures"$本文介绍了一种基于区域对比预训练的医学图像检索方法(RegionMIR)，它展示了利用类似解剖区域进行医学图像检索的可行性。RegionMIR解决了医学图像检索中的两个主要挑战：i)标准化临床相关搜索标准(例如，基于解剖学、病理学的标准)，ii)定位具有语义意义的解剖区域。我们的方法利用基于兴趣区域(ROI)的图像搜索，在规模上工作，使临床医生能够搜索并检索选定的与相同解剖学和/或相似病理状态对应的ROIs。与以前的方法不同，我们的方法捕获了特定解剖区域的细节。我们提出了一种ROI图像检索图像网络，通过提取解剖特征(通过边界框)来检索具有相似解剖结构的图像，并使用对比学习在查询与图像数据库之间比较解剖学类别特征对间的相似性。在预训练编码器的对比微调下，ROI查询被编码，生成与区域相关的图像检索的解剖特定潜在空间。在检索过程中，我们比较解剖编码的查询，以找到与训练样本中具有相似特征的图像，然后从训练样本中检索具有相似区域的图像。我们在Chest ImaGenome数据集上对我们的方法进行了解剖学分类和图像检索任务的评估。我们的策略将解剖学分类的准确性从92.24提高到94.12(2.03%)，并且我们在多个具有不同形态的解剖学上 qualitatively 评估了图像检索性能的泛化能力。$http://arxiv.org/pdf/2305.05598v1
Privacy-Preserving Collaborative Chinese Text Recognition with Federated  Learning$  In Chinese text recognition, to compensate for the insufficient local dataand improve the performance of local few-shot character recognition, it isoften necessary for one organization to collect a large amount of data fromsimilar organizations. However, due to the natural presence of privateinformation in text data, different organizations are unwilling to shareprivate data, such as addresses and phone numbers. Therefore, it becomesincreasingly important to design a privacy-preserving collaborative trainingframework for the Chinese text recognition task. In this paper, we introducepersonalized federated learning (pFL) into the Chinese text recognition taskand propose the pFedCR algorithm, which significantly improves the modelperformance of each client (organization) without sharing private data.Specifically, based on CRNN, to handle the non-iid problem of client data, weadd several attention layers to the model and design a two-stage trainingapproach for the client. In addition, we fine-tune the output layer of themodel using a virtual dataset on the server, mitigating the problem ofcharacter imbalance in Chinese documents. The proposed approach is validated onpublic benchmarks and two self-built real-world industrial scenario datasets.The experimental results show that the pFedCR algorithm can improve theperformance of local personalized models while also improving theirgeneralization performance on other client data domains. Compared to localtraining within an organization, pFedCR improves model performance by about20%. Compared to other state-of-the-art personalized federated learningmethods, pFedCR improves performance by 6%~8%. Moreover, through federatedlearning, pFedCR can correct erroneous information in the ground truth.$Keywords: Privacy preservation, Collaborative learning, Chinese text recognition, Federated learning, Personalized federated learning, Character imbalance.$"作者：苏上超，于海洋，李斌，薛祥阳（复旦大学计算机学院）

机构：复旦大学计算机科学系"$本文提出了一种隐私保护的协作训练框架pFedCR，用于改进中文文本识别任务中局部客户端（组织）的模型性能，而不需要共享私有数据。该框架基于个性化联合学习（pFL），在CRNN基础上引入了多个注意力层来处理客户端数据的不平衡性，并采用虚拟数据集来解决字符不平衡问题。实验结果表明，pFedCR算法不仅可以改进本地个性化模型的性能，还可以提高其在其他客户端数据领域的泛化性能。与本地训练相比，pFedCR可以提高模型性能约20％，并且比其他最先进的个性化联合学习方法提高了6％~8％的性能。此外，通过联合学习，pFedCR可以纠正地面真实性中的错误信息。$http://arxiv.org/pdf/2305.05602v1
SwinIA: Self-Supervised Blind-Spot Image Denoising with Zero  Convolutions$  The essence of self-supervised image denoising is to restore the signal fromthe noisy image alone. State-of-the-art solutions for this task rely on theidea of masking pixels and training a fully-convolutional neural network toimpute them. This most often requires multiple forward passes, informationabout the noise model, and intricate regularization functions. In this paper,we propose a Swin Transformer-based Image Autoencoder (SwinIA), the firstconvolution-free architecture for self-supervised denoising. It can be trainedend-to-end with a simple mean squared error loss without masking and does notrequire any prior knowledge about clean data or noise distribution. Despite itssimplicity, SwinIA establishes state-of-the-art on several common benchmarks.$Image denoising, self-supervised learning, Swin Transformer, convolution-free, blind-spot network, neural networks, deep learning, noise model, regularization functions, mean squared error loss, benchmarks.$"作者名（机构）：Mikhail Papkov, Pavel Chizhov（塔尔图大学计算机科学研究所）
论文标题：SwinIA: Self-Supervised Blind-Spot Image Denoising with Zero Convolutions"$这篇论文介绍了一个新的自监督图像去噪算法，SwinIA。现有的算法多依赖于遮盖像素和卷积神经网络模型训练来推断缺失像素，但这会导致耗时、需要先前的知识和复杂的正则化函数。相比之下，SwinIA是第一个无需卷积的架构，可以通过简单的均方误差损失进行端到端训练，无需遮盖像素并且不需要关于噪声分布的先前知识。该算法在多项基准测试中取得了最好的效果。论文同时介绍了图像去噪的重要性和现有的解决方案，说明自监督去噪的优势在于无需成对的干净/噪声图像，而是从训练集本身学习。$http://arxiv.org/pdf/2305.05651v1
InternChat: Solving Vision-Centric Tasks by Interacting with Chatbots  Beyond Language$  We present an interactive visual framework named InternChat, or iChat forshort. The framework integrates chatbots that have planning and reasoningcapabilities, such as ChatGPT, with non-verbal instructions like pointingmovements that enable users to directly manipulate images or videos on thescreen. Pointing (including gestures, cursors, etc.) movements can provide moreflexibility and precision in performing vision-centric tasks that requirefine-grained control, editing, and generation of visual content. The nameInternChat stands for interaction, nonverbal, and chatbots. Different fromexisting interactive systems that rely on pure language, by incorporatingpointing instructions, the proposed iChat significantly improves the efficiencyof communication between users and chatbots, as well as the accuracy ofchatbots in vision-centric tasks, especially in complicated visual scenarioswhere the number of objects is greater than 2. Additionally, in iChat, anauxiliary control mechanism is used to improve the control capability of LLM,and a large vision-language model termed Husky is fine-tuned for high-qualitymulti-modal dialogue (impressing ChatGPT-3.5-turbo with 93.89% GPT-4 Quality).We hope this work can spark new ideas and directions for future interactivevisual systems. Welcome to watch the code athttps://github.com/OpenGVLab/InternChat.$Keywords: Vision-centric tasks, Chatbots, Non-verbal instructions, Pointing movements, Interactive visual framework, Planning, Reasoning, Language models, Image manipulation, Video editing.$"作者：Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Qinglong Zhang, Yang Yang, Qingyun Li, Jiashuo Yu, Kunchang Li, Zhe Chen, Xue Yang, Xizhou Zhu, Yali Wang, Limin Wang, Ping Luo, Jifeng Dai, Yu Qiao
机构：OpenGVLab, Shanghai AI Laboratory; The University of Hong Kong; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Nanjing University; SenseTime Research; Tsinghua University
GitHub链接：https://github.com/OpenGVLab/InternChat"$本文介绍了一种名为InternChat的交互式视觉框架。该框架结合了具有计划和推理能力的聊天机器人（例如ChatGPT）和非语言指令（如指向性动作），使用户可以直接操纵屏幕上的图像或视频。指向性动作提供了更大的灵活性和精度，对于需要精细控制、编辑和生成视觉内容的视觉中心任务尤为重要。不同于现有的纯语言交互系统，通过加入指向性指令，本文提出的iChat显著改善了用户和聊天机器人之间的沟通效率，以及聊天机器人在具有大量对象的复杂视觉场景中的准确性。此外，iChat使用了辅助控制机制来提高LLM的控制能力，并对一个名为Husky的大型视觉语言模型进行了微调，以实现高质量的多模态对话。希望这项工作能引发未来交互式视觉系统的新想法和方向。$http://arxiv.org/pdf/2305.05662v1
Indoor Localization and Multi-person Tracking Using Privacy Preserving  Distributed Camera Network with Edge Computing$  Localization of individuals in a built environment is a growing researchtopic. Estimating the positions, face orientation (or gaze direction) andtrajectories of people through space has many uses, such as in crowdmanagement, security, and healthcare. In this work, we present an open-source,low-cost, scalable and privacy-preserving edge computing framework formulti-person localization, i.e. estimating the positions, orientations, andtrajectories of multiple people in an indoor space. Our computing frameworkconsists of 38 Tensor Processing Unit (TPU)-enabled edge computing camerasystems placed in the ceiling of the indoor therapeutic space. The edge computesystems are connected to an on-premise fog server through a secure and privatenetwork. A multi-person detection algorithm and a pose estimation model run onthe edge TPU in real-time to collect features which are used, instead of rawimages, for downstream computations. This ensures the privacy of individuals inthe space, reduces data transmission/storage and improves scalability. Weimplemented a Kalman filter-based multi-person tracking method and astate-of-the-art body orientation estimation method to determine the positionsand facing orientations of multiple people simultaneously in the indoor space.For our study site with size of 18,000 square feet, our system demonstrated anaverage localization error of 1.41 meters, a multiple-object tracking accuracyscore of 62%, and a mean absolute body orientation error of 29{\\deg}, which issufficient for understanding group activity behaviors in indoor environments.Additionally, our study provides practical guidance for deploying the proposedsystem by analyzing various elements of the camera installation with respect totracking accuracy.$Indoor localization, multi-person tracking, privacy-preserving, edge computing, Tensor Processing Unit (TPU), Kalman filter, body orientation estimation, group activity behaviors.$作者：HYEOKHYEN KWON（Emory University），CHAITRA HEDGE（Georgia Institute of Technology），YASHAR KIARASHI（Emory University），VENKATA SIVA KRISHNA MADALA（Georgia Institute of Technology），RATAN SINGH（Georgia Institute of Technology），ARJUNSINH NAKUM（Georgia Institute of Technology），ROBERT TWEEDY（Emory University），LEANDRO MILETTO TONETTO（Georgia Institute of Technology），CRAIG M. ZIMRING（Georgia Institute of Technology），GARI D. CLIFFORD（Emory University）。 机构：Emory University，Georgia Institute of Technology。$本篇论文介绍了一种用于多人室内定位和追踪的隐私保护分布式相机网络与边缘计算的系统框架，该框架采用低成本边缘计算相机系统，实现了对多人的实时、精确定位和面向方向追踪。该相机系统与本地雾服务器连接，可以保证数据的安全性和隐私性。实验结果表明，在系统实现了卡尔曼滤波的情况下，该系统可以达到平均定位误差1.41米、多对象追踪准确性得分62%和平均绝对人体方向误差29°的效果。此外，本文还讨论了系统的实际部署，并对相机的布局和环境因素进行了分析，以便提高多人追踪的准确率。$http://arxiv.org/pdf/2305.05062v1
Atmospheric Turbulence Correction via Variational Deep Diffusion$  Atmospheric Turbulence (AT) correction is a challenging restoration task asit consists of two distortions: geometric distortion and spatially variantblur. Diffusion models have shown impressive accomplishments in photo-realisticimage synthesis and beyond. In this paper, we propose a novel deep conditionaldiffusion model under a variational inference framework to solve the ATcorrection problem. We use this framework to improve performance by learninglatent prior information from the input and degradation processes. We use thelearned information to further condition the diffusion model. Experiments areconducted in a comprehensive synthetic AT dataset. We show that the proposedframework achieves good quantitative and qualitative results.$Atmospheric turbulence correction, variational inference, deep diffusion model, image restoration, spatially variant blur, geometrical distortion, deep learning, image synthesis.$"作者：Xijun Wang, Santiago López-Tapia, Aggelos K. Katsaggelos

机构：1北西大学计算机科学系，伊万斯顿，美国；2北西大学电气和计算机工程系，伊万斯顿，美国。"$本文提出了一种新颖的基于变分深度扩散模型的大气湍流校正方法。这是一项具有挑战性的任务，因为它涉及到两种扭曲：几何形变和空间变异的模糊。本文的主要贡献在于，首次将扩散模型应用于解决泛泛的大气湍流校正问题。同时，引入了一种变分推理图像复原框架，从输入和退化过程中学习与任务有关的潜在先验知识，并将其作为条件注入到扩散模型中。实验结果表明，所提出的深度扩散模型在合成大气湍流数据集上具有优异的定量和定性表现。$http://arxiv.org/pdf/2305.05077v1
Less is More: Removing Text-regions Improves CLIP Training Efficiency  and Robustness$  The CLIP (Contrastive Language-Image Pre-training) model and its variants arebecoming the de facto backbone in many applications. However, training a CLIPmodel from hundreds of millions of image-text pairs can be prohibitivelyexpensive. Furthermore, the conventional CLIP model doesn\'t differentiatebetween the visual semantics and meaning of text regions embedded in images.This can lead to non-robustness when the text in the embedded region doesn\'tmatch the image\'s visual appearance. In this paper, we discuss two effectiveapproaches to improve the efficiency and robustness of CLIP training: (1)augmenting the training dataset while maintaining the same number ofoptimization steps, and (2) filtering out samples that contain text regions inthe image. By doing so, we significantly improve the classification andretrieval accuracy on public benchmarks like ImageNet and CoCo. Filtering outimages with text regions also protects the model from typographic attacks. Toverify this, we build a new dataset named ImageNet with Adversarial TextRegions (ImageNet-Attr). Our filter-based CLIP model demonstrates a top-1accuracy of 68.78\\%, outperforming previous models whose accuracy was all below50\\%.$Keywords: CLIP, text-regions, training efficiency, robustness, visual semantics, augmentation, filtering, classifcation, retrieval accuracy, ImageNet, typographic attacks.$"作者名（机构）：Liangliang Cao, Bowen Zhang, Chen Chen, Yinfei Yang,
Xianzhi Du, Wencong Zhang, Zhiyun Lu, Yantao Zheng （Apple AI/ML）"$本论文探讨了如何提高CLIP模型的训练效率和鲁棒性，因为从数以亿计的图像-文本对训练CLIP模型是昂贵的。作者提出了两种有效的方法来实现这一目标：(1)在保持相同优化步骤数量的情况下增加训练数据集，(2)过滤掉包含图像文本区域的样本。通过这样做，作者在像ImageNet和CoCo这样的公共基准测试中显著提高了分类和检索准确性。过滤掉带有文本区域的图像还可以保护模型免受印刷攻击。作者建立了一个新的数据集ImageNet with Adversarial Text Regions (ImageNet-Attr)来验证这一点。他们的基于过滤的CLIP模型表现出68.78%的top-1准确率，优于所有低于50%准确率的先前模型。作者的研究动机是对对比损失的观察。$http://arxiv.org/pdf/2305.05095v1
Localisation of Mammographic masses by Greedy Backtracking of  Activations in the Stacked Auto-Encoders$  Mammographic image analysis requires accurate localisation of salientmammographic masses. In mammographic computer-aided diagnosis, mass or Regionof Interest (ROI) is often marked by physicians and features are extracted fromthe marked ROI. In this paper, we present a novel mammographic masslocalisation framework, based on the maximal class activations of the stackedauto-encoders. We hypothesize that the image regions activating abnormalclasses in mammographic images will be the breast masses which causes theanomaly. The experiment is conducted using randomly selected 200 mammographicimages (100 normal and 100 abnormal) from IRMA mammographic dataset. Abnormalmass regions marked by an expert radiologist are used as the ground truth. Theproposed method outperforms existing Deep Convolutional Neural Network (DCNN)based techniques in terms of salient region detection accuracy. The proposedgreedy backtracking method is more efficient and does not require a vast numberof labelled training images as in DCNN based method. Such automaticlocalisation method will assist physicians to make accurate decisions on biopsyrecommendations and treatment evaluations.$Keywords: Mammographic image analysis, Salient Region Detection, Auto-encoder, Deep Convolutional Neural Network, Breast mass detection, Biopsy recommendations, Treatment evaluations.$"作者：Shamna Pootheri (新加坡南洋理工大学研究员，前研究学者) 和 V K Govindan (印度国家技术卡利卡特国家工程学院名誉教授)
机构：a新加坡南洋理工大学，邮编639798；b印度国家技术卡利卡特国家工程学院，邮编673 601"$本研究提出了一种新的乳腺X线摄影质量分析方法，使用堆叠自动编码器的最大类激活进行乳腺肿块识别。本研究假设在乳腺X线摄影图像中，异常类别的图像区域将是导致异常的乳腺肿块。实验使用了来自IRMA数据库的200张随机选取的乳腺X线摄影图像（100张正常，100张异常）。根据专家放射科医师标记的异常肿块区域作为参考标准。结果显示，该方法在突出区域检测准确性方面优于现有的基于DCNN的技术。该贪婪回溯方法更加高效，并且不需要像DCNN方法那样大量标记的训练图像。此类自动化定位方法将有助于医生准确决定活检建议和治疗评估。$http://arxiv.org/pdf/2305.05136v1
Adapt and Align to Improve Zero-Shot Sketch-Based Image Retrieval$  Zero-shot sketch-based image retrieval (ZS-SBIR) is challenging due to thecross-domain nature of sketches and photos, as well as the semantic gap betweenseen and unseen image distributions. Previous methods fine-tune pre-trainedmodels with various side information and learning strategies to learn a compactfeature space that (\\romannumeral1) is shared between the sketch and photodomains and (\\romannumeral2) bridges seen and unseen classes. However, theseefforts are inadequate in adapting domains and transferring knowledge from seento unseen classes. In this paper, we present an effective \\emph{``Adapt andAlign\'\'} approach to address the key challenges. Specifically, we insert simpleand lightweight domain adapters to learn new abstract concepts of the sketchdomain and improve cross-domain representation capabilities. Inspired by recentadvances in image-text foundation models (\\textit{e.g.}, CLIP) on zero-shotscenarios, we explicitly align the learned image embedding with a more semantictext embedding to achieve the desired knowledge transfer from seen to unseenclasses. Extensive experiments on three benchmark datasets and two popularbackbones demonstrate the superiority of our method in terms of retrievalaccuracy and flexibility.$Keywords: zero-shot learning, sketch-based image retrieval, domain adaptation, vision-language alignment, feature representation.$"作者：Shiyin Dong, Mingrui Zhu, Nannan Wang, Heng Yang, Xinbo Gao（来自中国西安的西安电子科技大学、深圳AiMall科技和重庆邮电大学。）
机构：State Key Laboratory of Integrated Services Networks, Xidian University, Xian, China；Shenzhen AiMall Tech, Shenzhen, China；Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China."$本文提出了一种“适应和对齐”的方法来解决零样本草图图像检索的挑战，该任务由于草图和照片的跨域性以及已知和未知图像分布之间的语义差距而变得具有挑战性。该方法通过插入轻量级领域适配器来学习草图域的新抽象概念，从而提高跨域表示能力；同时，受最近图像-文本基础模型的启发，本文明确了学习的图像嵌入与更语义的文本嵌入之间的对齐，以实现从已知到未知类的理想知识转移。实验证明，该方法在三个基准数据集和两个流行的骨干网络上具有出色的检索准确性和灵活性。$http://arxiv.org/pdf/2305.05144v1
SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with  Large Language Models$  Diffusion models, which have emerged to become popular text-to-imagegeneration models, can produce high-quality and content-rich images guided bytextual prompts. However, there are limitations to semantic understanding andcommonsense reasoning in existing models when the input prompts are concisenarrative, resulting in low-quality image generation. To improve the capacitiesfor narrative prompts, we propose a simple-yet-effective parameter-efficientfine-tuning approach called the Semantic Understanding and Reasoning adapter(SUR-adapter) for pre-trained diffusion models. To reach this goal, we firstcollect and annotate a new dataset SURD which consists of more than 57,000semantically corrected multi-modal samples. Each sample contains a simplenarrative prompt, a complex keyword-based prompt, and a high-quality image.Then, we align the semantic representation of narrative prompts to the complexprompts and transfer knowledge of large language models (LLMs) to ourSUR-adapter via knowledge distillation so that it can acquire the powerfulsemantic understanding and reasoning capabilities to build a high-qualitytextual semantic representation for text-to-image generation. We conductexperiments by integrating multiple LLMs and popular pre-trained diffusionmodels to show the effectiveness of our approach in enabling diffusion modelsto understand and reason concise natural language without image qualitydegradation. Our approach can make text-to-image diffusion models easier to usewith better user experience, which demonstrates our approach has the potentialfor further advancing the development of user-friendly text-to-image generationmodels by bridging the semantic gap between simple narrative prompts andcomplex keyword-based prompts.$Keywords: text-to-image generation, diffusion models, semantic understanding, large language models, adapter, knowledge distillation, multimodal image generation.$作者：Shanshan Zhong, Zhongzhan Huang, Wushao Wen, Jinghui Qin, Liang Lin（Sun Yat-sen University, Guangzhou, China；Guangdong University of Technology, Guangzhou, China）$本论文提出了一种名为Semantic Understanding and Reasoning adapter (SUR-adapter)的方法，以增强预训练扩散模型对大型语言模型的语义理解和常识推理能力，从而更好地生成文本指示下的图像。作者还收集和标注了一个名为SURD的新数据集，其中包含超过57,000个语义纠正的多模态样本。他们通过知识蒸馏的方式将LLMs的知识转移到SUR-adapter上，从而赋予其强大的语义理解和推理能力，以在简洁自然语言的情况下更好地生成图像。作者进行了多个实验，证明了其方法的有效性。$http://arxiv.org/pdf/2305.05189v1
LSAS: Lightweight Sub-attention Strategy for Alleviating Attention Bias  Problem$  In computer vision, the performance of deep neural networks (DNNs) is highlyrelated to the feature extraction ability, i.e., the ability to recognize andfocus on key pixel regions in an image. However, in this paper, wequantitatively and statistically illustrate that DNNs have a serious attentionbias problem on many samples from some popular datasets: (1) Position bias:DNNs fully focus on label-independent regions; (2) Range bias: The focusedregions from DNN are not completely contained in the ideal region. Moreover, wefind that the existing self-attention modules can alleviate these biases to acertain extent, but the biases are still non-negligible. To further mitigatethem, we propose a lightweight sub-attention strategy (LSAS), which utilizeshigh-order sub-attention modules to improve the original self-attentionmodules. The effectiveness of LSAS is demonstrated by extensive experiments onwidely-used benchmark datasets and popular attention networks. We release ourcode to help other researchers to reproduce the results ofLSAS~\\footnote{https://github.com/Qrange-group/LSAS}.$Computer vision, deep neural networks, attention bias, sub-attention, lightweight.$"作者：钟珊珊（中山大学）、温吴少（中山大学）、秦竞辉（广东工业大学）、陈强普（中山大学）、黄忠展（中山大学）
机构：中山大学、广东工业大学
论文题目：LSAS: Lightweight Sub-attention Strategy for Alleviating Attention Bias Problem*"$本篇论文提出了一种轻量级子注意力策略（LSAS），用于缓解深度神经网络在图像识别任务中存在的注意偏向问题。本文通过实验和统计数据量化揭示了DNN在某些常用数据集上存在的严重偏向问题，包括位置偏向和范围偏向，并发现现有的自注意机制只能在一定程度上缓解这些偏向。为此，本文提出了LSAS，通过利用高阶子注意力模块改进原始自注意力模块来进一步缓解偏向问题。大量实验结果表明，LSAS能够显著提高模型的性能，并且我们开源了代码以供其他研究者验证实验结果。$http://arxiv.org/pdf/2305.05200v1
FishRecGAN: An End to End GAN Based Network for Fisheye Rectification  and Calibration$  We propose an end-to-end deep learning approach to rectify fisheye images andsimultaneously calibrate camera intrinsic and distortion parameters. Our methodconsists of two parts: a Quick Image Rectification Module developed with aPix2Pix GAN and Wasserstein GAN (W-Pix2PixGAN), and a Calibration Module with aCNN architecture. Our Quick Rectification Network performs robust rectificationwith good resolution, making it suitable for constant calibration incamera-based surveillance equipment. To achieve high-quality calibration, weuse the straightened output from the Quick Rectification Module as aguidance-like semantic feature map for the Calibration Module to learn thegeometric relationship between the straightened feature and the distortedfeature. We train and validate our method with a large synthesized datasetlabeled with well-simulated parameters applied to a perspective image dataset.Our solution has achieved robust performance in high-resolution with asignificant PSNR value of 22.343.$Keywords: fisheye cameras, rectification, calibration, deep learning, GAN, CNN, Pix2Pix, Wasserstein GAN, synthetic dataset.$"作者名（机构）：Xin Shen, Kyungdon Joo, Jean Oh（卡内基梅隆大学）
文章标题：FishRecGAN: An End to End GAN Based Network
for Fisheye Rectiﬁcation and Calibration"$本文提出了一种基于生成对抗网络（GAN）和卷积神经网络（CNN）的端到端深度学习方法，用于修正鱼眼图像以及同时校准相机内在和畸变参数。方法包括一个通过Pix2Pix GAN和Wasserstein GAN开发的快速图像矫正模块（W-Pix2PixGAN），以及一个具有CNN架构的校准模块。快速矫正网络具有良好的分辨率性能和鲁棒性，适合于相机监控设备中的恒定校准。为了实现高质量的校准，作者利用来自快速矫正模块的直线输出作为校准模块的“指导”语义特征图，学习劣化特征和矫正特征之间的几何关系。作者使用包含不同畸变参数标签的大规模合成数据集，提出的方法具有鲁棒的性能和显著的峰值信噪比（PSNR）值。本文通过端到端深度学习方法提供了一种自动化，一致性和高效率的解决方案，可用于实时鱼眼图像矫正和相机校准。$http://arxiv.org/pdf/2305.05222v1
Rotation Synchronization via Deep Matrix Factorization$  In this paper we address the rotation synchronization problem, where theobjective is to recover absolute rotations starting from pairwise ones, wherethe unknowns and the measures are represented as nodes and edges of a graph,respectively. This problem is an essential task for structure from motion andsimultaneous localization and mapping. We focus on the formulation ofsynchronization via neural networks, which has only recently begun to beexplored in the literature. Inspired by deep matrix completion, we expressrotation synchronization in terms of matrix factorization with a deep neuralnetwork. Our formulation exhibits implicit regularization properties and, moreimportantly, is unsupervised, whereas previous deep approaches are supervised.Our experiments show that we achieve comparable accuracy to the closestcompetitors in most scenes, while working under weaker assumptions.$Keywords: rotation synchronization, structure from motion, simultaneous localization and mapping, deep matrix factorization, neural networks, special orthogonal group, gauge ambiguity.$"作者：GK Tejus, Giacomo Zara, Paolo Rota, Andrea Fusiello，Elisa Ricci和Federica Arrigoni

机构：1印度理工学院（ISM）丹巴德分校，2特伦托大学，3乌迪内大学，4米兰理工大学"$本文针对旋转同步问题，旨在通过神经网络实现同步。文章将同步问题表示为一种深度矩阵分解形式，并具有隐式正则化特性，且为无监督形式。此前的深度方法属于有监督形式。文章还探讨了存在噪声、离群数据和丢失数据情况下的问题，实验结果表明方法精度可与其他方法相媲美，但假设更弱。$http://arxiv.org/pdf/2305.05268v1
Mediapipe and CNNs for Real-Time ASL Gesture Recognition$  This research paper describes a realtime system for identifying American SignLanguage (ASL) movements that employs modern computer vision and machinelearning approaches. The suggested method makes use of the Mediapipe libraryfor feature extraction and a Convolutional Neural Network (CNN) for ASL gestureclassification. The testing results show that the suggested system can detectall ASL alphabets with an accuracy of 99.95%, indicating its potential for usein communication devices for people with hearing impairments. The proposedapproach can also be applied to additional sign languages with similar handmotions, potentially increasing the quality of life for people with hearingloss. Overall, the study demonstrates the effectiveness of using Mediapipe andCNN for real-time sign language recognition, making a significant contributionto the field of computer vision and machine learning.$Keywords: Mediapipe, CNN, real-time ASL gesture recognition, computer vision, machine learning, SLR, sign language, finger-spelled gestures, dynamic gestures, continuous recognition.$"作者名（机构）：Rupesh Kumar（印度加尔各答理工学院CSE系），Ashutosh Bajpai（印度加尔各答理工学院CSE系），Ayush Sinha（印度加尔各答理工学院CSE系），S.K Singh（印度加尔各答理工学院CSE系）

本研究论文介绍了一个利用现代计算机视觉和机器学习方法识别美国手语（ASL）动作的实时系统。该方法利用Mediapipe库进行特征提取和卷积神经网络（CNN）进行ASL手势分类。测试结果显示，所提出的系统可以检测所有ASL字母，精度为99.95％，表明它有潜力用于听力受损人士的通讯设备。该方法也可以应用于其他手势类似的手语，潜在地提高听力损失人士的生活质量。总的来说，本研究展示了利用Mediapipe和CNN进行实时手语识别的有效性，为计算机视觉和机器学习领域做出了重要贡献。"$本篇论文介绍了一种使用现代计算机视觉和机器学习方法进行实时检测美国手语运动的系统。该系统采用Mediapipe库进行特征提取，并使用卷积神经网络（CNN）进行ASL手势分类。测试结果显示，该系统可以以99.95％的准确率检测所有ASL字母，表明它在为听力受损人群提供通信设备方面具有潜力。这种方法还可以应用于其他具有类似手势的手语，从而可能提高听力受损者的生活质量。总体而言，该研究表明，使用Mediapipe和CNN进行实时手语识别是有效的，为计算机视觉和机器学习领域做出了显著贡献。$http://arxiv.org/pdf/2305.05296v1
Eiffel Tower: A Deep-Sea Underwater Dataset for Long-Term Visual  Localization$  Visual localization plays an important role in the positioning and navigationof robotics systems within previously visited environments. When visits occurover long periods of time, changes in the environment related to seasons orday-night cycles present a major challenge. Under water, the sources ofvariability are due to other factors such as water conditions or growth ofmarine organisms. Yet it remains a major obstacle and a much less studied one,partly due to the lack of data. This paper presents a new deep-sea dataset tobenchmark underwater long-term visual localization. The dataset is composed ofimages from four visits to the same hydrothermal vent edifice over the courseof five years. Camera poses and a common geometry of the scene were estimatedusing navigation data and Structure-from-Motion. This serves as a referencewhen evaluating visual localization techniques. An analysis of the dataprovides insights about the major changes observed throughout the years.Furthermore, several well-established visual localization methods are evaluatedon the dataset, showing there is still room for improvement in underwaterlong-term visual localization. The data is made publicly available athttps://www.seanoe.org/data/00810/92226/.$Keywords: Underwater dataset, long-term visual localization, deep sea, visual localization benchmark, Eiffel Tower vent edifice.$作者：Clémentin Boittiaux, Claire Dune, Maxime Ferrera, Aurélien Arnaubec, Ricard Marxer, Marjolaine Matabos, Loïc Van Audenhaege和Vincent Hugel。机构：Ifremer, Université de Toulon, Aix Marseille Univ, CNRS和Univ Brest。$本论文提出了一个新的深海数据集，用于评估水下长期视觉定位。该数据集由同一水热喷口堆的四次拜访期间所拍摄的图像组成，历时五年。使用导航数据和结构光扫描技术确定了相机位置和场景几何，可以用作评估视觉定位技术的参考。文章还分析了这些年份的主要变化，评估了几种视觉定位方法。结果表明，水下长期视觉定位仍有改进的空间。同时，数据集已经公开提供，可用于深海视觉定位的基准测试。$http://arxiv.org/pdf/2305.05301v1
Application of Artificial Intelligence in the Classification of  Microscopical Starch Images for Drug Formulation$  Starches are important energy sources found in plants with many uses in thepharmaceutical industry such as binders, disintegrants, bulking agents in drugsand thus require very careful physicochemical analysis for properidentification and verification which includes microscopy. In this work, weapplied artificial intelligence techniques (using transfer learning and deepconvolution neural network CNNs to microscopical images obtained from 9 starchsamples of different botanical sources. Our approach obtained an accuracy of61% when the machine learning model was pretrained on microscopic images fromMicroNet dataset. However the accuracy jumped to 81% for model pretrained onrandom day to day images obtained from Imagenet dataset. The model pretrainedon the imagenet dataset also showed a better precision, recall and f1 scorethan that pretrained on the imagenet dataset.$Keywords: Microscopy, starch, computer vision, convolution neural network, starch classification, transfer learning, artificial intelligence.$"作者名（机构）：Marvellous Ajala, Blessing Oko, David Oba-Fidelis, Joycelyn Iyasele, Joy I. Odimegwu（尼日利亚拉各斯大学药学院药剂学系）

摘要：
淀粉是植物中重要的能源来源，具有在制药工业中作为粘合剂、分散剂、增容剂等多种用途，因此需要非常仔细的物理化学分析进行正确的鉴定和验证，其中包括显微镜检查。
在这项工作中，我们应用了人工智能技术（使用转移学习和深度卷积神经网络CNN）来对来自9种不同植物来源的淀粉样品进行显微镜图像分析。当机器学习模型预先训练了来自MicroNet数据集的显微镜图像时，我们的方法得到了61％的准确率。然而，当模型预先训练在从Imagenet数据集中获取的随机日常图像上时，准确率跃升至81％。预先训练在imagenet数据集上的模型也比在imagenet数据集上预先训练的模型表现出更好的精确度、召回率和f1分数。
关键词：显微镜术，淀粉，计算机视觉，卷积神经网络，淀粉分类，转移学习。"$这篇论文主要研究如何利用人工智能技术对显微镜下的淀粉质图片进行分类分析，进而在药物制剂中的应用。由于淀粉质在制药中扮演着重要的作用，如成为药物的粘合剂、分散剂和增大剂，因此对淀粉质的物理化学分析十分关键。利用迁移学习和深度卷积神经网络，研究人员对来自9种植物样本的显微镜图片进行分析。该研究表明，对于从不同数据集预训练的模型，其精度、精确度、召回率和f1值均有所不同，其中以从Imagenet数据集预训练的模型表现最佳，其准确率可达81%。关键词包括：显微镜、淀粉质、计算机视觉、卷积神经网络、淀粉质分类和迁移学习。$http://arxiv.org/pdf/2305.05321v1
Trustworthy Multi-phase Liver Tumor Segmentation via Evidence-based  Uncertainty$  Multi-phase liver contrast-enhanced computed tomography (CECT) images conveythe complementary multi-phase information for liver tumor segmentation (LiTS),which are crucial to assist the diagnosis of liver cancer clinically. However,the performances of existing multi-phase liver tumor segmentation(MPLiTS)-based methods suffer from redundancy and weak interpretability, % ofthe fused result, resulting in the implicit unreliability of clinicalapplications. In this paper, we propose a novel trustworthy multi-phase livertumor segmentation (TMPLiTS), which is a unified framework jointly conductingsegmentation and uncertainty estimation. The trustworthy results could assistthe clinicians to make a reliable diagnosis. Specifically, Dempster-ShaferEvidence Theory (DST) is introduced to parameterize the segmentation anduncertainty as evidence following Dirichlet distribution. The reliability ofsegmentation results among multi-phase CECT images is quantified explicitly.Meanwhile, a multi-expert mixture scheme (MEMS) is proposed to fuse themulti-phase evidences, which can guarantee the effect of fusion procedure basedon theoretical analysis. Experimental results demonstrate the superiority ofTMPLiTS compared with the state-of-the-art methods. Meanwhile, the robustnessof TMPLiTS is verified, where the reliable performance can be guaranteedagainst the perturbations.$Keywords: liver tumor segmentation, multi-phase contrast-enhanced CT, deep learning, uncertainty estimation, Dempster-Shafer Evidence Theory.$"作者：Chuanfei Hu, Tianyi Xia, Ying Cui, Quchen Zou, Yuancheng Wang, Wenbo Xiao, Shenghong Ju, Xinde Li

机构：Southeast University, Jiangsu Key Laboratory of Molecular and Functional Imaging, Department of Radiology, Zhongda Hospital, School of Medicine, Zhejiang University School of Medicine"$本文介绍了一种新颖的、可信赖的多相肝肿瘤分割方法，它能够在联合分割和不确定性估计的基础上，量化在不同阶段CT图像中分割结果的可靠性，并利用Dempster-Shafer证据理论(DST)和多专家混合模式(MEMS)来融合多相证据，从而提高肝肿瘤分割的准确性。实验结果表明，TMPLiTS相较于现有的分割方法具有更优越的表现和稳健性，提高了医学分割的可靠性和可用性。$http://arxiv.org/pdf/2305.05344v1
Towards the Characterization of Representations Learned via  Capsule-based Network Architectures$  Capsule Networks (CapsNets) have been re-introduced as a more compact andinterpretable alternative to standard deep neural networks. While recentefforts have proved their compression capabilities, to date, theirinterpretability properties have not been fully assessed. Here, we conduct asystematic and principled study towards assessing the interpretability of thesetypes of networks. Moreover, we pay special attention towards analyzing thelevel to which part-whole relationships are indeed encoded within the learnedrepresentation. Our analysis in the MNIST, SVHN, PASCAL-part and CelebAdatasets suggest that the representations encoded in CapsNets might not be asdisentangled nor strictly related to parts-whole relationships as is commonlystated in the literature.$Capsule Networks, Interpretability, Part-Whole Relationships, Deep Neural Networks, Compression, Model Interpretation, Model Explanation, Methodology, Experimental Analysis.$作者名（机构）：Saja AL-Tawalbeh和José Oramas（比利时安特卫普大学，imec-IDLab）：Towards the Characterization of Representations Learned via Capsule-based Network Architectures$本文介绍了一种名为“Capsule Networks (CapsNets)”的神经网络结构，并重点探讨了该结构所建立的内部表征能否解释和识别数据集中的部分关系。Capsule Networks采用了一组神经元作为一个胶囊，用一个活跃向量表示一个特定类型的实体，比如对象或对象的一部分。不同于传统的卷积神经网络（CNNs），CapsNets使用连接列表显式地将一组胶囊与另一组胶囊相联系，模拟部分-整体关系。本文通过实验研究CapsNets的解释能力，提出了两种检验方法，并在MNIST、SVHN、PASCAL-part和CelebA等数据集上对CapsNets中学习的表征进行了分析。研究发现，CapsNets可能没有像文献中所述的那样明确编码部分-整体关系。此外，本文提出的方法可以用于提取重要的部分关系单元，这些单元可以提高CapsNet的表现水平。$http://arxiv.org/pdf/2305.05349v1
DC3DCD: unsupervised learning for multiclass 3D point cloud change  detection$  In a constant evolving world, change detection is of prime importance to keepupdated maps. To better sense areas with complex geometry (urban areas inparticular), considering 3D data appears to be an interesting alternative toclassical 2D images. In this context, 3D point clouds (PCs) obtained by LiDARor photogrammetry are very interesting. While recent studies showed theconsiderable benefit of using deep learning-based methods to detect andcharacterize changes into raw 3D PCs, these studies rely on large annotatedtraining data to obtain accurate results. The collection of these annotationsare tricky and time-consuming. The availability of unsupervised or weaklysupervised approaches is then of prime interest. In this paper, we propose anunsupervised method, called DeepCluster 3D Change Detection (DC3DCD), to detectand categorize multiclass changes at point level. We classify our approach inthe unsupervised family given the fact that we extract in a completelyunsupervised way a number of clusters associated with potential changes. Let usprecise that in the end of the process, the user has only to assign a label toeach of these clusters to derive the final change map. Our method builds uponthe DeepCluster approach, originally designed for image classification, tohandle complex raw 3D PCs and perform change segmentation task. An assessmentof the method on both simulated and real public dataset is provided. Theproposed method allows to outperform fully-supervised traditional machinelearning algorithm and to be competitive with fully-supervised deep learningnetworks applied on rasterization of 3D PCs with a mean of IoU over classes ofchange of 57.06% and 66.69% for the simulated and the real datasets,respectively.$Keywords: 3D point clouds, change detection, unsupervised deep learning, deep clustering, LiDAR, photogrammetry, Aerial Laser Scanning.$作者：Iris de Gélis, Sébastien Lefèvre和Thomas Corpetti，机构：Magellium，Toulouse，France；IRISA，UMR 6074，Université Bretagne Sud，Vannes，France；CNRS，LETG，UMR 6554，Rennes，France。$本论文提出了一种无监督的方法，名为DC3DCD，用于检测和分类多类别的点云变化。该方法建立在原本用于图像分类的DeepCluster方法之上，应用于处理复杂的3D点云数据以执行变化分割任务。与使用大量标注数据的监督学习方法相比，本方法在未标注数据集上表现出色，展现了其实用性和价值。通过模拟数据和真实公共数据集的评估，DC3DCD方法表现出了很好的效果，并且在同类别变化的IoU均值方面，与应用于3D点云光栅化的完全监督深度学习网络相当。其代码公开在github上已分享。该成果旨在为地图更新、自然灾害等方面提供基础支持。$http://arxiv.org/pdf/2305.05421v1
Egocentric Hierarchical Visual Semantics$  We are interested in aligning how people think about objects and whatmachines perceive, meaning by this the fact that object recognition, asperformed by a machine, should follow a process which resembles that followedby humans when thinking of an object associated with a certain concept. Theultimate goal is to build systems which can meaningfully interact with theirusers, describing what they perceive in the users\' own terms. As from the fieldof Lexical Semantics, humans organize the meaning of words in hierarchies wherethe meaning of, e.g., a noun, is defined in terms of the meaning of a moregeneral noun, its genus, and of one or more differentiating properties, itsdifferentia. The main tenet of this paper is that object recognition shouldimplement a hierarchical process which follows the hierarchical semanticstructure used to define the meaning of words. We achieve this goal byimplementing an algorithm which, for any object, recursively recognizes itsvisual genus and its visual differentia. In other words, the recognition of anobject is decomposed in a sequence of steps where the locally relevant visualfeatures are recognized. This paper presents the algorithm and a firstevaluation.$Image recognition, visual semantics, hierarchical structure, lexical semantics, genus and differentia, interactive machine learning.$作者名（机构）：Luca Erculiani, Andrea Bontempelli, Andrea Passerini, and Fausto Giunchiglia (Trento大学)$本文介绍了一种逐级检测对象特征的视觉语义层次分析算法，并且将其与词汇语义中的Genus和Differentia的层次结构相联系。该算法通过递归地识别目标的视觉Genus和视觉Differentia，将目标的识别分解为一系列的步骤。这种视觉语义层次分析算法的实现对于构建可以有意义地与用户交互的系统具有重要的意义。本文还对该算法进行了初步的评估。$http://arxiv.org/pdf/2305.05422v1
High-throughput Cotton Phenotyping Big Data Pipeline Lambda Architecture  Computer Vision Deep Neural Networks$  In this study, we propose a big data pipeline for cotton bloom detectionusing a Lambda architecture, which enables real-time and batch processing ofdata. Our proposed approach leverages Azure resources such as Data Factory,Event Grids, Rest APIs, and Databricks. This work is the first to develop anddemonstrate the implementation of such a pipeline for plant phenotyping throughAzure\'s cloud computing service. The proposed pipeline consists of datapreprocessing, object detection using a YOLOv5 neural network model trainedthrough Azure AutoML, and visualization of object detection bounding boxes onoutput images. The trained model achieves a mean Average Precision (mAP) scoreof 0.96, demonstrating its high performance for cotton bloom classification. Weevaluate our Lambda architecture pipeline using 9000 images yielding anoptimized runtime of 34 minutes. The results illustrate the scalability of theproposed pipeline as a solution for deep learning object detection, with thepotential for further expansion through additional Azure processing cores. Thiswork advances the scientific research field by providing a new method forcotton bloom detection on a large dataset and demonstrates the potential ofutilizing cloud computing resources, specifically Azure, for efficient andaccurate big data processing in precision agriculture.$"Keywords: high-throughput cotton phenotyping, big data pipeline, lambda architecture, computer vision, deep neural networks, cloud computing, Azure. 

Summary: This paper proposes a big data pipeline for cotton bloom detection using a Lambda architecture and leveraging Azure resources. The pipeline consists of data preprocessing, object detection using a YOLOv5 neural network model, and visualization of detection bounding boxes. The paper demonstrates the potential of cloud computing resources for efficient and accurate big data processing in precision agriculture. Ultimately, the proposed pipeline advances the scientific research field by providing a new method for cotton bloom detection on a large dataset."$"作者：Amanda Issac（美国乔治亚大学），Alireza Ebrahimi（美国南卡罗来纳大学），Javad Mohammadpour Velnib（美国乔治亚大学）和Glen Rainsc（美国乔治亚大学）
机构：
a. 电气与计算机工程学院，美国乔治亚大学，美国乔治亚州雅典市
b. 机械工程系，美国南卡罗来纳大学，美国南卡罗来纳州克莱姆森市
c. 昆虫学系，美国乔治亚大学，美国乔治亚州蒂夫顿市"$本文提出了一种基于Lambda架构的大数据管道，用于棉花开花检测，实现了实时和批处理的数据处理。该管道利用Azure资源，如数据工厂、事件网格、Rest API和Databricks，其中包括数据预处理，使用Azure AutoML训练的YOLOv5神经网络模型进行目标检测，并在输出图像上对目标检测边界框进行可视化。训练模型的mAP得分为0.96，表明其用于棉花开花分类的性能较高。通过使用9,000个图像评估我们的Lambda架构管道，得出34分钟的优化运行时，说明该管道具有良好的可扩展性，并具有进一步通过Azure处理核心进行扩展的潜力。这项工作通过提供大型数据集的新方法来检测棉花开花，同时展示了利用云计算资源，特别是Azure，进行高效和精确的大数据处理在精准农业中的潜力。$http://arxiv.org/pdf/2305.05423v1
Bone Marrow Cytomorphology Cell Detection using InceptionResNetV2$  Critical clinical decision points in haematology are influenced by therequirement of bone marrow cytology for a haematological diagnosis. Bone marrowcytology, however, is restricted to reference facilities with expertise, andlinked to inter-observer variability which requires a long time to process thatcould result in a delayed or inaccurate diagnosis, leaving an unmet need forcutting-edge supporting technologies. This paper presents a novel transferlearning model for Bone Marrow Cell Detection to provide a solution to all thedifficulties faced for the task along with considerable accuracy. The proposedmodel achieved 96.19\\% accuracy which can be used in the future for analysis ofother medical images in this domain.$The article focuses on bone marrow cytology and cell detection, and proposes a new deep learning model called InceptionResNetV2.$"作者：Raisa Fairooz Meem 和 Khandaker Tabin Hasan （来自未知机构）
Graphical Abstract：无图形摘要，仅提供文章标题和作者信息"$本文提出了一种基于InceptionRes-NetV2模型的骨髓细胞检测方法。该方法通过对数字图像进行预处理和增强，获取细胞图像，并通过InceptionRes-NetV2模型对细胞进行分类和定位。实验结果表明，该方法具有高度的精确性和召回率，可以有效地用于骨髓细胞的检测和定位。$http://arxiv.org/pdf/2305.05430v1
WikiWeb2M: A Page-Level Multimodal Wikipedia Dataset$  Webpages have been a rich resource for language and vision-language tasks.Yet only pieces of webpages are kept: image-caption pairs, long text articles,or raw HTML, never all in one place. Webpage tasks have resultingly receivedlittle attention and structured image-text data underused. To study multimodalwebpage understanding, we introduce the Wikipedia Webpage 2M (WikiWeb2M) suite;the first to retain the full set of images, text, and structure data availablein a page. WikiWeb2M can be used for tasks like page description generation,section summarization, and contextual image captioning.$"Keywords: Wiki Workshop, Multimodal Data, Webpages, Machine Learning, Text Generation, Vision and Language. 

The article introduces a new dataset, Wikipedia Webpage 2M (WikiWeb2M), which retains the full set of images, text, and structure data available in a page, allowing for greater study of multimodal content understanding with many-to-many text and image relationships. The dataset can be used for tasks such as page description generation, section summarization, and contextual image captioning, and has potential applications in improving interaction with web content and modern content generation. The article discusses the creation process and additional processing done on the dataset, as well as its comparison to other datasets, such as the Wikipedia Image Text (WIT) dataset."$"作者：Andrea Burns、Krishna Srinivasan、Joshua Ainslie、Geoff Brown、Kate Saenko、Bryan A. Plummer、Jianmo Ni、Mandy Guo；
机构：Boston University、Google、FAIR"$本文提出了一个新的多模态数据集——Wikipedia Webpage 2M（简称WikiWeb2M），这是第一个将web pages的所有内容——图片、文字和结构数据都保留下来的数据集，它可以用于页描述生成、段落摘要和上下文图像字幕等任务，并为多模态内容理解的研究提供了大幅度的帮助。此外，文中还介绍了如何构建数据集和后续处理过程，包括WIT数据集的重采样和在处理图像时的筛选方法等。最后，文中给出了实验结果，证明了该数据集在多任务学习上的有效性。$http://arxiv.org/pdf/2305.05432v1
Multiscale Augmented Normalizing Flows for Image Compression$  Most learning-based image compression methods lack efficiency for high imagequality due to their non-invertible design. The decoding function of thefrequently applied compressive autoencoder architecture is only an approximatedinverse of the encoding transform. This issue can be resolved by usinginvertible latent variable models, which allow a perfect reconstruction if noquantization is performed. Furthermore, many traditional image and video codersapply dynamic block partitioning to vary the compression of certain imageregions depending on their content. Inspired by this approach, hierarchicallatent spaces have been applied to learning-based compression networks. In thispaper, we present a novel concept, which adapts the hierarchical latent spacefor augmented normalizing flows, an invertible latent variable model. Our bestperforming model achieved average rate savings of more than 7% over comparablesingle-scale models.$Keywords: learning-based image compression, augmented normalizing flows, hierarchical latent space, rate distortion optimization, end-to-end image compression.$作者名（机构）：Marc Windsheimer, Fabian Brand, Andr ´e Kaup（Friedrich-Alexander-Universit ¨at Erlangen-N ¨urnberg，Multimedia Communications and Signal Processing）$本文介绍了一种新颖的多尺度数据增广归一化流（multiscale augmented normalizing flows, MANF）模型，用于图像压缩。学习式图像压缩在高品质图像方面缺乏效率，由于它们的非可逆设计。使用不可逆潜变量模型，如增强型归一化流（ANF），可以改善学习式图像压缩的性能。本文还将分层潜变量空间应用于MANF网络，以适应图像内容。实验结果表明，与单一尺度模型相比，本文提出的MANF模型产生了平均节省7％以上的比特率。$http://arxiv.org/pdf/2305.05451v1
Style-A-Video: Agile Diffusion for Arbitrary Text-based Video Style  Transfer$  Large-scale text-to-video diffusion models have demonstrated an exceptionalability to synthesize diverse videos. However, due to the lack of extensivetext-to-video datasets and the necessary computational resources for training,directly applying these models for video stylization remains difficult. Also,given that the noise addition process on the input content is random anddestructive, fulfilling the style transfer task\'s content preservation criteriais challenging. This paper proposes a zero-shot video stylization method namedStyle-A-Video, which utilizes a generative pre-trained transformer with animage latent diffusion model to achieve a concise text-controlled videostylization. We improve the guidance condition in the denoising process,establishing a balance between artistic expression and structure preservation.Furthermore, to decrease inter-frame flicker and avoid the formation ofadditional artifacts, we employ a sampling optimization and a temporalconsistency module. Extensive experiments show that we can attain superiorcontent preservation and stylistic performance while incurring less consumptionthan previous solutions. Code will be available athttps://github.com/haha-lisa/Style-A-Video.$Keywords: Style transfer, text-to-video, video stylization, generative pre-trained transformer, diffusion models, content preservation, temporal consistency.$作者名（机构）：Nisha Huang（中国科学院自动化研究所人工智能学院）、Yuxin Zhang（中国科学院自动化研究所人工智能学院）、Weiming Dong（中国科学院自动化研究所人工智能学院）。$本文提出了一种零样本视频风格化方法，名为Style-A-Video。该方法利用了预先训练的生成式转换器及图像潜在扩散模型，实现了简洁的文本控制视频风格化。为了达到对艺术表现和结构保护之间的平衡，文章改善了去噪过程中的指导条件。此外，为了减少帧间闪烁并避免形成额外的伪影，文章还采用了抽样优化和时态一致性模块。实验结果表明，相较于以往的方法，该方法在达到优秀的内容保护和风格表现的同时，消耗更少的资源。文章还探讨了在文本与视频匹配数据收集方面的限制，以及如何利用现有的文本-图像模型生成视频的实践意义。$http://arxiv.org/pdf/2305.05464v1
Integrating Holistic and Local Information to Estimate Emotional  Reaction Intensity$  Video-based Emotional Reaction Intensity (ERI) estimation measures theintensity of subjects\' reactions to stimuli along several emotional dimensionsfrom videos of the subject as they view the stimuli. We propose a multi-modalarchitecture for video-based ERI combining video and audio information. Videoinput is encoded spatially first, frame-by-frame, combining features encodingholistic aspects of the subjects\' facial expressions and features encodingspatially localized aspects of their expressions. Input is then combined acrosstime: from frame-to-frame using gated recurrent units (GRUs), then globally bya transformer. We handle variable video length with a regression token thataccumulates information from all frames into a fixed-dimensional vectorindependent of video length. Audio information is handled similarly: spectralinformation extracted within each frame is integrated across time by a cascadeof GRUs and a transformer with regression token. The video and audio regressiontokens\' outputs are merged by concatenation, then input to a final fullyconnected layer producing intensity estimates. Our architecture achievedexcellent performance on the Hume-Reaction dataset in the ERI EsimationChallenge of the Fifth Competition on Affective Behavior Analysis in-the-Wild(ABAW5). The Pearson Correlation Coefficients between estimated and subjectself-reported scores, averaged across all emotions, were 0.455 on thevalidation dataset and 0.4547 on the test dataset, well above the baselines.The transformer\'s self-attention mechanism enables our architecture to focus onthe most critical video frames regardless of length. Ablation experimentsestablish the advantages of combining holistic/local features and ofmulti-modal integration. Code available at https://github.com/HKUST-NISL/ABAW5.$Emotional Reaction Intensity (ERI) estimation, video-based affective behavior analysis, gated recurrent units (GRUs), transformer, multi-modal integration, Hume-Reaction dataset, Affective Behavior Analysis in-the-Wild (ABAW5), holistic/local features, facial expressions.$"作者：Yini Fang, Liang Wu, Frederic Jumelle和Bertram Shi

机构：香港科技大学（Hong Kong University of Science and Technology）和Bright Nation Limited"$本论文提出一种多模态的架构，用于基于视频和音频信息来估计情感反应强度（ERI）。视频输入首先通过帧与帧之间的门控循环单元（GRUs）进行空间编码，结合编码主体面部表情整体方面和编码空间局部方面的功能。然后，在时间上进行整合：通过变换器处理从帧到帧，并使用回归标记处理可变视频长度。音频信息同样处理: 每帧提取出频谱信息，再通过GRUs和变换器进行时间整合，最后通过连接融合视频和音频回归标记的输出，输入到最终的完全连接层中进行强度估计。实验表明，综合使用整体/局部特征和多模态整合的优点。这种架构在ABAW5的ERI估计挑战中具有出色的性能。文章认为，多模态整合的关键在于变换器的自我关注机制，使架构能够聚焦于最关键的视频帧而不受长度限制。$http://arxiv.org/pdf/2305.05534v1
Fashion CUT: Unsupervised domain adaptation for visual pattern  classification in clothes using synthetic data and pseudo-labels$  Accurate product information is critical for e-commerce stores to allowcustomers to browse, filter, and search for products. Product data quality isaffected by missing or incorrect information resulting in poor customerexperience. While machine learning can be used to correct inaccurate or missinginformation, achieving high performance on fashion image classification tasksrequires large amounts of annotated data, but it is expensive to generate dueto labeling costs. One solution can be to generate synthetic data whichrequires no manual labeling. However, training a model with a dataset of solelysynthetic images can lead to poor generalization when performing inference onreal-world data because of the domain shift. We introduce a new unsuperviseddomain adaptation technique that converts images from the synthetic domain intothe real-world domain. Our approach combines a generative neural network and aclassifier that are jointly trained to produce realistic images whilepreserving the synthetic label information. We found that using real-worldpseudo-labels during training helps the classifier to generalize in thereal-world domain, reducing the synthetic bias. We successfully train a visualpattern classification model in the fashion domain without real-worldannotations. Experiments show that our method outperforms other unsuperviseddomain adaptation algorithms.$Fashion CUT: Unsupervised domain adaptation, synthetic data, pattern classification, fashion image classification, generative neural network, pseudo-labels.$"作者：Enric Moreu, Alex Martinelli, Martina Naughton, Philip Kelly, Noel E. O'Connor
机构：1Zalando SE, Valeska-Gert-Straße 5, 10243 Berlin, Germany；2Insight Centre for Data Analytics, Dublin City University, Dublin, Ireland"$本文介绍了一种无监督域自适应技术，将来自合成域的图像转换为真实世界中的图像，并在此过程中保留标签信息，以提高视觉模式分类模型在真实世界中的泛化能力。文章提出使用合成数据来训练模型，配合真实世界伪标签进行训练，以达到提高产品数据质量的目的。作者在时尚领域的实验结果表明，本方法优于其他无监督域自适应算法。关键词：域自适应、合成数据、模式分类。$http://arxiv.org/pdf/2305.05580v1
Can point cloud networks learn statistical shape models of anatomies?$  Statistical Shape Modeling (SSM) is a valuable tool for investigating andquantifying anatomical variations within populations of anatomies. However,traditional correspondence-based SSM generation methods require atime-consuming re-optimization process each time a new subject is added to thecohort, making the inference process prohibitive for clinical research.Additionally, they require complete geometric proxies (e.g., high-resolutionbinary volumes or surface meshes) as input shapes to construct the SSM.Unordered 3D point cloud representations of shapes are more easily acquiredfrom various medical imaging practices (e.g., thresholded images and surfacescanning). Point cloud deep networks have recently achieved remarkable successin learning permutation-invariant features for different point cloud tasks(e.g., completion, semantic segmentation, classification). However, theirapplication to learning SSM from point clouds is to-date unexplored. In thiswork, we demonstrate that existing point cloud encoder-decoder-based completionnetworks can provide an untapped potential for SSM, capturing population-levelstatistical representations of shapes while reducing the inference burden andrelaxing the input requirement. We discuss the limitations of these techniquesto the SSM application and suggest future improvements. Our work paves the wayfor further exploration of point cloud deep learning for SSM, a promisingavenue for advancing shape analysis literature and broadening SSM to diverseuse cases.$"Keywords: Statistical Shape Modeling, Point Cloud Deep Networks, Morphometrics, Medical Imaging, Deep Learning. 

This paper explores the potential for point cloud deep networks to learn statistical shape models (SSMs) of anatomical variations within populations of anatomies, which can be used for medical research and clinical applications. Traditional SSM generation methods require complete geometric proxies and time-consuming re-optimization when new subjects are added, while point cloud representations are more easily acquired from medical imaging practices. The authors demonstrate that existing point cloud encoder-decoder-based completion networks can provide an untapped potential for SSM, capturing population-level statistical representations of shapes while reducing the inference burden and relaxing the input requirement. The paper also discusses the limitations and suggests future improvements, paving the way for further exploration of point cloud deep learning for SSM and advancing shape analysis literature."$作者名（机构）： Jadie Adams（Scientific Computing and Imaging Institute, University of Utah, UT, USA）; Shireen Elhabian（Scientific Computing and Imaging Institute, University of Utah, UT, USA； School of Computing, University of Utah, UT, USA）$本文探讨了点云深度网络是否可以学习解剖学族群的统计形状模型。传统的对应SSM生成方法需要在将新的主题添加到队列时进行耗时的重新优化过程，使得推理过程对临床研究来说是有障碍的。此外，它们需要完整的几何代理（例如高分辨率二进制体积或表面网格）作为输入形状来构建SSM。由于医学成像实践（例如阈值图像和表面扫描）更容易获得无序的3D点云表示形状，因此点云深度网络近期在学习不同点云任务（例如完成，语义分割，分类）的排列不变特征方面取得了显着的成功。本文表明现有的点云编码器-解码器完成网络可以为SSM提供未开发的潜力，捕捉形状的族群级统计表示，同时减轻推理负担并减少输入要求。本文讨论了这些技术在SSM应用中的局限性并提出了未来的改进。我们的工作为进一步探索点云深度学习的SSM铺平了道路，这是提高形状分析文献水平和扩展各种用例的有前途的途径。$http://arxiv.org/pdf/2305.05610v1
Adaptive Domain Generalization for Digital Pathology Images$"  In AI-based histopathology, domain shifts are common and well-studied.However, this research focuses on stain and scanner variations, which do notshow the full picture -- shifts may be combinations of other shifts, or""invisible"" shifts that are not obvious but still damage performance of machinelearning models. Furthermore, it is important for models to generalize to theseshifts without expensive or scarce annotations, especially in thehistopathology space and if wanting to deploy models on a larger scale. Thus,there is a need for ""reactive"" domain generalization techniques: ones thatadapt to domain shifts at test-time rather than requiring predictions of orexamples of the shifts at training time. We conduct a literature review andintroduce techniques that react to domain shifts rather than requiring aprediction of them in advance. We investigate test time training, a techniquefor domain generalization that adapts model parameters at test-time throughoptimization of a secondary self-supervised task."$Digital pathology, Domain generalization, Adaptive learning.$"作者：Andrew John Walker（明尼苏达大学）
机构：明尼苏达大学"$这篇论文是关于数字病理学图像的自适应领域泛化的研究，旨在解决在测试数据中存在不同于训练数据的领域漂移问题。作者提出了一种基于适应性域分类器的方法，通过在训练和推理过程中使用抗混淆损失来实现自适应领域泛化。结果表明，该方法提高了数字病理学图像分类模型的泛化能力，有效地解决了领域漂移问题。$http://arxiv.org/pdf/2305.05100v1
Towards unraveling calibration biases in medical image analysis$  In recent years the development of artificial intelligence (AI) systems forautomated medical image analysis has gained enormous momentum. At the sametime, a large body of work has shown that AI systems can systematically andunfairly discriminate against certain populations in various applicationscenarios. These two facts have motivated the emergence of algorithmic fairnessstudies in this field. Most research on healthcare algorithmic fairness to datehas focused on the assessment of biases in terms of classical discriminationmetrics such as AUC and accuracy. Potential biases in terms of modelcalibration, however, have only recently begun to be evaluated. This isespecially important when working with clinical decision support systems, aspredictive uncertainty is key for health professionals to optimally evaluateand combine multiple sources of information. In this work we studydiscrimination and calibration biases in models trained for automatic detectionof malignant dermatological conditions from skin lesions images. Importantly,we show how several typically employed calibration metrics are systematicallybiased with respect to sample sizes, and how this can lead to erroneousfairness analysis if not taken into consideration. This is of particularrelevance to fairness studies, where data imbalance results in drastic samplesize differences between demographic sub-groups, which, if not taken intoaccount, can act as confounders.$Medical image analysis, algorithmic fairness, bias, calibration, skin lesion analysis. This paper highlights the importance of evaluating biases in terms of model calibration, especially for clinical decision support systems. The study focuses on discrimination and calibration biases in models trained for automatic detection of malignant dermatological conditions from skin lesion images. The paper also highlights how several calibration metrics are systematically biased with respect to sample sizes, which can lead to erroneous fairness analysis if not taken into consideration. Finally, the authors emphasize the relevance of fairness studies for developing trustworthy AI systems in healthcare.$作者名（机构）：Mar´ıa Agustina Ricci Lara1,2, Candelaria Mosquera1,2, Enzo Ferrante3, Rodrigo Echeveste3；1 医学信息学系，布宜诺斯艾利斯意大利医院，阿根廷布宜诺斯艾利斯；2 阿根廷布宜诺斯艾利斯国立技术大学；3 信号、系统和计算智能研究所 sinc(i) （FICH-UNL/CONICET），阿根廷圣菲。$这篇论文讨论了在医学图像分析中发现和评估算法公平性所面临的挑战。作者研究了自动检测恶性皮肤病变模型的歧视和校准偏差，并探讨了几种常用的校准评估指标在样本规模方面的偏差，以及这可能导致错误的公平性分析。在这个领域中，公平性研究的数据不平衡会导致不同人口子群之间的样本量差异，在未考虑这一因素时，结果会出现混淆误差。因此，评估AI算法的公平性是构建可信系统的重要步骤。作者认为，在医学影像计算领域，评估分类性能应理解为判别能力和校准能力的组合，需要进行全面的评估。$http://arxiv.org/pdf/2305.05101v1
DeepTree: Modeling Trees with Situated Latents$  In this paper, we propose DeepTree, a novel method for modeling trees basedon learning developmental rules for branching structures instead of manuallydefining them. We call our deep neural model situated latent because itsbehavior is determined by the intrinsic state -- encoded as a latent space of adeep neural model -- and by the extrinsic (environmental) data that is situatedas the location in the 3D space and on the tree structure. We use a neuralnetwork pipeline to train a situated latent space that allows us to locallypredict branch growth only based on a single node in the branch graph of a treemodel. We use this representation to progressively develop new branch nodes,thereby mimicking the growth process of trees. Starting from a root node, atree is generated by iteratively querying the neural network on the newly addednodes resulting in the branching structure of the whole tree. Our methodenables generating a wide variety of tree shapes without the need to defineintricate parameters that control their growth and behavior. Furthermore, weshow that the situated latents can also be used to encode the environmentalresponse of tree models, e.g., when trees grow next to obstacles. We validatethe effectiveness of our method by measuring the similarity of our tree modelsand by procedurally generated ones based on a number of established metrics fortree form.$Key words: Deep Learning, Situated Latent, Tree Modeling, Procedural Modeling, Botanical Tree Models, Shape Modeling, Generative Methods, Computer Vision Problems, Developmental Modeling.$作者：XIAOCHEN ZHOU, BOSHENG LI, BEDRICH BENES, SONGLIN FEI 机构：Purdue University, USA；SÖREN PIRK, 机构：Adobe Research, USA。$这篇论文介绍了一种名为DeepTree的生成方法，通过学习树木分枝结构的发育规律，而非手动定义分枝结构来模拟树木的生长过程。作者使用神经网络管道训练一个名为“situated latent”的模型，它的行为由隐空间编码的内在状态和“situated”在3D空间和树结构上的外源性数据决定。作者通过局部预测单个节点的分支生长来逐步开发新的分支节点，从而模拟树木的生长过程。该方法无需定义复杂的参数即可生成多样的树木形状。作者还说明了在DeepTree中，situated latents可以用于编码树木模型的环境响应，例如当树木生长在障碍物旁边时。作者通过多个树形态度量进行验证，证明该方法的有效性。$http://arxiv.org/pdf/2305.05153v1
Semantic Embedded Deep Neural Network: A Generic Approach to Boost  Multi-Label Image Classification Performance$  Fine-grained multi-label classification models have broad applications inAmazon production features, such as visual based label predictions ranging fromfashion attribute detection to brand recognition. One challenge to achievesatisfactory performance for those classification tasks in real world is thewild visual background signal that contains irrelevant pixels which confusesmodel to focus onto the region of interest and make prediction upon thespecific region. In this paper, we introduce a generic semantic-embedding deepneural network to apply the spatial awareness semantic feature incorporating achannel-wise attention based model to leverage the localization guidance toboost model performance for multi-label prediction. We observed an Avg.relativeimprovement of 15.27% in terms of AUC score across all labels compared to thebaseline approach. Core experiment and ablation studies involve multi-labelfashion attribute classification performed on Instagram fashion apparels\'image. We compared the model performances among our approach, baselineapproach, and 3 alternative approaches to leverage semantic features. Resultsshow favorable performance for our approach.$Keywords: Semantic-embedding, deep neural network, multi-label image classification, spatial awareness, attention-based model, fashion attribute detection, wild visual backgrounds, regional semantic information, class activation maps, imbalanced distribution.$"作者：Xin Shen, Xiaonan Zhao, Rui Luo
机构：Amazon.com"$本文提出了一种基于深度神经网络的通用语义嵌入方法，通过引入基于通道的注意力模型来应用空间感知语义特征，从而提升多标签图像分类模型的性能。在Instagram时尚服装图像上进行的多标签时尚属性分类实验以及对比实验显示出了本方法的优越性，相比基线方法，平均AUC评分相对提升了15.27%。该方法的主要贡献在于将分类标签用作类激活映射生成器，学习各像素的语义嵌入，并将其与原始图像张量连接作为输入特征。该方法不需要语义分割模型和物体检测模型的预训练，适用性更强。$http://arxiv.org/pdf/2305.05228v1
DietCNN: Multiplication-free Inference for Quantized CNNs$  The rising demand for networked embedded systems with machine intelligencehas been a catalyst for sustained attempts by the research community toimplement Convolutional Neural Networks (CNN) based inferencing on embeddedresource-limited devices. Redesigning a CNN by removing costly multiplicationoperations has already shown promising results in terms of reducing inferenceenergy usage. This paper proposes a new method for replacing multiplications ina CNN by table look-ups. Unlike existing methods that completely modify the CNNoperations, the proposed methodology preserves the semantics of the major CNNoperations. Conforming to the existing mechanism of the CNN layer operationsensures that the reliability of a standard CNN is preserved. It is shown thatthe proposed multiplication-free CNN, based on a single activation codebook,can achieve 4.7x, 5.6x, and 3.5x reduction in energy per inference in an FPGAimplementation of MNIST-LeNet-5, CIFAR10-VGG-11, and Tiny ImageNet-ResNet-18respectively. Our results show that the DietCNN approach significantly improvesthe resource consumption and latency of deep inference for smaller models,often used in embedded systems. Our code is available at:https://github.com/swadeykgp/DietCNN$Specific domains and keywords discussed in this paper include: networked embedded systems, Convolutional Neural Networks (CNNs), multiplication-free inference, table look-ups, resource-limited devices, energy efficiency, CNN operations, activation codebook, FPGA implementation, MNIST-LeNet-5, CIFAR10-VGG-11, Tiny ImageNet-ResNet-18, deep inference, resource consumption, latency, small models, and edge-enabled IoT devices.$"作者名（机构）：Swarnava Dey（Tata Consultancy Services Ltd. & Indian Institute of Technology Kharagpur）、Pallab Dasgupta和Partha P Chakrabarti（Indian Institute of Technology Kharagpur）

论文标题：DietCNN: Multiplication-free Inference for Quantized CNNs (Supplementary Material and author's Draft)

摘要：本文提出了一种新的方法，通过表查找来替换CNN中的乘法运算。与现有方法完全修改CNN操作不同，该方法保留了主要CNN操作的语义。符合CNN层操作的现有机制可以确保标准CNN的可靠性。将基于单个激活代码本的提出的无乘法CNN应用于FPGA实现的MNIST-LeNet-5、CIFAR10-VGG-11和Tiny ImageNet-ResNet-18，分别可以获得4.7倍、5.6倍和3.5倍的推理能量消耗减小。这种方法显著改善了在嵌入式系统中使用的小型模型的资源消耗和延迟。该方法的代码可在https://github.com/swadeykgp/DietCNN获得。"$本文介绍了一种针对卷积神经网络（CNNs）进行量化推理的无乘法方法。该方法通过表查找替代乘法，相比于已有的方法，该方法保留了CNN操作的语义，以确保标准CNN的可靠性和正确性。作者还使用单一激活码本的基于DietCNN无乘法CNN，在FPGA实现MNIST-LeNet-5、CIFAR10-VGG-11和Tiny ImageNet-ResNet-18的推理过程中，分别实现了4.7x、5.6x和3.5x的能耗降低。该方法使小型模型嵌入式系统中的资源消耗和延迟得到了显著改善。$http://arxiv.org/pdf/2305.05274v1
Learning Dynamic Point Cloud Compression via Hierarchical Inter-frame  Block Matching$  3D dynamic point cloud (DPC) compression relies on mining its temporalcontext, which faces significant challenges due to DPC\'s sparsity andnon-uniform structure. Existing methods are limited in capturing sufficienttemporal dependencies. Therefore, this paper proposes a learning-based DPCcompression framework via hierarchical block-matching-based inter-predictionmodule to compensate and compress the DPC geometry in latent space.Specifically, we propose a hierarchical motion estimation and motioncompensation (Hie-ME/MC) framework for flexible inter-prediction, whichdynamically selects the granularity of optical flow to encapsulate the motioninformation accurately. To improve the motion estimation efficiency of theproposed inter-prediction module, we further design a KNN-attention blockmatching (KABM) network that determines the impact of potential correspondingpoints based on the geometry and feature correlation. Finally, we compress theresidual and the multi-scale optical flow with a fully-factorized deep entropymodel. The experiment result on the MPEG-specified Owlii Dynamic Human DynamicPoint Cloud (Owlii) dataset shows that our framework outperforms the previousstate-of-the-art methods and the MPEG standard V-PCC v18 in inter-framelow-delay mode.$Keywords: dynamic point cloud compression, inter-frame block matching, hierarchical motion estimation, optical flow estimation, deep learning.$"作者：夏舒婷、范庭宇、许亦凌、Jenq-Neng Hwang、李竹

机构：上海交通大学合作式多媒体创新中心、华盛顿大学、密苏里大学堪萨斯城"$本研究提出了一种基于层级块匹配的交叉预测模块的学习型DPC压缩框架，该框架通过在潜在空间中补偿和压缩DPC几何体。作者进一步设计了一个KNN-attention块匹配网络，以确定潜在对应点的影响，并结合多尺度光流压缩残差。在MPEG指定的Owlii人类动态点云数据集上的实验表明，该框架在交叉帧低延迟模式下优于先前的最先进方法和MPEG标准V-PCC v18。$http://arxiv.org/pdf/2305.05356v1
Investigating the Corruption Robustness of Image Classifiers with Random  Lp-norm Corruptions$  Robustness is a fundamental property of machine learning classifiers toachieve safety and reliability. In the fields of adversarial robustness andformal robustness verification of image classification models, robustness iscommonly defined as the stability to all input variations within an Lp-normdistance. However, robustness to random corruptions is usually improved andevaluated using variations observed in the real-world, while mathematicallydefined Lp-norm corruptions are rarely considered. This study investigates theuse of random Lp-norm corruptions to augment the training and test data ofimage classifiers. We adapt an approach from the field of adversarialrobustness to assess the model robustness to imperceptible random corruptions.We empirically and theoretically investigate whether robustness is transferableacross different Lp-norms and derive conclusions on which Lp-norm corruptions amodel should be trained and evaluated on. We find that training dataaugmentation with L0-norm corruptions improves corruption robustness whilemaintaining accuracy compared to standard training and when applied on top ofselected state-of-the-art data augmentation techniques.$Keywords: Machine learning; Image classification; Robustness; Lp-norm corruptions; Data augmentation.$作者名（机构）：Georg Siedel（德国联邦职业安全与健康研究所）、Silvia Vocka和Andrey Morozov（斯图加特大学）。$本论文探究了利用随机Lp范数抗污染来增强图像分类器的训练和测试数据。文章的背景是在实际应用中，在保证精度的情况下，分类器必须足够鲁棒性。作者用一种在对抗鲁棒性领域经常使用的方法来评估模型对误差的鲁棒性。文章从实验和定量研究两方面探究了不同Lp范数下的鲁棒性问题，发现通过训练数据加强，L0范数污染可以提高模型的污染鲁棒性。同时，该论文探究了不同的Lp范数和鲁棒性目标对于模型Lp范数下的污染鲁棒性的影响。最终，作者提出了结论，建议在标准训练基础上引入L0范数污染，来提高污染鲁棒性。$http://arxiv.org/pdf/2305.05400v1
Echo from noise: synthetic ultrasound image generation using diffusion  models for real image segmentation$"  We propose a novel pipeline for the generation of synthetic images viaDenoising Diffusion Probabilistic Models (DDPMs) guided by cardiac ultrasoundsemantic label maps. We show that these synthetic images can serve as a viablesubstitute for real data in the training of deep-learning models for medicalimage analysis tasks such as image segmentation. To demonstrate theeffectiveness of this approach, we generated synthetic 2D echocardiographyimages and trained a neural network for segmentation of the left ventricle andleft atrium. The performance of the network trained on exclusively syntheticimages was evaluated on an unseen dataset of real images and yielded mean Dicescores of 88.5 $\\pm 6.0$ , 92.3 $\\pm 3.9$, 86.3 $\\pm 10.7$ \\% for leftventricular endocardial, epicardial and left atrial segmentation respectively.This represents an increase of $9.09$, $3.7$ and $15.0$ \\% in Dice scorescompared to the previous state-of-the-art. The proposed pipeline has thepotential for application to a wide range of other tasks across various medicalimaging modalities."$Keywords: Echo, Ultrasound, Image segmentation, Deep learning, Synthetic images, Denoising Diffusion Probabilistic Models (DDPMs), Medical image analysis.$"作者：David Stojanovski, Uxio Hermida, Pablo Lamata, Arian Beqiri, 和 Alberto Gomez
机构：1）国王学院伦敦；2）Ultromics

摘要：本文提出了一种基于去噪扩散概率模型（DDPMs）和心脏超声语义标签图的合成图像生成方法。我们展示了这些合成图像可以作为训练深度学习模型用于医学图像分析任务（如图像分割）中的真实数据的可行替代品。为了展示这种方法的有效性，我们生成了人工合成的2D心脏超声图像，并训练了一个神经网络用于分割左心室和左心房。在仅使用人工合成图像进行训练的网络的性能在真实图像的未见数据集上被评估，并得到了88.5±6.0％，92.3±3.9％，86.3±10.7％的均值Dice得分，分别用于左室内膜、外膜和左心房分割。相比之前的最先进方法，这表示Dice得分分别增加了9.09％、3.7％和15.0％。该方法有潜力应用于各种医学成像模态的其他任务中。

关键词：扩散模型·图像合成·超声"$本文提出了一种基于去噪扩散概率模型（DDPMs）和心脏超声语义标签地图引导合成图像的新型流程。我们展示了这些合成图像可以作为训练医学图像分析任务（如图像分割）的深度学习模型的可行替代真实数据的方法。为了证明该方法的有效性，我们生成了 2D 超声心动图，并训练了一个神经网络，用于左心室和左心房的分割。在仅使用合成图像训练的网络在未见过的真实图像数据集上的表现优于之前的最先进方法，分别得到左心室内膜、心外膜和左心房分割的 Dice 分数均值为 88.5±6.0%、92.3±3.9% 和 86.3±10.7%。该方法有潜在的应用于各种医学影像模态下的其他任务。$http://arxiv.org/pdf/2305.05424v1
StyleSync: High-Fidelity Generalized and Personalized Lip Sync in  Style-based Generator$  Despite recent advances in syncing lip movements with any audio waves,current methods still struggle to balance generation quality and the model\'sgeneralization ability. Previous studies either require long-term data fortraining or produce a similar movement pattern on all subjects with lowquality. In this paper, we propose StyleSync, an effective framework thatenables high-fidelity lip synchronization. We identify that a style-basedgenerator would sufficiently enable such a charming property on both one-shotand few-shot scenarios. Specifically, we design a mask-guided spatialinformation encoding module that preserves the details of the given face. Themouth shapes are accurately modified by audio through modulated convolutions.Moreover, our design also enables personalized lip-sync by introducing stylespace and generator refinement on only limited frames. Thus the identity andtalking style of a target person could be accurately preserved. Extensiveexperiments demonstrate the effectiveness of our method in producinghigh-fidelity results on a variety of scenes. Resources can be found athttps://hangz-nju-cuhk.github.io/projects/StyleSync.$Keywords: lip sync, style-based generator, personalized optimization, modulated convolutions.$作者名（机构）：管家智（百度）、张展旺（百度）、周航（百度）、胡天舒（百度）、王开思远（悉尼大学）、何东亮（百度）、冯浩程（百度）、刘敬拓（百度）、丁尔睿（百度）、刘紫薇（南洋理工大学）、王京东（百度）。$本文提出了一种名为StyleSync的框架，可以实现高保真的唇部同步。该方法提出了基于样式的生成器，以实现一次性和少量训练数据的高质量输出。通过掩模引导的空间信息编码模块，能够保留给定面部的细节信息；通过调制卷积可以准确修改音频下的嘴形。此外，该设计还通过引入样式空间和生成器优化，仅对有限的帧进行个性化的唇部同步，从而可以准确保留目标人物的身份和口头风格。实验表明了该方法在各种场景下生成高保真的结果的有效性。$http://arxiv.org/pdf/2305.05445v1
Effects of Real-Life Traffic Sign Alteration on YOLOv7- an Object  Recognition Model$  The advancement of Image Processing has led to the widespread use of ObjectRecognition (OR) models in various applications, such as airport security andmail sorting. These models have become essential in signifying the capabilitiesof AI and supporting vital services like national postal operations. However,the performance of OR models can be impeded by real-life scenarios, such astraffic sign alteration. Therefore, this research investigates the effects ofaltered traffic signs on the accuracy and performance of object recognitionmodels. To this end, a publicly available dataset was used to create differenttypes of traffic sign alterations, including changes to size, shape, color,visibility, and angles. The impact of these alterations on the YOLOv7 (You OnlyLook Once) model\'s detection and classification abilities were analyzed. Itreveals that the accuracy of object detection models decreases significantlywhen exposed to modified traffic signs under unlikely conditions. This studyhighlights the significance of enhancing the robustness of object detectionmodels in real-life scenarios and the need for further investigation in thisarea to improve their accuracy and reliability.$Keywords: object recognition, traffic sign alteration, YOLOv7 model, accuracy, performance, real-world scenarios, image processing, artificial intelligence.$作者名（机构）：Farhin Farhad Riya, Shahinul Hoque, Md Saif Hassan Onim, Edward Michaud, and Edmon Begoli（田纳西大学电子工程与计算机科学系）$本文研究了交通标志的实际改变对物体识别模型YOLOv7的影响。文章指出，现实生活中的场景，如交通标志的改变会影响物体识别模型的性能，而识别交通标志的准确性对于确保道路安全至关重要，因此需要探究交通标志改变对物体识别模型性能的影响，提高其在实际情况下的鲁棒性。文章使用公开数据集创建了不同类型的交通标志改变，并评估了它们对YOLOv7模型的检测和分类能力的影响。研究发现，当暴露于改变后的交通标志在不太可能出现的条件下时，物体识别模型的准确性显著降低。本文的研究表明，需要进一步研究在实际情况下提高物体识别模型的精度和可靠性的必要性和重要性。$http://arxiv.org/pdf/2305.05499v1
Recursions Are All You Need: Towards Efficient Deep Unfolding Networks$  The use of deep unfolding networks in compressive sensing (CS) has seen widesuccess as they provide both simplicity and interpretability. However, sincemost deep unfolding networks are iterative, this incurs significantredundancies in the network. In this work, we propose a novel recursion-basedframework to enhance the efficiency of deep unfolding models. First, recursionsare used to effectively eliminate the redundancies in deep unfolding networks.Secondly, we randomize the number of recursions during training to decrease theoverall training time. Finally, to effectively utilize the power of recursions,we introduce a learnable unit to modulate the features of the model based onboth the total number of iterations and the current iteration index. Toevaluate the proposed framework, we apply it to both ISTA-Net+ and COAST.Extensive testing shows that our proposed framework allows the network to cutdown as much as 75% of its learnable parameters while mostly maintaining itsperformance, and at the same time, it cuts around 21% and 42% from the trainingtime for ISTA-Net+ and COAST respectively. Moreover, when presented with alimited training dataset, the recursive models match or even outperform theirrespective non-recursive baseline. Codes and pretrained models are available athttps://github.com/Rawwad-Alhejaili/Recursions-Are-All-You-Need .$Keywords: compressive sensing, deep unfolding networks, recursion-based framework, learnable unit, ISTA-Net+, COAST.$作者名（机构）：Rawwad Alhejaili, Motaz Alfarraj, Hamzah Luqman, and Ali Al-Shaikhi（King Fahd University of Petroleum and Minerals, Saudi Arabia）$本论文提出一种基于递归的框架，以提高深度展开模型的效率。该框架首先使用递归来有效消除深度展开网络中的冗余。其次，我们通过在训练过程中随机选择递归次数来减少总体训练时间。最后，为了有效利用递归的能力，我们引入了一个可学习单元，基于迭代次数和当前迭代索引来调节模型的特征。作者将该框架应用于ISTA-Net+和COAST，并进行了广泛的测试。结果表明，该框架可以使网络削减高达75%的可学习参数，同时大幅减少ISTA-Net+和COAST的训练时间。此外，在只有有限训练数据的情况下，递归模型与其非递归基线相当甚至更优。$http://arxiv.org/pdf/2305.05505v1
AudioSlots: A slot-centric generative model for audio separation$  In a range of recent works, object-centric architectures have been shown tobe suitable for unsupervised scene decomposition in the vision domain. Inspiredby these methods we present AudioSlots, a slot-centric generative model forblind source separation in the audio domain. AudioSlots is built usingpermutation-equivariant encoder and decoder networks. The encoder network basedon the Transformer architecture learns to map a mixed audio spectrogram to anunordered set of independent source embeddings. The spatial broadcast decodernetwork learns to generate the source spectrograms from the source embeddings.We train the model in an end-to-end manner using a permutation invariant lossfunction. Our results on Libri2Mix speech separation constitute a proof ofconcept that this approach shows promise. We discuss the results andlimitations of our approach in detail, and further outline potential ways toovercome the limitations and directions for future work.$Keywords: audio separation, blind source separation, set-structured data, permutation-equivariant encoder, transformer architecture, slot-based attention mechanism.$作者：Pradyumna Reddy（University College London），Scott Wisdom, Klaus Greff, John R. Hershey, Thomas Kipf（Google Research）$这篇论文提出了一种针对音频分离的基于插槽（Slot-Centric）的生成模型——AudioSlots。该模型利用置换等变编/解码器网络，其中编码器网络基于Transformer架构，学习将混合音频谱图映射到独立源嵌入的无序集合上，而空间广播解码器网络则学习从源嵌入生成源谱图。通过匹配损失来训练模型，将混合音频信号转换为独立的源信号。作者在Libri2Mix语音分离数据集上进行了实验验证，结果表明这种方法具有很大的潜力，但还存在一些挑战，例如难以生成高频率的详细信息，需要依赖启发式方法来拼接独立预测的音频块，以及仍需要使用地面真实参考音频源进行训练等。作者对如何解决这些问题进行了讨论，并提出了未来工作的可能方向。$http://arxiv.org/pdf/2305.05591v1
PET-NeuS: Positional Encoding Tri-Planes for Neural Surfaces$  A signed distance function (SDF) parametrized by an MLP is a commoningredient of neural surface reconstruction. We build on the successful recentmethod NeuS to extend it by three new components. The first component is toborrow the tri-plane representation from EG3D and represent signed distancefields as a mixture of tri-planes and MLPs instead of representing it with MLPsonly. Using tri-planes leads to a more expressive data structure but will alsointroduce noise in the reconstructed surface. The second component is to use anew type of positional encoding with learnable weights to combat noise in thereconstruction process. We divide the features in the tri-plane into multiplefrequency scales and modulate them with sin and cos functions of differentfrequencies. The third component is to use learnable convolution operations onthe tri-plane features using self-attention convolution to produce featureswith different frequency bands. The experiments show that PET-NeuS achieveshigh-fidelity surface reconstruction on standard datasets. Following previouswork and using the Chamfer metric as the most important way to measure surfacereconstruction quality, we are able to improve upon the NeuS baseline by 57% onNerf-synthetic (0.84 compared to 1.97) and by 15.5% on DTU (0.71 compared to0.84). The qualitative evaluation reveals how our method can better control theinterference of high-frequency noise. Code available at\\url{https://github.com/yiqun-wang/PET-NeuS}.$Keywords: neural surfaces, signed distance function, MLP, tri-plane representation, positional encoding, learnable convolution, self-attention convolution, surface reconstruction.$"作者名（机构）：王一群（重庆大学，沙特阿拉伯国王科技大学）、Ivan Skorokhodov（沙特阿拉伯国王科技大学）、Peter Wonka（沙特阿拉伯国王科技大学）
文章标题：PET-NeuS: Positional Encoding Tri-Planes for Neural Surfaces"$本文提出了一种名为PET-NeuS的神经表面重构方法，通过引入三个新组件来扩展已有方法NeuS。第一个组件是借鉴EG3D的三面体表示方法，将带符号距离场表示为三面体和MLPs的混合物，从而提高表达能力。第二个组件是采用一种新的可学习权重的位置编码来减少重构过程中的噪声。第三个组件是使用自我注意卷积操作对三面体特征进行卷积操作，以产生不同频带的特征。实验表明，PET-NeuS在标准数据集上实现了高保真度的表面重构，并且在Nerf-synthetic数据集上比NeuS基线提高了57%，在DTU数据集上提高了15.5%。该方法能够更好地控制高频噪声的干扰，有效地利用了其改进的局部表达能力来重构局部细节。$http://arxiv.org/pdf/2305.05594v1
Predicting Cardiovascular Disease Risk using Photoplethysmography and  Deep Learning$  Cardiovascular diseases (CVDs) are responsible for a large proportion ofpremature deaths in low- and middle-income countries. Early CVD detection andintervention is critical in these populations, yet many existing CVD riskscores require a physical examination or lab measurements, which can bechallenging in such health systems due to limited accessibility. Here weinvestigated the potential to use photoplethysmography (PPG), a sensingtechnology available on most smartphones that can potentially enablelarge-scale screening at low cost, for CVD risk prediction. We developed a deeplearning PPG-based CVD risk score (DLS) to predict the probability of havingmajor adverse cardiovascular events (MACE: non-fatal myocardial infarction,stroke, and cardiovascular death) within ten years, given only age, sex,smoking status and PPG as predictors. We compared the DLS with the office-basedrefit-WHO score, which adopts the shared predictors from WHO and Globoriskscores (age, sex, smoking status, height, weight and systolic blood pressure)but refitted on the UK Biobank (UKB) cohort. In UKB cohort, DLS\'s C-statistic(71.1%, 95% CI 69.9-72.4) was non-inferior to office-based refit-WHO score(70.9%, 95% CI 69.7-72.2; non-inferiority margin of 2.5%, p&lt;0.01). Thecalibration of the DLS was satisfactory, with a 1.8% mean absolute calibrationerror. Adding DLS features to the office-based score increased the C-statisticby 1.0% (95% CI 0.6-1.4). DLS predicts ten-year MACE risk comparable with theoffice-based refit-WHO score. It provides a proof-of-concept and suggests thepotential of a PPG-based approach strategies for community-based primaryprevention in resource-limited regions.$The article focuses on predicting cardiovascular disease risk using photoplethysmography and deep learning, authored by a team from Google LLC. The key areas of focus include machine learning, cardiovascular disease, risk prediction, photoplethysmography, and epidemiology.$"作者：Wei-Hung Weng（Google LLC）、Sebastien Baur（Google LLC）、Mayank Daswani（Google LLC）、Christina Chen（Google LLC）、Lauren Harrell（Google LLC）、Sujay Kakarmath（Google LLC）、Mariam Jabara（Google LLC）、Babak Behsaz（Google LLC）、Cory Y. McLean（Google LLC）、Yossi Matias（Google LLC）、Greg S. Corrado（Google LLC）、Shravya Shetty（Google LLC）、Shruthi Prabhakara（Google LLC）、Yun Liu（Google LLC）、Diego Ardila（Google LLC）
机构：Google LLC, Mountain View, CA, USA；Department of Global Health and Population, Department of Epidemiology, Harvard School of Public Health, Boston, MA, USA"$本文探讨了使用光电测容技术和深度学习来预测心血管疾病风险的可能性。该研究由谷歌公司的一组科学家和哈佛公共卫生学院的一名学者合作完成。作者们使用了大型心血管健康研究中心收集的匿名数据集，通过训练神经网络模型，成功地预测了心血管疾病风险。该方法可能有助于早期识别心血管疾病风险，并可为医疗保健提供更加个性化和有效的预防和治疗。$http://arxiv.org/pdf/2305.05648v1
ImageBind: One Embedding Space To Bind Them All$  We present ImageBind, an approach to learn a joint embedding across sixdifferent modalities - images, text, audio, depth, thermal, and IMU data. Weshow that all combinations of paired data are not necessary to train such ajoint embedding, and only image-paired data is sufficient to bind themodalities together. ImageBind can leverage recent large scale vision-languagemodels, and extends their zero-shot capabilities to new modalities just byusing their natural pairing with images. It enables novel emergent applications\'out-of-the-box\' including cross-modal retrieval, composing modalities witharithmetic, cross-modal detection and generation. The emergent capabilitiesimprove with the strength of the image encoder and we set a newstate-of-the-art on emergent zero-shot recognition tasks across modalities,outperforming specialist supervised models. Finally, we show strong few-shotrecognition results outperforming prior work, and that ImageBind serves as anew way to evaluate vision models for visual and non-visual tasks.$Keywords: Multimodal learning, joint embedding, cross-modal retrieval, embedding-space arithmetic, audio to image generation.$"作者：Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra
机构：FAIR, Meta AI"$这篇论文介绍了一种学习六种不同模态-图像、文本、音频、深度、热度和IMU数据的联合嵌入方法，称为IMAGE BIND。该方法可以通过使用图像的自然配对来扩展最近大规模视觉语言模型的零样本能力以适应新的模态。通过在嵌入空间中将这些模态进行对齐，IMAGE BIND可以实现新型的跨模态检索、模态组合与算术、跨模态检测和生成等应用。实验结果表明，与专门的监督模型相比，IMAGE BIND在紧急零样本识别任务上取得了最新的国际先进水平，并且在少量数据识别方面表现出良好的性能。$http://arxiv.org/pdf/2305.05665v1
TidyBot: Personalized Robot Assistance with Large Language Models$  For a robot to personalize physical assistance effectively, it must learnuser preferences that can be generally reapplied to future scenarios. In thiswork, we investigate personalization of household cleanup with robots that cantidy up rooms by picking up objects and putting them away. A key challenge isdetermining the proper place to put each object, as people\'s preferences canvary greatly depending on personal taste or cultural background. For instance,one person may prefer storing shirts in the drawer, while another may preferthem on the shelf. We aim to build systems that can learn such preferences fromjust a handful of examples via prior interactions with a particular person. Weshow that robots can combine language-based planning and perception with thefew-shot summarization capabilities of large language models (LLMs) to infergeneralized user preferences that are broadly applicable to futureinteractions. This approach enables fast adaptation and achieves 91.2% accuracyon unseen objects in our benchmark dataset. We also demonstrate our approach ona real-world mobile manipulator called TidyBot, which successfully puts away85.0% of objects in real-world test scenarios.$Keywords: service robotics, mobile manipulation, personalized assistance, household cleanup, large language models, user preferences, few-shot learning, language-based planning, perception.$"作者：Jimmy Wu（Princeton University）、Rika Antonova（Stanford University）、Adam Kan（The Nueva School）、Marion Lepert（Stanford University）、Andy Zeng（Google）、Shuran Song（Columbia University）、Jeannette Bohg（Stanford University）、Szymon Rusinkiewicz（Princeton University）、Thomas Funkhouser（Princeton University、Google）。

机构：1. Princeton University, Princeton, NJ, USA. 2. Stanford University, Stanford, CA, USA. 3. The Nueva School, San Mateo, CA, USA. 4. Google, Mountain View, CA, USA. 5. Columbia University, New York, NY, USA."$本文介绍了利用大型语言模型（LLM）进行个性化家居清洁服务的研究。针对如何确定每个物品的正确放置位置，探讨了通过机器人与用户的交互来学习用户偏好的方法。通过将自然语言处理和感知技术与LLM结合，机器人可以从少量的交互中推断出用户的普遍偏好，并在未来的交互中应用。实验证明这种方法实现了快速适应，并在测试数据集上达到91.2%的准确率。文中还介绍了真实世界移动机械臂TidyBot的应用，成功将85.0%的物品放回其正确位置。关键词：服务机器人，移动机械臂，大型语言模型。$http://arxiv.org/pdf/2305.05658v1
ShapeCoder: Discovering Abstractions for Visual Programs from  Unstructured Primitives$"  Programs are an increasingly popular representation for visual data, exposingcompact, interpretable structure that supports manipulation. Visual programsare usually written in domain-specific languages (DSLs). Finding ""good""programs, that only expose meaningful degrees of freedom, requires access to aDSL with a ""good"" library of functions, both of which are typically authored bydomain experts. We present ShapeCoder, the first system capable of taking adataset of shapes, represented with unstructured primitives, and jointlydiscovering (i) useful abstraction functions and (ii) programs that use theseabstractions to explain the input shapes. The discovered abstractions capturecommon patterns (both structural and parametric) across the dataset, so thatprograms rewritten with these abstractions are more compact, and expose fewerdegrees of freedom. ShapeCoder improves upon previous abstraction discoverymethods, finding better abstractions, for more complex inputs, under lessstringent input assumptions. This is principally made possible by twomethodological advancements: (a) a shape to program recognition network thatlearns to solve sub-problems and (b) the use of e-graphs, augmented with aconditional rewrite scheme, to determine when abstractions with complexparametric expressions can be applied, in a tractable manner. We evaluateShapeCoder on multiple datasets of 3D shapes, where primitive decompositionsare either parsed from manual annotations or produced by an unsupervised cuboidabstraction method. In all domains, ShapeCoder discovers a library ofabstractions that capture high-level relationships, remove extraneous degreesof freedom, and achieve better dataset compression compared with alternativeapproaches. Finally, we investigate how programs rewritten to use discoveredabstractions prove useful for downstream tasks."$Keywords: Shape modeling, procedural modeling, visual programs, shape analysis, shape abstraction, library learning, e-graph. The article describes ShapeCoder, which discovers useful abstraction functions and programs that use these abstractions to explain unstructured primitive representations of shapes. The discovered abstractions capture common patterns so that programs rewritten with these abstractions are more compact and better constrain the visual data they represent. This is achieved through methodological advancements such as a shape-to-program recognition network and the use of e-graphs. The system is evaluated on multiple datasets of 3D shapes and proves useful for downstream tasks.$作者：R. Kenny Jones (Brown University, USA)，Paul Guerrero (Adobe Research, United Kingdom)，Niloy J. Mitra (University College London and Adobe Research, United Kingdom)，Daniel Ritchie (Brown University, USA)。$这篇论文介绍了一种名为ShapeCoder的系统，可以从用非结构化基元表示的形状数据集中发现有用的抽象函数和程序，以更紧凑地解释输入形状。ShapeCoder的抽象捕捉数据集中的常见模式，利用这些抽象重写的程序更紧凑，抑制了虚假的自由度。ShapeCoder通过两个方法论进步实现了更好的抽象发现方法，找到了更好的抽象函数，适用于更复杂的输入，不受严格的输入假设的限制。它首先通过一组手动注释解析或聚类抽象算法生成基元分解的3D形状数据集进行了测试，然后发现了一组抽象函数库，从而更好地压缩数据集并提高了数据集的压缩性能。最后，通过下游任务的研究，证明了利用所发现的抽象的程序的实用性。$http://arxiv.org/pdf/2305.05661v1
