title$summary$tag$affiliation$summary_zh$url
DocDiff: Document Enhancement via Residual Diffusion Models$  Removing degradation from document images not only improves their visualquality and readability, but also enhances the performance of numerousautomated document analysis and recognition tasks. However, existingregression-based methods optimized for pixel-level distortion reduction tend tosuffer from significant loss of high-frequency information, leading todistorted and blurred text edges. To compensate for this major deficiency, wepropose DocDiff, the first diffusion-based framework specifically designed fordiverse challenging document enhancement problems, including documentdeblurring, denoising, and removal of watermarks and seals. DocDiff consists oftwo modules: the Coarse Predictor (CP), which is responsible for recovering theprimary low-frequency content, and the High-Frequency Residual Refinement (HRR)module, which adopts the diffusion models to predict the residual(high-frequency information, including text edges), between the ground-truthand the CP-predicted image. DocDiff is a compact and computationally efficientmodel that benefits from a well-designed network architecture, an optimizedtraining loss objective, and a deterministic sampling process with short timesteps. Extensive experiments demonstrate that DocDiff achieves state-of-the-art(SOTA) performance on multiple benchmark datasets, and can significantlyenhance the readability and recognizability of degraded document images.Furthermore, our proposed HRR module in pre-trained DocDiff is plug-and-playand ready-to-use, with only 4.17M parameters. It greatly sharpens the textedges generated by SOTA deblurring methods without additional joint training.Available codes: https://github.com/Royalvice/DocDiff$"Keywords: Document Enhancement; Conditional Diffusion Models; Frequency Separation; Document Analysis. 

Summary: The article proposes a document enhancement framework called DocDiff that uses two modules, Coarse Predictor (CP) and High-Frequency Residual Refinement (HRR), to remove degradation from document images. DocDiff is a diffusion-based framework that is compact, computationally efficient, and benefits from well-designed network architecture, optimized training loss objective, and deterministic sampling process. It achieves state-of-the-art performance on multiple benchmark datasets for document deblurring, denoising, and removal of watermarks and seals. Additionally, the proposed HRR module is plug-and-play, which greatly sharpens the text edges generated by SOTA deblurring methods without additional joint training."$作者机构：北京邮电大学，中国，北京。$本文提出了 DocDiff 框架，采用残差扩散模型实现文档的去模糊、去噪和去水印等问题的增强，有效提高了文档图像的可读性和识别率，取得了最新成果。$http://arxiv.org/pdf/2305.03892v1
Prompt What You Need: Enhancing Segmentation in Rainy Scenes with  Anchor-based Prompting$  Semantic segmentation in rainy scenes is a challenging task due to thecomplex environment, class distribution imbalance, and limited annotated data.To address these challenges, we propose a novel framework that utilizessemi-supervised learning and pre-trained segmentation foundation model toachieve superior performance. Specifically, our framework leverages thesemi-supervised model as the basis for generating raw semantic segmentationresults, while also serving as a guiding force to prompt pre-trained foundationmodel to compensate for knowledge gaps with entropy-based anchors. In addition,to minimize the impact of irrelevant segmentation masks generated by thepre-trained foundation model, we also propose a mask filtering and fusionmechanism that optimizes raw semantic segmentation results based on theprinciple of minimum risk. The proposed framework achieves superiorsegmentation performance on the Rainy WCity dataset and is awarded the firstprize in the sub-track of STRAIN in ICME 2023 Grand Challenges.$Keywords: semantic segmentation, rainy scenes, anchor-based prompting, semi-supervised learning, pre-trained segmentation foundation model, entropy-based anchors, mask filtering and fusion mechanism, minimum risk principle, STRAIN, ICME 2023 Grand Challenges.$本文的作者机构是北京交通大学软件工程学院，作者包括Xiaoyu Guo、Xiang Wei、Qi Su、Huiqin Zhao和Shunli Zhang。$本文提出了一种新的框架，利用半监督学习和预训练的分割基础模型来增强雨天场景中的分割效果，解决了环境复杂性、类别分布不平衡和缺乏标注数据等问题。该框架充分利用半监督模型作为生成初始结果的基础，并以熵为基准生成适用于预训练模型的锚点，同时提出了一种最小化风险原则优化的分割结果的方法。该框架在Rainy WCity数据集上取得了优异的分割效果。$http://arxiv.org/pdf/2305.03902v1
Listen to Look into the Future: Audio-Visual Egocentric Gaze  Anticipation$  Egocentric gaze anticipation serves as a key building block for the emergingcapability of Augmented Reality. Notably, gaze behavior is driven by bothvisual cues and audio signals during daily activities. Motivated by thisobservation, we introduce the first model that leverages both the video andaudio modalities for egocentric gaze anticipation. Specifically, we propose aContrastive Spatial-Temporal Separable (CSTS) fusion approach that adopts twomodules to separately capture audio-visual correlations in spatial and temporaldimensions, and applies a contrastive loss on the re-weighted audio-visualfeatures from fusion modules for representation learning. We conduct extensiveablation studies and thorough analysis using two egocentric video datasets:Ego4D and Aria, to validate our model design. We also demonstrate improvementsover prior state-of-the-art methods. Moreover, we provide visualizations toshow the gaze anticipation results and provide additional insights intoaudio-visual representation learning.$Key domains: Audio-visual egocentric gaze anticipation, augmented reality, wearable computing, cognitive processes, decision making, attention anticipation, egocentric scene content, dynamic gaze behaviors.$作者机构：Bolin Lai、Fiona Ryan、Wenqi Jia，Georgia Institute of Technology；Miao Liu，Meta AI；James M. Rehg，Georgia Institute of Technology。$本文提出了一种同时利用视觉和音频模态的egocentric gaze anticipation模型，提供了对audio-visual representation learning的更深入的理解。该模型在两个egocentric video datasets: Ego4D 和 Aria上都有提升。$http://arxiv.org/pdf/2305.03907v1
DBAT: Dynamic Backward Attention Transformer for Material Segmentation  with Cross-Resolution Patches$  The objective of dense material segmentation is to identify the materialcategories for every image pixel. Recent studies adopt image patches to extractmaterial features. Although the trained networks can improve the segmentationperformance, their methods choose a fixed patch resolution which fails to takeinto account the variation in pixel area covered by each material. In thispaper, we propose the Dynamic Backward Attention Transformer (DBAT) toaggregate cross-resolution features. The DBAT takes cropped image patches asinput and gradually increases the patch resolution by merging adjacent patchesat each transformer stage, instead of fixing the patch resolution duringtraining. We explicitly gather the intermediate features extracted fromcross-resolution patches and merge them dynamically with predicted attentionmasks. Experiments show that our DBAT achieves an accuracy of 86.85%, which isthe best performance among state-of-the-art real-time models. Like othersuccessful deep learning solutions with complex architectures, the DBAT alsosuffers from lack of interpretability. To address this problem, this paperexamines the properties that the DBAT makes use of. By analysing thecross-resolution features and the attention weights, this paper interprets howthe DBAT learns from image patches. We further align features to semanticlabels, performing network dissection, to infer that the proposed model canextract material-related features better than other methods. We show that theDBAT model is more robust to network initialisation, and yields fewer variablepredictions compared to other models. The project code is available athttps://github.com/heng-yuwen/Dynamic-Backward-Attention-Transformer.$Keywords: Dense material segmentation, Cross-resolution patches, Dynamic Backward Attention Transformer (DBAT), Image processing, Neural networks, Network interpretability.$作者机构：英国南安普顿大学电子与计算机科学学院（School of Electronics and Computer Science）$本文提出了一种名为Dynamic Backward Attention Transformer (DBAT)的新型神经网络架构，在密集材料分割任务中，通过聚合多分辨率图像片段中提取的特征，来提高分割性能，并在实验中获得了最佳的性能表现。除此之外，本文还深入研究了该网络的特性与可解释性。$http://arxiv.org/pdf/2305.03919v1
Annotation-efficient learning for OCT segmentation$  Deep learning has been successfully applied to OCT segmentation. However, fordata from different manufacturers and imaging protocols, and for differentregions of interest (ROIs), it requires laborious and time-consuming dataannotation and training, which is undesirable in many scenarios, such assurgical navigation and multi-center clinical trials. Here we propose anannotation-efficient learning method for OCT segmentation that couldsignificantly reduce annotation costs. Leveraging self-supervised generativelearning, we train a Transformer-based model to learn the OCT imagery. Then weconnect the trained Transformer-based encoder to a CNN-based decoder, to learnthe dense pixel-wise prediction in OCT segmentation. These training phases useopen-access data and thus incur no annotation costs, and the pre-trained modelcan be adapted to different data and ROIs without re-training. Based on thegreedy approximation for the k-center problem, we also introduce an algorithmfor the selective annotation of the target data. We verified our method onpublicly-available and private OCT datasets. Compared to the widely-used U-Netmodel with 100% training data, our method only requires ~10% of the data forachieving the same segmentation accuracy, and it speeds the training up to ~3.5times. Furthermore, our proposed method outperforms other potential strategiesthat could improve annotation efficiency. We think this emphasis on learningefficiency may help improve the intelligence and application penetration ofOCT-based technologies. Our code and pre-trained model are publicly availableathttps://github.com/SJTU-Intelligent-Optics-Lab/Annotation-efficient-learning-for-OCT-segmentation.$Annotation-efficient learning, OCT segmentation, deep learning, self-supervised generative learning, Transformer-based model, CNN-based decoder, k-center problem, selective annotation, clinical OCT data.$作者机构：上海交通大学生物医学工程学院、上海交通大学医学院附属新华医院眼科、上海交通大学智能光学实验室。$这篇文章提出了一种高效的OCT图像分割学习方法，通过自监督生成学习和基于转换器的模型，可以显著降低数据标注的成本，同时提高分割精度和速度。$http://arxiv.org/pdf/2305.03936v1
Structural and Statistical Texture Knowledge Distillation for Semantic  Segmentation$  Existing knowledge distillation works for semantic segmentation mainly focuson transferring high-level contextual knowledge from teacher to student.However, low-level texture knowledge is also of vital importance forcharacterizing the local structural pattern and global statistical property,such as boundary, smoothness, regularity and color contrast, which may not bewell addressed by high-level deep features. In this paper, we are intended totake full advantage of both structural and statistical texture knowledge andpropose a novel Structural and Statistical Texture Knowledge Distillation(SSTKD) framework for semantic segmentation. Specifically, for structuraltexture knowledge, we introduce a Contourlet Decomposition Module (CDM) thatdecomposes low-level features with iterative Laplacian pyramid and directionalfilter bank to mine the structural texture knowledge. For statisticalknowledge, we propose a Denoised Texture Intensity Equalization Module (DTIEM)to adaptively extract and enhance statistical texture knowledge throughheuristics iterative quantization and denoised operation. Finally, eachknowledge learning is supervised by an individual loss function, forcing thestudent network to mimic the teacher better from a broader perspective.Experiments show that the proposed method achieves state-of-the-art performanceon Cityscapes, Pascal VOC 2012 and ADE20K datasets.$Structural, statistical texture, knowledge distillation, semantic segmentation, CNN backbone, contourlet decomposition, denoised texture intensity equalization, loss function.$作者机构：Alibaba Cloud Computing Ltd. 和上海交通大学计算机科学与工程系。作者分别为：Deyi Ji、Haoran Wang、Mingyuan Tao、Jianqiang Huang、Xian-Sheng Hua和Hongtao Lu。$本文提出了一种新颖的结构和统计纹理知识蒸馏框架(SSTKD)，旨在从低层次特征中提取结构和统计纹理知识，将该知识迁移给学生网络以提高其性能，实现在各类数据集上的良好表现。$http://arxiv.org/pdf/2305.03944v1
Feature Chirality in Deep Learning Models$  As deep learning applications extensively increase by leaps and bounds, theirinterpretability has become increasingly prominent. As a universal property,chirality exists widely in nature, and applying it to the explanatory researchof deep learning may be helpful to some extent. Inspired by a recent study thatused CNN (convolutional neural network), which applied visual chirality, todistinguish whether an image is flipped or not. In this paper, we study featurechirality innovatively, which shows how the statistics of deep learning models\'feature data are changed by training. We rethink the feature-level chiralityproperty, propose the feature chirality, and give the measure. Our analysis offeature chirality on AlexNet, VGG, and ResNet reveals similar but surprisingresults, including the prevalence of feature chirality in these models, theinitialization methods of the models do not affect feature chirality. Our workshows that feature chirality implies model evaluation, interpretability of themodel, and model parameters optimization.$Keywords: deep learning model, feature chirality, kernel similarity, interpretability, discriminant analysis.$作者机构：中国人民解放军陆军工程大学指挥控制工程学院，南京，中国。$本文介绍了一个新概念——“feature chirality”，通过深度学习模型的特征数据在训练过程中的变化，探讨了模型可解释性和模型参数优化等问题，并在AlexNet、VGG和ResNet等模型上进行了实验研究。$http://arxiv.org/pdf/2305.03966v1
Multi-object Video Generation from Single Frame Layouts$  In this paper, we study video synthesis with emphasis on simplifying thegeneration conditions. Most existing video synthesis models or datasets aredesigned to address complex motions of a single object, lacking the ability ofcomprehensively understanding the spatio-temporal relationships among multipleobjects. Besides, current methods are usually conditioned on intricateannotations (e.g. video segmentations) to generate new videos, beingfundamentally less practical. These motivate us to generate multi-object videosconditioning exclusively on object layouts from a single frame. To solve abovechallenges and inspired by recent research on image generation from layouts, wehave proposed a novel video generative framework capable of synthesizing globalscenes with local objects, via implicit neural representations and layoutmotion self-inference. Our framework is a non-trivial adaptation from imagegeneration methods, and is new to this field. In addition, our model has beenevaluated on two widely-used video recognition benchmarks, demonstratingeffectiveness compared to the baseline model.$Key Words: Multi-object video generation, single frame layouts, generative modeling, implicit neural representations, spatial transform neural layers, video recognition benchmarks.$作者机构：中山大学计算机科学与工程学院、广东省信息安全技术重点实验室。$本文主要介绍了一种基于单帧布局实现多目标视频合成的方法，采用隐式神经表示和布局运动自我推断来实现全局场景和局部物体的显式生成，目标是简化生成条件和提高模型的实用性。作者还在两个经典的视频识别基准上进行了评估，并与基线模型进行了比较，取得了较好的效果。$http://arxiv.org/pdf/2305.03983v1
LEO: Generative Latent Image Animator for Human Video Synthesis$  Spatio-temporal coherency is a major challenge in synthesizing high qualityvideos, particularly in synthesizing human videos that contain rich global andlocal deformations. To resolve this challenge, previous approaches haveresorted to different features in the generation process aimed at representingappearance and motion. However, in the absence of strict mechanisms toguarantee such disentanglement, a separation of motion from appearance hasremained challenging, resulting in spatial distortions and temporal jitteringthat break the spatio-temporal coherency. Motivated by this, we here proposeLEO, a novel framework for human video synthesis, placing emphasis onspatio-temporal coherency. Our key idea is to represent motion as a sequence offlow maps in the generation process, which inherently isolate motion fromappearance. We implement this idea via a flow-based image animator and a LatentMotion Diffusion Model (LMDM). The former bridges a space of motion codes withthe space of flow maps, and synthesizes video frames in a warp-and-inpaintmanner. LMDM learns to capture motion prior in the training data bysynthesizing sequences of motion codes. Extensive quantitative and qualitativeanalysis suggests that LEO significantly improves coherent synthesis of humanvideos over previous methods on the datasets TaichiHD, FaceForensics andCelebV-HQ. In addition, the effective disentanglement of appearance and motionin LEO allows for two additional tasks, namely infinite-length human videosynthesis, as well as content-preserving video editing.$Keywords: video synthesis, generative models, latent image animator, spatio-temporal coherency, human videos, appearance, motion.$作者机构：1 上海人工智能实验室，2 莫纳什大学，3 Inria，Université Côte d'Azur。$这篇文章介绍了一种基于生成模型的动态图像合成方法，可用于无条件视频生成、基于单个图像的条件生成和从起始图像进行视频编辑等任务。$http://arxiv.org/pdf/2305.03989v1
Weighted Point Cloud Normal Estimation$  Existing normal estimation methods for point clouds are often less robust tosevere noise and complex geometric structures. Also, they usually ignore thecontributions of different neighbouring points during normal estimation, whichleads to less accurate results. In this paper, we introduce a weighted normalestimation method for 3D point cloud data. We innovate in two key points: 1) wedevelop a novel weighted normal regression technique that predicts point-wiseweights from local point patches and use them for robust, feature-preservingnormal regression; 2) we propose to conduct contrastive learning between pointpatches and the corresponding ground-truth normals of the patches\' centralpoints as a pre-training process to facilitate normal regression. Comprehensiveexperiments demonstrate that our method can robustly handle noisy and complexpoint clouds, achieving state-of-the-art performance on both synthetic andreal-world datasets.$Keywords: Point Cloud, Normal Estimation, Weighted Regression, Feature-Preserving, Noise-Resisting.$"作者机构：澳大利亚迪肯大学（Deakin University），OPT Machine Vision Tech Co.，Ltd日本。
作者：Weijia Wang，Xuequan Lu，Di Shao，Xiao Liu，Richard Dazeley，Antonio Robles-Kelly，Wei Pan。"$本文提出了一种基于加权法的点云法线估计方法，创新性地利用邻居点的贡献权重来进行法线回归，从而提高法线预测的准确性和鲁棒性。同时，作者通过对比实验证明其在处理噪声和复杂点云方面具有先进性和实用性。$http://arxiv.org/pdf/2305.04007v1
Exploring One-shot Semi-supervised Federated Learning with A Pre-trained  Diffusion Model$  Federated learning is a privacy-preserving collaborative learning approach.Recently, some studies have proposed the semi-supervised federated learningsetting to handle the commonly seen real-world scenarios with labeled data onthe server and unlabeled data on the clients. However, existing methods stillface challenges such as high communication costs, training pressure on theclient devices, and distribution differences among the server and the clients.In this paper, we introduce the powerful pre-trained diffusion models intofederated learning and propose FedDISC, a Federated Diffusion InspiredSemi-supervised Co-training method, to address these challenges. Specifically,we first extract prototypes from the labeled data on the server and send themto the clients. The clients then use these prototypes to predict pseudo-labelsof the local data, and compute the cluster centroids and domain-specificfeatures to represent their personalized distributions. After adding noise, theclients send these features and their corresponding pseudo-labels back to theserver, which uses a pre-trained diffusion model to conditionally generatepseudo-samples complying with the client distributions and train an aggregatedmodel on them. Our method does not require local training and only involvesforward inference on the clients. Our extensive experiments on DomainNet,Openimage, and NICO++ demonstrate that the proposed FedDISC method effectivelyaddresses the one-shot semi-supervised problem on Non-IID clients andoutperforms the compared SOTA methods. We also demonstrate throughvisualization that it is of neglectable possibility for FedDISC to leakprivacy-sensitive information of the clients.$"Keywords: Federated Learning, Semi-supervised Learning, Diffusion Model, Communication Efficiency, Distribution Differences, Privacy Protection.

Summary: The article proposes a novel approach called FedDISC, which incorporates pre-trained diffusion models to improve the one-shot semi-supervised federated learning setting. The approach aims to address challenges such as high communication costs, training pressure on client devices, and distribution differences between server and clients. FedDISC extracts prototypes from labeled server data to generate pseudo-labels for unlabeled client data. The generated labels, along with domain-specific features extracted from the client data, are sent back to the server. The server uses a pre-trained diffusion model to produce pseudo-samples, which are utilized to train an aggregated model. The approach does not require local training, involves only forward inference on clients, and effectively solves the semi-supervised problem, especially on Non-IID clients, without compromising privacy."$本文的作者机构为复旦大学计算机科学系，作者为Mingzhao Yang、Shangchao Su、Bin Li和Xiangyang Xue。$本文提出了一种基于预训练扩散模型的联邦学习方法，名为FedDISC，用于处理半监督联邦学习中存在的通信成本高、客户端训练压力大和服务器与客户端之间存在的分布差异等挑战。该方法不需要客户端进行本地训练，只涉及前向推断，通过从服务器标记数据中提取原型，向客户端发送这些原型，并使用这些原型来预测本地数据的伪标签，计算聚类中心和特定于领域的特征，以表示其个性化分布，并在添加噪声后将这些特征和相应的伪标签发送回服务器，使用预训练扩散模型有条件地生成遵守客户端分布的伪样本并对其进行聚合模型的培训。 本文在DomainNet，Openimage和NICO ++上进行了广泛的实验，证明了所提出的FedDISC方法有效地解决了非独立和非同分布客户端上的单次半监督问题，并优于SOTA方法。同时，作者还通过可视化验证了FedDISC不太可能泄漏客户敏感信息。$http://arxiv.org/pdf/2305.04063v1
PointCMP: Contrastive Mask Prediction for Self-supervised Learning on  Point Cloud Videos$  Self-supervised learning can extract representations of good quality fromsolely unlabeled data, which is appealing for point cloud videos due to theirhigh labelling cost. In this paper, we propose a contrastive mask prediction(PointCMP) framework for self-supervised learning on point cloud videos.Specifically, our PointCMP employs a two-branch structure to achievesimultaneous learning of both local and global spatio-temporal information. Ontop of this two-branch structure, a mutual similarity based augmentation moduleis developed to synthesize hard samples at the feature level. By maskingdominant tokens and erasing principal channels, we generate hard samples tofacilitate learning representations with better discrimination andgeneralization performance. Extensive experiments show that our PointCMPachieves the state-of-the-art performance on benchmark datasets and outperformsexisting full-supervised counterparts. Transfer learning results demonstratethe superiority of the learned representations across different datasets andtasks.$PointCMP, self-supervised learning, point cloud videos, contrastive mask prediction, spatio-temporal information, hard samples, feature level augmentation, global dynamics, sample generation, positive and negative samples.$"作者机构：Zhiqiang Shen1;2*, Xiaoxiao Sheng1*, Longguang Wang3†, Yulan Guo4, Qiong Liu2, Xi Zhou1;2
1上海交通大学，2云从科技，3空军工程大学，4中山大学"$本文提出了一个基于对比度掩码预测的自监督学习框架，用于点云视频数据，同时考虑了局部和全局时空信息的学习，并通过互为相似性的增强模块生成难样本，实现了更好的判别和泛化能力。通过大量实验表明，该方法在基准数据集上实现了最先进的性能，超过了现有的全监督对应物。转移学习结果表明，所学习的表示在不同数据集和任务之间具有优越性。$http://arxiv.org/pdf/2305.04075v1
Transform-Equivariant Consistency Learning for Temporal Sentence  Grounding$  This paper addresses the temporal sentence grounding (TSG). Although existingmethods have made decent achievements in this task, they not only severely relyon abundant video-query paired data for training, but also easily fail into thedataset distribution bias. To alleviate these limitations, we introduce a novelEquivariant Consistency Regulation Learning (ECRL) framework to learn morediscriminative query-related frame-wise representations for each video, in aself-supervised manner. Our motivation comes from that the temporal boundary ofthe query-guided activity should be consistently predicted under variousvideo-level transformations. Concretely, we first design a series ofspatio-temporal augmentations on both foreground and background video segmentsto generate a set of synthetic video samples. In particular, we devise aself-refine module to enhance the completeness and smoothness of the augmentedvideo. Then, we present a novel self-supervised consistency loss (SSCL) appliedon the original and augmented videos to capture their invariant query-relatedsemantic by minimizing the KL-divergence between the sequence similarity of twovideos and a prior Gaussian distribution of timestamp distance. At last, ashared grounding head is introduced to predict the transform-equivariantquery-guided segment boundaries for both the original and augmented videos.Extensive experiments on three challenging datasets (ActivityNet, TACoS, andCharades-STA) demonstrate both effectiveness and efficiency of our proposedECRL framework.$Temporal Sentence Grounding, Equivariant Consistency Learning, Spatio-temporal augmentations, Self-supervised consistency loss, Query-related frame-wise representations, Shared grounding head.$本文作者机构：中国华中科技大学、中国浙江工商大学、中国大连理工大学、美国 Protagolabs Inc.、中国清华大学、美国微软研究院。$本文提出一种新颖的等变一致性学习框架解决时间句子定位任务，通过自监督方式学习更具有区分性的查询相关帧级表示，并引入一系列时空数据增强和自我完善模块来提高模型鲁棒性和泛化能力。同时在三个具有挑战性的数据集上进行了大量实验验证，证明了该框架的有效性和效率性。$http://arxiv.org/pdf/2305.04123v1
Context-Aware Chart Element Detection$  As a prerequisite of chart data extraction, the accurate detection of chartbasic elements is essential and mandatory. In contrast to object detection inthe general image domain, chart element detection relies heavily on contextinformation as charts are highly structured data visualization formats. Toaddress this, we propose a novel method CACHED, which stands for Context-AwareChart Element Detection, by integrating a local-global context fusion moduleconsisting of visual context enhancement and positional context encoding withthe Cascade R-CNN framework. To improve the generalization of our method forbroader applicability, we refine the existing chart element categorization andstandardized 18 classes for chart basic elements, excluding plot elements. OurCACHED method, with the updated category of chart elements, achievesstate-of-the-art performance in our experiments, underscoring the importance ofcontext in chart element detection. Extending our method to the bar plotdetection task, we obtain the best result on the PMC test dataset.$Context-Aware Chart Element Detection, Chart Detection, Chart Data Extraction, Chart Understanding, Document Analysis, Cascade R-CNN, Visual Context Enhancement, Positional Context Encoding, Bar Plot Detection.$作者机构：纽约大学计算机科学和工程系，Pengyu Yan、Saleem Ahmed和David Doermann。$本文提出了一种上下文感知的图表元素检测方法，称为CACHED，通过结合本地和全局上下文信息，来提高对图表元素的检测准确性，以及拓展其应用性和通用性。同时，在更新图表元素的分类和定义后，该方法在实验中取得了最优的性能表现。$http://arxiv.org/pdf/2305.04151v1
PhysBench: A Benchmark Framework for Remote Physiological Sensing with  New Dataset and Baseline$  In recent years, due to the widespread use of internet videos, physiologicalremote sensing has gained more and more attention in the fields of affectivecomputing and telemedicine. Recovering physiological signals from facial videosis a challenging task that involves a series of preprocessing, imagealgorithms, and post-processing to finally restore waveforms. We propose acomplete and efficient end-to-end training and testing framework that providesfair comparisons for different algorithms through unified preprocessing andpost-processing. In addition, we introduce a highly synchronized losslessformat dataset along with a lightweight algorithm. The dataset contains over 32hours (3.53M frames) of video from 58 subjects; by training on our collecteddataset both our proposed algorithm as well as existing ones can achieveimprovements.$Physiological remote sensing, affective computing, telemedicine, video preprocessing, image algorithms, post-processing, lossless format dataset, synchronized dataset, Blood Volume Pulse signals, remote Photoplethysmography, raw RGB data, YUV format videos.$作者机构：华中师范大学人工智能教育学院，中国湖北省武汉市，邮编430079。作者包括：Kegang Wang、Yantao Wei、Mingwen Tong、Jie Gao、Yi Tian、YuJian Ma、ZhongJin Zhao。$本文提出了一个用于远程生理感应的基准框架PhysBench，包括一个新数据集和基准线，以及具有公正性的图像处理算法。同时，还介绍了一个高度同步的无损格式数据集和一种轻量级算法。$http://arxiv.org/pdf/2305.04161v1
YOLOCS: Object Detection based on Dense Channel Compression for Feature  Spatial Solidification$  In this study, we examine the associations between channel features andconvolutional kernels during the processes of feature purification and gradientbackpropagation, with a focus on the forward and backward propagation withinthe network. Consequently, we propose a method called Dense Channel Compressionfor Feature Spatial Solidification. Drawing upon the central concept of thismethod, we introduce two innovative modules for backbone and head networks: theDense Channel Compression for Feature Spatial Solidification Structure (DCFS)and the Asymmetric Multi-Level Compression Decoupled Head (ADH). Whenintegrated into the YOLOv5 model, these two modules demonstrate exceptionalperformance, resulting in a modified model referred to as YOLOCS. Evaluated onthe MSCOCO dataset, the large, medium, and small YOLOCS models yield AP of50.1%, 47.6%, and 42.5%, respectively. Maintaining inference speeds remarkablysimilar to those of the YOLOv5 model, the large, medium, and small YOLOCSmodels surpass the YOLOv5 model\'s AP by 1.1%, 2.3%, and 5.2%, respectively.$domain keywords: YOLO, object detection, dense channel compression, feature spatial solidification, backbone network, head network, convolutional kernels, gradient backpropagation, image preprocessing, Non-Maximum Suppression (NMS), loss functions, feature pyramid networks (FPN), spatial pyramid pooling (SPP)$作者机构：重庆邮电大学、深圳大学、浪潮工业互联网股份有限公司、重庆臻源创新科技有限公司。$本文介绍了一种基于密集通道压缩的物体检测方法，重点探讨了特征净化和梯度反向传播过程中通道特征与卷积核之间的关联，并提出了一种名为DCFS的结构，以及一个名为ADH的多层压缩解耦头部网络模块，将其整合到YOLOv5模型中，得到了性能卓越的改进模型YOLOCS。$http://arxiv.org/pdf/2305.04170v1
Video-Specific Query-Key Attention Modeling for Weakly-Supervised  Temporal Action Localization$  Weakly-supervised temporal action localization aims to identify and localizethe action instances in the untrimmed videos with only video-level actionlabels. When humans watch videos, we can adapt our abstract-level knowledgeabout actions in different video scenarios and detect whether some actions areoccurring. In this paper, we mimic how humans do and bring a new perspectivefor locating and identifying multiple actions in a video. We propose a networknamed VQK-Net with a video-specific query-key attention modeling that learns aunique query for each action category of each input video. The learned queriesnot only contain the actions\' knowledge features at the abstract level but alsohave the ability to fit this knowledge into the target video scenario, and theywill be used to detect the presence of the corresponding action along thetemporal dimension. To better learn these action category queries, we exploitnot only the features of the current input video but also the correlationbetween different videos through a novel video-specific action category querylearner worked with a query similarity loss. Finally, we conduct extensiveexperiments on three commonly used datasets (THUMOS14, ActivityNet1.2, andActivityNet1.3) and achieve state-of-the-art performance.$Temporal action localization, weakly supervised, query learner, query-key attention modeling.$作者机构：1. Northwestern University, Evanston, IL, USA.$这篇文章提出了一个新模型VQK-Net，通过视频特定的查询键关注建模，学习每个视频的每个动作类别的独特查询，以学习动作的抽象级别知识并适用于目标视频场景，实现弱监督时间动作定位。$http://arxiv.org/pdf/2305.04186v1
Robust Image Ordinal Regression with Controllable Image Generation$  Image ordinal regression has been mainly studied along the line of exploitingthe order of categories. However, the issues of class imbalance and categoryoverlap that are very common in ordinal regression were largely overlooked. Asa result, the performance on minority categories is often unsatisfactory. Inthis paper, we propose a novel framework called CIG based on controllable imagegeneration to directly tackle these two issues. Our main idea is to generateextra training samples with specific labels near category boundaries, and thesample generation is biased toward the less-represented categories. To achievecontrollable image generation, we seek to separate structural and categoricalinformation of images based on structural similarity, categorical similarity,and reconstruction constraints. We evaluate the effectiveness of our new CIGapproach in three different image ordinal regression scenarios. The resultsdemonstrate that CIG can be flexibly integrated with off-the-shelf imageencoders or ordinal regression models to achieve improvement, and further, theimprovement is more significant for minority categories.$Keywords: Robust Image Ordinal Regression, Category Imbalance, Category Overlap, Controllable Image Generation, Structural Similarity, Categorical Similarity, Reconstruction Constraints.$本文的作者机构为：浙江大学软件技术学院、浙江大学公共卫生学院、阿里巴巴集团、浙江大学计算机科学与技术学院、山东大学计算机科学与技术学院、圣母大学计算机科学和工程系、浙江大学医学院第二附属医院。$本文提出了一种基于可控图像生成的鲁棒性图像序数回归框架(CIG)，用于解决序数回归中的类别不平衡和重叠问题。其主要思想是生成具有特定标签的额外训练样本，使样本生成偏向于少数类别。为实现可控的图像生成，该框架通过结构相似性、类别相似性和重构约束来分离图像的结构和类别信息。实验证明，CIG能够与现有的图像编码器或序数回归模型灵活集成，进而实现性能提升，特别是对于少数类别。$http://arxiv.org/pdf/2305.04213v1
Visual Causal Scene Refinement for Video Question Answering$  Existing methods for video question answering (VideoQA) often suffer fromspurious correlations between different modalities, leading to a failure inidentifying the dominant visual evidence and the intended question. Moreover,these methods function as black boxes, making it difficult to interpret thevisual scene during the QA process. In this paper, to discover critical videosegments and frames that serve as the visual causal scene for generatingreliable answers, we present a causal analysis of VideoQA and propose aframework for cross-modal causal relational reasoning, named Visual CausalScene Refinement (VCSR). Particularly, a set of causal front-door interventionoperations is introduced to explicitly find the visual causal scenes at bothsegment and frame levels. Our VCSR involves two essential modules: i) theQuestion-Guided Refiner (QGR) module, which refines consecutive video framesguided by the question semantics to obtain more representative segment featuresfor causal front-door intervention; ii) the Causal Scene Separator (CSS)module, which discovers a collection of visual causal and non-causal scenesbased on the visual-linguistic causal relevance and estimates the causal effectof the scene-separating intervention in a contrastive learning manner.Extensive experiments on the NExT-QA, Causal-VidQA, and MSRVTT-QA datasetsdemonstrate the superiority of our VCSR in discovering visual causal scene andachieving robust video question answering.$Video question answering, causal analysis, cross-modal causal relational reasoning, visual causal scene refinement, causal front-door intervention, segment and frame levels, Question-Guided Refiner module, Causal Scene Separator module.$作者机构：中山大学。Yushen Wei、Yang Liu、Hong Yan、Guanbin Li、Liang Lin是该论文的作者。$这篇文章提出了一种视觉因果场景细化的方法，用于视频问答任务，以发现关键视频段和帧，并生成可靠的答案。该方法包括两个关键模块：问题导向的细化器（QGR）模块和因果场景分隔器（CSS）模块。实验结果表明，该方法在发现视觉因果场景和实现稳健的视频问答方面具有卓越性能。$http://arxiv.org/pdf/2305.04224v1
CatFLW: Cat Facial Landmarks in the Wild Dataset$  Animal affective computing is a quickly growing field of research, where onlyrecently first efforts to go beyond animal tracking into recognizing theirinternal states, such as pain and emotions, have emerged. In most mammals,facial expressions are an important channel for communicating information aboutthese states. However, unlike the human domain, there is an acute lack ofdatasets that make automation of facial analysis of animals feasible.  This paper aims to fill this gap by presenting a dataset called Cat FacialLandmarks in the Wild (CatFLW) which contains 2016 images of cat faces indifferent environments and conditions, annotated with 48 facial landmarksspecifically chosen for their relationship with underlying musculature, andrelevance to cat-specific facial Action Units (CatFACS). To the best of ourknowledge, this dataset has the largest amount of cat facial landmarksavailable.  In addition, we describe a semi-supervised (human-in-the-loop) method ofannotating images with landmarks, used for creating this dataset, whichsignificantly reduces the annotation time and could be used for creatingsimilar datasets for other animals.  The dataset is available on request.$Animal affective computing, facial expression analysis, cat-specific facial Action Units (CatFACS), pain assessment in cats, animal welfare research, geometric landmarks, machine learning models.$文章作者机构：以色列海法大学：George Martvel、Nareed Farhat、Ilan Shimshoni、Anna Zamansky。$本文介绍了一种名为Cat Facial Landmarks in the Wild (CatFLW)的数据集，其中包含了2016张猫的面部图像，用48个面部地标进行注释，这些地标与猫特定表情单元 (CatFACS) 的相关性非常高，可用于推断猫的情感状态。此外，作者还提出了一种半监督方法，即使用人与机器配合的方式进行图像地标的标注，从而显著减少了标注时间。$http://arxiv.org/pdf/2305.04232v1
RFR-WWANet: Weighted Window Attention-Based Recovery Feature Resolution  Network for Unsupervised Image Registration$  The Swin transformer has recently attracted attention in medical imageanalysis due to its computational efficiency and long-range modelingcapability, which enables the establishment of more distant relationshipsbetween corresponding voxels. However, transformer-based models split imagesinto tokens, which results in transformers that can only model and outputcoarse-grained spatial information representations. To address this issue, wepropose Recovery Feature Resolution Network (RFRNet), which enables thetransformer to contribute with fine-grained spatial information and richsemantic correspondences. Furthermore, shifted window partitioning operationsare inflexible, indicating that they cannot perceive the semantic informationover uncertain distances and automatically bridge the global connectionsbetween windows. Therefore, we present a Weighted Window Attention (WWA) toautomatically build global interactions between windows after the regular andcyclic shifted window partitioning operations for Swin transformer blocks. Theproposed unsupervised deformable image registration model, named RFR-WWANet,senses the long-range correlations, thereby facilitating meaningful semanticrelevance of anatomical structures. Qualitative and quantitative results showthat RFR-WWANet achieves significant performance improvements over baselinemethods. Ablation experiments demonstrate the effectiveness of the RFRNet andWWA designs.$Keywords: Swin transformer, medical image analysis, unsupervised image registration, deformable image registration, weighted window attention, recovery feature resolution network.$Mingrui Ma, Tao Wang, Lei Song, and Guixia Liu are from the Computer Science and Technology of Jilin University and Weijie Wang is from the Department of Information Engineering and Computer Science at the University of Trento.$本文介绍了一种基于加权窗口注意力机制的恢复特征分辨率网络，可以用于无监督医学图像配准。通过此网络，可以实现细颗粒度的空间信息和丰富的语义对应关系，并自动构建窗口之间的全局关系。该网络在医学图像领域的试验结果表明，性能优于基准方法。$http://arxiv.org/pdf/2305.04236v1
Estimation of control area in badminton doubles with pose information  from top and back view drone videos$  The application of visual tracking to the performance analysis of sportsplayers in dynamic competitions is vital for effective coaching. In racketsports, most previous studies have focused on analyzing and assessing singlesplayers without occlusion in broadcast videos and discrete representations(e.g., stroke) that ignore meaningful spatial distributions. In this work, wepresent the first annotated drone dataset from top and back views in badmintondoubles and propose a framework to estimate the control area probability map,which can be used to evaluate teamwork performance. We present an efficientframework of deep neural networks that enables the calculation of fullprobability surfaces, which utilizes the embedding of a Gaussian mixture map ofplayers\' positions and graph convolution of their poses. In the experiment, weverify our approach by comparing various baselines and discovering thecorrelations between the score and control area. Furthermore, we propose thepractical application of assessing optimal positioning to provide instructionsduring a game. Our approach can visually and quantitatively evaluate players\'movements, providing valuable insights into doubles teamwork.$Keywords: Visual tracking, Deep Learning, Drone Dataset, Racket Sports, Control Area, Probability Map, Pose Information, Badminton, Doubles, Teamwork Performance.$作者机构：日本名古屋大学信息学研究生院（Ning Ding, Kazuya Takeda, Keisuke Fujii），中国芜湖职业技术学院体育教研室（Wenhui Jin），安徽师范大学体育科学学院（Yingjiu Bei）。$本文描述了利用顶部和背面视角的无人机视频中的姿势信息估计羽毛球双打控制区域的方法，以评估团队合作表现并提供游戏指令的实际应用程序。利用深度神经网络的有效框架计算完整的概率分布，可以视觉和定量地评估球员的运动，并提供有价值的洞察双打合作。$http://arxiv.org/pdf/2305.04247v1
Multi-Space Neural Radiance Fields$  Existing Neural Radiance Fields (NeRF) methods suffer from the existence ofreflective objects, often resulting in blurry or distorted rendering. Insteadof calculating a single radiance field, we propose a multi-space neuralradiance field (MS-NeRF) that represents the scene using a group of featurefields in parallel sub-spaces, which leads to a better understanding of theneural network toward the existence of reflective and refractive objects. Ourmulti-space scheme works as an enhancement to existing NeRF methods, with onlysmall computational overheads needed for training and inferring the extra-spaceoutputs. We demonstrate the superiority and compatibility of our approach usingthree representative NeRF-based models, i.e., NeRF, Mip-NeRF, and Mip-NeRF 360.Comparisons are performed on a novelly constructed dataset consisting of 25synthetic scenes and 7 real captured scenes with complex reflection andrefraction, all having 360-degree viewpoints. Extensive experiments show thatour approach significantly outperforms the existing single-space NeRF methodsfor rendering high-quality scenes concerned with complex light paths throughmirror-like objects. Our code and dataset will be publicly available athttps://zx-yin.github.io/msnerf.$Neural Radiance Fields, multi-space, reflective objects, refractive objects, rendering, MLPs, unbounded scenes, moving objects.$作者机构：南开大学计算机科学系，VCIP实验室。作者为：尹泽昕，邱家雄，程明明，任波。$本文提出了一种多空间神经辐射场（MS-NeRF）方法，旨在提高对反射和折射物体的理解，以实现更高质量的渲染。通过在平行子空间中表示场景，可以有效地处理复杂的镜面反射和折射，同时仅需要较小的计算代价。实验证明，该方法相比现有的单空间NeRF方法在渲染具有镜面反射的高质量场景方面显著优于现有方法。$http://arxiv.org/pdf/2305.04268v1
Neural Voting Field for Camera-Space 3D Hand Pose Estimation$  We present a unified framework for camera-space 3D hand pose estimation froma single RGB image based on 3D implicit representation. As opposed to recentworks, most of which first adopt holistic or pixel-level dense regression toobtain relative 3D hand pose and then follow with complex second-stageoperations for 3D global root or scale recovery, we propose a novel unified 3Ddense regression scheme to estimate camera-space 3D hand pose via dense 3Dpoint-wise voting in camera frustum. Through direct dense modeling in 3D domaininspired by Pixel-aligned Implicit Functions for 3D detailed reconstruction,our proposed Neural Voting Field (NVF) fully models 3D dense local evidence andhand global geometry, helping to alleviate common 2D-to-3D ambiguities.Specifically, for a 3D query point in camera frustum and its pixel-alignedimage feature, NVF, represented by a Multi-Layer Perceptron, regresses: (i) itssigned distance to the hand surface; (ii) a set of 4D offset vectors (1D votingweight and 3D directional vector to each hand joint). Following a vote-castingscheme, 4D offset vectors from near-surface points are selected to calculatethe 3D hand joint coordinates by a weighted average. Experiments demonstratethat NVF outperforms existing state-of-the-art algorithms on FreiHAND datasetfor camera-space 3D hand pose estimation. We also adapt NVF to the classic taskof root-relative 3D hand pose estimation, for which NVF also obtainsstate-of-the-art results on HO3D dataset.$Neural Voting Field, camera-space, 3D hand pose estimation, RGB input, 3D implicit function, dense regression, 3D point-wise voting, camera frustum, local evidence, global geometry.$作者机构：Lin Huang（University at Buffalo）、Chung-Ching Lin、Kevin Lin、Lin Liang、Lijuan Wang、Zicheng Liu（Microsoft）$本文提出了一种基于三维隐式表示的相机空间三维手部姿态估计的统一框架，通过密集三维点投票实现了相机空间三维手势姿势的估计，并在FreiHAND和HO3D数据集上实现了取得了优越的结果。$http://arxiv.org/pdf/2305.04328v1
Segmentation of the veterinary cytological images for fast neoplastic  tumors diagnosis$  This paper shows the machine learning system which performs instancesegmentation of cytological images in veterinary medicine. Eleven cell typeswere used directly and indirectly in the experiments, including damaged andunrecognized categories. The deep learning models employed in the systemachieve a high score of average precision and recall metrics, i.e. 0.94 and 0.8respectively, for the selected three types of tumors. This variety of labeltypes allowed us to draw a meaningful conclusion that there are relatively fewmistakes for tumor cell types. Additionally, the model learned tumor cellfeatures well enough to avoid misclassification mistakes of one tumor type intoanother. The experiments also revealed that the quality of the results improveswith the dataset size (excluding the damaged cells). It is worth noting thatall the experiments were done using a custom dedicated dataset provided by thecooperating vet doctors.$$$$http://arxiv.org/pdf/2305.04332v1
Living in a Material World: Learning Material Properties from  Full-Waveform Flash Lidar Data for Semantic Segmentation$  Advances in lidar technology have made the collection of 3D point clouds fastand easy. While most lidar sensors return per-point intensity (or reflectance)values along with range measurements, flash lidar sensors are able to provideinformation about the shape of the return pulse. The shape of the returnwaveform is affected by many factors, including the distance that the lightpulse travels and the angle of incidence with a surface. Importantly, the shapeof the return waveform also depends on the material properties of thereflecting surface. In this paper, we investigate whether the material type orclass can be determined from the full-waveform response. First, as a proof ofconcept, we demonstrate that the extra information about material class, ifknown accurately, can improve performance on scene understanding tasks such assemantic segmentation. Next, we learn two different full-waveform materialclassifiers: a random forest classifier and a temporal convolutional neuralnetwork (TCN) classifier. We find that, in some cases, material types can bedistinguished, and that the TCN generally performs better across a wider rangeof materials. However, factors such as angle of incidence, material colour, andmaterial similarity may hinder overall performance.$Keywords: lidar technology, 3D point clouds, full-waveform flash lidar data, material properties, semantic segmentation, material class, random forest classifier, temporal convolutional neural network, scene reconstruction, robotic applications.$作者机构：1. 多伦多大学空间与陆地自主机器人系统实验室；2. LeddarTech Inc.（加拿大魁北克）。$本文探讨了通过全波形闪光激光雷达数据学习物体材料属性，以提高语义分割任务的性能的可能性，并建立了两种不同的全波形材料分类器。$http://arxiv.org/pdf/2305.04334v1
Spatiotemporally Consistent HDR Indoor Lighting Estimation$  We propose a physically-motivated deep learning framework to solve a generalversion of the challenging indoor lighting estimation problem. Given a singleLDR image with a depth map, our method predicts spatially consistent lightingat any given image position. Particularly, when the input is an LDR videosequence, our framework not only progressively refines the lighting predictionas it sees more regions, but also preserves temporal consistency by keeping therefinement smooth. Our framework reconstructs a spherical Gaussian lightingvolume (SGLV) through a tailored 3D encoder-decoder, which enables spatiallyconsistent lighting prediction through volume ray tracing, a hybrid blendingnetwork for detailed environment maps, an in-network Monte-Carlo renderinglayer to enhance photorealism for virtual object insertion, and recurrentneural networks (RNN) to achieve temporally consistent lighting prediction witha video sequence as the input. For training, we significantly enhance theOpenRooms public dataset of photorealistic synthetic indoor scenes with around360K HDR environment maps of much higher resolution and 38K video sequences,rendered with GPU-based path tracing. Experiments show that our frameworkachieves lighting prediction with higher quality compared to state-of-the-artsingle-image or video-based methods, leading to photorealistic AR applicationssuch as object insertion.$Keywords: Spatiotemporally Consistent, HDR, Indoor Lighting Estimation, Deep Learning, Photorealism, Object Insertion, Spherical Gaussian Lighting Volume, Volume Ray Tracing, Hybrid Blending Network, Recurrent Neural Networks, Photorealistic AR Applications.$作者机构：ZHENGQIN LI, Meta Reality Labs Research, UC San Diego, USA; LI YU, Meta Reality Labs, USA; MIKHAIL OKUNEV, Meta Reality Labs Research, USA; MANMOHAN CHANDRAKER, UC San Diego, USA; ZHAO DONG, Meta Reality Labs Research, USA.$这篇文章提出了一个物理上驱动的深度学习框架，用于解决室内光照预测问题，可以在空间和时间上保持高一致性，且能够生成具有高品质的室内HDR光照图，帮助实现逼真增强现实应用。$http://arxiv.org/pdf/2305.04374v1
SegGPT Meets Co-Saliency Scene$  Co-salient object detection targets at detecting co-existed salient objectsamong a group of images. Recently, a generalist model for segmenting everythingin context, called SegGPT, is gaining public attention. In view of itsbreakthrough for segmentation, we can hardly wait to probe into itscontribution to the task of co-salient object detection. In this report, wefirst design a framework to enable SegGPT for the problem of co-salient objectdetection. Proceed to the next step, we evaluate the performance of SegGPT onthe problem of co-salient object detection on three available datasets. Weachieve a finding that co-saliency scenes challenges SegGPT due to contextdiscrepancy within a group of co-saliency images.$Keywords: Co-salient object detection, segmentation, SegGPT, context, salient object detector, IC algorithm, prompt segmentation, co-salient maps, evaluation, CoSOD3k, CoCA, CoSal2015.$作者机构: Yi Liu, Shoukun Xu, Dingwen Zhang 和 Jungong Han分别来自中国常州大学计算机科学与人工智能学院、阿里云大数据学院、中国合肥综合国家科学中心人工智能研究所和英国谢菲尔德大学计算机科学系。$这篇文章介绍了一种使用SegGPT模型来进行共同显著性对象检测的方法，并对其在三个公共数据集上进行了评估和分析。作者发现，虽然SegGPT在分割方面有很好的表现，但在共同显著性场景下，由于组内图像的上下文差异，其表现不如预期。$http://arxiv.org/pdf/2305.04396v1
TaLU: A Hybrid Activation Function Combining Tanh and Rectified Linear  Unit to Enhance Neural Networks$  The application of the deep learning model in classification plays animportant role in the accurate detection of the target objects. However, theaccuracy is affected by the activation function in the hidden and output layer.In this paper, an activation function called TaLU, which is a combination ofTanh and Rectified Linear Units (ReLU), is used to improve the prediction. ReLUactivation function is used by many deep learning researchers for itscomputational efficiency, ease of implementation, intuitive nature, etc.However, it suffers from a dying gradient problem. For instance, when the inputis negative, its output is always zero because its gradient is zero. A numberof researchers used different approaches to solve this issue. Some of the mostnotable are LeakyReLU, Softplus, Softsign, Elu, ThresholdedReLU, etc. Thisresearch developed TaLU, a modified activation function combining Tanh andReLU, which mitigates the dying gradient problem of ReLU. The deep learningmodel with the proposed activation function was tested on MNIST and CIFAR-10,and it outperforms ReLU and some other studied activation functions in terms ofaccuracy(from 0\\% upto 6\\% in most cases, when used with Batch Normalizationand a reasonable learning rate).$Springer Nature, 2021, LATEX template, deep learning, activation function, TaLU, Tanh, Rectified Linear Unit, neural networks, classification, accuracy, dying gradient problem, LeakyReLU, Softplus, Softsign, Elu, ThresholdedReLU, MNIST, CIFAR-10, Batch Normalization, learning rate.$作者机构：孟加拉国拉杰沙希工程大学计算机科学与工程系（Department of Computer Science & Engineering, Rajshahi University of Engineering and Technology, Kazla, Rajshahi, 6204, Bangladesh）$本文提出了一种名为TaLU的激活函数，通过结合Tanh和ReLU解决了ReLU存在的梯度消失问题，从而在深度学习模型中的分类任务中提高了准确性。$http://arxiv.org/pdf/2305.04402v1
Improving 2D face recognition via fine-level facial depth generation and  RGB-D complementary feature learning$  Face recognition in complex scenes suffers severe challenges coming fromperturbations such as pose deformation, ill illumination, partial occlusion.Some methods utilize depth estimation to obtain depth corresponding to RGB toimprove the accuracy of face recognition. However, the depth generated by themsuffer from image blur, which introduces noise in subsequent RGB-D facerecognition tasks. In addition, existing RGB-D face recognition methods areunable to fully extract complementary features. In this paper, we propose afine-grained facial depth generation network and an improved multimodalcomplementary feature learning network. Extensive experiments on the Lock3DFacedataset and the IIIT-D dataset show that the proposed FFDGNet and I MCFLNet canimprove the accuracy of RGB-D face recognition while achieving thestate-of-the-art performance.$$$$http://arxiv.org/pdf/2305.04426v1
Adversarial Examples Detection with Enhanced Image Difference Features  based on Local Histogram Equalization$  Deep Neural Networks (DNNs) have recently made significant progress in manyfields. However, studies have shown that DNNs are vulnerable to adversarialexamples, where imperceptible perturbations can greatly mislead DNNs even ifthe full underlying model parameters are not accessible. Various defensemethods have been proposed, such as feature compression and gradient masking.However, numerous studies have proven that previous methods create detection ordefense against certain attacks, which renders the method ineffective in theface of the latest unknown attack methods. The invisibility of adversarialperturbations is one of the evaluation indicators for adversarial exampleattacks, which also means that the difference in the local correlation ofhigh-frequency information in adversarial examples and normal examples can beused as an effective feature to distinguish the two. Therefore, we propose anadversarial example detection framework based on a high-frequency informationenhancement strategy, which can effectively extract and amplify the featuredifferences between adversarial examples and normal examples. Experimentalresults show that the feature augmentation module can be combined with existingdetection models in a modular way under this framework. Improve the detector\'sperformance and reduce the deployment cost without modifying the existingdetection model.$Key words: Adversarial Examples Detection, Image Enhancement, Local Histogram Equalization, Deep Learning.$作者机构：东华大学通信与电子工程学院、清华大学计算机科学与技术系、OPPO智能互动研究院、安徽大学。$本文提出了一种基于局部直方图均衡的高频信息增强策略来检测对抗样本的方法。该方法能够有效地提取和放大对抗样本和正常样本之间的特征差异，提高检测器的性能，并降低部署成本。$http://arxiv.org/pdf/2305.04436v1
Vision Transformer Off-the-Shelf: A Surprising Baseline for Few-Shot  Class-Agnostic Counting$  Class-agnostic counting (CAC) aims to count objects of interest from a queryimage given few exemplars. This task is typically addressed by extracting thefeatures of query image and exemplars respectively with (un)shared featureextractors and by matching their feature similarity, leading to anextract-\\textit{then}-match paradigm. In this work, we show that CAC can besimplified in an extract-\\textit{and}-match manner, particularly using apretrained and plain vision transformer (ViT) where feature extraction andsimilarity matching are executed simultaneously within the self-attention. Wereveal the rationale of such simplification from a decoupled view of theself-attention and point out that the simplification is only made possible ifthe query and exemplar tokens are concatenated as input. The resulting model,termed CACViT, simplifies the CAC pipeline and unifies the feature spacesbetween the query image and exemplars. In addition, we find CACViT naturallyencodes background information within self-attention, which helps reducebackground disturbance. Further, to compensate the loss of the scale and theorder-of-magnitude information due to resizing and normalization in ViT, wepresent two effective strategies for scale and magnitude embedding. Extensiveexperiments on the FSC147 and the CARPK datasets show that CACViT significantlyoutperforms state-of-the-art CAC approaches in both effectiveness (23.60% errorreduction) and generalization, which suggests CACViT provides a concise andstrong baseline for CAC. Code will be available.$Specific Field Keywords: Computer Vision, Object Counting, Class-Agnostic Counting, Vision Transformer, Self-Attention, Pretrained Models, Few-Shot Learning, Scale and Magnitude Embedding.$文章的作者机构为华中科技大学人工智能与自动化学院。作者包括：Zhicheng Wang、Liwen Xiao、Zhiguo Cao和Hao Lu。$本文介绍了一种基于预训练的Vision Transformer的Class-Agnostic Counting方法，该方法在自注意力机制中同时执行特征提取和相似度匹配，相对于传统的提取-匹配模式更加简化和一致，并通过额外的缩放嵌入和数量级嵌入实现信息的补偿。在FSC147和CARPK数据集上的实验表明，该方法在效果和泛化性方面显著优于现有的CAC方法，提供了一种简洁而强大的CAC基线。$http://arxiv.org/pdf/2305.04440v1
Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion  Models$  Recently large-scale language-image models (e.g., text-guided diffusionmodels) have considerably improved the image generation capabilities togenerate photorealistic images in various domains. Based on this success,current image editing methods use texts to achieve intuitive and versatilemodification of images. To edit a real image using diffusion models, one mustfirst invert the image to a noisy latent from which an edited image is sampledwith a target text prompt. However, most methods lack one of the following:user-friendliness (e.g., additional masks or precise descriptions of the inputimage are required), generalization to larger domains, or high fidelity to theinput image. In this paper, we design an accurate and quick inversiontechnique, Prompt Tuning Inversion, for text-driven image editing.Specifically, our proposed editing method consists of a reconstruction stageand an editing stage. In the first stage, we encode the information of theinput image into a learnable conditional embedding via Prompt Tuning Inversion.In the second stage, we apply classifier-free guidance to sample the editedimage, where the conditional embedding is calculated by linearly interpolatingbetween the target embedding and the optimized one obtained in the first stage.This technique ensures a superior trade-off between editability and highfidelity to the input image of our method. For example, we can change the colorof a specific object while preserving its original shape and background underthe guidance of only a target text prompt. Extensive experiments on ImageNetdemonstrate the superior editing performance of our method compared to thestate-of-the-art baselines.$Keywords: text-driven image editing, large-scale language-image models, diffusion models, Prompt Tuning Inversion, editability, fidelity.$作者机构：百度VIS、北京航空航天大学。$这篇文章介绍了一种基于扩散模型的文本驱动图像编辑方法，通过使用Prompt Tuning Inversion技术将输入图像反演为噪声潜变量，然后进行编辑以生成符合目标文本提示的图像，该方法保证了编辑性和对输入图像的高保真度。$http://arxiv.org/pdf/2305.04441v1
FashionTex: Controllable Virtual Try-on with Text and Texture$  Virtual try-on attracts increasing research attention as a promising way forenhancing the user experience for online cloth shopping. Though existingmethods can generate impressive results, users need to provide a well-designedreference image containing the target fashion clothes that often do not exist.To support user-friendly fashion customization in full-body portraits, wepropose a multi-modal interactive setting by combining the advantages of bothtext and texture for multi-level fashion manipulation. With the carefullydesigned fashion editing module and loss functions, FashionTex framework cansemantically control cloth types and local texture patterns without annotatedpairwise training data. We further introduce an ID recovery module to maintainthe identity of input portrait. Extensive experiments have demonstrated theeffectiveness of our proposed pipeline.$FashionTex, virtual try-on, multi-modal interactive setting, text and texture, fashion editing module, loss functions, ID recovery module, controllable fashion generation, image manipulation, computer vision.$"本文的作者机构为：
Anran Lin - 香港中文大学（深圳）软件工程学院（SSE）
Nanxuan Zhao - Adobe Research
Shuliang Ning - 香港中文大学（深圳）FNii研究所、软件工程学院（SSE）
Yuda Qiu - 香港中文大学（深圳）FNii研究所、软件工程学院（SSE）
Baoyuan Wang - Xiaobing.AI
Xiaoguang Han - 香港中文大学（深圳）FNii研究所、软件工程学院（SSE）（通讯作者）"$文章介绍了FashionTex框架，一个能够通过文字与纹理图案互动实现服装个性化定制的虚拟试穿系统。通过该系统，用户可以方便地设计出符合个人喜好的服装。$http://arxiv.org/pdf/2305.04451v1
Real-World Denoising via Diffusion Model$  Real-world image denoising is an extremely important image processingproblem, which aims to recover clean images from noisy images captured innatural environments. In recent years, diffusion models have achieved verypromising results in the field of image generation, outperforming previousgeneration models. However, it has not been widely used in the field of imagedenoising because it is difficult to control the appropriate position of theadded noise. Inspired by diffusion models, this paper proposes a novel generaldenoising diffusion model that can be used for real-world image denoising. Weintroduce a diffusion process with linear interpolation, and the intermediatenoisy image is interpolated from the original clean image and the correspondingreal-world noisy image, so that this diffusion model can handle the level ofadded noise. In particular, we also introduce two sampling algorithms for thisdiffusion model. The first one is a simple sampling procedure defined accordingto the diffusion process, and the second one targets the problem of the firstone and makes a number of improvements. Our experimental results show that ourproposed method with a simple CNNs Unet achieves comparable results compared tothe Transformer architecture. Both quantitative and qualitative evaluations onreal-world denoising benchmarks show that the proposed general diffusion modelperforms almost as well as against the state-of-the-art methods.$real-world image denoising, diffusion model, linear interpolation, added noise, sampling algorithms, CNNs, Unet, Transformer architecture, quantitative evaluation, qualitative evaluation, image processing, computer vision, convolutional neural networks, hand-crafted features, mathematical models, wavelet transform, Markov random field, multi-layer perceptron models, deep learning methods, autoencoder.$作者机构：大连理工大学数学科学学院，分别为Cheng Yang、Lijing Liang和Zhixun Su。$本文提出了一种基于扩散模型的图像去噪方法，采用线性插值来控制添加噪声的位置，提供了两种采样算法，并经过真实世界去噪基准测试，结果表明该方法在定量和定性上与最先进的方法相当。$http://arxiv.org/pdf/2305.04457v1
Generalized Universal Domain Adaptation with Generative Flow Networks$  We introduce a new problem in unsupervised domain adaptation, termed asGeneralized Universal Domain Adaptation (GUDA), which aims to achieve preciseprediction of all target labels including unknown categories. GUDA bridges thegap between label distribution shift-based and label space mismatch-basedvariants, essentially categorizing them as a unified problem, guiding to acomprehensive framework for thoroughly solving all the variants. The keychallenge of GUDA is developing and identifying novel target categories whileestimating the target label distribution. To address this problem, we takeadvantage of the powerful exploration capability of generative flow networksand propose an active domain adaptation algorithm named GFlowDA, which selectsdiverse samples with probabilities proportional to a reward function. Toenhance the exploration capability and effectively perceive the target labeldistribution, we tailor the states and rewards, and introduce an efficientsolution for parent exploration and state transition. We also propose atraining paradigm for GUDA called Generalized Universal Adversarial Network(GUAN), which involves collaborative optimization between GUAN and GFlowNet.Theoretical analysis highlights the importance of exploration, and extensiveexperiments on benchmark datasets demonstrate the superiority of GFlowDA.$Keywords: Generalized Universal Domain Adaptation (GUDA), unsupervised domain adaptation (UDA), label distribution shift, label space mismatch, label prediction space, generative flow network, active domain adaptation, exploration capability, collaborative optimization, label heterogeneity.$作者机构：1,2,3浙江大学，华为诺亚方舟实验室，天津大学。$本文提出了一种新的无监督域适应问题，称为广义通用域适应(GUDA)，其目的是实现所有目标标签的精确预测，包括未知分类。作者提出了一个主动域适应算法GFlowDA，利用生成流网络的强探索能力来开发和识别新的目标类别，从而解决GUDA中的关键挑战。同时，作者也提出了一种针对GUDA的培训范式，称为广义通用对抗网络(GUAN)。通过广泛的实验，作者证明了GFlowDA的优越性。$http://arxiv.org/pdf/2305.04466v1
Video Object Segmentation in Panoptic Wild Scenes$  In this paper, we introduce semi-supervised video object segmentation (VOS)to panoptic wild scenes and present a large-scale benchmark as well as abaseline method for it. Previous benchmarks for VOS with sparse annotations arenot sufficient to train or evaluate a model that needs to process all possibleobjects in real-world scenarios. Our new benchmark (VIPOSeg) containsexhaustive object annotations and covers various real-world object categorieswhich are carefully divided into subsets of thing/stuff and seen/unseen classesfor comprehensive evaluation. Considering the challenges in panoptic VOS, wepropose a strong baseline method named panoptic object association withtransformers (PAOT), which uses panoptic identification to associate objectswith a pyramid architecture on multiple scales. Experimental results show thatVIPOSeg can not only boost the performance of VOS models by panoptic trainingbut also evaluate them comprehensively in panoptic scenes. Previous methods forclassic VOS still need to improve in performance and efficiency when dealingwith panoptic scenes, while our PAOT achieves SOTA performance with goodefficiency on VIPOSeg and previous VOS benchmarks. PAOT also ranks 1st in theVOT2022 challenge. Our dataset is available athttps://github.com/yoxu515/VIPOSeg-Benchmark.$Video object segmentation, Panoptic Wild Scenes, Benchmark, Semi-supervised learning, Object annotations, Object categories, Thing/Stuff classes, Seen/Unseen classes, Panoptic identification, Pyramid architecture, Transformers, VOT2022 challenge.$作者机构：1. Zhejiang大学ReLER, CCAI；2. Baidu Research。$本文介绍了一种在广阔自然环境中进行视频目标分割的方法和数据集VIPOSeg，同时提出了一个名为PAOT的强基线模型，可有效地处理来自不同比例、种类和参考掩模的目标，此外，该数据集还提供了全面的评估方法，并在VOT2022挑战中获得了第一名。$http://arxiv.org/pdf/2305.04470v1
DiffBFR: Bootstrapping Diffusion Model Towards Blind Face Restoration$  Blind face restoration (BFR) is important while challenging. Prior worksprefer to exploit GAN-based frameworks to tackle this task due to the balanceof quality and efficiency. However, these methods suffer from poor stabilityand adaptability to long-tail distribution, failing to simultaneously retainsource identity and restore detail. We propose DiffBFR to introduce DiffusionProbabilistic Model (DPM) for BFR to tackle the above problem, given itssuperiority over GAN in aspects of avoiding training collapse and generatinglong-tail distribution. DiffBFR utilizes a two-step design, that first restoresidentity information from low-quality images and then enhances texture detailsaccording to the distribution of real faces. This design is implemented withtwo key components: 1) Identity Restoration Module (IRM) for preserving theface details in results. Instead of denoising from pure Gaussian randomdistribution with LQ images as the condition during the reverse process, wepropose a novel truncated sampling method which starts from LQ images with partnoise added. We theoretically prove that this change shrinks the evidence lowerbound of DPM and then restores more original details. With theoretical proof,two cascade conditional DPMs with different input sizes are introduced tostrengthen this sampling effect and reduce training difficulty in thehigh-resolution image generated directly. 2) Texture Enhancement Module (TEM)for polishing the texture of the image. Here an unconditional DPM, a LQ-freemodel, is introduced to further force the restorations to appear realistic. Wetheoretically proved that this unconditional DPM trained on pure HQ imagescontributes to justifying the correct distribution of inference images outputfrom IRM in pixel-level space. Truncated sampling with fractional time step isutilized to polish pixel-level textures while preserving identity information.$Keywords: Blind face restoration, Diffusion Probabilistic Model, identity restoration, texture enhancement.$作者机构：中国科学院大学，北京，中国，以及美图研究院，北京，中国。$这篇文章提出了一种新的盲人脸恢复方法，称为DiffBFR，它利用Diffusion Probabilistic Model来解决GAN方法在稳定性和适应长尾分布方面的问题，具有更好的细节恢复和身份信息保留能力。$http://arxiv.org/pdf/2305.04517v1
Scene Text Recognition with Image-Text Matching-guided Dictionary$  Employing a dictionary can efficiently rectify the deviation between thevisual prediction and the ground truth in scene text recognition methods.However, the independence of the dictionary on the visual features may lead toincorrect rectification of accurate visual predictions. In this paper, wepropose a new dictionary language model leveraging the Scene Image-TextMatching(SITM) network, which avoids the drawbacks of the explicit dictionarylanguage model: 1) the independence of the visual features; 2) noisy choice incandidates etc. The SITM network accomplishes this by using Image-TextContrastive (ITC) Learning to match an image with its corresponding text amongcandidates in the inference stage. ITC is widely used in vision-languagelearning to pull the positive image-text pair closer in feature space. Inspiredby ITC, the SITM network combines the visual features and the text features ofall candidates to identify the candidate with the minimum distance in thefeature space. Our lexicon method achieves better results(93.8\\% accuracy) thanthe ordinary method results(92.1\\% accuracy) on six mainstream benchmarks.Additionally, we integrate our method with ABINet and establish newstate-of-the-art results on several benchmarks.$Keywords: Scene Text Recognition, Image-Text Matching, Dictionary Language Model, Image-Text Contrastive Learning.$作者机构：1. 华东师范大学多维信息处理上海市重点实验室，中国上海；2. 华东师范大学重庆研究院，中国重庆；3. 印度统计研究所计算机视觉与模式识别研究组，印度加尔各答。$本文介绍一种新的基于SceneImage-TextMatching(SITM)网络的词典语言模型，在推理阶段使用Image-Text Contrastive (ITC) learning来匹配图像和其对应的文本，以标记有最小距离的候选项。该模型的结果在六个主流基准测试中表现更好，并且与ABINet结合后，建立了新的基准测试的最新结果。$http://arxiv.org/pdf/2305.04524v1
SNT: Sharpness-Minimizing Network Transformation for Fast  Compression-friendly Pretraining$  Model compression has become the de-facto approach for optimizing theefficiency of vision models. Recently, the focus of most compression effortshas shifted to post-training scenarios due to the very high cost of large-scalepretraining. This has created the need to build compressible models fromscratch, which can effectively be compressed after training. In this work, wepresent a sharpness-minimizing network transformation (SNT) method appliedduring pretraining that can create models with desirable compressibility andgeneralizability features. We compare our approach to a well-knownsharpness-minimizing optimizer to validate its efficacy in creating a flat losslandscape. To the best of our knowledge, SNT is the first pretraining methodthat uses an architectural transformation to generate compression-friendlynetworks. We find that SNT generalizes across different compression tasks andnetwork backbones, delivering consistent improvements over the ADAM baselinewith up to 2% accuracy improvement on weight pruning and 5.4% accuracyimprovement on quantization. Code to reproduce our results will be madepublicly available.$"Specific domain keywords:
- Model compression
- Pretraining
- Sharpness-minimizing
- Architecture transformation
- Post-training compression
- Pruning
- Quantization
- Generalizability"$本文作者机构：南加州大学（University of Southern California）$本文提出了一种名为SNT的预训练方法，通过使用锐度最小化的网络转换来创建具有可压缩性和泛化性能的模型，实现预训练阶段的压缩优化。与现有预训练方法不同的是，SNT是第一种使用架构变换来生成压缩友好型网络的预训练方法。作者在多个压缩任务和网络骨架中验证了SNT方法的有效性，并将代码公开。$http://arxiv.org/pdf/2305.04526v1
Smart Home Device Detection Algorithm Based on FSA-YOLOv5$  Smart home device detection is a critical aspect of human-computerinteraction. However, detecting targets in indoor environments can bechallenging due to interference from ambient light and background noise. Inthis paper, we present a new model called FSA-YOLOv5, which addresses thelimitations of traditional convolutional neural networks by introducing theTransformer to learn long-range dependencies. Additionally, we propose a newattention module, the full-separation attention module, which integratesspatial and channel dimensional information to learn contextual information. Toimprove tiny device detection, we include a prediction head for the indoorsmart home device detection task. We also release the Southeast UniversityIndoor Smart Speaker Dataset (SUSSD) to supplement existing data samples.Through a series of experiments on SUSSD, we demonstrate that our methodoutperforms other methods, highlighting the effectiveness of FSA-YOLOv5.$Smart home, indoor environments, human-computer interaction, computer vision technology, detection, convolutional neural network, Transformer, attention module, tiny device detection, indoor smart home device detection, dataset.$"作者机构：张佳峰1，蒲雪晶2
1天津师范大学
2东南大学"$本文提出了一种新的智能家居设备检测模型——FSA-YOLOv5，并结合全分离注意力模块和Transformer模块，解决了传统卷积神经网络在室内环境下的限制和噪声干扰问题，同时提供了一个适用于室内智能家居设备检测任务的预测头，实验结果表明FSA-YOLOv5的表现优于其他方法。$http://arxiv.org/pdf/2305.04534v1
LMPT: Prompt Tuning with Class-Specific Embedding Loss for Long-tailed  Multi-Label Visual Recognition$  Long-tailed multi-label visual recognition (LTML) task is a highlychallenging task due to the label co-occurrence and imbalanced datadistribution. In this work, we propose a unified framework for LTML, namelyprompt tuning with class-specific embedding loss (LMPT), capturing the semanticfeature interactions between categories by combining text and image modalitydata and improving the performance synchronously on both head and tail classes.Specifically, LMPT introduces the embedding loss function with class-aware softmargin and re-weighting to learn class-specific contexts with the benefit oftextual descriptions (captions), which could help establish semanticrelationships between classes, especially between the head and tail classes.Furthermore, taking into account the class imbalance, the distribution-balancedloss is adopted as the classification loss function to further improve theperformance on the tail classes without compromising head classes. Extensiveexperiments are conducted on VOC-LT and COCO-LT datasets, which demonstratesthat the proposed method significantly surpasses the previous state-of-the-artmethods and zero-shot CLIP in LTML. Our codes are fully available at\\url{https://github.com/richard-peng-xia/LMPT}.$LMPT, prompt tuning, class-specific embedding loss, long-tailed multi-label visual recognition, label co-occurrence, imbalanced data distribution, text and image modality data, semantic feature interactions, embedding loss function, class-aware soft margin, re-weighting, textual descriptions, class imbalance, distribution-balanced loss, VOC-LT dataset, COCO-LT dataset, state-of-the-art methods, zero-shot CLIP.$作者机构：澳大利亚莫纳什大学AIM实验室、Airdoc-Monash Research、莫纳什大学工程学院、中国苏州大学计算机科学与技术学院、英国伦敦帝国学院地球科学与工程学院、沙特阿拉伯阿卜杜拉国王科技大学。$本文提出了一种针对长尾多标签视觉识别的LMPT框架，通过结合文本和图像模态数据捕捉类别间的语义特征交互，并通过引入类别感知的软边界和重新加权等措施，学习类别特定的语境以建立类别间的语义关系，从而在头部和尾部类别上同步提高性能。经过在VOC-LT和COCO-LT数据集上的实验，证明该方法显著优于现有的最先进方法和零样本CLIP。$http://arxiv.org/pdf/2305.04536v1
High Quality Large-Scale 3-D Urban Mapping with Multi-Master TomoSAR$  Multi-baseline interferometric synthetic aperture radar (InSAR) techniquesare effective approaches for retrieving the 3-D information of urban areas. Inorder to obtain a plausible reconstruction, it is necessary to use large-stackinterferograms. Hence, these methods are commonly not appropriate forlarge-scale 3-D urban mapping using TanDEM-X data where only a few acquisitionsare available in average for each city. This work proposes a new SARtomographic processing framework to work with those extremely small stacks,which integrates the non-local filtering into SAR tomography inversion. Theapplicability of the algorithm is demonstrated using a TanDEM-X multi-baselinestack with 5 bistatic interferograms over the whole city of Munich, Germany.Systematic comparison of our result with airborne LiDAR data shows that therelative height accuracy of two third buildings is within two meters, whichoutperforms the TanDEM-X raw DEM. The promising performance of the proposedalgorithm paved the first step towards high quality large-scale 3-D urbanmapping.$Keywords: High Quality Large-Scale 3-D Urban Mapping, Multi-Master TomoSAR, Interferometric Synthetic Aperture Radar, TanDEM-X, Non-Local Filtering, SAR Tomography, Bistatic Interferograms, Differential TomoSAR, Compressive Sensing, Spectral Estimation.$作者机构：德国慕尼黑工业大学（Technische Universität München，简称TUM-LMF和TUM-SIPEO）和德国航空航天中心（German Aerospace Center，简称DLR）。作者包括Yilei Shi（TUM-LMF）、Richard Bamler（DLR和TUM-LMF）、Yuanyuan Wang（TUM-SIPEO）和Xiao Xiang Zhu（DLR和TUM-SIPEO）。$这篇文章介绍了一种新的雷达成像技术——多主雷达干涉成像雷达（InSAR）技术，将其应用于高质量大规模城市三维地图绘制。该技术在使用TanDEM-X数据时，将非局部滤波融入SAR层析成像，能够适用于仅有少量采集数据的情况下绘制三维城市地图，并证明了其相对高度精度可以优于TanDEM-X原始DEM。$http://arxiv.org/pdf/2305.04541v1
Privacy-Preserving Representations are not Enough -- Recovering Scene  Content from Camera Poses$  Visual localization is the task of estimating the camera pose from which agiven image was taken and is central to several 3D computer visionapplications. With the rapid growth in the popularity of AR/VR/MR devices andcloud-based applications, privacy issues are becoming a very important aspectof the localization process. Existing work on privacy-preserving localizationaims to defend against an attacker who has access to a cloud-based service. Inthis paper, we show that an attacker can learn about details of a scene withoutany access by simply querying a localization service. The attack is based onthe observation that modern visual localization algorithms are robust tovariations in appearance and geometry. While this is in general a desiredproperty, it also leads to algorithms localizing objects that are similarenough to those present in a scene. An attacker can thus query a server with alarge enough set of images of objects, \\eg, obtained from the Internet, andsome of them will be localized. The attacker can thus learn about objectplacements from the camera poses returned by the service (which is the minimalinformation returned by such a service). In this paper, we develop aproof-of-concept version of this attack and demonstrate its practicalfeasibility. The attack does not place any requirements on the localizationalgorithm used, and thus also applies to privacy-preserving representations.Current work on privacy-preserving representations alone is thus insufficient.$Visual localization, camera pose, privacy, cloud-based localization, augmented reality, mixed reality, virtual reality, XR applications, client-server mechanism, privacy-preserving localization, privacy-preserving representations.$本文作者机构为：瑞典查尔莫斯理工大学（Chalmers University of Technology），捷克特克尼卡大学（Czech Technical University in Prague）信息学、机器人和控制工程研究所以及该校电气工程系的视觉识别小组（Visual Recognition Group）。$本文研究通过查询定位服务，使用现代视觉定位算法的健壮性，实现未经授权获取场景内容的攻击，并且证明现有的隐私保护方案不足以抵御此类攻击。$http://arxiv.org/pdf/2305.04603v1
Target-driven One-Shot Unsupervised Domain Adaptation$  In this paper, we introduce a novel framework for the challenging problem ofOne-Shot Unsupervised Domain Adaptation (OSUDA), which aims to adapt to atarget domain with only a single unlabeled target sample. Unlike existingapproaches that rely on large labeled source and unlabeled target data, ourTarget-driven One-Shot UDA (TOS-UDA) approach employs a learnable augmentationstrategy guided by the target sample\'s style to align the source distributionwith the target distribution. Our method consists of three modules: anaugmentation module, a style alignment module, and a classifier. Unlikeexisting methods, our augmentation module allows for strong transformations ofthe source samples, and the style of the single target sample available isexploited to guide the augmentation by ensuring perceptual similarity.Furthermore, our approach integrates augmentation with style alignment,eliminating the need for separate pre-training on additional datasets. Ourmethod outperforms or performs comparably to existing OS-UDA methods on theDigits and DomainNet benchmarks.$Target-driven One-Shot Unsupervised Domain Adaptation, Data Augmentation, One-Shot, Unsupervised Domain Adaptation.$"作者机构：Julio Ivan Davila Carrazco1;2, Suvarna Kishorkumar Kadam1, Pietro Morerio1,
Alessio Del Bue1, and Vittorio Murino1;3
1意大利技术学院模式分析与计算机视觉实验室（PAVIS），意大利，热那亚
2热那亚大学海洋、电气、电子与通信工程系，意大利，热那亚
3维罗纳大学计算机科学系，意大利，维罗纳
{julio.davila, suvarna.kadam, pietro.morerio, alessio.delbue,
vittorio.murino}@iit.it"$本文提出了一种新颖的框架，用于解决仅有单个未标记目标样本的单次无监督域适应问题，采用可学习的数据扩增策略和目标样本的风格指导源样本的扩增，通过与目标分布对齐来适应目标域，消除了在其他数据集上的预训练需求，实验结果在Digits和DomainNet基准测试上表现优异。$http://arxiv.org/pdf/2305.04628v1
ReGeneration Learning of Diffusion Models with Rich Prompts for  Zero-Shot Image Translation$  Large-scale text-to-image models have demonstrated amazing ability tosynthesize diverse and high-fidelity images. However, these models are oftenviolated by several limitations. Firstly, they require the user to provideprecise and contextually relevant descriptions for the desired imagemodifications. Secondly, current models can impose significant changes to theoriginal image content during the editing process. In this paper, we exploreReGeneration learning in an image-to-image Diffusion model (ReDiffuser), thatpreserves the content of the original image without human prompting and therequisite editing direction is automatically discovered within the textembedding space. To ensure consistent preservation of the shape during imageediting, we propose cross-attention guidance based on regeneration learning.This novel approach allows for enhanced expression of the target domainfeatures while preserving the original shape of the image. In addition, weintroduce a cooperative update strategy, which allows for efficientpreservation of the original shape of an image, thereby improving the qualityand consistency of shape preservation throughout the editing process. Ourproposed method leverages an existing pre-trained text-image diffusion modelwithout any additional training. Extensive experiments show that the proposedmethod outperforms existing work in both real and synthetic image editing.$Keywords: image-to-image translation, text-to-image models, zero-shot learning, shape consistency, cross-attention guidance, regeneration learning, cooperative update strategy, diffusion models.$作者机构：广东工业大学、悉尼大学、安徽大学。$文章探索了在图像到图像扩散模型中使用ReGeneration学习来保留原始图像内容，并自动在文本嵌入空间中发现所需的编辑方向，同时保持形状一致性的新方法。这种方法可以提高目标域特征的表达，并保持图像的原始形状。同时，这篇文章提出了合作更新策略，可以有效地保留图像的原始形状，从而提高形状保留的质量和一致性。$http://arxiv.org/pdf/2305.04651v1
Self-supervised Learning for Pre-Training 3D Point Clouds: A Survey$  Point cloud data has been extensively studied due to its compact form andflexibility in representing complex 3D structures. The ability of point clouddata to accurately capture and represent intricate 3D geometry makes it anideal choice for a wide range of applications, including computer vision,robotics, and autonomous driving, all of which require an understanding of theunderlying spatial structures. Given the challenges associated with annotatinglarge-scale point clouds, self-supervised point cloud representation learninghas attracted increasing attention in recent years. This approach aims to learngeneric and useful point cloud representations from unlabeled data,circumventing the need for extensive manual annotations. In this paper, wepresent a comprehensive survey of self-supervised point cloud representationlearning using DNNs. We begin by presenting the motivation and general trendsin recent research. We then briefly introduce the commonly used datasets andevaluation metrics. Following that, we delve into an extensive exploration ofself-supervised point cloud representation learning methods based on thesetechniques. Finally, we share our thoughts on some of the challenges andpotential issues that future research in self-supervised learning forpre-training 3D point clouds may encounter.$Keywords: Self-supervised learning, point clouds, pre-training, object & indoor scene-level data, outdoor scene-level data, transfer learning.$文章作者机构：复旦大学计算机科学学院（Ben Fei, Liwen Liu, Tianyue Luo, Rui Zhang, Weidong Yang），香港理工大学应用数学系（Yixuan Li），南洋理工大学计算机科学与工程学院（Ying He）。$本文综述了最近几年基于深度神经网络的自监督学习方法在3D点云表示学习中的应用。通过设计预文本任务，这些方法能够从大规模无标签点云数据中学习通用的、有用的点云表示，从而避免了大量手动标注数据的需求。本文将介绍自监督学习的动机、常用数据集、评价指标和各种技术，并对其方法进行深入探索，最后讨论未来研究中可能面临的挑战和潜在问题。$http://arxiv.org/pdf/2305.04691v1
Learning to Generate Poetic Chinese Landscape Painting with Calligraphy$  In this paper, we present a novel system (denoted as Polaca) to generatepoetic Chinese landscape painting with calligraphy. Unlike previous singleimage-to-image painting generation, Polaca takes the classic poetry as inputand outputs the artistic landscape painting image with the correspondingcalligraphy. It is equipped with three different modules to complete the wholepiece of landscape painting artwork: the first one is a text-to-image module togenerate landscape painting image, the second one is an image-to-image moduleto generate stylistic calligraphy image, and the third one is an image fusionmodule to fuse the two images into a whole piece of aesthetic artwork.$Keywords: Chinese landscape painting, calligraphy, deep learning, Generative Adversarial Networks (GANs), text-to-image, image-to-image, image fusion, poetry.$"作者机构：
1. 京东AI，中国北京
2. 中央美术学院，中国北京"$本文提出了一个名为Polaca的新系统，旨在以诗歌为输入，输出相应的艺术山水画和对应的书法，其中包括文本转图像模块、图像转图像样式模块和图像融合模块，以完成整个艺术作品的绘制。$http://arxiv.org/pdf/2305.04719v1
Understanding Gaussian Attention Bias of Vision Transformers Using  Effective Receptive Fields$  Vision transformers (ViTs) that model an image as a sequence of partitionedpatches have shown notable performance in diverse vision tasks. Becausepartitioning patches eliminates the image structure, to reflect the order ofpatches, ViTs utilize an explicit component called positional embedding.However, we claim that the use of positional embedding does not simplyguarantee the order-awareness of ViT. To support this claim, we analyze theactual behavior of ViTs using an effective receptive field. We demonstrate thatduring training, ViT acquires an understanding of patch order from thepositional embedding that is trained to be a specific pattern. Based on thisobservation, we propose explicitly adding a Gaussian attention bias that guidesthe positional embedding to have the corresponding pattern from the beginningof training. We evaluated the influence of Gaussian attention bias on theperformance of ViTs in several image classification, object detection, andsemantic segmentation experiments. The results showed that proposed method notonly facilitates ViTs to understand images but also boosts their performance onvarious datasets, including ImageNet, COCO 2017, and ADE20K.$Keywords: Vision transformers, Gaussian attention bias, effective receptive fields, patch order, positional embeddings, image classification, object detection, semantic segmentation.$文章的作者机构：POSTECH（韩国科学技术院）的Bum Jun Kim、Hyeyeon Choi、Hyeonah Jang和Sang Woo Kim。$本文针对ViT对于输入patch的顺序理解的问题，通过分析有效感受野，提出在位置嵌入中注入高斯注意偏置的方法，以获得一种天生的空间理解视觉变换器。经过实验证明，该方法不仅能提高ViT在各种数据集上的性能，而且能使ViT更好地理解图像。$http://arxiv.org/pdf/2305.04722v1
Large-scale and Efficient Texture Mapping Algorithm via Loopy Belief  Propagation$  Texture mapping as a fundamental task in 3D modeling has been wellestablished for well-acquired aerial assets under consistent illumination, yetit remains a challenge when it is scaled to large datasets with images undervarying views and illuminations. A well-performed texture mapping algorithmmust be able to efficiently select views, fuse and map textures from theseviews to mesh models, at the same time, achieve consistent radiometry over theentire model. Existing approaches achieve efficiency either by limiting thenumber of images to one view per face, or simplifying global inferences to onlyachieve local color consistency. In this paper, we break this tie by proposinga novel and efficient texture mapping framework that allows the use of multipleviews of texture per face, at the same time to achieve global colorconsistency. The proposed method leverages a loopy belief propagation algorithmto perform an efficient and global-level probabilistic inferences to rankcandidate views per face, which enables face-level multi-view texture fusionand blending. The texture fusion algorithm, being non-parametric, bringsanother advantage over typical parametric post color correction methods, due toits improved robustness to non-linear illumination differences. The experimentson three different types of datasets (i.e. satellite dataset, unmanned-aerialvehicle dataset and close-range dataset) show that the proposed method hasproduced visually pleasant and texturally consistent results in all scenarios,with an added advantage of consuming less running time as compared to the stateof the art methods, especially for large-scale dataset such assatellite-derived models.$Texture mapping, Belief propagation, Multiple labels, Image blending, 3D modeling, Mesh models, Polyhedral models, Aerial assets, Illumination, Global color consistency, Probabilistic inferences, Non-parametric algorithm, Non-linear illumination differences, Satellite dataset, Unmanned-aerial vehicle dataset, Close-range dataset.$作者机构：Geospatial Data Analytics Laboratory, The Ohio State University, Department of Civil, Environmental and Geodetic Engineering, The Ohio State University, College of Astronautics, Nanjing University Of Aeronautics And Astronautics.$本文提出了一种基于循环置信传播算法的大规模高效纹理映射算法，能够有效地实现纹理选择、融合和映射，同时实现全局颜色一致性，并且在不同类型的数据集上表现良好。$http://arxiv.org/pdf/2305.04763v1
OSTA: One-shot Task-adaptive Channel Selection for Semantic Segmentation  of Multichannel Images$  Semantic segmentation of multichannel images is a fundamental task for manyapplications. Selecting an appropriate channel combination from the originalmultichannel image can improve the accuracy of semantic segmentation and reducethe cost of data storage, processing and future acquisition. Existing channelselection methods typically use a reasonable selection procedure to determine adesirable channel combination, and then train a semantic segmentation networkusing that combination. In this study, the concept of pruning from a supernetis used for the first time to integrate the selection of channel combinationand the training of a semantic segmentation network. Based on this concept, aOne-Shot Task-Adaptive (OSTA) channel selection method is proposed for thesemantic segmentation of multichannel images. OSTA has three stages, namely thesupernet training stage, the pruning stage and the fine-tuning stage. Theoutcomes of six groups of experiments (L7Irish3C, L7Irish2C, L8Biome3C,L8Biome2C, RIT-18 and Semantic3D) demonstrated the effectiveness and efficiencyof OSTA. OSTA achieved the highest segmentation accuracies in all tests (62.49%(mIoU), 75.40% (mIoU), 68.38% (mIoU), 87.63% (mIoU), 66.53% (mA) and 70.86%(mIoU), respectively). It even exceeded the highest accuracies of exhaustivetests (61.54% (mIoU), 74.91% (mIoU), 67.94% (mIoU), 87.32% (mIoU), 65.32% (mA)and 70.27% (mIoU), respectively), where all possible channel combinations weretested. All of this can be accomplished within a predictable and relativelyefficient timeframe, ranging from 101.71% to 298.1% times the time required totrain the segmentation network alone. In addition, there were interestingfindings that were deemed valuable for several fields.$ultichannel images, semantic segmentation, channel selection, pruning, supernet training, fine-tuning, remote sensing, Landsat, cloud detection, domain adaptation, multispectral images, supervised band selection.$Xi’an Jiaotong-Liverpool University, Suzhou 215000, China, and also with the School of Engineering, University of Liverpool, L69 3BX Liverpool, U.K. Jagannath Aryal is with the Department of Infrastructure Engineering, Faculty of Engineering and IT, The University of Melbourne, Melbourne VIC 3010, Australia. Lei Fan is with the Department of Civil Engineering, Design School, Xi’an Jiaotong-Liverpool University, Suzhou 215000, China.$本文提出了一种基于剪枝超网的一次性任务自适应通道选择（OSTA）方法，用于多通道图像的语义分割，通过选择适当的通道组合提高语义分割的准确性并减少数据存储、处理和未来采集的成本。该方法在超网训练阶段、剪枝阶段和微调阶段三个阶段进行，实验结果表明其有效性和效率，并超出全面测试的最高准确性。$http://arxiv.org/pdf/2305.04766v1
SignBERT+: Hand-model-aware Self-supervised Pre-training for Sign  Language Understanding$  Hand gesture serves as a crucial role during the expression of sign language.Current deep learning based methods for sign language understanding (SLU) areprone to over-fitting due to insufficient sign data resource and suffer limitedinterpretability. In this paper, we propose the first self-supervisedpre-trainable SignBERT+ framework with model-aware hand prior incorporated. Inour framework, the hand pose is regarded as a visual token, which is derivedfrom an off-the-shelf detector. Each visual token is embedded with gesturestate and spatial-temporal position encoding. To take full advantage of currentsign data resource, we first perform self-supervised learning to model itsstatistics. To this end, we design multi-level masked modeling strategies(joint, frame and clip) to mimic common failure detection cases. Jointly withthese masked modeling strategies, we incorporate model-aware hand prior tobetter capture hierarchical context over the sequence. After the pre-training,we carefully design simple yet effective prediction heads for downstream tasks.To validate the effectiveness of our framework, we perform extensiveexperiments on three main SLU tasks, involving isolated and continuous signlanguage recognition (SLR), and sign language translation (SLT). Experimentalresults demonstrate the effectiveness of our method, achieving newstate-of-the-art performance with a notable gain.$Keywords: Sign language understanding, self-supervised pre-training, masked modeling strategies, model-aware hand prior.$作者机构：中国科学技术大学，电子工程与信息科学系。$本文提出了一种手模型感知的自监督预训练方法，用于手语理解任务的深度学习。通过利用模型感知的手先验和多层面掩码建模策略，实现了预训练的有效性验证，并在三个主要手语理解任务中取得了新的最高性能。$http://arxiv.org/pdf/2305.04868v1
PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR  Point Clouds$  In order to deal with the sparse and unstructured raw point clouds, LiDARbased 3D object detection research mostly focuses on designing dedicated localpoint aggregators for fine-grained geometrical modeling. In this paper, werevisit the local point aggregators from the perspective of allocatingcomputational resources. We find that the simplest pillar based models performsurprisingly well considering both accuracy and latency. Additionally, we showthat minimal adaptions from the success of 2D object detection, such asenlarging receptive field, significantly boost the performance. Extensiveexperiments reveal that our pillar based networks with modernized designs interms of architecture and training render the state-of-the-art performance onthe two popular benchmarks: Waymo Open Dataset and nuScenes. Our resultschallenge the common intuition that the detailed geometry modeling is essentialto achieve high performance for 3D object detection.$Keywords: 3D object detection, LiDAR point clouds, local point aggregators, computational resources, pillar-based models, receptive field, network architecture, Waymo Open Dataset, nuScenes.$作者机构：Jinyu Li, Chenxu Luo, Xiaodong Yang*，QCraft。$本文重新审视了基于LiDAR点云的3D物体检测中的本地点聚合器，发现最简单的柱状模型在准确性和延迟方面都表现出色，并证明了一些2D物体检测中广泛应用的技术可以显著提高性能，挑战了现有的3D物体检测性能实现需要详细的几何建模的观点。该文提出的PillarNeXt网络设计在Waymo Open数据集和nuScenes数据集上表现出最先进的性能。$http://arxiv.org/pdf/2305.04925v1
RelPose++: Recovering 6D Poses from Sparse-view Observations$  We address the task of estimating 6D camera poses from sparse-view image sets(2-8 images). This task is a vital pre-processing stage for nearly allcontemporary (neural) reconstruction algorithms but remains challenging givensparse views, especially for objects with visual symmetries and texture-lesssurfaces. We build on the recent RelPose framework which learns a network thatinfers distributions over relative rotations over image pairs. We extend thisapproach in two key ways; first, we use attentional transformer layers toprocess multiple images jointly, since additional views of an object mayresolve ambiguous symmetries in any given image pair (such as the handle of amug that becomes visible in a third view). Second, we augment this network toalso report camera translations by defining an appropriate coordinate systemthat decouples the ambiguity in rotation estimation from translationprediction. Our final system results in large improvements in 6D poseprediction over prior art on both seen and unseen object categories and alsoenables pose estimation and 3D reconstruction for in-the-wild objects.$Keywords: 6D Camera Poses, Sparse-view Image Sets, Attentional Transformer Layers, Neural Reconstruction Algorithms, 3D Reconstruction, Multi-view Cues.$作者机构：卡内基梅隆大学（Carnegie Mellon University）Amy Lin、Jason Y. Zhang、Deva Ramanan、Shubham Tulsiani。$本文提出了一个名为RelPose++的框架，用于从稀疏视图中恢复物体的6D姿态，通过引入注意力转换层和解耦旋转估计和位移预测来提高相对旋转和相对位移的准确性。$http://arxiv.org/pdf/2305.04926v1
Physics-based network fine-tuning for robust quantitative susceptibility  mapping from high-pass filtered phase$  Purpose: To improve the generalization ability of convolutional neuralnetwork (CNN) based prediction of quantitative susceptibility mapping (QSM)from high-pass filtered phase (HPFP) image. Methods: The proposed networkaddresses two common generalization issues that arise when using a pre-trainednetwork to predict QSM from HPFP: a) data with unseen voxel sizes, and b) datawith unknown high-pass filter parameters. A network fine-tuning step based on ahigh-pass filtering dipole convolution forward model is proposed to reduce thegeneralization error of the pre-trained network. A progressive Unetarchitecture is proposed to improve prediction accuracy without increasingfine-tuning computational cost. Results: In retrospective studies using RMSE,PSNR, SSIM and HFEN as quality metrics, the performance of both Unet andprogressive Unet was improved after physics-based fine-tuning at all voxelsizes and most high-pass filtering cutoff frequencies tested in the experiment.Progressive Unet slightly outperformed Unet both before and after fine-tuning.In a prospective study, image sharpness was improved after physics-basedfine-tuning for both Unet and progressive Unet. Compared to Unet, progressiveUnet had better agreement of regional susceptibility values with reference QSM.Conclusion: The proposed method shows improved robustness compared to thepre-trained network without fine-tuning when the test dataset deviates fromtraining. Our code is available at https://github.com/Jinwei1209/SWI_to_QSM/$Physics-based, network fine-tuning, robust, quantitative susceptibility mapping, high-pass filtered phase, biomedical engineering, radiology, electrical and computer engineering, applied physics.$文章的作者机构为：康奈尔大学生物医学工程Meinig学院，康奈尔医学院放射科，康奈尔大学电气和计算机工程系，康奈尔大学应用物理系。作者之一Yi Wang是美国纽约威尔·康奈尔医学院放射科的博士，是本文的通讯作者。$本篇文章提出了一种物理基础的神经网络调整方法（SWI2QSM），用于从高通滤波相位中进行鲁棒的定量磁敏感度映像微调。$http://arxiv.org/pdf/2305.03844v1
White Matter Hyperintensities Segmentation Using Probabilistic TransUNet$  White Matter Hyperintensities (WMH) are areas of the brain that have higherintensity than other normal brain regions on Magnetic Resonance Imaging (MRI)scans. WMH is often associated with small vessel disease in the brain, makingearly detection of WMH important. However, there are two common issues in thedetection of WMH: high ambiguity and difficulty in detecting small WMH. In thisstudy, we propose a method called Probabilistic TransUNet to address theprecision of small object segmentation and the high ambiguity of medicalimages. To measure model performance, we conducted a k-fold cross validationand cross dataset robustness experiment. Based on the experiments, the additionof a probabilistic model and the use of a transformer-based approach were ableto achieve better performance.$Keywords: White Matter Hyperintensities, Medical Image Segmentation, Probabilistic Model, UNet, TransUNet, Probabilistic UNet, Robustness, Deep Learning, MRI, Global Spatial Information, Convolutional Neural Networks, Fully Convolutional Networks, Transformer.$作者机构：印度尼西亚大学计算机科学学院，日本理化学研究所大脑科学中心大脑图像分析单元。$这篇文章介绍了一个名为Probabilistic TransUNet的方法来解决医学图像高模糊性和难以检测小WMH的问题，使用交叉验证和跨数据集鲁棒性实验来衡量模型性能。文章讨论了使用深度学习进行白质高信号（WMH）自动分割的相关研究，并介绍了使用Transformer对全局信息上下文进行建模的研究。$http://arxiv.org/pdf/2305.03912v1
Adaptive loose optimization for robust question answering$  Question answering methods are well-known for leveraging data bias, such asthe language prior in visual question answering and the position bias inmachine reading comprehension (extractive question answering). Currentdebiasing methods often come at the cost of significant in-distributionperformance to achieve favorable out-of-distribution generalizability, whilenon-debiasing methods sacrifice a considerable amount of out-of-distributionperformance in order to obtain high in-distribution performance. Therefore, itis challenging for them to deal with the complicated changing real-worldsituations. In this paper, we propose a simple yet effective novel lossfunction with adaptive loose optimization, which seeks to make the best of bothworlds for question answering. Our main technical contribution is to reduce theloss adaptively according to the ratio between the previous and currentoptimization state on mini-batch training data. This loose optimization can beused to prevent non-debiasing methods from overlearning data bias whileenabling debiasing methods to maintain slight bias learning. Experiments on thevisual question answering datasets, including VQA v2, VQA-CP v1, VQA-CP v2,GQA-OOD, and the extractive question answering dataset SQuAD demonstrate thatour approach enables QA methods to obtain state-of-the-art in- andout-of-distribution performance in most cases. The source code has beenreleased publicly in \\url{https://github.com/reml-group/ALO}.$Keywords: question answering, data bias, debiasing, visual question answering, extractive question answering, language bias, position bias, multi-modality learning, robustness.$作者机构：西安交通大学网络空间安全学院，中国移动研究所，西安交通大学计算机科学与技术学院，智能网络与网络安全教育部重点实验室。$这篇文章介绍了一个自适应松弛优化方法，用于提高问题回答的鲁棒性，避免数据偏差和位置偏差对回答的影响。该方法可以平衡数据内外的表现，提高QA方法的性能。$http://arxiv.org/pdf/2305.03971v1
Towards Prompt-robust Face Privacy Protection via Adversarial Decoupling  Augmentation Framework$  Denoising diffusion models have shown remarkable potential in variousgeneration tasks. The open-source large-scale text-to-image model, StableDiffusion, becomes prevalent as it can generate realistic artistic or facialimages with personalization through fine-tuning on a limited number of newsamples. However, this has raised privacy concerns as adversaries can acquirefacial images online and fine-tune text-to-image models for malicious editing,leading to baseless scandals, defamation, and disruption to victims\' lives.Prior research efforts have focused on deriving adversarial loss fromconventional training processes for facial privacy protection throughadversarial perturbations. However, existing algorithms face two issues: 1)they neglect the image-text fusion module, which is the vital module oftext-to-image diffusion models, and 2) their defensive performance is unstableagainst different attacker prompts. In this paper, we propose the AdversarialDecoupling Augmentation Framework (ADAF), addressing these issues by targetingthe image-text fusion module to enhance the defensive performance of facialprivacy protection algorithms. ADAF introduces multi-level text-relatedaugmentations for defense stability against various attacker prompts.Concretely, considering the vision, text, and common unit space, we proposeVision-Adversarial Loss, Prompt-Robust Augmentation, and Attention-DecouplingLoss. Extensive experiments on CelebA-HQ and VGGFace2 demonstrate ADAF\'spromising performance, surpassing existing algorithms.$"Area: Computer Vision, Privacy Protection, Adversarial Learning, Text-to-Image Models. 

Keywords: Denoising diffusion models, Stable Diffusion, Adversarial Decoupling Augmentation Framework, Facial Privacy Protection, DeepFakes, Fine-tuning, Image-Text Fusion Module, Adversarial Noise, Multi-level Augmentations, Vision-Adversarial Loss, Prompt-Robust Augmentation, Attention-Decoupling Loss."$作者机构：SenseTime Research和中国科学院大学电子、电气、通信工程学院。$本文提出了一种对抗解耦增强框架（ADAF），以目标是加强面部隐私保护算法的防御性能。ADAF通过定位图像-文本融合模块，引入多级文本相关增强来提高防御稳定性，以应对各种攻击者提示。该方法在CelebA-HQ和VGGFace2上展示出良好的性能，超过了现有算法。$http://arxiv.org/pdf/2305.03980v1
Unlocking Low-Light-Rainy Image Restoration by Pairwise Degradation  Feature Vector Guidance$"  Rain in the dark is a common natural phenomenon. Photos captured in such acondition significantly impact the performance of various nighttime activities,such as autonomous driving, surveillance systems, and night photography. Whileexisting methods designed for low-light enhancement or deraining show promisingperformance, they have limitations in simultaneously addressing the task ofbrightening low light and removing rain. Furthermore, using a cascade approach,such as ``deraining followed by low-light enhancement\'\' or vice versa, may leadto difficult-to-handle rain patterns or excessively blurred and overexposedimages. To overcome these limitations, we propose an end-to-end network called$L^{2}RIRNet$ which can jointly handle low-light enhancement and deraining. Ournetwork mainly includes a Pairwise Degradation Feature Vector ExtractionNetwork (P-Net) and a Restoration Network (R-Net). P-Net can learn degradationfeature vectors on the dark and light areas separately, using contrastivelearning to guide the image restoration process. The R-Net is responsible forrestoring the image. We also introduce an effective Fast Fourier - ResNetDetail Guidance Module (FFR-DG) that initially guides image restoration usingdetail image that do not contain degradation information but focus on texturedetail information. Additionally, we contribute a dataset containing syntheticand real-world low-light-rainy images. Extensive experiments demonstrate thatour $L^{2}RIRNet$ outperforms existing methods in both synthetic and complexreal-world scenarios."$Low-Light-Rainy Image Restoration, Pairwise Degradation, Feature Vector Guidance, End-to-End Network, Low-Light Enhancement, Deraining, Autonomous Driving, Surveillance Systems, Night Photography.$文章作者机构：Xin Lin（四川大学），Jingtong Yue（四川大学），Chao Ren（四川大学），Chun-Le Guo（南开大学TMCC），Chongyi Li（南洋理工大学S-Lab）。$本文提出了一种新的图像恢复方法𝐿2𝑅𝐼𝑅𝑁𝑒𝑡，可以同时处理低光照、下雨天气等多种难以处理的情况。同时，文章还对现有的低光照增强和去雨算法进行了比较。$http://arxiv.org/pdf/2305.03997v1
AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion$  Recent advances in diffusion models have showcased promising results in thetext-to-video (T2V) synthesis task. However, as these T2V models solely employtext as the guidance, they tend to struggle in modeling detailed temporaldynamics. In this paper, we introduce a novel T2V framework that additionallyemploy audio signals to control the temporal dynamics, empowering anoff-the-shelf T2I diffusion to generate audio-aligned videos. We proposeaudio-based regional editing and signal smoothing to strike a good balancebetween the two contradicting desiderata of video synthesis, i.e., temporalflexibility and coherence. We empirically demonstrate the effectiveness of ourmethod through experiments, and further present practical applications forcontents creation.$Audio-aligned video synthesis, text-to-image diffusion, diffusion models, temporal dynamics, audio modality, image translation, contents creation, image editing, generative models.$作者机构：首尔国立大学；NA VER。$本文介绍了一种使用音频来控制视频时序的文本到视频生成框架，以生成与音频同步的视频。作者使用了一个文本到图像扩散模型，同时使用文本和音频来指导视频综合，前者聚焦于可视化场景语义，而后者更负责细粒度的时序动态控制。作者提出了基于区域编辑和信号平滑的音频方法，以在视频合成的两个相互矛盾的期望（即时序灵活性和一致性）之间取得良好的平衡。作者通过实验证明了该方法的有效性，并进一步提出了将其应用于内容创建的实际应用。$http://arxiv.org/pdf/2305.04001v1
Degradation-Noise-Aware Deep Unfolding Transformer for Hyperspectral  Image Denoising$  Hyperspectral imaging (HI) has emerged as a powerful tool in diverse fieldssuch as medical diagnosis, industrial inspection, and agriculture, owing to itsability to detect subtle differences in physical properties through highspectral resolution. However, hyperspectral images (HSIs) are often quite noisybecause of narrow band spectral filtering. To reduce the noise in HSI datacubes, both model-driven and learning-based denoising algorithms have beenproposed. However, model-based approaches rely on hand-crafted priors andhyperparameters, while learning-based methods are incapable of estimating theinherent degradation patterns and noise distributions in the imaging procedure,which could inform supervised learning. Secondly, learning-based algorithmspredominantly rely on CNN and fail to capture long-range dependencies,resulting in limited interpretability. This paper proposes aDegradation-Noise-Aware Unfolding Network (DNA-Net) that addresses theseissues. Firstly, DNA-Net models sparse noise, Gaussian noise, and explicitlyrepresent image prior using transformer. Then the model is unfolded into anend-to-end network, the hyperparameters within the model are estimated from thenoisy HSI and degradation model and utilizes them to control each iteration.Additionally, we introduce a novel U-Shaped Local-Non-local-SpectralTransformer (U-LNSA) that captures spectral correlation, local contents, andnon-local dependencies simultaneously. By integrating U-LNSA into DNA-Net, wepresent the first Transformer-based deep unfolding HSI denoising method.Experimental results show that DNA-Net outperforms state-of-the-art methods,and the modeling of noise distributions helps in cases with heavy noise.$"Keywords: 
- Hyperspectral imaging (HSI)
- Denoising
- Transformer
- Model-based methods
- Learning-based methods
- Gaussian noise
- Sparse noise
- Image priors
- End-to-end (E2E) deep neural networks
- Deep unfolding methods
- Plug-and-play (PnP) algorithms
- U-Shaped Local-Non-local-Spectral Transformer (U-LNSA)"$"机构：
- IMEC-UGent, Ghent, Belgium
- Computer Vision Lab, ETH, Zurich, Switzerland
- NPU, Xi’an, China
- CUG, Wuhan, China
- KAIST SCENE-02"$本文提出了一种Degradation-Noise-Aware Deep Unfolding Transformer方法，用于高光谱图像去噪，通过建模稀疏噪声、高斯噪声和使用变压器显式表示图像先验，以捕获局部内容、非局部依赖和谱相关性，同时集成了一种新的U型局部-非局部-谱变换器，以解决传统基于模型或基于学习的方法存在的问题和局限性。实验结果表明，该方法优于现有方法。$http://arxiv.org/pdf/2305.04047v1
SST-ReversibleNet: Reversible-prior-based Spectral-Spatial Transformer  for Efficient Hyperspectral Image Reconstruction$  Spectral image reconstruction is an important task in snapshot compressedimaging. This paper aims to propose a new end-to-end framework with iterativecapabilities similar to a deep unfolding network to improve reconstructionaccuracy, independent of optimization conditions, and to reduce the number ofparameters. A novel framework called the reversible-prior-based method isproposed. Inspired by the reversibility of the optical path, thereversible-prior-based framework projects the reconstructions back into themeasurement space, and then the residuals between the projected data and thereal measurements are fed into the network for iteration. The reconstructionsubnet in the network then learns the mapping of the residuals to the truevalues to improve reconstruction accuracy. In addition, a novelspectral-spatial transformer is proposed to account for the global correlationof spectral data in both spatial and spectral dimensions while balancingnetwork depth and computational complexity, in response to the shortcomings ofexisting transformer-based denoising modules that ignore spatial texturefeatures or learn local spatial features at the expense of global spatialfeatures. Extensive experiments show that our SST-ReversibleNet significantlyoutperforms state-of-the-art methods on simulated and real HSI datasets, whilerequiring lower computational and storage costs.https://github.com/caizeyu1992/SST$$$$http://arxiv.org/pdf/2305.04054v1
SynthMix: Mixing up Aligned Synthesis for Medical Cross-Modality Domain  Adaptation$  The adversarial methods showed advanced performance by producing syntheticimages to mitigate the domain shift, a common problem due to the hardship ofacquiring labelled data in medical field. Most existing studies focus onmodifying the network architecture, but little has worked on the GAN trainingstrategy. In this work, we propose SynthMix, an add-on module with a naturalyet effective training policy that can promote synthetic quality withoutaltering the network architecture. Following the adversarial philosophy of GAN,we designed a mix-up synthesis scheme termed SynthMix. It coherently mixed upaligned images of real and synthetic samples to stimulate the generation offine-grained features, examined by an associated Inspector for thedomain-specific details. We evaluated our method on two segmentation benchmarksamong three publicly available datasets, where our method showed a significantperformance gain compared with existing state-of-the-art approaches.$Specific keywords related to the article's domain include: unsupervised domain adaptation, medical image analysis, deep learning, adversarial learning, cross-modality domain adaptation, mixup, data augmentation, segmentation, synthetic image alignment, and GAN-based image synthesis.$作者机构：澳大利亚悉尼大学计算机学院（School of Computer Science, University of Sydney, Australia）$本文提出了一种名为SynthMix的方法，通过合并真实和合成的样本的对齐图像来刺激生成更精细特征的混合合成方案，从而增强了跨模态适应性的对抗方法在医疗图像分割中的性能。同时，通过Mixup Inspector加速SynthMix的训练。$http://arxiv.org/pdf/2305.04156v1
UIT-OpenViIC: A Novel Benchmark for Evaluating Image Captioning in  Vietnamese$  Image Captioning is one of the vision-language tasks that still interest theresearch community worldwide in the 2020s. MS-COCO Caption benchmark iscommonly used to evaluate the performance of advanced captioning models,although it was published in 2015. Recent captioning models trained on theMS-COCO Caption dataset only have good performance in language patterns ofEnglish; they do not have such good performance in contexts captured in Vietnamor fluently caption images using Vietnamese. To contribute to the low-resourcesresearch community as in Vietnam, we introduce a novel image captioning datasetin Vietnamese, the Open-domain Vietnamese Image Captioning dataset(UIT-OpenViIC). The introduced dataset includes complex scenes captured inVietnam and manually annotated by Vietnamese under strict rules andsupervision. In this paper, we present in more detail the dataset creationprocess. From preliminary analysis, we show that our dataset is challenging torecent state-of-the-art (SOTA) Transformer-based baselines, which performedwell on the MS COCO dataset. Then, the modest results prove that UIT-OpenViIChas room to grow, which can be one of the standard benchmarks in Vietnamese forthe research community to evaluate their captioning models. Furthermore, wepresent a CAMO approach that effectively enhances the image representationability by a multi-level encoder output fusion mechanism, which helps improvethe quality of generated captions compared to previous captioning models.$Vietnamese Image Captioning, Open-domain Image Captioning, Transformer, Vision-language Task, Image Captioning Dataset, MS-COCO Caption, Flickr30k, GoodNews, TextCaps, Crowd-Caption, VisualGenome, CAMO Approach, Encoder Output Fusion Mechanism, Low-resource Language.$作者机构：University of Information Technology, Ho Chi Minh city, Vietnam，Vietnam National University, Ho Chi Minh City, Vietnam。$本文介绍了一种用越南语评估图像字幕的新方法，并详细描述了数据集的创建过程和一种有效的CAMO方法来提高生成字幕的质量。作者介绍了UIT-OpenViIC，这是一个包含越南日常生活和文化相关背景的开放领域越南数据集，并评估了在此数据集上的最新SOTA方法，为越南的视觉语言研究提供了新的基准。$http://arxiv.org/pdf/2305.04166v1
Cross-Modal Retrieval for Motion and Text via MildTriple Loss$  Cross-modal retrieval has become a prominent research topic in computervision and natural language processing with advances made in image-text andvideo-text retrieval technologies. However, cross-modal retrieval between humanmotion sequences and text has not garnered sufficient attention despite theextensive application value it holds, such as aiding virtual realityapplications in better understanding users\' actions and language. This taskpresents several challenges, including joint modeling of the two modalities,demanding the understanding of person-centered information from text, andlearning behavior features from 3D human motion sequences. Previous work onmotion data modeling mainly relied on autoregressive feature extractors thatmay forget previous information, while we propose an innovative model thatincludes simple yet powerful transformer-based motion and text encoders, whichcan learn representations from the two different modalities and capturelong-term dependencies. Furthermore, the overlap of the same atomic actions ofdifferent human motions can cause semantic conflicts, leading us to explore anew triplet loss function, MildTriple Loss. it leverages the similarity betweensamples in intra-modal space to guide soft-hard negative sample mining in thejoint embedding space to train the triplet loss and reduce the violation causedby false negative samples. We evaluated our model and method on the latestHumanML3D and KIT Motion-Language datasets, achieving a 62.9\\% recall formotion retrieval and a 71.5\\% recall for text retrieval (based on R@10) on theHumanML3D dataset. Our code is available athttps://github.com/eanson023/rehamot.$Cross-modal retrieval, motion-text retrieval, triplet loss, contrastive learning, 3D human motion sequences, natural language processing, transformer-based motion and text encoders, long-term dependencies, MildTriple Loss.$机构：重庆理工大学人工智能学院，北京大学深圳研究生院机器感知重点实验室。$这篇文章提出了一种基于MildTriple Loss的交叉模态检索方法，用于在人类运动序列和文本之间检索，解决了自动匹配自然语言描述和精确的3D人类运动的问题，具有广泛的应用价值。$http://arxiv.org/pdf/2305.04195v1
Unlocking the Power of Open Set : A New Perspective for Open-set Noisy  Label Learning$  Learning from noisy data has attracted much attention, where most methodsfocus on closed-set label noise. However, a more common scenario in the realworld is the presence of both open-set and closed-set noise. Existing methodstypically identify and handle these two types of label noise separately bydesigning a specific strategy for each type. However, in many real-worldscenarios, it would be challenging to identify open-set examples, especiallywhen the dataset has been severely corrupted. Unlike the previous works, weexplore how models behave when faced open-set examples, and find that a part ofopen-set examples gradually get integrated into certain known classes, which isbeneficial for the seperation among known classes. Motivated by the phenomenon,in this paper, we propose a novel two-step contrastive learning method calledCECL, which aims to deal with both types of label noise by exploiting theuseful information of open-set examples. Specifically, we incorporate someopen-set examples into closed-set classes to enhance performance while treatingothers as delimiters to improve representative ability. Extensive experimentson synthetic and real-world datasets with diverse label noise demonstrate thatCECL can outperform state-of-the-art methods.$Keywords: open-set noisy label learning, closed-set noise, open-set noise, deep neural networks, contrastive learning.$作者机构：Wenhai Wan，Institution1，Institution1地址，firstauthor@i1.org；Xinrui Wang，Institution2，Institution2地址第一行，secondauthor@i2.org。$文章提出了一种新的、探索开放集（open-set）概念的、旨在处理标签噪声的方法，称为CECL，它能够更好地处理在现实世界中普遍存在的开放集和封闭集标签噪声。$http://arxiv.org/pdf/2305.04203v1
RATs-NAS: Redirection of Adjacent Trails on GCN for Neural Architecture  Search$  Various hand-designed CNN architectures have been developed, such as VGG,ResNet, DenseNet, etc., and achieve State-of-the-Art (SoTA) levels on differenttasks. Neural Architecture Search (NAS) now focuses on automatically findingthe best CNN architecture to handle the above tasks. However, the verificationof a searched architecture is very time-consuming and makes predictor-basedmethods become an essential and important branch of NAS. Two commonly usedtechniques to build predictors are graph-convolution networks (GCN) andmultilayer perceptron (MLP). In this paper, we consider the difference betweenGCN and MLP on adjacent operation trails and then propose the RedirectedAdjacent Trails NAS (RATs-NAS) to quickly search for the desired neural networkarchitecture. The RATs-NAS consists of two components: the Redirected AdjacentTrails GCN (RATs-GCN) and the Predictor-based Search Space Sampling (P3S)module. RATs-GCN can change trails and their strengths to search for a betterneural network architecture. DSS can rapidly focus on tighter intervals ofFLOPs in the search space. Based on our observations on cell-based NAS, webelieve that architectures with similar FLOPs will perform similarly. Finally,the RATs-NAS consisting of RATs-GCN and DSS beats WeakNAS, Arch-Graph, andothers by a significant margin on three sub-datasets of NASBench-201.$Neural Architecture Search (NAS), graph-convolution networks (GCN), multilayer perceptron (MLP), predictor-based NAS, cell-based NAS, FLOPs, adjacency matrix, operation matrix, architecture search space.$"作者机构：

1.国立中央大学计算机科学与信息工程系，台湾桃园；
2.国立阳明交通大学人工智能与绿色能源学院，台湾新竹。"$本文提出了一种快速搜索所需神经网络结构的方法--RATs-NAS，即通过考虑GCN和MLP之间在相邻操作路径（即邻接矩阵）上的差异，提出一种重定向邻接路径的GCN，结合基于GCN的预测器和基于定点抽样的漫游搜索，以快速搜索最佳神经网络结构。$http://arxiv.org/pdf/2305.04206v1
Segmentation and Vascular Vectorization for Coronary Artery by  Geometry-based Cascaded Neural Network$  Segmentation of the coronary artery is an important task for the quantitativeanalysis of coronary computed tomography angiography (CCTA) images and is beingstimulated by the field of deep learning. However, the complex structures withtiny and narrow branches of the coronary artery bring it a great challenge.Coupled with the medical image limitations of low resolution and poor contrast,fragmentations of segmented vessels frequently occur in the prediction.Therefore, a geometry-based cascaded segmentation method is proposed for thecoronary artery, which has the following innovations: 1) Integrating geometricdeformation networks, we design a cascaded network for segmenting the coronaryartery and vectorizing results. The generated meshes of the coronary artery arecontinuous and accurate for twisted and sophisticated coronary arterystructures, without fragmentations. 2) Different from mesh annotationsgenerated by the traditional marching cube method from voxel-based labels, afiner vectorized mesh of the coronary artery is reconstructed with theregularized morphology. The novel mesh annotation benefits the geometry-basedsegmentation network, avoiding bifurcation adhesion and point cloud dispersionin intricate branches. 3) A dataset named CCA-200 is collected, consisting of200 CCTA images with coronary artery disease. The ground truths of 200 casesare coronary internal diameter annotations by professional radiologists.Extensive experiments verify our method on our collected dataset CCA-200 andpublic ASOCA dataset, with a Dice of 0.778 on CCA-200 and 0.895 on ASOCA,showing superior results. Especially, our geometry-based model generates anaccurate, intact and smooth coronary artery, devoid of any fragmentations ofsegmented vessels.$"Keywords:
- Segmentation
- Vascular vectorization
- Coronary artery
- Geometry-based
- Mesh annotation
- Computed tomography angiography (CCTA)
- Deep learning
- Medical imaging"$本文作者机构：1、同济大学电子与信息工程学院，2、上海人工智能实验室，3、香港中文大学感知与交互智能中心，4、香港中文大学影像与介入放射学部门，5、商汤科技研究所，6、香港中文大学电子工程系。$本文提出了一种基于几何的级联神经网络的冠状动脉分割和血管向量化方法，可以避免分割结果中经常出现的断裂，并且在复杂结构下也能生成连续和准确的冠状动脉网格注释。$http://arxiv.org/pdf/2305.04208v1
Design, Implementation and Evaluation of an External Pose-Tracking  System for Underwater Cameras$  In order to advance underwater computer vision and robotics from labenvironments and clear water scenarios to the deep dark ocean or murky coastalwaters, representative benchmarks and realistic datasets with ground truthinformation are required. In particular, determining the camera pose isessential for many underwater robotic or photogrammetric applications and knownground truth is mandatory to evaluate the performance of e.g., simultaneouslocalization and mapping approaches in such extreme environments. This paperpresents the conception, calibration and implementation of an externalreference system for determining the underwater camera pose in real-time. Theapproach, based on an HTC Vive tracking system in air, calculates theunderwater camera pose by fusing the poses of two controllers tracked above thewater surface of a tank. It is shown that the mean deviation of this approachto an optical marker based reference in air is less than 3 mm and 0.3{\\deg}.Finally, the usability of the system for underwater applications isdemonstrated.$Keywords: tracking, pose estimation, Hand-Eye calibration, UKF, underwater vision, ROS, ground truth, HTC Vive.$作者机构：Birger Winkel, David Nakath, Felix Woelk, Kevin K oser，GEOMAR Helmholtz Centre for Ocean Research Kiel和University of Applied Sciences Kiel。$这篇文章介绍了一种基于HTC Vive跟踪系统的外部参考系统，在海底相机的实时定位方面进行设计、实现和评估，以提高海底计算机视觉和机器人技术在极端环境下的性能。$http://arxiv.org/pdf/2305.04226v1
Instance-Variant Loss with Gaussian RBF Kernel for 3D Cross-modal  Retriveal$  3D cross-modal retrieval is gaining attention in the multimedia community.Central to this topic is learning a joint embedding space to represent datafrom different modalities, such as images, 3D point clouds, and polygon meshes,to extract modality-invariant and discriminative features. Hence, theperformance of cross-modal retrieval methods heavily depends on therepresentational capacity of this embedding space. Existing methods treat allinstances equally, applying the same penalty strength to instances with varyingdegrees of difficulty, ignoring the differences between instances. This canresult in ambiguous convergence or local optima, severely compromising theseparability of the feature space. To address this limitation, we propose anInstance-Variant loss to assign different penalty strengths to differentinstances, improving the space separability. Specifically, we assign differentpenalty weights to instances positively related to their intra-class distance.Simultaneously, we reduce the cross-modal discrepancy between features bylearning a shared weight vector for the same class data from differentmodalities. By leveraging the Gaussian RBF kernel to evaluate samplesimilarity, we further propose an Intra-Class loss function that minimizes theintra-class distance among same-class instances. Extensive experiments on three3D cross-modal datasets show that our proposed method surpasses recentstate-of-the-art approaches.$Specific domain keywords: 3D cross-modal retrieval, deep metric learning, cross-domain feature learning, Gaussian RBF kernel, modality-invariant, discriminative features, intra-class distance, inter-class distance, multi-modal data, point clouds, polygon meshes.$作者机构：中国电子科技大学$本文提出了一种基于高斯RBF核的实例变异损失用于3D跨模态检索，解决了现有方法中没有考虑实例之间难易程度差异的问题，提高了嵌入空间的可分性。同时，作者还使用一个共享的权重向量学习不同模态下相同类别的数据，减小跨模态差异。最后，作者还提出了一个基于高斯RBF核的Intra-Class损失函数来减小同类实例之间的距离，实验结果表明该方法优于当前最先进的方法。$http://arxiv.org/pdf/2305.04239v1
Dual Residual Attention Network for Image Denoising$  In image denoising, deep convolutional neural networks (CNNs) can obtainfavorable performance on removing spatially invariant noise. However, many ofthese networks cannot perform well on removing the real noise (i.e. spatiallyvariant noise) generated during image acquisition or transmission, whichseverely sets back their application in practical image denoising tasks.Instead of continuously increasing the network depth, many researchers haverevealed that expanding the width of networks can also be a useful way toimprove model performance. It also has been verified that feature filtering canpromote the learning ability of the models. Therefore, in this paper, wepropose a novel Dual-branch Residual Attention Network (DRANet) for imagedenoising, which has both the merits of a wide model architecture andattention-guided feature learning. The proposed DRANet includes two differentparallel branches, which can capture complementary features to enhance thelearning ability of the model. We designed a new residual attention block (RAB)and a novel hybrid dilated residual attention block (HDRAB) for the upper andthe lower branches, respectively. The RAB and HDRAB can capture rich localfeatures through multiple skip connections between different convolutionallayers, and the unimportant features are dropped by the residual attentionmodules. Meanwhile, the long skip connections in each branch, and the globalfeature fusion between the two parallel branches can capture the globalfeatures as well. Moreover, the proposed DRANet uses downsampling operationsand dilated convolutions to increase the size of the receptive field, which canenable DRANet to capture more image context information. Extensive experimentsdemonstrate that compared with other state-of-the-art denoising methods, ourDRANet can produce competitive denoising performance both on synthetic andreal-world noise removal.$"- Image denoising
- Deep convolutional neural networks (CNNs)
- Spatially invariant noise
- Spatially variant noise
- Dual-branch Residual Attention Network (DRANet)
- Wide model architecture
- Attention-guided feature learning
- Residual attention block (RAB)
- Hybrid dilated residual attention block (HDRAB)
- Multiple skip connections
- Global feature fusion
- Downsampling operations
- Dilated convolutions
- Receptive field
- Synthetic noise removal
- Real-world noise removal
- Dual deep convolutional network
- Hybrid residual attention learning."$作者机构：云南师范大学信息科学与技术学院（Yunnan Normal University, School of Information Science and Technology），中国云南省昆明市650500。$本文提出了一种新型的图像去噪网络——双分支残差关注网络(DRANet)，它结合了宽模型架构和关注引导特征学习的优点，具有良好的去噪性能。该网络采用两个不同的平行分支来捕捉互补特征，其中上分支和下分支分别采用了新设计的残差注意力块(RAB)和混合空洞残差注意力块(HDRAB)，可以通过多个跳跃连接捕捉丰富的本地特征，同时通过残差注意机制抑制不重要的特征。另外，该网络采用下采样和空洞卷积增加感受野，以便更好地捕捉图像上下文信息。实验表明，DRANet相对于其他最先进的去噪方法，在合成和实际噪声去除上都具有竞争性的去噪性能。$http://arxiv.org/pdf/2305.04269v1
RSC-VAE: Recoding Semantic Consistency Based VAE for One-Class Novelty  Detection$  In recent years, there is an increasing interests in reconstruction basedgenerative models for image One-Class Novelty Detection, most of which onlyfocus on image-level information. While in this paper, we further exploit thelatent space of Variational Auto-encoder (VAE), a typical reconstruction basedmodel, and we innovatively divide it into three regions:Normal/Anomalous/Unknown-semantic-region. Based on this hypothesis, we proposea new VAE architecture, Recoding Semantic Consistency Based VAE (RSC-VAE),combining VAE with recoding mechanism and constraining the semantic consistencyof two encodings. We come up with three training modes of RSC-VAE: 1. One-ClassTraining Mode, alleviating False Positive problem of normal samples; 2.Distributionally-Shifted Training Mode, alleviating False Negative problem ofanomalous samples; 3. Extremely-Imbalanced Training Mode, introducing a smallnumber of anomalous samples for training to enhance the second mode. Theexperimental results on multiple datasets demonstrate that our mechanismachieves state-of-the-art performance in various baselines including VAE.$Keywords: One-Class Novelty Detection, Variational Auto-encoder (VAE), latent space, Normal/Anomalous/Unknown Semantic Regions, recoding mechanism, semantic consistency, False Positive problem, False Negative problem, extremely imbalanced data.$作者机构：浙江大学、太原理工大学。$本文提出了一种基于VAE的新架构RSC-VAE，将潜空间分为正常/异常/未知语义区域，并提出了三种训练模式以实现单类新奇性检测。实验结果表明，该机制在多个基线中实现了现有技术水平的性能。$http://arxiv.org/pdf/2305.04275v1
Learning from synthetic data generated with GRADE$  Recently, synthetic data generation and realistic rendering has advancedtasks like target tracking and human pose estimation. Simulations for mostrobotics applications are obtained in (semi)static environments, with specificsensors and low visual fidelity. To solve this, we present a fully customizableframework for generating realistic animated dynamic environments (GRADE) forrobotics research, first introduced in [1]. GRADE supports full simulationcontrol, ROS integration, realistic physics, while being in an engine thatproduces high visual fidelity images and ground truth data. We use GRADE togenerate a dataset focused on indoor dynamic scenes with people and flyingobjects. Using this, we evaluate the performance of YOLO and Mask R-CNN on thetasks of segmenting and detecting people. Our results provide evidence thatusing data generated with GRADE can improve the model performance when used fora pre-training step. We also show that, even training using only syntheticdata, can generalize well to real-world images in the same application domainsuch as the ones from the TUM-RGBD dataset. The code, results, trained models,and the generated data are provided as open-source athttps://eliabntt.github.io/grade-rr.$Key domain-specific keywords in this article include: robotics research, synthetic data, photorealism, dynamic entities, simulation engines, target tracking, human pose estimation, computer vision community, path-tracing rendering, material reflections, pre-training.$作者机构：Elia Bonetto（马普智能系统研究所）、Chenghao Xu（德尔夫特理工大学）、Aamir Ahmad（斯图加特大学）$文章介绍了一个名为GRADE的框架，用于生成可定制的、真实的、动态的环境，用于机器人研究，并证明使用该框架生成的数据可以提高模型的性能，即使只使用合成数据进行培训也可以在同一应用领域的真实世界图像上获得良好的泛化能力。$http://arxiv.org/pdf/2305.04282v1
PELE scores: Pelvic X-ray Landmark Detection by Pelvis Extraction and  Enhancement$  The pelvis, the lower part of the trunk, supports and balances the trunk.Landmark detection from a pelvic X-ray (PXR) facilitates downstream analysisand computer-assisted diagnosis and treatment of pelvic diseases. Although PXRshave the advantages of low radiation and reduced cost compared to computedtomography (CT) images, their 2D pelvis-tissue superposition of 3D structuresconfuses clinical decision-making. In this paper, we propose a PELvisExtraction (PELE) module that utilizes 3D prior anatomical knowledge in CT toguide and well isolate the pelvis from PXRs, thereby eliminating the influenceof soft tissue. We conduct an extensive evaluation based on two public datasetsand one private dataset, totaling 850 PXRs. The experimental results show thatthe proposed PELE module significantly improves the accuracy of PXRs landmarkdetection and achieves state-of-the-art performances in several benchmarkmetrics, thus better serving downstream tasks.$Keywords: PELE scores, Pelvic X-ray, Landmark detection, Pelvis Extraction, Enhancement, Bone extraction.$文章作者机构：中国科学技术大学计算机科学系、生物医学工程学院和苏州研究院医学影像、机器人和分析计算实验室，东南大学，中国科学院计算技术研究所，智慧医疗信息处理技术重点实验室，Z²Sky技术公司，贵州医科大学附属医院。$本文提出了一种基于PELE模块的Pelvic X-ray地标检测方法，利用先前在CT中得到的3D解剖知识来指导和分离PXRs中的骨盆部位，从而提高了准确度，达到了表现最优秀的标准，更好地服务于PXRs的下游任务。$http://arxiv.org/pdf/2305.04294v1
HashCC: Lightweight Method to Improve the Quality of the Camera-less  NeRF Scene Generation$  Neural Radiance Fields has become a prominent method of scene generation viaview synthesis. A critical requirement for the original algorithm to learnmeaningful scene representation is camera pose information for each image in adata set. Current approaches try to circumnavigate this assumption withmoderate success, by learning approximate camera positions alongside learningneural representations of a scene. This requires complicated camera models,causing a long and complicated training process, or results in a lack oftexture and sharp details in rendered scenes. In this work we introduce HashColor Correction (HashCC) -- a lightweight method for improving Neural RadianceFields rendered image quality, applicable also in situations where camerapositions for a given set of images are unknown.$Keywords: Neural Radiance Fields, view synthesis, camera pose, HashNeRF, camera-less regime, color correction.$作者机构：华沙大学（University of Warsaw）的Jan Olszewski。$本文介绍了一种名为HashCC的轻量级方法，通过添加颜色校正模块来提高NeRF无相机场景生成算法的渲染图片质量，适用于相机位置未知的情况。$http://arxiv.org/pdf/2305.04296v1
Poses as Queries: Image-to-LiDAR Map Localization with Transformers$  High-precision vehicle localization with commercial setups is a crucialtechnique for high-level autonomous driving tasks. Localization with amonocular camera in LiDAR map is a newly emerged approach that achievespromising balance between cost and accuracy, but estimating pose by findingcorrespondences between such cross-modal sensor data is challenging, therebydamaging the localization accuracy. In this paper, we address the problem byproposing a novel Transformer-based neural network to register 2D images into3D LiDAR map in an end-to-end manner. Poses are implicitly represented ashigh-dimensional feature vectors called pose queries and can be iterativelyupdated by interacting with the retrieved relevant information from cross-modelfeatures using attention mechanism in a proposed POse Estimator Transformer(POET) module. Moreover, we apply a multiple hypotheses aggregation method thatestimates the final poses by performing parallel optimization on multiplerandomly initialized pose queries to reduce the network uncertainty.Comprehensive analysis and experimental results on public benchmark concludethat the proposed image-to-LiDAR map localization network could achievestate-of-the-art performances in challenging cross-modal localization tasks.$Specific domain keywords: high-precision vehicle localization, autonomous driving, LiDAR map, monocular camera, cross-modal sensor data, Transformer-based neural network, pose estimation, attention mechanism, multiple hypotheses aggregation, state-of-the-art performance.$作者机构：清华大学车辆与运动性能研究所，中国北京，以及蔚来汽车自动驾驶事业部，中国北京。$本文提出了一种基于Transformer的神经网络，用于将2D图像注册到3D LiDAR地图中，隐式地将姿态表示为称为姿态查询的高维特征向量，并通过交互和关注机制来更新这些姿态查询，从而提高交叉模态本地化任务的性能。同时，采用多假设聚合方法来减少网络不确定性，最终实现在挑战性交叉模态本地化任务中实现最先进的性能表现。$http://arxiv.org/pdf/2305.04298v1
Data Efficient Training with Imbalanced Label Sample Distribution for  Fashion Detection$  Multi-label classification models have a wide range of applications inE-commerce, including visual-based label predictions and language-basedsentiment classifications. A major challenge in achieving satisfactoryperformance for these tasks in the real world is the notable imbalance in datadistribution. For instance, in fashion attribute detection, there may be onlysix \'puff sleeve\' clothes among 1000 products in most E-commerce fashioncatalogs. To address this issue, we explore more data-efficient model trainingtechniques rather than acquiring a huge amount of annotations to collectsufficient samples, which is neither economic nor scalable. In this paper, wepropose a state-of-the-art weighted objective function to boost the performanceof deep neural networks (DNNs) for multi-label classification with long-taileddata distribution. Our experiments involve image-based attribute classificationof fashion apparels, and the results demonstrate favorable performance for thenew weighting method compared to non-weighted and inverse-frequency-basedweighting mechanisms. We further evaluate the robustness of the new weightingmechanism using two popular fashion attribute types in today\'s fashionindustry: sleevetype and archetype.$Keywords: Multi-label classification, data imbalance, deep neural networks, weighted objective function, long-tailed data distribution, fashion attribute detection, label weighting.$作者机构：亚马逊公司，Xin Shen、Praful Agrawal、Zhongwei Cheng。$本文提出了一种新的加权目标函数来增强深度神经网络对不平衡数据分布的多标签分类的性能，以解决在实际应用中数据分布不平衡的问题。作者通过对时尚图像的属性分类进行了实验，结果表明新的加权方法比非加权和基于逆频率加权机制具有更好的性能。同时，作者还评估了新的加权机制的鲁棒性。$http://arxiv.org/pdf/2305.04379v1
Few Shot Learning for Medical Imaging: A Comparative Analysis of  Methodologies and Formal Mathematical Framework$"  Deep learning becomes an elevated context regarding disposing of many machinelearning tasks and has shown a breakthrough upliftment to extract features fromunstructured data. Though this flourishing context is developing in the medicalimage processing sector, scarcity of problem-dependent training data has becomea larger issue in the way of easy application of deep learning in the medicalsector. To unravel the confined data source, researchers have developed a modelthat can solve machine learning problems with fewer data called ``Few shotlearning"". Few hot learning algorithms determine to solve the data limitationproblems by extracting the characteristics from a small dataset throughclassification and segmentation methods. In the medical sector, there isfrequently a shortage of available datasets in respect of some confidentialdiseases. Therefore, Few shot learning gets the limelight in this data scarcitysector. In this chapter, the background and basic overview of a few shots oflearning is represented. Henceforth, the classification of few-shot learning isdescribed also. Even the paper shows a comparison of methodological approachesthat are applied in medical image analysis over time. The current advancementin the implementation of few-shot learning concerning medical imaging isillustrated. The future scope of this domain in the medical imaging sector isfurther described."$Keywords: Few-shot learning, medical imaging, deep learning, machine learning, classification, segmentation, data scarcity, methodological approaches, future scope.$"本文作者机构：

Jannatul Nayem
孟加拉国国际伊斯兰大学，电气与电子工程系，库米拉，锡塔昆杜，吉大港-4318，孟加拉国

Sayed Sahriar Hasan
孟加拉国国际伊斯兰大学，电气与电子工程系，库米拉，锡塔昆杜，吉大港-4318，孟加拉国

Noshin Amina
孟加拉国国际伊斯兰大学，电气与电子工程系，库米拉，锡塔昆杜，吉大港-4318，孟加拉国

Bristy Das
孟加拉国国际伊斯兰大学，电气与电子工程系，库米拉，锡塔昆杜，吉大港-4318，孟加拉国

Md Shahin Ali
孟加拉国伊斯兰大学生物医学工程系，库希亚，7003，孟加拉国

Md Manjurul Ahsan
俄克拉荷马大学工业与系统工程学院，诺曼，俄克拉荷马州-73019

Shivakumar Raman
俄克拉荷马大学工业与系统工程学院，诺曼，俄克拉荷马州-73019"$本文对比了医学影像中少样本学习的不同方法论和形式数学框架，并讨论了其在医学影像处理领域的应用及未来发展前景。$http://arxiv.org/pdf/2305.04401v1
Towards Accurate Human Motion Prediction via Iterative Refinement$  Human motion prediction aims to forecast an upcoming pose sequence given apast human motion trajectory. To address the problem, in this work we proposeFreqMRN, a human motion prediction framework that takes into account both thekinematic structure of the human body and the temporal smoothness nature ofmotion. Specifically, FreqMRN first generates a fixed-size motion historysummary using a motion attention module, which helps avoid inaccurate motionpredictions due to excessively long motion inputs. Then, supervised by theproposed spatial-temporal-aware, velocity-aware and global-smoothness-awarelosses, FreqMRN iteratively refines the predicted motion though the proposedmotion refinement module, which converts motion representations back and forthbetween pose space and frequency space. We evaluate FreqMRN on several standardbenchmark datasets, including Human3.6M, AMASS and 3DPW. Experimental resultsdemonstrate that FreqMRN outperforms previous methods by large margins for bothshort-term and long-term predictions, while demonstrating superior robustness.$"Key domain-specific keywords in this article include: 

- Human motion prediction 
- Kinematic structure 
- Temporal smoothness 
- Motion attention module 
- Spatial-temporal-aware 
- Velocity-aware 
- Global-smoothness-aware 
- Motion refinement module 
- Pose space 
- Frequency space 
- Recurrent Neural Networks (RNNs) 
- Convolutional Neural Networks (CNNs) 
- Attention mechanism 
- Discrete Cosine Transform (DCT) 
- Graph Convolutional Networks (GCNs) 
- Human tracking 
- Motion generation 
- Robotics 
- Autonomous driving 
- Markov models 
- Gaussian processes 
- Binary latent variables 
- Human3.6M dataset 
- AMASS dataset 
- 3DPW dataset."$文章作者机构：伊利诺伊大学香槟分校电气与计算机工程系，作者为 Jiarui Sun 和 Girish Chowdhary。$本文提出了一个名为FreqMRN的人体运动预测框架，利用运动注意力模块生成一个固定大小的运动历史摘要，并通过运动细化模块迭代地优化预测的运动，同时考虑人体的运动结构和运动的时间平滑性。该方法在多个标准基准数据集上得到实验验证，展示了在短期和长期预测方面超过先前方法的明显优势。$http://arxiv.org/pdf/2305.04443v1
Locally Attentional SDF Diffusion for Controllable 3D Shape Generation$  Although the recent rapid evolution of 3D generative neural networks greatlyimproves 3D shape generation, it is still not convenient for ordinary users tocreate 3D shapes and control the local geometry of generated shapes. To addressthese challenges, we propose a diffusion-based 3D generation framework --locally attentional SDF diffusion, to model plausible 3D shapes, via 2D sketchimage input. Our method is built on a two-stage diffusion model. The firststage, named occupancy-diffusion, aims to generate a low-resolution occupancyfield to approximate the shape shell. The second stage, named SDF-diffusion,synthesizes a high-resolution signed distance field within the occupied voxelsdetermined by the first stage to extract fine geometry. Our model is empoweredby a novel view-aware local attention mechanism for image-conditioned shapegeneration, which takes advantage of 2D image patch features to guide 3D voxelfeature learning, greatly improving local controllability and modelgeneralizability. Through extensive experiments in sketch-conditioned andcategory-conditioned 3D shape generation tasks, we validate and demonstrate theability of our method to provide plausible and diverse 3D shapes, as well asits superior controllability and generalizability over existing work. Our codeand trained models are available athttps://zhengxinyang.github.io/projects/LAS-Diffusion.html$3D shape generation, diffusion model, sketch-conditioned, local attention.$作者机构：郑昕阳（清华大学，中国），潘浩（微软亚洲研究院，中国），王鹏帅（北京大学，中国），童欣（微软亚洲研究院，中国），刘洋（微软亚洲研究院，中国），岑享洋（清华大学和国际数字经济学院，中国）。$本文提出了一种基于扩散的三维形状生成框架，可通过2D示意图输入生成逼真的三维形状并控制局部几何，采用新颖的局部关注机制和视角感知机制，极大地提高了对局部的可控性和模型的普适性。$http://arxiv.org/pdf/2305.04461v1
Vision Lanauge Pre-training by Contrastive Learning with Cross-Modal  Similarity Regulation$  Cross-modal contrastive learning in vision language pretraining (VLP) facesthe challenge of (partial) false negatives. In this paper, we study thisproblem from the perspective of Mutual Information (MI) optimization. It iscommon sense that InfoNCE loss used in contrastive learning will maximize thelower bound of MI between anchors and their positives, while we theoreticallyprove that MI involving negatives also matters when noises commonly exist.Guided by a more general lower bound form for optimization, we propose acontrastive learning strategy regulated by progressively refined cross-modalsimilarity, to more accurately optimize MI between an image/text anchor and itsnegative texts/images instead of improperly minimizing it. Our method performscompetitively on four downstream cross-modal tasks and systematically balancesthe beneficial and harmful effects of (partial) false negative samples undertheoretical guidance.$Vision-language pre-training, cross-modal contrastive learning, mutual information optimization, self-supervised learning, large-scale image-text pairs, downstream cross-modal tasks, partial false negatives, cross-modal similarity regularization.$本文作者机构：北京大学软件工程国家工程研究中心，阿里巴巴达摩院。$本文研究了视觉语言预训练中跨模态对比学习中（部分）假阴性的挑战，并提出了一种逐步精炼的跨模态相似性调控方法来更准确地优化图像/文本锚点和它的负文本/图像之间的互信息最大化问题。通过理论指导，本方法在四个下游跨模态任务中表现出竞争力并系统平衡了（部分）假阴性样本的有益和有害影响。$http://arxiv.org/pdf/2305.04474v1
IIITD-20K: Dense captioning for Text-Image ReID$  Text-to-Image (T2I) ReID has attracted a lot of attention in the recent past.CUHK-PEDES, RSTPReid and ICFG-PEDES are the three available benchmarks toevaluate T2I ReID methods. RSTPReid and ICFG-PEDES comprise of identities fromMSMT17 but due to limited number of unique persons, the diversity is limited.On the other hand, CUHK-PEDES comprises of 13,003 identities but has relativelyshorter text description on average. Further, these datasets are captured in arestricted environment with limited number of cameras. In order to furtherdiversify the identities and provide dense captions, we propose a novel datasetcalled IIITD-20K. IIITD-20K comprises of 20,000 unique identities captured inthe wild and provides a rich dataset for text-to-image ReID. With a minimum of26 words for a description, each image is densely captioned. We furthersynthetically generate images and fine-grained captions using Stable-diffusionand BLIP models trained on our dataset. We perform elaborate experiments usingstate-of-art text-to-image ReID models and vision-language pre-trained modelsand present a comprehensive analysis of the dataset. Our experiments alsoreveal that synthetically generated data leads to a substantial performanceimprovement in both same dataset as well as cross dataset settings. Our datasetis available at https://bit.ly/3pkA3Rj.$Specific domain keywords: Text-to-Image ReID, benchmark, synthetic data, IIITD-20K dataset, global feature embedding, attention-based Re-ID methods.$作者机构：IIITD-20K，印度的IIITD大学。$该文章介绍了一个新的数据集IIITD-20K，用于跨模态的文本图像ReID任务，并展示了如何使用该数据集进行实验。同时，文章还强调了IIITD-20K数据集与现有数据集的不同之处，包括更多的不同身份和更丰富的文本描述。$http://arxiv.org/pdf/2305.04497v1
Building Footprint Extraction with Graph Convolutional Network$  Building footprint information is an essential ingredient for 3-Dreconstruction of urban models. The automatic generation of building footprintsfrom satellite images presents a considerable challenge due to the complexityof building shapes. Recent developments in deep convolutional neural networks(DCNNs) have enabled accurate pixel-level labeling tasks. One central issueremains, which is the precise delineation of boundaries. Deep architecturesgenerally fail to produce fine-grained segmentation with accurate boundariesdue to progressive downsampling. In this work, we have proposed a end-to-endframework to overcome this issue, which uses the graph convolutional network(GCN) for building footprint extraction task. Our proposed frameworkoutperforms state-of-the-art methods.$Key domain-specific keywords of this article include building footprint extraction, deep convolutional neural networks, and graph convolutional networks. The article discusses the challenges of automatic generation of building footprints from satellite images due to the complexity of building shapes. The article proposes an end-to-end framework that uses the graph convolutional network (GCN) for the building footprint extraction task, which outperforms state-of-the-art methods. The article also discusses various deep learning architectures, such as ResNet and U-Net, and their applications in remote sensing tasks. Overall, the article focuses on the use of advanced deep learning methodologies for accurate and efficient building footprint extraction from satellite images.$"本文的作者机构为：
Yilei Shi1，Qinyu Li2，Xiaoxiang Zhu2,3
1 Technical University of Munich, Germany
2 Technical University of Munich, Germany
3 German Aerospace Center (DLR), Germany"$文章提出了一种基于图卷积网络的建筑物轮廓提取方法，可以解决深度卷积神经网络在精细边界分割方面的不足，并在建筑物轮廓提取任务上优于现有方法。$http://arxiv.org/pdf/2305.04499v1
Pedestrian Behavior Maps for Safety Advisories: CHAMP Framework and  Real-World Data Analysis$  It is critical for vehicles to prevent any collisions with pedestrians.Current methods for pedestrian collision prevention focus on integrating visualpedestrian detectors with Automatic Emergency Braking (AEB) systems which cantrigger warnings and apply brakes as a pedestrian enters a vehicle\'s path.Unfortunately, pedestrian-detection-based systems can be hindered in certainsituations such as night-time or when pedestrians are occluded. Our systemaddresses such issues using an online, map-based pedestrian detectionaggregation system where common pedestrian locations are learned after repeatedpasses of locations. Using a carefully collected and annotated dataset in LaJolla, CA, we demonstrate the system\'s ability to learn pedestrian zones andgenerate advisory notices when a vehicle is approaching a pedestrian despitechallenges like dark lighting or pedestrian occlusion. Using the number ofcorrect advisories, false advisories, and missed advisories to define precisionand recall performance metrics, we evaluate our system and discuss futurepositive effects with further data collection. We have made our code availableat https://github.com/s7desai/ped-mapping, and a video demonstration of theCHAMP system at https://youtu.be/dxeCrS_Gpkw.$Keywords: pedestrian safety, autonomous vehicles, high definition maps, advanced driver assistance systems, pedestrian detection, automatic emergency braking, precision, recall.$该文章的作者机构为加州大学圣地亚哥分校智能和安全汽车实验室（LISA），作者包括Ross Greer、Samveed Desai、Lulua Rakla、Akshay Gopalkrishnan、Afnan Alofi和Mohan Trivedi。$这篇文章介绍了一种在线地图式行人检测聚合系统（CHAMP），通过学习常见行人位置、解决光线暗、行人遮挡等问题，在车辆接近行人时发出警告，提高行人安全性能。作者还介绍了他们利用在加利福尼亚洛杉矶的样本数据对系统进行评估并讨论了未来的数据收集和应用。$http://arxiv.org/pdf/2305.04506v1
Multi-Temporal Lip-Audio Memory for Visual Speech Recognition$  Visual Speech Recognition (VSR) is a task to predict a sentence or word fromlip movements. Some works have been recently presented which use audio signalsto supplement visual information. However, existing methods utilize onlylimited information such as phoneme-level features and soft labels of AutomaticSpeech Recognition (ASR) networks. In this paper, we present a Multi-TemporalLip-Audio Memory (MTLAM) that makes the best use of audio signals to complementinsufficient information of lip movements. The proposed method is mainlycomposed of two parts: 1) MTLAM saves multi-temporal audio features producedfrom short- and long-term audio signals, and the MTLAM memorizes avisual-to-audio mapping to load stored multi-temporal audio features fromvisual features at the inference phase. 2) We design an audio temporal model toproduce multi-temporal audio features capturing the context of neighboringwords. In addition, to construct effective visual-to-audio mapping, the audiotemporal models can generate audio features time-aligned with visual features.Through extensive experiments, we validate the effectiveness of the MTLAMachieving state-of-the-art performances on two public VSR datasets.$Keywords: Visual Speech Recognition, Lip reading, Multi-Temporal Lip-Audio Memory, Memory Network, audio signals, short- and long-term audio signals, audio temporal model, visual-to-audio mapping, context information.$本文的作者机构为韩国KAIST电气工程学院。其中，作者Jeong Hun Yeo、Minsu Kim和Yong Man Roy的电子邮件地址分别为fsedne246、ms.k和ymrog@kaist.ac.kr。$本文介绍一种多时间唇音记忆的方法，利用音频信号来补充视觉信息，提高视觉语音识别的精度，并在两个公共数据集上取得了最先进的性能。$http://arxiv.org/pdf/2305.04542v1
SwinDocSegmenter: An End-to-End Unified Domain Adaptive Transformer for  Document Instance Segmentation$  Instance-level segmentation of documents consists in assigning a class-awareand instance-aware label to each pixel of the image. It is a key step indocument parsing for their understanding. In this paper, we present a unifiedtransformer encoder-decoder architecture for en-to-end instance segmentation ofcomplex layouts in document images. The method adapts a contrastive trainingwith a mixed query selection for anchor initialization in the decoder. Lateron, it performs a dot product between the obtained query embeddings and thepixel embedding map (coming from the encoder) for semantic reasoning. Extensiveexperimentation on competitive benchmarks like PubLayNet, PRIMA, HistoricalJapanese (HJ), and TableBank demonstrate that our model with SwinL backboneachieves better segmentation performance than the existing state-of-the-artapproaches with the average precision of \\textbf{93.72}, \\textbf{54.39},\\textbf{84.65} and \\textbf{98.04} respectively under one billion parameters.The code is made publicly available at:\\href{https://github.com/ayanban011/SwinDocSegmenter}{github.com/ayanban011/SwinDocSegmenter}$Document Layout Analysis, Instance-Level Segmentation, Swin Transformer, Contrastive Learning.$作者机构：1. 西班牙阿图纳自治大学（Universitat Autonoma de Barcelona）计算机视觉中心（Computer Vision Center）和计算机科学系（Computer Science Department）；2. 印度统计研究所（Indian Statistical Institute）计算机视觉和模式识别（CVPR）单元。$本文提出了一种面向文档实例分割的统一领域自适应变压器SwinDocSegmenter，并表明该方法在PubLayNet、PRIMA、Historical Japanese (HJ)、TableBank等竞争性基准测试中均优于现有最先进的方法，在10亿个参数下的平均精度分别为93.72、54.39、84.65和98.04。$http://arxiv.org/pdf/2305.04609v1
Riesz networks: scale invariant neural networks in a single forward pass$  Scale invariance of an algorithm refers to its ability to treat objectsequally independently of their size. For neural networks, scale invariance istypically achieved by data augmentation. However, when presented with a scalefar outside the range covered by the training set, neural networks may fail togeneralize.  Here, we introduce the Riesz network, a novel scale invariant neural network.Instead of standard 2d or 3d convolutions for combining spatial information,the Riesz network is based on the Riesz transform which is a scale equivariantoperation. As a consequence, this network naturally generalizes to unseen oreven arbitrary scales in a single forward pass. As an application example, weconsider detecting and segmenting cracks in tomographic images of concrete. Inthis context, \'scale\' refers to the crack thickness which may vary stronglyeven within the same sample. To prove its scale invariance, the Riesz networkis trained on one fixed crack width. We then validate its performance insegmenting simulated and real tomographic images featuring a wide range ofcrack widths. An additional experiment is carried out on the MNIST Large Scaledata set.$$$$http://arxiv.org/pdf/2305.04665v1
ElasticHash: Semantic Image Similarity Search by Deep Hashing with  Elasticsearch$  We present ElasticHash, a novel approach for high-quality, efficient, andlarge-scale semantic image similarity search. It is based on a deep hashingmodel to learn hash codes for fine-grained image similarity search in naturalimages and a two-stage method for efficiently searching binary hash codes usingElasticsearch (ES). In the first stage, a coarse search based on short hashcodes is performed using multi-index hashing and ES terms lookup of neighboringhash codes. In the second stage, the list of results is re-ranked by computingthe Hamming distance on long hash codes. We evaluate the retrieval performanceof \\textit{ElasticHash} for more than 120,000 query images on about 6.9 milliondatabase images of the OpenImages data set. The results show that our approachachieves high-quality retrieval results and low search latencies.$Key words: deep hashing, similarity search, Elasticsearch, semantic image similarity, natural images, multi-index hashing, retrieval performance, query-by-content, feature representations, convolutional neural networks, query images.$本文作者机构：德国马尔堡大学数学和计算机科学系，Nikolaus Korfhage、Markus Muhling、Bernd Freisleben。邮箱：fkorfhage、muehling、freisleb g@informatik.uni-marburg.de。$本文提出了一种基于深度哈希模型结合Elasticsearch的高质量、高效、大规模图像语义相似性检索方法，通过短哈希码进行粗略搜索，再通过长哈希码进行重新排序，以提高检索性能。文章通过对超过120,000个查询图像和约6.9百万个OpenImages数据集中的数据库图像进行评估，证明了该方法具有高质量的检索结果和低的搜索时延。$http://arxiv.org/pdf/2305.04710v1
Strategy for Rapid Diabetic Retinopathy Exposure Based on Enhanced  Feature Extraction Processing$  In the modern world, one of the most severe eye infections brought on bydiabetes is known as diabetic retinopathy, which will result in retinal damage,and, thus, lead to blindness. Diabetic retinopathy can be well treated withearly diagnosis. Retinal fundus images of humans are used to screen for lesionsin the retina. However, detecting DR in the early stages is challenging due tothe minimal symptoms. Furthermore, the occurrence of diseases linked tovascular anomalies brought on by DR aids in diagnosing the condition.Nevertheless, the resources required for manually identifying the lesions arehigh. Similarly, training for Convolutional Neural Networks is moretime-consuming. This proposed research aims to improve diabetic retinopathydiagnosis by developing an enhanced deep learning model for timely DRidentification that is potentially more accurate than existing CNN-basedmodels. The proposed model will detect various lesions from retinal images inthe early stages. First, characteristics are retrieved from the retinal funduspicture and put into the EDLM for classification. For dimensionality reduction,EDLM is used. Additionally, the classification and feature extraction processesare optimized using the stochastic gradient descent optimizer. The EDLMeffectiveness is assessed on the KAG GLE dataset with 3459 retinal images, andresults are compared over VGG16, VGG19, RESNET18, RESNET34, and RESNET50.$$$$http://arxiv.org/pdf/2305.04724v1
Controllable Light Diffusion for Portraits$  We introduce light diffusion, a novel method to improve lighting inportraits, softening harsh shadows and specular highlights while preservingoverall scene illumination. Inspired by professional photographers\' diffusersand scrims, our method softens lighting given only a single portrait photo.Previous portrait relighting approaches focus on changing the entire lightingenvironment, removing shadows (ignoring strong specular highlights), orremoving shading entirely. In contrast, we propose a learning based method thatallows us to control the amount of light diffusion and apply it on in-the-wildportraits. Additionally, we design a method to synthetically generate plausibleexternal shadows with sub-surface scattering effects while conforming to theshape of the subject\'s face. Finally, we show how our approach can increase therobustness of higher level vision applications, such as albedo estimation,geometry estimation and semantic segmentation.$Controllable Light Diffusion, Portraits, Lighting, Diffusers, Scrims, Specular/Shadow maps, Albedo estimation, Geometry estimation, Semantic segmentation.$"作者机构：
1. Google Research
2. CTU in Prague, FEE
3. University of Washington"$这篇文章提出了一种控制肖像照片中光线扩散的方法，通过提取高光和阴影图来生成全面扩散的图像，并展示了如何逐渐增加光线扩散的过程。同时，这种方法可以提高视觉应用程序的鲁棒性。$http://arxiv.org/pdf/2305.04745v1
Toeplitz Neural Network for Sequence Modeling$  Sequence modeling has important applications in natural language processingand computer vision. Recently, the transformer-based models have shown strongperformance on various sequence modeling tasks, which rely on attention tocapture pairwise token relations, and position embedding to inject positionalinformation. While showing good performance, the transformer models areinefficient to scale to long input sequences, mainly due to the quadraticspace-time complexity of attention. To overcome this inefficiency, we proposeto model sequences with a relative position encoded Toeplitz matrix and use aToeplitz matrix-vector production trick to reduce the space-time complexity ofthe sequence modeling to log linear. A lightweight sub-network called relativeposition encoder is proposed to generate relative position coefficients with afixed budget of parameters, enabling the proposed Toeplitz neural network todeal with varying sequence lengths. In addition, despite being trained on512-token sequences, our model can extrapolate input sequence length up to 14Ktokens in inference with consistent performance. Extensive experiments onautoregressive and bidirectional language modeling, image modeling, and thechallenging Long-Range Arena benchmark show that our method achieves betterperformance than its competitors in most downstream tasks while beingsignificantly faster. The code is available athttps://github.com/OpenNLPLab/Tnn.$Keywords: sequence modeling, natural language processing, computer vision, transformer-based models, attention, position embedding, quadratic space-time complexity, Toeplitz matrix, relative position encoding, autoregressive language modeling, bidirectional language modeling, image modeling, Long-Range Arena benchmark.$作者机构：上海人工智能实验室，商汤研究院，澳大利亚国立大学，西北工业大学和香港大学。$本文提出了一种用Toeplitz矩阵进行序列建模的神经网络，通过使用相对位置编码和Toeplitz矩阵向量乘法技巧来降低序列建模的空间时间复杂度，从而在自回归和双向语言建模、图像建模等任务上取得了较好的表现。$http://arxiv.org/pdf/2305.04749v1
AvatarReX: Real-time Expressive Full-body Avatars$  We present AvatarReX, a new method for learning NeRF-based full-body avatarsfrom video data. The learnt avatar not only provides expressive control of thebody, hands and the face together, but also supports real-time animation andrendering. To this end, we propose a compositional avatar representation, wherethe body, hands and the face are separately modeled in a way that thestructural prior from parametric mesh templates is properly utilized withoutcompromising representation flexibility. Furthermore, we disentangle thegeometry and appearance for each part. With these technical designs, we proposea dedicated deferred rendering pipeline, which can be executed in real-timeframerate to synthesize high-quality free-view images. The disentanglement ofgeometry and appearance also allows us to design a two-pass training strategythat combines volume rendering and surface rendering for network training. Inthis way, patch-level supervision can be applied to force the network to learnsharp appearance details on the basis of geometry estimation. Overall, ourmethod enables automatic construction of expressive full-body avatars withreal-time rendering capability, and can generate photo-realistic images withdynamic details for novel body motions and facial expressions.$"- Avatar modeling
- Full-body avatars
- Real-time rendering
- Neural radiance fields
- Computer vision
- Rendering
- Photorealistic images"$作者机构：清华大学自动化系，中国，以及NNKosmos Technology公司，中国。$这篇文章介绍了一个名为AvatarReX的方法，可以从视频数据中学习NeRF-based的全身化身，提供全方位的身体姿势、手部姿势和面部表情控制，并支持实时动画和渲染。并介绍了一些技术实现，包括身体、手和面部的组合式化身表示、几何和外观的分离等，并提出了一种延迟渲染管道，可以以实时帧速率执行，以合成高质量的自由视图图像。最终实现了自动构建具有实时渲染能力的全身化身，并且可以为新的运动和面部表情生成带有动态细节的照片般逼真的图像。$http://arxiv.org/pdf/2305.04789v1
MultiModal-GPT: A Vision and Language Model for Dialogue with Humans$  We present a vision and language model named MultiModal-GPT to conductmulti-round dialogue with humans. MultiModal-GPT can follow variousinstructions from humans, such as generating a detailed caption, counting thenumber of interested objects, and answering general questions from users.MultiModal-GPT is parameter-efficiently fine-tuned from OpenFlamingo, withLow-rank Adapter (LoRA) added both in the cross-attention part and theself-attention part of the language model. We first construct instructiontemplates with vision and language data for multi-modality instruction tuningto make the model understand and follow human instructions. We find the qualityof training data is vital for the dialogue performance, where few datacontaining short answers can lead the model to respond shortly to anyinstructions. To further enhance the ability to chat with humans of theMultiModal-GPT, we utilize language-only instruction-following data to trainthe MultiModal-GPT jointly. The joint training of language-only andvisual-language instructions with the \\emph{same} instruction templateeffectively improves dialogue performance. Various demos show the ability ofcontinuous dialogue of MultiModal-GPT with humans. Code and demo are athttps://github.com/open-mmlab/Multimodal-GPT$Vision and language, Multi-modal dialogues, GPT, Language model, Instruction following, Low-rank adapter, Fine-tuning, Self-attention, Multi-modality instruction tuning, Dialogue performance.$作者机构：1. 上海人工智能实验室；2.香港大学；3.天津大学电子与信息工程学院。$这篇论文介绍了一个名为MultiModal-GPT的多模态视觉语言模型，能够与人类进行多轮对话，并通过结合视觉和语言数据的指令模板来实现对多样化指令的理解和遵循。同时使用语言输入数据提高对话效果，并通过低秩适配器（LoRA）提高模型的效率。$http://arxiv.org/pdf/2305.04790v1
Compressed Video Quality Assessment for Super-Resolution: a Benchmark  and a Quality Metric$  We developed a super-resolution (SR) benchmark to analyze SR\'s capacity toupscale compressed videos. Our dataset employed video codecs based on fivecompression standards: H.264, H.265, H.266, AV1, and AVS3. We assessed 17state-ofthe-art SR models using our benchmark and evaluated their ability topreserve scene context and their susceptibility to compression artifacts. Toget an accurate perceptual ranking of SR models, we conducted a crowd-sourcedside-by-side comparison of their outputs. The benchmark is publicly availableathttps://videoprocessing.ai/benchmarks/super-resolutionfor-video-compression.html.We also analyzed benchmark results and developed anobjective-quality-assessment metric based on the current bestperformingobjective metrics. Our metric outperforms others, according to Spearmancorrelation with subjective scores for compressed video upscaling. It ispublicly available athttps://github.com/EvgeneyBogatyrev/super-resolution-metric.$"Key domain keywords:

1. Compressed video quality assessment
2. Super-resolution
3. Benchmark
4. Quality metric
5. Video processing
6. Dataset
7. Video codecs
8. Video quality metric
9. Objective quality assessment
10. Subjective evaluation
11. Neural networks
12. Bandwidth consumption
13. Perceptual quality
14. Compression standards."$作者机构：Lomonosov Moscow State University, MSU Institute for Artificial Intelligence$该研究旨在研究超分辨率技术在压缩视频中的应用，开发了一个基准数据集和一个评估模型质量的客观指标，并对SR算法进行了分析和评估，提供了有助于选择最佳算法的建议。$http://arxiv.org/pdf/2305.04844v1
Learning to Evaluate the Artness of AI-generated Images$  Assessing the artness of AI-generated images continues to be a challengewithin the realm of image generation. Most existing metrics cannot be used toperform instance-level and reference-free artness evaluation. This paperpresents ArtScore, a metric designed to evaluate the degree to which an imageresembles authentic artworks by artists (or conversely photographs), therebyoffering a novel approach to artness assessment. We first blend pre-trainedmodels for photo and artwork generation, resulting in a series of mixed models.Subsequently, we utilize these mixed models to generate images exhibitingvarying degrees of artness with pseudo-annotations. Each photorealistic imagehas a corresponding artistic counterpart and a series of interpolated imagesthat range from realistic to artistic. This dataset is then employed to train aneural network that learns to estimate quantized artness levels of arbitraryimages. Extensive experiments reveal that the artness levels predicted byArtScore align more closely with human artistic evaluation than existingevaluation metrics, such as Gram loss and ArtFID.$Keywords: image generation, artness evaluation, ArtScore, neural style transfer, generative adversarial networks, diffusion models, metrics, instance-level, reference-free, dataset, neural network training.$作者机构：罗彻斯特大学（University of Rochester），Junyu Chen, Jie An, Hanjia Lyu, Jiebo Luo。$本文提出了一种名为ArtScore的度量方法，用于评价人工智能生成图像的艺术性，通过混合训练预训练模型生成一系列混合模型来评估图像与真实艺术品之间的相似程度，训练出可量化评估任意图像艺术程度的神经网络模型。$http://arxiv.org/pdf/2305.04923v1
Fairness in Image Search: A Study of Occupational Stereotyping in Image  Retrieval and its Debiasing$"  Multi-modal search engines have experienced significant growth and widespreaduse in recent years, making them the second most common internet use. Whilesearch engine systems offer a range of services, the image search field hasrecently become a focal point in the information retrieval community, as theadage goes, ""a picture is worth a thousand words"". Although popular searchengines like Google excel at image search accuracy and agility, there is anongoing debate over whether their search results can be biased in terms ofgender, language, demographics, socio-cultural aspects, and stereotypes. Thispotential for bias can have a significant impact on individuals\' perceptionsand influence their perspectives.  In this paper, we present our study on bias and fairness in web search, witha focus on keyword-based image search. We first discuss several kinds of biasesthat exist in search systems and why it is important to mitigate them. Wenarrow down our study to assessing and mitigating occupational stereotypes inimage search, which is a prevalent fairness issue in image retrieval. For theassessment of stereotypes, we take gender as an indicator. We explore variousopen-source and proprietary APIs for gender identification from images. Withthese, we examine the extent of gender bias in top-tanked image search resultsobtained for several occupational keywords. To mitigate the bias, we thenpropose a fairness-aware re-ranking algorithm that optimizes (a) relevance ofthe search result with the keyword and (b) fairness w.r.t genders identified.We experiment on 100 top-ranked images obtained for 10 occupational keywordsand consider random re-ranking and re-ranking based on relevance as baselines.Our experimental results show that the fairness-aware re-ranking algorithmproduces rankings with better fairness scores and competitive relevance scoresthan the baselines."$Pre-print version, bias, fairness, image search, occupational stereotypes, gender, API, re-ranking algorithm.$作者机构：Swagatika Dash, Information School, University of Washington, Seattle, WA 98105, USA 和Yunhe Feng, Department of Computer Science, University of North Texas, Dallas, TX 75201, USA$本文研究了图像检索中职业刻板印象的偏见问题，并提出了一种公平性感知的重新排序算法以减少这种偏见。同时，文章讨论了多模态搜索引擎中存在的偏见，以及评估和减少偏见的挑战。$http://arxiv.org/pdf/2305.03881v1
NL-CS Net: Deep Learning with Non-Local Prior for Image Compressive  Sensing$"  Deep learning has been applied to compressive sensing (CS) of imagessuccessfully in recent years. However, existing network-based methods are oftentrained as the black box, in which the lack of prior knowledge is often thebottleneck for further performance improvement. To overcome this drawback, thispaper proposes a novel CS method using non-local prior which combines theinterpretability of the traditional optimization methods with the speed ofnetwork-based methods, called NL-CS Net. We unroll each phase from iteration ofthe augmented Lagrangian method solving non-local and sparse regularizedoptimization problem by a network. NL-CS Net is composed of the up-samplingmodule and the recovery module. In the up-sampling module, we use learnableup-sampling matrix instead of a predefined one. In the recovery module,patch-wise non-local network is employed to capture long-range featurecorrespondences. Important parameters involved (e.g. sampling matrix, nonlineartransforms, shrinkage thresholds, step size, $etc.$) are learned end-to-end,rather than hand-crafted. Furthermore, to facilitate practical implementation,orthogonal and binary constraints on the sampling matrix are simultaneouslyadopted. Extensive experiments on natural images and magnetic resonance imaging(MRI) demonstrate that the proposed method outperforms the state-of-the-artmethods while maintaining great interpretability and speed."$Keywords: compressive sensing, deep learning, non-local prior, augmented Lagrangian method, up-sampling, recovery module, patch-wise non-local network, end-to-end learning, orthogonal and binary constraints.$作者机构：中国东北大学医学与生物信息工程学院，美国史蒂文斯理工学院电气与计算机工程系，中国教育部智能医学计算重点实验室。$本文提出了一种基于非局部先验的图像压缩感知方法NL-CS Net，利用深度学习技术将传统优化技术的可解释性和网络方法的速度相结合，提高了性能表现。$http://arxiv.org/pdf/2305.03899v1
HateMM: A Multi-Modal Dataset for Hate Video Classification$  Hate speech has become one of the most significant issues in modern society,having implications in both the online and the offline world. Due to this, hatespeech research has recently gained a lot of traction. However, most of thework has primarily focused on text media with relatively little work on imagesand even lesser on videos. Thus, early stage automated video moderationtechniques are needed to handle the videos that are being uploaded to keep theplatform safe and healthy. With a view to detect and remove hateful contentfrom the video sharing platforms, our work focuses on hate video detectionusing multi-modalities. To this end, we curate ~43 hours of videos fromBitChute and manually annotate them as hate or non-hate, along with the framespans which could explain the labelling decision. To collect the relevantvideos we harnessed search keywords from hate lexicons. We observe various cuesin images and audio of hateful videos. Further, we build deep learningmulti-modal models to classify the hate videos and observe that using all themodalities of the videos improves the overall hate speech detection performance(accuracy=0.798, macro F1-score=0.790) by ~5.7% compared to the best uni-modalmodel in terms of macro F1 score. In summary, our work takes the first steptoward understanding and modeling hateful videos on video hosting platformssuch as BitChute.$Keywords: hate speech, video moderation, multi-modal dataset, hate video classification, deep learning models, search keywords, hate lexicons, image and audio cues, BitChute platform, community guidelines, Alt-Tech platforms, hate speech detection, machine learning algorithms, hate speech in videos, multi-frame video processing, speech processing signals.$本文的作者机构为：印度理工学院（IIT），加尔各策普尔（Kharagpur）的Mithun Das、Rohit Raj、Punyajoy Saha、Binny Mathew和Animesh Mukherjee，以及印度微软公司（Microsoft, India）的Manish Gupta。$本文介绍了一个多模式数据集HateMM，旨在用多模式的方式来识别和辨别视频中的仇恨言论，帮助平台识别和清除仇恨内容，借此保持平台的安全和健康。对于当前关注文本媒体的仇恨言论研究很多，而对于图像和视频的研究相对较少，因此本文尝试将多个模态应用到视频分类中，提高了分类的性能。$http://arxiv.org/pdf/2305.03915v1
Beyond the Model: Data Pre-processing Attack to Deep Learning Models in  Android Apps$  The increasing popularity of deep learning (DL) models and the advantages ofcomputing, including low latency and bandwidth savings on smartphones, have ledto the emergence of intelligent mobile applications, also known as DL apps, inrecent years. However, this technological development has also given rise toseveral security concerns, including adversarial examples, model stealing, anddata poisoning issues. Existing works on attacks and countermeasures foron-device DL models have primarily focused on the models themselves. However,scant attention has been paid to the impact of data processing disturbance onthe model inference. This knowledge disparity highlights the need foradditional research to fully comprehend and address security issues related todata processing for on-device models. In this paper, we introduce a dataprocessing-based attacks against real-world DL apps. In particular, our attackcould influence the performance and latency of the model without affecting theoperation of a DL app. To demonstrate the effectiveness of our attack, we carryout an empirical study on 517 real-world DL apps collected from Google Play.Among 320 apps utilizing MLkit, we find that 81.56\\% of them can besuccessfully attacked.  The results emphasize the importance of DL app developers being aware of andtaking actions to secure on-device models from the perspective of dataprocessing.$Keywords: deep learning, on-device models, data processing, security concerns, adversarial examples, model stealing, data poisoning, attacks, countermeasures, DL app developers, performance, latency, empirical study, MLkit, mobile apps, artificial intelligence features, cloud models, inference task, privacy issues, customization, robustness, black-box attack, white-box attack.$"作者机构： 
Ye Sang - 莫纳什大学，墨尔本，维多利亚州，澳大利亚
Yujin Huang - 莫纳什大学，墨尔本，维多利亚州，澳大利亚
Shuo Huang - 西北工业大学，西安，中国；莫纳什大学，墨尔本，维多利亚州，澳大利亚
Helei Cui - 西北工业大学，西安，中国"$这篇文章研究了基于数据预处理攻击的深度学习模型在安卓应用程序中的影响，重点在于对模型推理的影响而非仅仅是模型本身，并发现在实际情况中，81.56%的使用MLkit的应用程序容易受到攻击。文章强调了开发人员在保障设备安全方面，应该重视并采取行动以保证数据处理的安全性。$http://arxiv.org/pdf/2305.03963v1
A Sea-Land Clutter Classification Framework for Over-the-Horizon-Radar  Based on Weighted Loss Semi-supervised GAN$  Deep convolutional neural network has made great achievements in sea-landclutter classification for over-the-horizon-radar (OTHR). The premise is that alarge number of labeled training samples must be provided for a sea-landclutter classifier. In practical engineering applications, it is relativelyeasy to obtain label-free sea-land clutter samples. However, the labelingprocess is extremely cumbersome and requires expertise in the field of OTHR. Tosolve this problem, we propose an improved generative adversarial network,namely weighted loss semi-supervised generative adversarial network (WL-SSGAN).Specifically, we propose a joint feature matching loss by weighting the middlelayer features of the discriminator of semi-supervised generative adversarialnetwork. Furthermore, we propose the weighted loss of WL-SSGAN by linearlyweighting standard adversarial loss and joint feature matching loss. Thesemi-supervised classification performance of WL-SSGAN is evaluated on asea-land clutter dataset. The experimental results show that WL-SSGAN canimprove the performance of the fully supervised classifier with only a smallnumber of labeled samples by utilizing a large number of unlabeled sea-landclutter samples. Further, the proposed weighted loss is superior to both theadversarial loss and the feature matching loss. Additionally, we compareWL-SSGAN with conventional semi-supervised classification methods anddemonstrate that WL-SSGAN achieves the highest classification accuracy.$海陆杂波分类，超视距雷达，深度学习，半监督学习，生成对抗网络，特征匹配$作者机构：西北工业大学自动化学院, 教育部信息融合技术重点实验室、南京电子技术研究所、天虹联合实验室。$该文章提出了一种基于加权损失半监督生成对抗网络的海陆杂波分类框架，用于未来的超视距雷达应用。该框架可以在少量标记样本和大量无标记样本的情况下，提高海陆杂波分类的性能和准确性。$http://arxiv.org/pdf/2305.04021v1
Gradient Leakage Defense with Key-Lock Module for Federated Learning$  Federated Learning (FL) is a widely adopted privacy-preserving machinelearning approach where private data remains local, enabling securecomputations and the exchange of local model gradients between local clientsand third-party parameter servers. However, recent findings reveal that privacymay be compromised and sensitive information potentially recovered from sharedgradients. In this study, we offer detailed analysis and a novel perspective onunderstanding the gradient leakage problem. These theoretical works lead to anew gradient leakage defense technique that secures arbitrary modelarchitectures using a private key-lock module. Only the locked gradient istransmitted to the parameter server for global model aggregation. Our proposedlearning method is resistant to gradient leakage attacks, and the key-lockmodule is designed and trained to ensure that, without the private informationof the key-lock module: a) reconstructing private training data from the sharedgradient is infeasible; and b) the global model\'s inference performance issignificantly compromised. We discuss the theoretical underpinnings of whygradients can leak private information and provide theoretical proof of ourmethod\'s effectiveness. We conducted extensive empirical evaluations with atotal of forty-four models on several popular benchmarks, demonstrating therobustness of our proposed approach in both maintaining model performance anddefending against gradient leakage attacks.$Keywords: Federated Learning, Data Privacy, Gradient Leakage Defense, Distributed Learning, Deep Neural Network, Key-Lock Module, Private Key, Model Performance.$该篇文章的作者机构为：Hanchi Ren（IEEE学生会员）、Jingjing Deng（IEEE会员）、Xianghua Xie（IEEE资深会员）、Xiaoke Ma与Jianfeng Ma（IEEE会员）。其中，Hanchi Ren和Xianghua Xie就职于英国斯旺西大学计算机科学系，Jingjing Deng就职于英国杜伦大学计算机科学系，而Xiaoke Ma和Jianfeng Ma则就职于中国西安电子科技大学计算机学院和网络空间安全学院。$本文提出了一种新的联邦学习隐私保护技术——FedKL，旨在解决目前梯度泄露攻击带来的隐私泄露问题，通过在模型中添加私密钥匙锁模块，将梯度加锁，只有加锁的梯度被发送到参数服务器进行全局模型聚合，从而保证了数据私密性，同时保持模型性能。$http://arxiv.org/pdf/2305.04095v1
Transformer-Based Hierarchical Clustering for Brain Network Analysis$  Brain networks, graphical models such as those constructed from MRI, havebeen widely used in pathological prediction and analysis of brain functions.Within the complex brain system, differences in neuronal connection strengthsparcellate the brain into various functional modules (network communities),which are critical for brain analysis. However, identifying such communitieswithin the brain has been a nontrivial issue due to the complexity of neuronalinteractions. In this work, we propose a novel interpretable transformer-basedmodel for joint hierarchical cluster identification and brain networkclassification. Extensive experimental results on real-world brain networkdatasets show that with the help of hierarchical clustering, the model achievesincreased accuracy and reduced runtime complexity while providing plausibleinsight into the functional organization of brain regions. The implementationis available at https://github.com/DDVD233/THC.$Keywords: Brain networks, MRI, hierarchical clustering, transformer-based model, community detection, graph neural networks, machine learning, neural imaging analysis.$作者机构：Wei Dai, Hejie Cui, Xuan Kan, Ying Guo, Sanne van Rooijz, Carl Yang，分别隶属于斯坦福大学和埃默里大学。$本文提出了一种基于Transformer的层次聚类方法，针对脑网络分析进行了优化，并在实际数据集上进行了广泛的实验验证其准确性和可解释性。该模型通过将聚类层和Transformer编码器协同训练，以连续调整多层次聚类分配，从而能够更好地捕获脑区之间的高阶连接模式。$http://arxiv.org/pdf/2305.04142v1
X-LLM: Bootstrapping Advanced Large Language Models by Treating  Multi-Modalities as Foreign Languages$  Large language models (LLMs) have demonstrated remarkable language abilities.GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilitiesbeyond previous visual language models. We attribute this to the use of moreadvanced LLMs compared with previous multimodal models. Unfortunately, themodel architecture and training strategies of GPT-4 are unknown. To endow LLMswith multimodal capabilities, we propose X-LLM, which converts Multi-modalities(images, speech, videos) into foreign languages using X2L interfaces and inputsthem into a large Language model (ChatGLM). Specifically, X-LLM aligns multiplefrozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X\'\'denotes multi-modalities such as image, speech, and videos, and ``L\'\' denoteslanguages. X-LLM\'s training consists of three stages: (1) Converting MultimodalInformation: The first stage trains each X2L interface to align with itsrespective single-modal encoder separately to convert multimodal informationinto languages. (2) Aligning X2L representations with the LLM: single-modalencoders are aligned with the LLM through X2L interfaces independently. (3)Integrating multiple modalities: all single-modal encoders are aligned with theLLM through X2L interfaces to integrate multimodal capabilities into the LLM.Our experiments show that X-LLM demonstrates impressive multimodel chatabilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseenimages/instructions, and yields a 84.5\\% relative score compared with GPT-4 ona synthetic multimodal instruction-following dataset. And we also conductquantitative tests on using LLM for ASR and multimodal ASR, hoping to promotethe era of LLM-based speech recognition.$Keywords: large language models, multimodal language models, X2L interfaces, image captioning, visual question answering, visual dialog, video captioning, spoken dialogue, GPT-4, model architecture$该文章的作者机构为中国科学院自动化研究所、中国科学院未来技术学院和中国科学院大学。$本研究提出了一种名为X-LLM的模型，通过将多模态信息（如图像、语音、视频）转换为外语，并输入大型语言模型中进行训练，以增强其多模态能力。通过对比测试，结果显示X-LLM在合成多模态数据集方面的得分相对于GPT-4为84.5%，此外还探讨了将LLM应用于自动语音识别（ASR）和多模态ASR。$http://arxiv.org/pdf/2305.04160v1
Text-to-Image Diffusion Models can be Easily Backdoored through  Multimodal Data Poisoning$  With the help of conditioning mechanisms, the state-of-the-art diffusionmodels have achieved tremendous success in guided image generation,particularly in text-to-image synthesis. To gain a better understanding of thetraining process and potential risks of text-to-image synthesis, we perform asystematic investigation of backdoor attack on text-to-image diffusion modelsand propose BadT2I, a general multimodal backdoor attack framework that tamperswith image synthesis in diverse semantic levels. Specifically, we performbackdoor attacks on three levels of the vision semantics: Pixel-Backdoor,Object-Backdoor and Style-Backdoor. By utilizing a regularization loss, ourmethods efficiently inject backdoors into a large-scale text-to-image diffusionmodel while preserving its utility with benign inputs. We conduct empiricalexperiments on Stable Diffusion, the widely-used text-to-image diffusion model,demonstrating that the large-scale diffusion model can be easily backdooredwithin a few fine-tuning steps. We conduct additional experiments to explorethe impact of different types of textual triggers. Besides, we discuss thebackdoor persistence during further training, the findings of which provideinsights for the development of backdoor defense methods.$Text-to-Image Synthesis, Diffusion Models, Backdoor Attacks, Multimodal Data Poisoning, Pixel-Backdoor, Object-Backdoor, Style-Backdoor.$文章作者机构：北京大学软件与微电子学院、清华大学计算机科学与技术系、北京盛数科技有限公司。$这篇文章揭示了文本到图像扩散模型易受多模态数据污染反向攻击的风险，提出了BadT2I框架，并进行了三种级别的图像语义背门攻击实验。$http://arxiv.org/pdf/2305.04175v1
Bi-Mapper: Holistic BEV Semantic Mapping for Autonomous Driving$  A semantic map of the road scene, covering fundamental road elements, is anessential ingredient in autonomous driving systems. It provides importantperception foundations for positioning and planning when rendered in theBird\'s-Eye-View (BEV). Currently, the prior knowledge of hypothetical depth canguide the learning of translating front perspective views into BEV directlywith the help of calibration parameters. However, it suffers from geometricdistortions in the representation of distant objects. In addition, anotherstream of methods without prior knowledge can learn the transformation betweenfront perspective views and BEV implicitly with a global view. Considering thatthe fusion of different learning methods may bring surprising beneficialeffects, we propose a Bi-Mapper framework for top-down road-scene semanticunderstanding, which incorporates a global view and local prior knowledge. Toenhance reliable interaction between them, an asynchronous mutual learningstrategy is proposed. At the same time, an Across-Space Loss (ASL) is designedto mitigate the negative impact of geometric distortions. Extensive results onnuScenes and Cam2BEV datasets verify the consistent effectiveness of eachmodule in the proposed Bi-Mapper framework. Compared with exiting road mappingnetworks, the proposed Bi-Mapper achieves 5.0 higher IoU on the nuScenesdataset. Moreover, we verify the generalization performance of Bi-Mapper in areal-world driving scenario. Code will be available athttps://github.com/lynn-yu/Bi-Mapper.$Autonomous driving, semantic mapping, Bird's-Eye-View (BEV), depth estimation, inverse perspective mapping (IPM), deep learning, view-transformer learning, local-self view (LV), global-cross view (GV), mutual learning strategy, Across-Space Loss (ASL).$文章作者机构： 湖南大学机器人研究院，浙江大学现代光学仪器国家重点实验室，卡尔斯鲁厄理工学院人类与机器智能研究所，牛津大学工程科学系，湖南大学计算机科学与电子工程学院$本文介绍了一种面向自动驾驶的全局语义地图构建方法——Bi-Mapper框架，旨在从前置摄像头拍摄的场景中构建包括车道、人行横道等基本道路元素的高质量的Bird’s-Eye-View（BEV）地图。该框架同时考虑了利用IPM算法进行的局部自监督训练和利用前景转换器进行的全局交叉训练的产生的相互补充效果。并且，还设计了Across-Space Loss（ASL）来缓解相机取景角度可能导致的地图中远距离对象无法清晰表示的问题。$http://arxiv.org/pdf/2305.04205v1
AdaptiveClick: Clicks-aware Transformer with Adaptive Focal Loss for  Interactive Image Segmentation$  Interactive Image Segmentation (IIS) has emerged as a promising technique fordecreasing annotation time. Substantial progress has been made in pre- andpost-processing for IIS, but the critical issue of interaction ambiguitynotably hindering segmentation quality, has been under-researched. To addressthis, we introduce AdaptiveClick -- a clicks-aware transformer incorporating anadaptive focal loss, which tackles annotation inconsistencies with tools formask- and pixel-level ambiguity resolution. To the best of our knowledge,AdaptiveClick is the first transformer-based, mask-adaptive segmentationframework for IIS. The key ingredient of our method is the Clicks-awareMask-adaptive Transformer Decoder (CAMD), which enhances the interactionbetween clicks and image features. Additionally, AdaptiveClick enablespixel-adaptive differentiation of hard and easy samples in the decision space,independent of their varying distributions. This is primarily achieved byoptimizing a generalized Adaptive Focal Loss (AFL) with a theoreticalguarantee, where two adaptive coefficients control the ratio of gradient valuesfor hard and easy pixels. Our analysis reveals that the commonly used Focal andBCE losses can be considered special cases of the proposed AFL loss. With aplain ViT backbone, extensive experimental results on nine datasets demonstratethe superiority of AdaptiveClick compared to state-of-the-art methods. Codewill be publicly available at https://github.com/lab206/AdaptiveClick.$Key areas of this article include Interactive Image Segmentation (IIS), Clicks-aware transformer, Adaptive Focal Loss, interaction ambiguity, mask-adaptive segmentation, vision transformers, and machine learning loss optimization. The authors propose a new technique for interactive image segmentation using AdaptiveClick, which incorporates an adaptive focal loss and click-aware transformer to tackle annotation inconsistencies and interaction ambiguity. The proposed method is the first transformer-based, mask-adaptive segmentation framework for IIS, which enhances interaction between clicks and image features, enables pixel-adaptive differentiation and optimizes a generalized Adaptive Focal Loss with a theoretical guarantee. The article also includes a comparative analysis of the proposed method with state-of-the-art techniques on nine datasets, demonstrating its superiority.$作者机构：Jiacheng Lin, Jiajun Chen, Kailun Yang, Alina Roitberg, Siyu Li, Zhiyong Li, and Shutao Li，分别隶属于中国湖南大学的计算机科学与电子工程学院、机器人学院和电气信息学院以及人工智能和视觉认知湖南省重点实验室，以及德国卡尔斯鲁厄理工学院人类和机器人技术研究所。$本文介绍了AdaptiveClick，一种基于transformer的、具有自适应focal loss的交互式图像分割方法，能够解决标注不一致和像素级别歧义的问题。通过优化自适应focal loss来进行像素级别的难易样本区分，适应不同分布，实现更好的图像分割效果。$http://arxiv.org/pdf/2305.04276v1
Performance Gaps of Artificial Intelligence Models Screening Mammography  -- Towards Fair and Interpretable Models$  Purpose: To analyze the demographic and imaging characteristics associatedwith increased risk of failure for abnormality classification in screeningmammograms. Materials and Methods: This retrospective study used data from theEmory BrEast Imaging Dataset (EMBED) which includes mammograms from 115,931patients imaged at Emory University Healthcare between 2013 to 2020. Clinicaland imaging data includes Breast Imaging Reporting and Data System (BI-RADS)assessment, region of interest coordinates for abnormalities, imaging features,pathologic outcomes, and patient demographics. Multiple deep learning modelswere developed to distinguish between patches of abnormal tissue and randomlyselected patches of normal tissue from the screening mammograms. We assessedmodel performance overall and within subgroups defined by age, race, pathologicoutcome, and imaging characteristics to evaluate reasons formisclassifications. Results: On a test set size of 5,810 studies (13,390patches), a ResNet152V2 model trained to classify normal versus abnormal tissuepatches achieved an accuracy of 92.6% (95% CI = 92.0-93.2%), and area under thereceiver operative characteristics curve 0.975 (95% CI = 0.972-0.978). Imagingcharacteristics associated with higher misclassifications of images includehigher tissue densities (risk ratio [RR]=1.649; p=.010, BI-RADS density C andRR=2.026; p=.003, BI-RADS density D), and presence of architectural distortion(RR=1.026; p&lt;.001). Conclusion: Even though deep learning models forabnormality classification can perform well in screening mammography, wedemonstrate certain imaging features that result in worse model performance.This is the first such work to systematically evaluate breast abnormalityclassification by various subgroups and better-informed developers andend-users of population subgroups which are likely to experience biased modelperformance.$Keywords: artificial intelligence, screening mammography, performance gaps, deep learning, BI-RADS, abnormality classification, patient demographics, imaging characteristics, pathologic outcomes.$"作者机构：
1. Kennesaw State University数据科学与分析学院，3391 Town Point Dr NW，Kennesaw，GA 30144
2. Kennesaw State University信息技术系，1100 South Marietta Pkwy，Marietta，GA 30060
3. Emory University影像科学与放射学系，1364 E Clifton Rd NE，Atlanta，GA 30322
4. 华盛顿大学圣路易斯医学院计算成像研究中心，4525 Scott Avenue，St. Louis，MO 63110
5. Mayo Clinic Arizona放射科，13400 E Shea Blvd，Scottsdale，AZ 85259
6. 亚利桑那州立大学计算和增强智能学院，699 S Mill Ave，Tempe，AZ 85281
7. 约克大学电气工程和计算机科学系，4700 Keele St，Toronto，Ontario，Canada M3J 1P3"$这篇文章分析了筛查乳腺X线摄影中人工智能模型的性能差距，旨在开发更公平、可解释的模型，通过对患者的人口统计和影像特征进行分析，评估错误分类的原因。$http://arxiv.org/pdf/2305.04422v1
Breaking Through the Haze: An Advanced Non-Homogeneous Dehazing Method  based on Fast Fourier Convolution and ConvNeXt$  Haze usually leads to deteriorated images with low contrast, color shift andstructural distortion. We observe that many deep learning based models exhibitexceptional performance on removing homogeneous haze, but they usually fail toaddress the challenge of non-homogeneous dehazing. Two main factors account forthis situation. Firstly, due to the intricate and non uniform distribution ofdense haze, the recovery of structural and chromatic features with highfidelity is challenging, particularly in regions with heavy haze. Secondly, theexisting small scale datasets for non-homogeneous dehazing are inadequate tosupport reliable learning of feature mappings between hazy images and theircorresponding haze-free counterparts by convolutional neural network(CNN)-based models. To tackle these two challenges, we propose a novel twobranch network that leverages 2D discrete wavelete transform (DWT), fastFourier convolution (FFC) residual block and a pretrained ConvNeXt model.Specifically, in the DWT-FFC frequency branch, our model exploits DWT tocapture more high-frequency features. Moreover, by taking advantage of thelarge receptive field provided by FFC residual blocks, our model is able toeffectively explore global contextual information and produce images withbetter perceptual quality. In the prior knowledge branch, an ImageNetpretrained ConvNeXt as opposed to Res2Net is adopted. This enables our model tolearn more supplementary information and acquire a stronger generalizationability. The feasibility and effectiveness of the proposed method isdemonstrated via extensive experiments and ablation studies. The code isavailable at https://github.com/zhouh115/DWT-FFC.$Keywords: image dehazing, non-homogeneous dehazing, deep learning, convolutional neural network, Fourier convolution, Discrete Wavelet Transform, ConvNeXt.$作者机构：Han Zhou1, Wei Dong2, Yangyi Liu1, Jun Chen1 （1 McMaster University, 加拿大Hamilton，Department of Electrical and Computer Engineering；2 University of Alberta, 加拿大Edmonton，Department of Electrical and Computer Engineering）$该文章提出了一种基于快速傅里叶卷积和ConvNeXt的高级非均质去雾方法，利用2D离散小波变换（DWT）和预训练的ConvNeXt模型构建了一个新颖的两分支网络来解决既有的密集雾的分布和现有小型数据集对卷积神经网络（CNN）的影响。通过广泛的实验和消融研究验证了该方法的可行性和有效性。$http://arxiv.org/pdf/2305.04430v1
Robust Traffic Light Detection Using Salience-Sensitive Loss:  Computational Framework and Evaluations$  One of the most important tasks for ensuring safe autonomous driving systemsis accurately detecting road traffic lights and accurately determining how theyimpact the driver\'s actions. In various real-world driving situations, a scenemay have numerous traffic lights with varying levels of relevance to thedriver, and thus, distinguishing and detecting the lights that are relevant tothe driver and influence the driver\'s actions is a critical safety task. Thispaper proposes a traffic light detection model which focuses on this task byfirst defining salient lights as the lights that affect the driver\'s futuredecisions. We then use this salience property to construct the LAVA SalientLights Dataset, the first US traffic light dataset with an annotated salienceproperty. Subsequently, we train a Deformable DETR object detection transformermodel using Salience-Sensitive Focal Loss to emphasize stronger performance onsalient traffic lights, showing that a model trained with this loss functionhas stronger recall than one trained without.$Keywords: autonomous driving, traffic light detection, salience-sensitive loss, object detection, transformer model, machine learning, dataset, salient lights, auxiliary loss function, ADAS features, object salience.$作者机构：加州大学圣地亚哥分校智能与安全汽车实验室(Laboratory for Intelligent & Safe Automobiles (LISA))，作者包括Ross Greer、Akshay Gopalkrishnan、Jacob Landgren、Lulua Rakla、Anish Gopalan和Mohan Trivedi。$本文提出了一个交通信号灯检测模型，该模型通过定义显著信号灯来聚焦于影响驾驶员决策的灯光，使用Salience-Sensitive Focal Loss训练一个Deformable DETR目标检测变压器模型，以强调对显著交通信号灯的强大性能表现。同时，构建了第一个带注释显著性质的美国交通信号灯数据集LA V A Salient Lights Dataset。$http://arxiv.org/pdf/2305.04516v1
Latest Trends in Artificial Intelligence Technology: A Scoping Review$  Artificial intelligence is more ubiquitous in multiple domains. Smartphones,social media platforms, search engines, and autonomous vehicles are just a fewexamples of applications that utilize artificial intelligence technologies toenhance their performance. This study carries out a scoping review of thecurrent state-of-the-art artificial intelligence technologies following thePRISMA framework. The goal was to find the most advanced technologies used indifferent domains of artificial intelligence technology research. Threerecognized journals were used from artificial intelligence and machine learningdomain: Journal of Artificial Intelligence Research, Journal of MachineLearning Research, and Machine Learning, and articles published in 2022 wereobserved. Certain qualifications were laid for the technological solutions: thetechnology must be tested against comparable solutions, commonly approved orotherwise well justified datasets must be used while applying, and results mustshow improvements against comparable solutions. One of the most important partsof the technology development appeared to be how to process and exploit thedata gathered from multiple sources. The data can be highly unstructured andthe technological solution should be able to utilize the data with minimummanual work from humans. The results of this review indicate that creatinglabeled datasets is very laborious, and solutions exploiting unsupervised orsemi-supervised learning technologies are more and more researched. Thelearning algorithms should be able to be updated efficiently, and predictionsshould be interpretable. Using artificial intelligence technologies inreal-world applications, safety and explainable predictions are mandatory toconsider before mass adoption can occur.$Keywords: artificial intelligence, deep learning, machine learning, natural language processing, reinforcement learning, scoping review.$作者机构：芬兰JAMK应用科学大学信息技术研究所（Institute of Information Technology, JAMK University of Applied Sciences）。作者包括Teemu Niskanen和Olli Väisänen，联系人为Olli Väisänen（olli.vaananen@jamk.fi）。$本文对当前最先进的人工智能技术进行了概览，关注人工智能在各个领域的应用以及解决方案所用的数据和技术。文章通过对人工智能和机器学习领域的三个期刊以及2022年发表的文章进行观察和分析，总结出数据的处理和利用对于技术开发的重要性，以及实时应用中考虑安全性和可解释性预测的必要性。$http://arxiv.org/pdf/2305.04532v1
Development of a Vision System to Enhance the Reliability of the  Pick-and-Place Robot for Autonomous Testing of Camera Module used in  Smartphones$  Pick-and-place robots are commonly used in modern industrial manufacturing.For complex devices/parts like camera modules used in smartphones, whichcontain optical parts, electrical components and interfacing connectors, theplacement operation may not absolutely accurate, which may cause damage in thedevice under test during the mechanical movement to make good contact forelectrical functions inspection. In this paper, we proposed an effective visionsystem including hardware and algorithm to enhance the reliability of thepick-and-place robot for autonomous testing memory of camera modules. Withlimited hardware based on camera and raspberry PI and using simplify imageprocessing algorithm based on histogram information, the vision system canconfirm the presence of the camera modules in feeding tray and the placementaccuracy of the camera module in test socket. Through that, the system can workwith more flexibility and avoid damaging the device under test. The system wasexperimentally quantified through testing approximately 2000 camera modules ina stable light condition. Experimental results demonstrate that the systemachieves accuracy of more than 99.92%. With its simplicity and effectiveness,the proposed vision system can be considered as a useful solution for using inpick-and-place systems in industry.$camera modules testing, pick-and-place robot, computer vision, image processing, reliability, accuracy, autonomous testing.$"作者机构： 
Hoang -Anh Phan, Bao-Anh Hoang, Duy Nam Bui, An Nguyen Ngoc, Tung Thanh Bui, Dong Tran Huu Quoc and Van Nguyen Thi Thanh 
Research Center for Electronics and Telecommunications, VNU University of Engineering and Technology, Hanoi, Vietnam 
Faculty of Electronics Engineering, Posts and Telecommunications Institute of Technology, Hanoi, Vietnam 
Faculty of Information Technology, VNU University of Engineering and Technology, Hanoi, Vietnam"$本文提出了一个基于视觉系统的方法来提高自动化测试智能手机摄像头模块的抓取放置机器人的可靠性，旨在解决一个复杂组件摆放的精确性问题，该组件包含光学部分、电子部分和接口连接器。文章提出的视觉系统结合硬件和算法，能够识别照相模块位置并检测抓取和放置的精确性，达到99.92%的准确率，并具有简单高效的优点，适用于制造业领域的抓取放置系统。$http://arxiv.org/pdf/2305.04605v1
The Treachery of Images: Bayesian Scene Keypoints for Deep Policy  Learning in Robotic Manipulation$  In policy learning for robotic manipulation, sample efficiency is ofparamount importance. Thus, learning and extracting more compactrepresentations from camera observations is a promising avenue. However,current methods often assume full observability of the scene and struggle withscale invariance. In many tasks and settings, this assumption does not hold asobjects in the scene are often occluded or lie outside the field of view of thecamera, rendering the camera observation ambiguous with regard to theirlocation. To tackle this problem, we present BASK, a Bayesian approach totracking scale-invariant keypoints over time. Our approach successfullyresolves inherent ambiguities in images, enabling keypoint tracking onsymmetrical objects and occluded and out-of-view objects. We employ our methodto learn challenging multi-object robot manipulation tasks from wrist cameraobservations and demonstrate superior utility for policy learning compared toother representation learning techniques. Furthermore, we show outstandingrobustness towards disturbances such as clutter, occlusions, and noisy depthmeasurements, as well as generalization to unseen objects both in simulationand real-world robotic experiments.$Robotics, policy learning, sample efficiency, camera observations, Bayesian approach, scale-invariant keypoint tracking, symmetrical objects, occluded objects, out-of-view objects, representation learning, semantic encoder network, visual features, 3D localization hypotheses, Bayes filter.$的作者机构：德国弗莱堡大学计算机科学系、纽伦堡工艺大学工程系。$文章介绍了一种利用贝叶斯方法跟踪场景的关键点的方法，可以解决图像中存在的多义性和场景中物体遮挡等问题，并在机器人操作中获得较好的表现。$http://arxiv.org/pdf/2305.04718v1
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning$  The ability of deep neural networks to continually learn and adapt to asequence of tasks has remained challenging due to catastrophic forgetting ofpreviously learned tasks. Humans, on the other hand, have a remarkable abilityto acquire, assimilate, and transfer knowledge across tasks throughout theirlifetime without catastrophic forgetting. The versatility of the brain can beattributed to the rehearsal of abstract experiences through a complementarylearning system. However, representation rehearsal in vision transformers lacksdiversity, resulting in overfitting and consequently, performance dropssignificantly compared to raw image rehearsal. Therefore, we propose BiRT, anovel representation rehearsal-based continual learning approach using visiontransformers. Specifically, we introduce constructive noises at various stagesof the vision transformer and enforce consistency in predictions with respectto an exponential moving average of the working model. Our method providesconsistent performance gain over raw image and vanilla representation rehearsalon several challenging CL benchmarks, while being memory efficient and robustto natural and adversarial corruptions.$Key domain-specific keywords in this article are: continual learning, catastrophic forgetting, deep neural networks, rehearsal-based approaches, vision transformers, representation rehearsal, and natural/adversarial robustness. The article presents a novel approach called BiRT, which is a representation rehearsal-based continual learning approach that uses vision transformers to address the problem of catastrophic forgetting in deep neural networks. The article compares the proposed approach with other methods on several challenging CL benchmarks and demonstrates its memory efficiency and robustness to natural and adversarial corruptions.$作者机构：1NavInfo Europe高级研究实验室，荷兰；2埃因霍芬科技大学数学与计算机科学系，荷兰。$本文提出了一种基于生物启发的回放方法BiRT，利用视觉变换器进行表示重复，以解决深度神经网络在继续学习过程中的灾难性遗忘问题。通过在视觉变换器的各个阶段引入建设性噪声并对预测的一致性进行强制性约束，该方法在多个具有挑战性的继续学习基准测试中提供了持续的性能提升，同时具有内存效率和自然和敌对失真的鲁棒性。$http://arxiv.org/pdf/2305.04769v1
A Variational Perspective on Solving Inverse Problems with Diffusion  Models$  Diffusion models have emerged as a key pillar of foundation models in visualdomains. One of their critical applications is to universally solve differentdownstream inverse tasks via a single diffusion prior without re-training foreach task. Most inverse tasks can be formulated as inferring a posteriordistribution over data (e.g., a full image) given a measurement (e.g., a maskedimage). This is however challenging in diffusion models since the nonlinear anditerative nature of the diffusion process renders the posterior intractable. Tocope with this challenge, we propose a variational approach that by designseeks to approximate the true posterior distribution. We show that our approachnaturally leads to regularization by denoising diffusion process (RED-Diff)where denoisers at different timesteps concurrently impose different structuralconstraints over the image. To gauge the contribution of denoisers fromdifferent timesteps, we propose a weighting mechanism based onsignal-to-noise-ratio (SNR). Our approach provides a new variationalperspective for solving inverse problems with diffusion models, allowing us toformulate sampling as stochastic optimization, where one can simply applyoff-the-shelf solvers with lightweight iterates. Our experiments for imagerestoration tasks such as inpainting and superresolution demonstrate thestrengths of our method compared with state-of-the-art sampling-based diffusionmodels.$Domain-specific keywords: diffusion models, visual domains, inverse problems, variational inference, regularization, denoising diffusion process, signal-to-noise ratio, image restoration, inpainting, superresolution.$作者机构：NVIDIA Inc. (Morteza Mardani, Jiaming Song, Jan Kautz, Arash Vahdat)$本文提出了一种使用变分推断的方法来解决扩散模型的反问题，通过加权处理噪声比例的方式，该方法能够有效地进行图像修复和超分辨率等任务。$http://arxiv.org/pdf/2305.04391v1
