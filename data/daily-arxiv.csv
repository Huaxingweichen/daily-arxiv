title$summary$tag$affiliation$summary_zh$tag_zh$url
The MuSe 2023 Multimodal Sentiment Analysis Challenge: Mimicked  Emotions, Cross-Cultural Humour, and Personalisation$  The MuSe 2023 is a set of shared tasks addressing three differentcontemporary multimodal affect and sentiment analysis problems: In the MimickedEmotions Sub-Challenge (MuSe-Mimic), participants predict three continuousemotion targets. This sub-challenge utilises the Hume-Vidmimic datasetcomprising of user-generated videos. For the Cross-Cultural Humour DetectionSub-Challenge (MuSe-Humour), an extension of the Passau Spontaneous FootballCoach Humour (Passau-SFCH) dataset is provided. Participants predict thepresence of spontaneous humour in a cross-cultural setting. The PersonalisationSub-Challenge (MuSe-Personalisation) is based on the Ulm-Trier Social StressTest (Ulm-TSST) dataset, featuring recordings of subjects in a stressedsituation. Here, arousal and valence signals are to be predicted, whereas partsof the test labels are made available in order to facilitate personalisation.MuSe 2023 seeks to bring together a broad audience from different researchcommunities such as audio-visual emotion recognition, natural languageprocessing, signal processing, and health informatics. In this baseline paper,we introduce the datasets, sub-challenges, and provided feature sets. As acompetitive baseline system, a Gated Recurrent Unit (GRU)-Recurrent NeuralNetwork (RNN) is employed. On the respective sub-challenges\' test datasets, itachieves a mean (across three continuous intensity targets) Pearson\'sCorrelation Coefficient of .4727 for MuSe-Mimic, an Area Under the Curve (AUC)value of .8310 for MuSe-Humor and Concordance Correlation Coefficient (CCC)values of .7482 for arousal and .7827 for valence in the MuSe-Personalisationsub-challenge.$Multimodal Sentiment Analysis; Affective Computing; Humour Detection; Emotion Recognition; Multimodal Fusion; Challenge; Benchmark$EIHW, University of Augsburg、Hume AI、University of Passau、University of Ulm、Nanyang Technological University、GLAM, Imperial College London$模仿情绪、跨文化幽默和个性化。本文提供了基线系统的算法，该算法在测试数据集上实现了很好的表现，并旨在将来自不同领域的研究人员聚集在一起。$多模态情感分析、情感识别、幽默检测、个性化、基准测试$http://arxiv.org/pdf/2305.03369v1
Out-of-Domain Intent Detection Considering Multi-turn Dialogue Contexts$"  Out-of-Domain (OOD) intent detection is vital for practical dialogue systems,and it usually requires considering multi-turn dialogue contexts. However, mostprevious OOD intent detection approaches are limited to single dialogue turns.In this paper, we introduce a context-aware OOD intent detection (Caro)framework to model multi-turn contexts in OOD intent detection tasks.Specifically, we follow the information bottleneck principle to extract robustrepresentations from multi-turn dialogue contexts. Two different views areconstructed for each input sample and the superfluous information not relatedto intent detection is removed using a multi-view information bottleneck loss.Moreover, we also explore utilizing unlabeled data in Caro. A two-stagetraining process is introduced to mine OOD samples from these unlabeled data,and these OOD samples are used to train the resulting model with abootstrapping approach. Comprehensive experiments demonstrate that Caroestablishes state-of-the-art performances on multi-turn OOD detection tasks byimproving the F1-OOD score of over $29\\%$ compared to the previous best method."$Out-of-Domain Intent Detection Considering Multi-turn Dialogue Contexts（在考虑多轮对话上下文的超出领域意图检测）介绍了一种上下文感知的超出领域意图检测(Caro)框架，以在ODD意图检测任务中模拟多轮对话上下文。实验表明，Caro在多轮OOD检测任务中实现了最先进的表现，相对于先前最佳方法的F1-OOD得分提高了超过29％。$Alibaba Group$$$http://arxiv.org/pdf/2305.03237v1
From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser  for Complex Question Answering over Knowledge Base$  Parsing questions into executable logical forms has showed impressive resultsfor knowledge-base question answering (KBQA). However, complex KBQA is a morechallenging task that requires to perform complex multi-step reasoning.Recently, a new semantic parser called KoPL has been proposed to explicitlymodel the reasoning processes, which achieved the state-of-the-art on complexKBQA. In this paper, we further explore how to unlock the reasoning ability ofsemantic parsers by a simple proposed parse-execute-refine paradigm. We refineand improve the KoPL parser by demonstrating the executed intermediatereasoning steps to the KBQA model. We show that such simple strategy cansignificantly improve the ability of complex reasoning. Specifically, wepropose three components: a parsing stage, an execution stage and a refinementstage, to enhance the ability of complex reasoning. The parser uses the KoPL togenerate the transparent logical forms. Then, the execution stage aligns andexecutes the logical forms over knowledge base to obtain intermediate reasoningprocesses. Finally, the intermediate step-by-step reasoning processes aredemonstrated to the KBQA model in the refinement stage. With the explicitreasoning processes, it is much easier to answer the complex questions.Experiments on benchmark dataset shows that the proposed PER-KBQA performssignificantly better than the stage-of-the-art baselines on the complex KBQA.$knowledge-base question answering, semantic parsing, complex reasoning$Sun Yat-Sen University$解析阶段，执行阶段和优化阶段，以增强语义解析器的复杂推理能力。最后，作者在基准数据集上进行了实验，表明所提出的解析-执行-优化（Parse-Execute-Reﬁne）模型相对于目前的基线方法在复杂KBQA上有显著的提高。$知识库问答，语义解析，复杂推理$http://arxiv.org/pdf/2305.03356v1
Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT  models$  We propose Retrieval Augmented Generation (RAG) as an approach for automatedradiology report writing that leverages multimodally aligned embeddings from acontrastively pretrained vision language model for retrieval of relevantcandidate radiology text for an input radiology image and a general domaingenerative model like OpenAI text-davinci-003, gpt-3.5-turbo and gpt-4 forreport generation using the relevant radiology text retrieved. This approachkeeps hallucinated generations under check and provides capabilities togenerate report content in the format we desire leveraging the instructionfollowing capabilities of these generative models. Our approach achieves betterclinical metrics with a BERTScore of 0.2865 ({\\Delta}+ 25.88%) and Semb scoreof 0.4026 ({\\Delta}+ 6.31%). Our approach can be broadly relevant for differentclinical settings as it allows to augment the automated radiology reportgeneration process with content relevant for that setting while also having theability to inject user intents and requirements in the prompts as part of thereport generation process to modulate the content and format of the generatedreports as applicable for that clinical setting.$English keywords: Automated Radiology Report Generation, multimodal embeddings, OpenAI GPT models, Retrieval Augmented Generation $Author's affiliation: Department of Computer Science, Bharathidasan University, Trichy, India; AI for Digital Health and Imaging, Indian Institute of Science, Bengaluru, India; Microsoft Research, Bengaluru, India; Databricks Inc., Bengaluru, India$本研究提出了基于多模态嵌入的检索辅助报告生成方法，利用经过对比预训练的视觉语言模型中与输入放射学图像对齐的嵌入，检索相关的候选放射学文本，然后使用像OpenAI文本-davinci-003、gpt-3.5-turbo和gpt-4这样的通用领域生成模型生成报告。该方法能够控制虚构生成而提供报告内容，以期望的格式生成，利用这些生成模型的指令遵循能力。本研究通过检索辅助报告生成方法，实现了更好的临床度量，BERTScore为0.2865（+25.88%），Sembscore为0.4026（+6.31%）。本方法可用于不同临床环境，可增强自动化放射学报告生成过程，使其具有与该环境相关的内容。同时，还能将用户意图和要求注入到提示中，作为报告生成过程的一部分来调节生成报告的内容和格式。$中文关键词: 自动化放射学报告撰写，多模式嵌入，OpenAI GPT模型，检索辅助报告生成$http://arxiv.org/pdf/2305.03660v1
LOGEN: Few-shot Logical Knowledge-Conditioned Text Generation with  Self-training$  Natural language generation from structured data mainly focuses onsurface-level descriptions, suffering from uncontrollable content selection andlow fidelity. Previous works leverage logical forms to facilitate logicalknowledge-conditioned text generation. Though achieving remarkable progress,they are data-hungry, which makes the adoption for real-world applicationschallenging with limited data. To this end, this paper proposes a unifiedframework for logical knowledge-conditioned text generation in the few-shotsetting. With only a few seeds logical forms (e.g., 20/100 shot), our approachleverages self-training and samples pseudo logical forms based on content andstructure consistency. Experimental results demonstrate that our approach canobtain better few-shot performance than baselines.$few-shot，self-training，text generation$Zhejiang University，National University of Singapore，Alibaba Group，Donghai Laboratory$这篇文章提出了一种在小样本场景下生成逻辑知识驱动文本的方法，使用少量种子逻辑形式并结合自训练技术，采样伪逻辑形式，并在内容和结构的一致性上进行约束，实验结果表明，这种方法能够获得比基线更好的小样本性能。$few-shot，self-training，文本生成$http://arxiv.org/pdf/2112.01404v3
Can In-context Learners Learn a Reasoning Concept from Demonstrations?$  Large language models show an emergent ability to learn a new task from asmall number of input-output demonstrations. However, recent work shows thatin-context learners largely rely on their pre-trained knowledge, such as thesentiment of the labels, instead of finding new associations in the input.However, the commonly-used few-shot evaluation settings using a randomselection of in-context demonstrations can not disentangle models\' ability tolearn a new skill from demonstrations, as most of the randomly-selecteddemonstrations do not present relations informative for prediction beyondexposing the new task distribution.  To disentangle models\' in-context learning ability independent of models\'memory, we introduce a Conceptual few-shot learning method selecting thedemonstrations sharing a possibly-informative concept with the predictedsample. We extract a set of such concepts from annotated explanations andmeasure how much can models benefit from presenting these concepts in few-shotdemonstrations.  We find that smaller models are more sensitive to the presented concepts.While some of the models are able to benefit from concept-presentingdemonstrations for each assessed concept, we find that none of the assessedin-context learners can benefit from all presented reasoning conceptsconsistently, leaving the in-context concept learning an open challenge.$$$$$http://arxiv.org/pdf/2212.01692v2
A Survey on Out-of-Distribution Detection in NLP$  Out-of-distribution (OOD) detection is essential for the reliable and safedeployment of machine learning systems in the real world. Great progress hasbeen made over the past years. This paper presents the first review of recentadvances in OOD detection with a particular focus on natural languageprocessing approaches. First, we provide a formal definition of OOD detectionand discuss several related fields. We then categorize recent algorithms intothree classes according to the data they used: (1) OOD data available, (2) OODdata unavailable + in-distribution (ID) label available, and (3) OOD dataunavailable + ID label unavailable. Third, we introduce datasets, applications,and metrics. Finally, we summarize existing work and present potential futureresearch topics.$$$$$http://arxiv.org/pdf/2305.03236v1
Generative Steganography Diffusion$"  Generative steganography (GS) is an emerging technique that generates stegoimages directly from secret data. Various GS methods based on GANs or Flow havebeen developed recently. However, existing GAN-based GS methods cannotcompletely recover the hidden secret data due to the lack of networkinvertibility, while Flow-based methods produce poor image quality due to thestringent reversibility restriction in each module. To address this issue, wepropose a novel GS scheme called ""Generative Steganography Diffusion"" (GSD) bydevising an invertible diffusion model named ""StegoDiffusion"". It not onlygenerates realistic stego images but also allows for 100\\% recovery of thehidden secret data. The proposed StegoDiffusion model leverages a non-Markovchain with a fast sampling technique to achieve efficient stego imagegeneration. By constructing an ordinary differential equation (ODE) based onthe transition probability of the generation process in StegoDiffusion, secretdata and stego images can be converted to each other through the approximatesolver of ODE -- Euler iteration formula, enabling the use of irreversible butmore expressive network structures to achieve model invertibility. Our proposedGSD has the advantages of both reversibility and high performance,significantly outperforming existing GS methods in all metrics."$$$$$http://arxiv.org/pdf/2305.03472v1
LOGO-Former: Local-Global Spatio-Temporal Transformer for Dynamic Facial  Expression Recognition$  Previous methods for dynamic facial expression recognition (DFER) in the wildare mainly based on Convolutional Neural Networks (CNNs), whose localoperations ignore the long-range dependencies in videos. Transformer-basedmethods for DFER can achieve better performances but result in higher FLOPs andcomputational costs. To solve these problems, the local-global spatio-temporalTransformer (LOGO-Former) is proposed to capture discriminative features withineach frame and model contextual relationships among frames while balancing thecomplexity. Based on the priors that facial muscles move locally and facialexpressions gradually change, we first restrict both the space attention andthe time attention to a local window to capture local interactions amongfeature tokens. Furthermore, we perform the global attention by querying atoken with features from each local window iteratively to obtain long-rangeinformation of the whole video sequence. In addition, we propose the compactloss regularization term to further encourage the learned features have theminimum intra-class distance and the maximum inter-class distance. Experimentson two in-the-wild dynamic facial expression datasets (i.e., DFEW and FERV39K)indicate that our method provides an effective way to make use of the spatialand temporal dependencies for DFER.$$$$$http://arxiv.org/pdf/2305.03343v1
AttentionViz: A Global View of Transformer Attention$  Transformer models are revolutionizing machine learning, but their innerworkings remain mysterious. In this work, we present a new visualizationtechnique designed to help researchers understand the self-attention mechanismin transformers that allows these models to learn rich, contextualrelationships between elements of a sequence. The main idea behind our methodis to visualize a joint embedding of the query and key vectors used bytransformer models to compute attention. Unlike previous attentionvisualization techniques, our approach enables the analysis of global patternsacross multiple input sequences. We create an interactive visualization tool,AttentionViz, based on these joint query-key embeddings, and use it to studyattention mechanisms in both language and vision transformers. We demonstratethe utility of our approach in improving model understanding and offering newinsights about query-key interactions through several application scenarios andexpert feedback.$$$$$http://arxiv.org/pdf/2305.03210v1
Survey and Systematization of 3D Object Detection Models and Methods$  Strong demand for autonomous vehicles and the wide availability of 3D sensorsare continuously fueling the proposal of novel methods for 3D object detection.In this paper, we provide a comprehensive survey of recent developments from2012-2021 in 3D object detection covering the full pipeline from input data,over data representation and feature extraction to the actual detectionmodules. We introduce fundamental concepts, focus on a broad range of differentapproaches that have emerged over the past decade, and propose asystematization that provides a practical framework for comparing theseapproaches with the goal of guiding future development, evaluation andapplication activities. Specifically, our survey and systematization of 3Dobject detection models and methods can help researchers and practitioners toget a quick overview of the field by decomposing 3DOD solutions into moremanageable pieces.$$$$$http://arxiv.org/pdf/2201.09354v2
Semantic Segmentation using Vision Transformers: A survey$  Semantic segmentation has a broad range of applications in a variety ofdomains including land coverage analysis, autonomous driving, and medical imageanalysis. Convolutional neural networks (CNN) and Vision Transformers (ViTs)provide the architecture models for semantic segmentation. Even though ViTshave proven success in image classification, they cannot be directly applied todense prediction tasks such as image segmentation and object detection sinceViT is not a general purpose backbone due to its patch partitioning scheme. Inthis survey, we discuss some of the different ViT architectures that can beused for semantic segmentation and how their evolution managed the above-statedchallenge. The rise of ViT and its performance with a high success ratemotivated the community to slowly replace the traditional convolutional neuralnetworks in various computer vision tasks. This survey aims to review andcompare the performances of ViT architectures designed for semanticsegmentation using benchmarking datasets. This will be worthwhile for thecommunity to yield knowledge regarding the implementations carried out insemantic segmentation and to discover more efficient methodologies using ViTs.$$$$$http://arxiv.org/pdf/2305.03273v1
Mining bias-target Alignment from Voronoi Cells$  Despite significant research efforts, deep neural networks are stillvulnerable to biases: this raises concerns about their fairness and limitstheir generalization. In this paper, we propose a bias-agnostic approach tomitigate the impact of bias in deep neural networks. Unlike traditionaldebiasing approaches, we rely on a metric to quantify ``biasalignment/misalignment\'\' on target classes, and use this information todiscourage the propagation of bias-target alignment information through thenetwork. We conduct experiments on several commonly used datasets for debiasingand compare our method to supervised and bias-specific approaches. Our resultsindicate that the proposed method achieves comparable performance tostate-of-the-art supervised approaches, although it is bias-agnostic, even inpresence of multiple biases in the same sample.$$$$$http://arxiv.org/pdf/2305.03691v1
Combating Online Misinformation Videos: Characterization, Detection, and  Future Directions$  With information consumption via online video streaming becoming increasinglypopular, misinformation video poses a new threat to the health of the onlineinformation ecosystem. Though previous studies have made much progress indetecting misinformation in text and image formats, video-based misinformationbrings new and unique challenges to automatic detection systems: 1) highinformation heterogeneity brought by various modalities, 2) blurred distinctionbetween misleading video manipulation and ubiquitous artistic video editing,and 3) new patterns of misinformation propagation due to the dominant role ofrecommendation systems on online video platforms. To facilitate research onthis challenging task, we conduct this survey to present advances inmisinformation video detection research. We first analyze and characterize themisinformation video from three levels including signals, semantics, andintents. Based on the characterization, we systematically review existing worksfor detection from features of various modalities to techniques for clueintegration. We also introduce existing resources including representativedatasets and widely used tools. Besides summarizing existing studies, wediscuss related areas and outline open issues and future directions toencourage and guide more research on misinformation video detection. Ourcorresponding public repository is available athttps://github.com/ICTMCG/Awesome-Misinfo-Video-Detection.$$$$$http://arxiv.org/pdf/2302.03242v2
A vector quantized masked autoencoder for audiovisual speech emotion  recognition$  While fully-supervised models have been shown to be effective for audiovisualspeech emotion recognition (SER), the limited availability of labeled dataremains a major challenge in the field. To address this issue, self-supervisedlearning approaches, such as masked autoencoders (MAEs), have gained popularityas potential solutions. In this paper, we propose the VQ-MAE-AV model, a vectorquantized MAE specifically designed for audiovisual speech self-supervisedrepresentation learning. Unlike existing multimodal MAEs that rely on theprocessing of the raw audiovisual speech data, the proposed method employs aself-supervised paradigm based on discrete audio and visual speechrepresentations learned by two pre-trained vector quantized variationalautoencoders. Experimental results show that the proposed approach, which ispre-trained on the VoxCeleb2 database and fine-tuned on standard emotionalaudiovisual speech datasets, outperforms the state-of-the-art audiovisual SERmethods.$self-supervised learning, masked autoencoder, multimodality, audiovisual speech, emotion recognition$CentraleSupélec, IETR UMR CNRS 6164, Rennes$本文提出了一种特别为视听说自监督表示学习设计的向量量化掩码自编码器模型VQ-MAE-AV。该方法不同于现有的多模态掩码自编码器，它使用了由两个预训练向量定量变分自编码器学习的离散音频和视觉语音表示的自监督范式。实验结果表明，该方法在VoxCeleb2数据库上进行预训练后，在标准情感视听说数据集上进行微调，优于现有的视听说情感识别方法。$自监督学习、掩码自编码器、多模态、视听说、情感识别$http://arxiv.org/pdf/2305.03568v1
A Multimodal Dynamical Variational Autoencoder for Audiovisual Speech  Representation Learning$  In this paper, we present a multimodal \\textit{and} dynamical VAE (MDVAE)applied to unsupervised audio-visual speech representation learning. The latentspace is structured to dissociate the latent dynamical factors that are sharedbetween the modalities from those that are specific to each modality. A staticlatent variable is also introduced to encode the information that is constantover time within an audiovisual speech sequence. The model is trained in anunsupervised manner on an audiovisual emotional speech dataset, in two stages.In the first stage, a vector quantized VAE (VQ-VAE) is learned independentlyfor each modality, without temporal modeling. The second stage consists inlearning the MDVAE model on the intermediate representation of the VQ-VAEsbefore quantization. The disentanglement between static versus dynamical andmodality-specific versus modality-common information occurs during this secondtraining stage. Extensive experiments are conducted to investigate howaudiovisual speech latent factors are encoded in the latent space of MDVAE.These experiments include manipulating audiovisual speech, audiovisual facialimage denoising, and audiovisual speech emotion recognition. The results showthat MDVAE effectively combines the audio and visual information in its latentspace. They also show that the learned static representation of audiovisualspeech can be used for emotion recognition with few labeled data, and withbetter accuracy compared with unimodal baselines and a state-of-the-artsupervised model based on an audiovisual transformer architecture.$$$$$http://arxiv.org/pdf/2305.03582v1
Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4$"  Harnessing logical reasoning ability is a comprehensive natural languageunderstanding endeavor. With the release of Generative Pretrained Transformer 4(GPT-4), highlighted as ""advanced"" at reasoning tasks, we are eager to learnthe GPT-4 performance on various logical reasoning tasks. This report analysesmultiple logical reasoning datasets, with popular benchmarks like LogiQA andReClor, and newly-released datasets like AR-LSAT. We test the multi-choicereading comprehension and natural language inference tasks with benchmarksrequiring logical reasoning. We further construct a logical reasoningout-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4.We also make a performance comparison between ChatGPT and GPT-4. Experimentresults show that ChatGPT performs significantly better than the RoBERTafine-tuning method on most logical reasoning benchmarks. With early access tothe GPT-4 API we are able to conduct intense experiments on the GPT-4 model.The results show GPT-4 yields even higher performance on most logical reasoningdatasets. Among benchmarks, ChatGPT and GPT-4 do relatively well on well-knowndatasets like LogiQA and ReClor. However, the performance drops significantlywhen handling newly released and out-of-distribution datasets. Logicalreasoning remains challenging for ChatGPT and GPT-4, especially onout-of-distribution and natural language inference datasets. We release theprompt-style logical reasoning datasets as a benchmark suite and name itLogiEval."$$$$$http://arxiv.org/pdf/2304.03439v3
Investigating Lexical Sharing in Multilingual Machine Translation for  Indian Languages$  Multilingual language models have shown impressive cross-lingual transferability across a diverse set of languages and tasks. To improve thecross-lingual ability of these models, some strategies include transliterationand finer-grained segmentation into characters as opposed to subwords. In thiswork, we investigate lexical sharing in multilingual machine translation (MT)from Hindi, Gujarati, Nepali into English. We explore the trade-offs that existin translation performance between data sampling and vocabulary size, and weexplore whether transliteration is useful in encouraging cross-scriptgeneralisation. We also verify how the different settings generalise to unseenlanguages (Marathi and Bengali). We find that transliteration does not givepronounced improvements and our analysis suggests that our multilingual MTmodels trained on original scripts seem to already be robust to cross-scriptdifferences even for relatively low-resource languages$$$$$http://arxiv.org/pdf/2305.03207v1
Low-Resource Multi-Granularity Academic Function Recognition Based on  Multiple Prompt Knowledge$  Fine-tuning pre-trained language models (PLMs), e.g., SciBERT, generallyrequires large numbers of annotated data to achieve state-of-the-artperformance on a range of NLP tasks in the scientific domain. However,obtaining the fine-tune data for scientific NLP task is still challenging andexpensive. Inspired by recent advancement in prompt learning, in this paper, wepropose the Mix Prompt Tuning (MPT), which is a semi-supervised method toalleviate the dependence on annotated data and improve the performance ofmulti-granularity academic function recognition tasks with a small number oflabeled examples. Specifically, the proposed method provides multi-perspectiverepresentations by combining manual prompt templates with automatically learnedcontinuous prompt templates to help the given academic function recognitiontask take full advantage of knowledge in PLMs. Based on these prompt templatesand the fine-tuned PLM, a large number of pseudo labels are assigned to theunlabeled examples. Finally, we fine-tune the PLM using the pseudo trainingset. We evaluate our method on three academic function recognition tasks ofdifferent granularity including the citation function, the abstract sentencefunction, and the keyword function, with datasets from computer science domainand biomedical domain. Extensive experiments demonstrate the effectiveness ofour method and statistically significant improvements against strong baselines.In particular, it achieves an average increase of 5% in Macro-F1 score comparedwith fine-tuning, and 6% in Macro-F1 score compared with other semi-supervisedmethod under low-resource settings. In addition, MPT is a general method thatcan be easily applied to other low-resource scientific classification tasks.$$$$$http://arxiv.org/pdf/2305.03287v1
Open Information Extraction via Chunks$  Open Information Extraction (OIE) aims to extract relational tuples fromopen-domain sentences. Existing OIE systems split a sentence into tokens andrecognize token spans as tuple relations and arguments. We instead proposeSentence as Chunk sequence (SaC) and recognize chunk spans as tuple relationsand arguments. We argue that SaC has better quantitative and qualitativeproperties for OIE than sentence as token sequence, and evaluate four choicesof chunks (i.e., CoNLL chunks, simple phrases, NP chunks, and spans fromSpanOIE) against gold OIE tuples. Accordingly, we propose a simple BERT-basedmodel for sentence chunking, and propose Chunk-OIE for tuple extraction on topof SaC. Chunk-OIE achieves state-of-the-art results on multiple OIE datasets,showing that SaC benefits OIE task.$$$$$http://arxiv.org/pdf/2305.03299v1
MindGames: Targeting Theory of Mind in Large Language Models with  Dynamic Epistemic Modal Logic$  Theory of Mind (ToM) is a critical component of intelligence, yet accuratelymeasuring it continues to be a subject of debate. Prior research has attemptedto apply human ToM assessments to natural language processing models usingeither human-created standardized tests or rule-based templates. However, thesemethods primarily focus on simplistic reasoning and require further validation.In this study, we utilize dynamic epistemic logic, which has establishedoverlaps with ToM, to generate more intricate problems. We also introduce novelverbalization techniques to express these problems using natural language. Ourfindings indicate that certain language model scaling (from 70M to 6B and 350Mto 174B) does not consistently yield results better than random chance. WhileGPT-4 demonstrates improved epistemic reasoning capabilities, there is stillroom for enhancement. Our code and datasets are publicly availablehttps://github.com/antoinelrnld/modloghttps://huggingface.co/datasets/sileod/mindgames$$$$$http://arxiv.org/pdf/2305.03353v1
Multi-View Graph Representation Learning for Answering Hybrid Numerical  Reasoning Question$  Hybrid question answering (HybridQA) over the financial report contains bothtextual and tabular data, and requires the model to select the appropriateevidence for the numerical reasoning task. Existing methods based onencoder-decoder framework employ a expression tree-based decoder to solvenumerical reasoning problems. However, encoders rely more on Machine ReadingComprehension (MRC) methods, which take table serialization and text splicingas input, damaging the granularity relationship between table and text as wellas the spatial structure information of table itself. In order to solve theseproblems, the paper proposes a Multi-View Graph (MVG) Encoder to take therelations among the granularity into account and capture the relations frommultiple view. By utilizing MVGE as a module, we constuct Tabular View,Relation View and Numerical View which aim to retain the originalcharacteristics of the hybrid data. We validate our model on the publiclyavailable table-text hybrid QA benchmark (TAT-QA) and outperform thestate-of-the-art model.$$$$$http://arxiv.org/pdf/2305.03458v1
Context-Aware Semantic Similarity Measurement for Unsupervised Word  Sense Disambiguation$  The issue of word sense ambiguity poses a significant challenge in naturallanguage processing due to the scarcity of annotated data to feed machinelearning models to face the challenge. Therefore, unsupervised word sensedisambiguation methods have been developed to overcome that challenge withoutrelying on annotated data. This research proposes a new context-aware approachto unsupervised word sense disambiguation, which provides a flexible mechanismfor incorporating contextual information into the similarity measurementprocess. We experiment with a popular benchmark dataset to evaluate theproposed strategy and compare its performance with state-of-the-artunsupervised word sense disambiguation techniques. The experimental resultsindicate that our approach substantially enhances disambiguation accuracy andsurpasses the performance of several existing techniques. Our findingsunderscore the significance of integrating contextual information in semanticsimilarity measurements to manage word sense ambiguity in unsupervisedscenarios effectively.$$$$$http://arxiv.org/pdf/2305.03520v1
In-context Learning as Maintaining Coherency: A Study of On-the-fly  Machine Translation Using Large Language Models$"  The phenomena of in-context learning has typically been thought of as""learning from examples"". In this work which focuses on Machine Translation, wepresent a perspective of in-context learning as the desired generation taskmaintaining coherency with its context, i.e., the prompt examples. We firstinvestigate randomly sampled prompts across 4 domains, and find thattranslation performance improves when shown in-domain prompts. Next, weinvestigate coherency for the in-domain setting, which uses prompt examplesfrom a moving window. We study this with respect to other factors that havepreviously been identified in the literature such as length, surface similarityand sentence embedding similarity. Our results across 3 models (GPTNeo2.7B,Bloom3B, XGLM2.9B), and three translation directions(\\texttt{en}$\\rightarrow$\\{\\texttt{pt, de, fr}\\}) suggest that the long-termcoherency of the prompts and the test sentence is a good indicator ofdownstream translation performance. In doing so, we demonstrate the efficacy ofIn-context Machine Translation for on-the-fly adaptation."$$$$$http://arxiv.org/pdf/2305.03573v1
Predicting COVID-19 and pneumonia complications from admission texts$  In this paper we present a novel approach to risk assessment for patientshospitalized with pneumonia or COVID-19 based on their admission reports. Weapplied a Longformer neural network to admission reports and other textual dataavailable shortly after admission to compute risk scores for the patients. Weused patient data of multiple European hospitals to demonstrate that ourapproach outperforms the Transformer baselines. Our experiments show that theproposed model generalises across institutions and diagnoses. Also, our methodhas several other advantages described in the paper.$$$$$http://arxiv.org/pdf/2305.03661v1
Vera: A General-Purpose Plausibility Estimation Model for Commonsense  Statements$  Despite the much discussed capabilities of today\'s language models, they arestill prone to silly and unexpected commonsense failures. We consider aretrospective verification approach that reflects on the correctness of LMoutputs, and introduce Vera, a general-purpose model that estimates theplausibility of declarative statements based on commonsense knowledge. Trainedon ~7M commonsense statements created from 19 QA datasets and two large-scaleknowledge bases, and with a combination of three training objectives, Vera is aversatile model that effectively separates correct from incorrect statementsacross diverse commonsense domains. When applied to solving commonsenseproblems in the verification format, Vera substantially outperforms existingmodels that can be repurposed for commonsense verification, and it furtherexhibits generalization capabilities to unseen tasks and provideswell-calibrated outputs. We find that Vera excels at filtering LM-generatedcommonsense knowledge and is useful in detecting erroneous commonsensestatements generated by models like ChatGPT in real-world settings.$$$$$http://arxiv.org/pdf/2305.03695v1
VideoOFA: Two-Stage Pre-Training for Video-to-Text Generation$  We propose a new two-stage pre-training framework for video-to-textgeneration tasks such as video captioning and video question answering: Agenerative encoder-decoder model is first jointly pre-trained on massiveimage-text data to learn fundamental vision-language concepts, and then adaptedto video data in an intermediate video-text pre-training stage to learnvideo-specific skills such as spatio-temporal reasoning. As a result, ourVideoOFA model achieves new state-of-the-art performance on four VideoCaptioning benchmarks, beating prior art by an average of 9.7 points in CIDErscore. It also outperforms existing models on two open-ended Video QuestionAnswering datasets, showcasing its generalization capability as a universalvideo-to-text model.$英文关键词:video-to-text generation, pre-training, video captioning, video question answering, image-text learning$机构:Meta AI$本文提出了一个新的两阶段预训练框架VideoOFA，用于视频到文本生成任务，如视频描述和视频问答。在第一阶段，模型联合预训练大规模的图像-文本数据集以学习视觉-语言的基本概念；在第二阶段，模型在中间的视频-文本预训练阶段上被调整到视频数据上，以学习视频特定的技能，如时空推理。实验结果表明，VideoOFA模型在四个视频描述基准测试上取得了最新的最佳表现，在CIDEr得分上平均超越先前的最佳成果9.7分。在两个开放式的视频问答数据集上也优于现有模型，展示了其作为通用视频到文本模型的泛化能力。$领域关键词:视频文本生成, 预训练, 视频描述, 视频问答, 图像-文本学习$http://arxiv.org/pdf/2305.03204v1
A Dual Semantic-Aware Recurrent Global-Adaptive Network For  Vision-and-Language Navigation$  Vision-and-Language Navigation (VLN) is a realistic but challenging task thatrequires an agent to locate the target region using verbal and visual cues.While significant advancements have been achieved recently, there are still twobroad limitations: (1) The explicit information mining for significant guidingsemantics concealed in both vision and language is still under-explored; (2)The previously structured map method provides the average historical appearanceof visited nodes, while it ignores distinctive contributions of various imagesand potent information retention in the reasoning process. This work proposes adual semantic-aware recurrent global-adaptive network (DSRG) to address theabove problems. First, DSRG proposes an instruction-guidance linguistic module(IGL) and an appearance-semantics visual module (ASV) for boosting vision andlanguage semantic learning respectively. For the memory mechanism, a globaladaptive aggregation module (GAA) is devised for explicit panoramic observationfusion, and a recurrent memory fusion module (RMF) is introduced to supplyimplicit temporal hidden states. Extensive experimental results on the R2R andREVERIE datasets demonstrate that our method achieves better performance thanexisting methods.$$$$$http://arxiv.org/pdf/2305.03602v1
A Suite of Generative Tasks for Multi-Level Multimodal Webpage  Understanding$  Webpages have been a rich, scalable resource for vision-language and languageonly tasks. Yet only pieces of webpages are kept: image-caption pairs, longtext articles, or raw HTML, never all in one place. Webpage tasks haveresultingly received little attention and structured image-text data leftunderused. To study multimodal webpage understanding, we introduce theWikipedia Webpage suite (WikiWeb2M) of 2M pages. We verify its utility on threegenerative tasks: page description generation, section summarization, andcontextual image captioning. We design a novel attention mechanism PrefixGlobal, which selects the most relevant image and text content as global tokensto attend to the rest of the webpage for context. By using page structure toseparate such tokens, it performs better than full attention with lowercomputational complexity. Experiments show that the new annotations fromWikiWeb2M improve task performance compared to data from prior work. We alsoinclude ablations on sequence length, input features, and model size.$$$$$http://arxiv.org/pdf/2305.03668v1
Otter: A Multi-Modal Model with In-Context Instruction Tuning$"  Large language models (LLMs) have demonstrated significant universalcapabilities as few/zero-shot learners in various tasks due to theirpre-training on vast amounts of text data, as exemplified by GPT-3, whichboosted to InstrctGPT and ChatGPT, effectively following natural languageinstructions to accomplish real-world tasks. In this paper, we propose tointroduce instruction tuning into multi-modal models, motivated by the Flamingomodel\'s upstream interleaved format pretraining dataset. We adopt a similarapproach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT)dataset. We then introduce Otter, a multi-modal model based on OpenFlamingo(open-sourced version of DeepMind\'s Flamingo), trained on MIMIC-IT andshowcasing improved instruction-following ability and in-context learning. Wealso optimize OpenFlamingo\'s implementation for researchers, democratizing therequired training resources from 1$\\times$ A100 GPU to 4$\\times$ RTX-3090 GPUs,and integrate both OpenFlamingo and Otter into Huggingface Transformers formore researchers to incorporate the models into their customized training andinference pipelines."$$$$$http://arxiv.org/pdf/2305.03726v1
A Subjective Dataset for Multi-Screen Video Streaming Applications$  In modern-era video streaming systems, videos are streamed and displayed on awide range of devices. Such devices vary from large-screen UHD and HDTVs tomedium-screen Desktop PCs and Laptops to smaller-screen devices such as mobilephones and tablets. It is well known that a video is perceived differently whendisplayed on different devices. The viewing experience for a particular videoon smaller screen devices such as smartphones and tablets, which have highpixel density, will be different with respect to the case where the same videois played on a large screen device such as a TV or PC monitor. Being able tomodel such relative differences in perception effectively can help in thedesign of better quality metrics and in the design of more efficient andoptimized encoding profiles, leading to lower storage, encoding, andtransmission costs. This paper presents a new, open-source dataset consistingof subjective ratings for various encoded video sequences of differentresolutions and bitrates (quality) when viewed on three devices of varyingscreen sizes: TV, Tablet, and Mobile. Along with the subjective scores, anevaluation of some of the most famous and commonly used open-source objectivequality metrics is also presented. It is observed that the performance of themetrics varies a lot across different device types, with the recentlystandardized ITU-T P.1204.3 Model, on average, outperforming theirfull-reference counterparts. The dataset consisting of the videos, along withtheir subjective and objective scores, is available freely on Github athttps://github.com/NabajeetBarman/Multiscreen-Dataset.$$$$$http://arxiv.org/pdf/2305.03138v1
NeRF-QA: Neural Radiance Fields Quality Assessment Database$  This short paper proposes a new database - NeRF-QA - containing 48 videossynthesized with seven NeRF based methods, along with their perceived qualityscores, resulting from subjective assessment tests; for the videos selection,both real and synthetic, 360 degrees scenes were considered. This database willallow to evaluate the suitability, to NeRF based synthesized views, of existingobjective quality metrics and also the development of new quality metrics,specific for this case.$$$$$http://arxiv.org/pdf/2305.03176v1
MOSAIC: Spatially-Multiplexed Edge AI Optimization over Multiple  Concurrent Video Sensing Streams$  Sustaining high fidelity and high throughput of perception tasks over visionsensor streams on edge devices remains a formidable challenge, especially giventhe continuing increase in image sizes (e.g., generated by 4K cameras) andcomplexity of DNN models. One promising approach involves criticality-awareprocessing, where the computation is directed selectively to critical portionsof individual image frames. We introduce MOSAIC, a novel system for suchcriticality-aware concurrent processing of multiple vision sensing streams thatprovides a multiplicative increase in the achievable throughput with negligibleloss in perception fidelity. MOSAIC determines critical regions from imagesreceived from multiple vision sensors and spatially bin-packs these regionsusing a novel multi-scale Mosaic Across Scales (MoS) tiling strategy into asingle canvas frame, sized such that the edge device can retain sufficientlyhigh processing throughput. Experimental studies using benchmark datasets fortwo tasks, Automatic License Plate Recognition and Drone-based PedestrianDetection, show that MOSAIC, executing on a Jetson TX2 edge device, can providedramatic gains in the throughput vs. fidelity tradeoff. For instance, fordrone-based pedestrian detection, for a batch size of 4, MOSAIC can pack inputframes from 6 cameras to achieve (a) 4.75x higher throughput (23 FPS percamera, cumulatively 138FPS) with less than 1% accuracy loss, compared to aFirst Come First Serve (FCFS) processing paradigm.$$$$$http://arxiv.org/pdf/2305.03222v1
Progressive-Hint Prompting Improves Reasoning in Large Language Models$  The performance of Large Language Models (LLMs) in reasoning tasks dependsheavily on prompt design, with Chain-of-Thought (CoT) and self-consistencybeing critical methods that enhance this ability. However, these methods do notfully exploit the answers generated by the LLM to guide subsequent responses.This paper proposes a new prompting method, named Progressive-Hint Prompting(PHP), that enables automatic multiple interactions between users and LLMs byusing previously generated answers as hints to progressively guide toward thecorrect answers. PHP is orthogonal to CoT and self-consistency, making it easyto combine with state-of-the-art techniques to further improve performance. Weconducted an extensive and comprehensive evaluation to demonstrate theeffectiveness of the proposed method. Our experimental results on sixbenchmarks show that combining CoT and self-consistency with PHP significantlyimproves accuracy while remaining highly efficient. For instance, withtext-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decodingcompared to Complex CoT, and a 46.17% reduction in sample paths withself-consistency. With GPT-4 and PHP, we achieve state-of-the-art performanceson SVAMP (89.1% -&gt; 91.9%), GSM8K (92% -&gt; 95.5%), AQuA (76.4% -&gt; 79.9%) and MATH(50.2% -&gt; 53.9%).$$$$$http://arxiv.org/pdf/2304.09797v2
Transfer and Active Learning for Dissonance Detection: Addressing the  Rare-Class Challenge$  While transformer-based systems have enabled greater accuracies with fewertraining examples, data acquisition obstacles still persist for rare-classtasks -- when the class label is very infrequent (e.g. &lt; 5% of samples). Activelearning has in general been proposed to alleviate such challenges, but choiceof selection strategy, the criteria by which rare-class examples are chosen,has not been systematically evaluated. Further, transformers enable iterativetransfer-learning approaches. We propose and investigate transfer- and activelearning solutions to the rare class problem of dissonance detection throughutilizing models trained on closely related tasks and the evaluation ofacquisition strategies, including a proposed probability-of-rare-class (PRC)approach. We perform these experiments for a specific rare class problem:collecting language samples of cognitive dissonance from social media. We findthat PRC is a simple and effective strategy to guide annotations and ultimatelyimprove model accuracy while transfer-learning in a specific order can improvethe cold-start performance of the learner but does not benefit iterations ofactive learning.$$$$$http://arxiv.org/pdf/2305.02459v2
Enhancing Pashto Text Classification using Language Processing  Techniques for Single And Multi-Label Analysis$"  Text classification has become a crucial task in various fields, leading to asignificant amount of research on developing automated text classificationsystems for national and international languages. However, there is a growingneed for automated text classification systems that can handle local languages.This study aims to establish an automated classification system for Pashtotext. To achieve this goal, we constructed a dataset of Pashto documents andapplied various models, including statistical and neural machine learningmodels such as DistilBERT-base-multilingual-cased, Multilayer Perceptron,Support Vector Machine, K Nearest Neighbor, decision tree, Gaussian na\\""iveBayes, multinomial na\\""ive Bayes, random forest, and logistic regression, toidentify the most effective approach. We also evaluated two different featureextraction methods, bag of words and Term Frequency Inverse Document Frequency.The study achieved an average testing accuracy rate of 94% using the MLPclassification algorithm and TFIDF feature extraction method in single-labelmulticlass classification. Similarly, MLP+TFIDF yielded the best results, withan F1-measure of 0.81. Furthermore, the use of pre-trained languagerepresentation models, such as DistilBERT, showed promising results for Pashtotext classification; however, the study highlights the importance of developinga specific tokenizer for a particular language to achieve reasonable results."$$$$$http://arxiv.org/pdf/2305.03201v1
Exploiting CNNs for Semantic Segmentation with Pascal VOC$  In this paper, we present a comprehensive study on semantic segmentation withthe Pascal VOC dataset. Here, we have to label each pixel with a class which inturn segments the entire image based on the objects/entities present. To tacklethis, we firstly use a Fully Convolution Network (FCN) baseline which gave71.31% pixel accuracy and 0.0527 mean IoU. We analyze its performance andworking and subsequently address the issues in the baseline with threeimprovements: a) cosine annealing learning rate scheduler(pixel accuracy:72.86%, IoU: 0.0529), b) data augmentation(pixel accuracy: 69.88%, IoU: 0.0585)c) class imbalance weights(pixel accuracy: 68.98%, IoU: 0.0596). Apart fromthese changes in training pipeline, we also explore three differentarchitectures: a) Our proposed model -- Advanced FCN (pixel accuracy: 67.20%,IoU: 0.0602) b) Transfer Learning with ResNet (Best performance) (pixelaccuracy: 71.33%, IoU: 0.0926 ) c) U-Net(pixel accuracy: 72.15%, IoU: 0.0649).We observe that the improvements help in greatly improving the performance, asreflected both, in metrics and segmentation maps. Interestingly, we observethat among the improvements, dataset augmentation has the greatestcontribution. Also, note that transfer learning model performs the best on thepascal dataset. We analyse the performance of these using loss, accuracy andIoU plots along with segmentation maps, which help us draw valuable insightsabout the working of the models.$$$$$http://arxiv.org/pdf/2304.13216v2
Smaller3d: Smaller Models for 3D Semantic Segmentation Using Minkowski  Engine and Knowledge Distillation Methods$  There are various optimization techniques in the realm of 3D, including pointcloud-based approaches that use mesh, texture, and voxels which optimize howyou store, and how do calculate in 3D. These techniques employ methods such asfeed-forward networks, 3D convolutions, graph neural networks, transformers,and sparse tensors. However, the field of 3D is one of the most computationallyexpensive fields, and these methods have yet to achieve their full potentialdue to their large capacity, complexity, and computation limits. This paperproposes the application of knowledge distillation techniques, especially forsparse tensors in 3D deep learning, to reduce model sizes while maintainingperformance. We analyze and purpose different loss functions, includingstandard methods and combinations of various losses, to simulate theperformance of state-of-the-art models of different Sparse Convolutional NNs.Our experiments are done on the standard ScanNet V2 dataset, and we achievedaround 2.6\\% mIoU difference with a 4 times smaller model and around 8\\% with a16 times smaller model on the latest state-of-the-art spacio-temporal conventsbased models.$$$$$http://arxiv.org/pdf/2305.03188v1
LLM2Loss: Leveraging Language Models for Explainable Model Diagnostics$  Trained on a vast amount of data, Large Language models (LLMs) have achievedunprecedented success and generalization in modeling fairly complex textualinputs in the abstract space, making them powerful tools for zero-shotlearning. Such capability is extended to other modalities such as the visualdomain using cross-modal foundation models such as CLIP, and as a result,semantically meaningful representation are extractable from visual inputs.  In this work, we leverage this capability and propose an approach that canprovide semantic insights into a model\'s patterns of failures and biases. Givena black box model, its training data, and task definition, we first calculateits task-related loss for each data point. We then extract a semanticallymeaningful representation for each training data point (such as CLIP embeddingsfrom its visual encoder) and train a lightweight diagnosis model which mapsthis semantically meaningful representation of a data point to its task loss.We show that an ensemble of such lightweight models can be used to generateinsights on the performance of the black-box model, in terms of identifying itspatterns of failures and biases.$Natural Language Processing, Neural Network, CLIP model$Netﬂix$本文提出了一种利用大型语言模型将模型故障和偏见进行语义诊断的方法。该方法使用CLIP模型提取图像和文本的语义表征，从而产生高度结构化且具有人类解释性的嵌入空间。最终，可以使用轻量级的诊断模型来对黑盒模型的性能进行诊断分析并发现其故障和偏见的模式。$自然语言处理，神经网络，CLIP模型$http://arxiv.org/pdf/2305.03212v1
Clothes Grasping and Unfolding Based on RGB-D Semantic Segmentation$  Clothes grasping and unfolding is a core step in robotic-assisted dressing.Most existing works leverage depth images of clothes to train a deeplearning-based model to recognize suitable grasping points. These methods oftenutilize physics engines to synthesize depth images to reduce the cost of reallabeled data collection. However, the natural domain gap between synthetic andreal images often leads to poor performance of these methods on real data.Furthermore, these approaches often struggle in scenarios where grasping pointsare occluded by the clothing item itself. To address the above challenges, wepropose a novel Bi-directional Fractal Cross Fusion Network (BiFCNet) forsemantic segmentation, enabling recognition of graspable regions in order toprovide more possibilities for grasping. Instead of using depth images only, wealso utilize RGB images with rich color features as input to our network inwhich the Fractal Cross Fusion (FCF) module fuses RGB and depth data byconsidering global complex features based on fractal geometry. To reduce thecost of real data collection, we further propose a data augmentation methodbased on an adversarial strategy, in which the color and geometrictransformations simultaneously process RGB and depth data while maintaining thelabel correspondence. Finally, we present a pipeline for clothes grasping andunfolding from the perspective of semantic segmentation, through the additionof a strategy for grasp point selection from segmentation regions based onclothing flatness measures, while taking into account the grasping direction.We evaluate our BiFCNet on the public dataset NYUDv2 and obtained comparableperformance to current state-of-the-art models. We also deploy our model on aBaxter robot, running extensive grasping and unfolding experiments as part ofour ablation studies, achieving an 84% success rate.$$$$$http://arxiv.org/pdf/2305.03259v1
BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks$  Recently, the Segment Anything Model (SAM) has gained significant attentionas an image segmentation foundation model due to its strong performance onvarious downstream tasks. However, it has been found that SAM does not alwaysperform satisfactorily when faced with challenging downstream tasks. This hasled downstream users to demand a customized SAM model that can be adapted tothese downstream tasks. In this paper, we present BadSAM, the first backdoorattack on the image segmentation foundation model. Our preliminary experimentson the CAMO dataset demonstrate the effectiveness of BadSAM.$$$$$http://arxiv.org/pdf/2305.03289v1
Contrastive Learning for Low-light Raw Denoising$  Image/video denoising in low-light scenes is an extremely challenging problemdue to limited photon count and high noise. In this paper, we propose a novelapproach with contrastive learning to address this issue. Inspired by thesuccess of contrastive learning used in some high-level computer vision tasks,we bring in this idea to the low-level denoising task. In order to achieve thisgoal, we introduce a new denoising contrastive regularization (DCR) to exploitthe information of noisy images and clean images. In the feature space, DCRmakes the denoised image closer to the clean image and far away from the noisyimage. In addition, we build a new feature embedding network called Wnet, whichis more effective to extract high-frequency information. We conduct theexperiments on a real low-light dataset that captures still images taken on amoonless clear night in 0.6 millilux and videos under starlight (no moonpresent, &lt;0.001 lux). The results show that our method can achieve a higherPSNR and better visual quality compared with existing methods$$$$$http://arxiv.org/pdf/2305.03352v1
Human Attention-Guided Explainable Artificial Intelligence for Computer  Vision Models$  We examined whether embedding human attention knowledge into saliency-basedexplainable AI (XAI) methods for computer vision models could enhance theirplausibility and faithfulness. We first developed new gradient-based XAImethods for object detection models to generate object-specific explanations byextending the current methods for image classification models. Interestingly,while these gradient-based methods worked well for explaining imageclassification models, when being used for explaining object detection models,the resulting saliency maps generally had lower faithfulness than humanattention maps when performing the same task. We then developed HumanAttention-Guided XAI (HAG-XAI) to learn from human attention how to bestcombine explanatory information from the models to enhance explanationplausibility by using trainable activation functions and smoothing kernels tomaximize XAI saliency map\'s similarity to human attention maps. While for imageclassification models, HAG-XAI enhanced explanation plausibility at the expenseof faithfulness, for object detection models it enhanced plausibility andfaithfulness simultaneously and outperformed existing methods. The learnedfunctions were model-specific, well generalizable to other databases.$$$$$http://arxiv.org/pdf/2305.03601v1
LMEye: An Interactive Perception Network for Large Language Models$  Training a Large Visual Language Model (LVLM) from scratch, like GPT-4, isresource-intensive. Our paper proposes an alternative method called LMEye, aplay-plug-in Interactive Perception Network for Large Language Models (LLMs),aiming to improve the accuracy of image understanding for the LVLM. Previousmethods that infuse visual information into LLMs utilize a static visualmapping network, but lack dynamic interaction between the LLMs and visualinformation. LMEye addresses this issue by allowing the LLM to incorporate thevisual information that aligned with human instruction. Specifically, the LMEyenetwork consists of a static visual mapping network to provide the basicperception of an image to LLMs. Then, it also contains additional linear layersresponsible for acquiring requests from LLMs, decomposing image features, andtransmitting the interleaved information to LLMs, respectively. In this way,LLMs act to be in charge of understanding human instructions, sending it to theinteractive perception network, and generating the response based on theinterleaved multimodal information. We evaluate LMEye through extensiveexperiments on multimodal question answering and reasoning tasks, demonstratingthat it significantly improves the zero-shot performance of LLMs on multimodaltasks compared to previous methods.$$$$$http://arxiv.org/pdf/2305.03701v1
Not what you\'ve signed up for: Compromising Real-World LLM-Integrated  Applications with Indirect Prompt Injection$  Large Language Models (LLMs) are increasingly being integrated into variousapplications. The functionalities of recent LLMs can be flexibly modulated vianatural language prompts. This renders them susceptible to targeted adversarialprompting, e.g., Prompt Injection (PI) attacks enable attackers to overrideoriginal instructions and employed controls. So far, it was assumed that theuser is directly prompting the LLM. But, what if it is not the user prompting?We argue that LLM-Integrated Applications blur the line between data andinstructions. We reveal new attack vectors, using Indirect Prompt Injection,that enable adversaries to remotely (without a direct interface) exploitLLM-integrated applications by strategically injecting prompts into data likelyto be retrieved. We derive a comprehensive taxonomy from a computer securityperspective to systematically investigate impacts and vulnerabilities,including data theft, worming, information ecosystem contamination, and othernovel security risks. We demonstrate our attacks\' practical viability againstboth real-world systems, such as Bing\'s GPT-4 powered Chat and code-completionengines, and synthetic applications built on GPT-4. We show how processingretrieved prompts can act as arbitrary code execution, manipulate theapplication\'s functionality, and control how and if other APIs are called.Despite the increasing integration and reliance on LLMs, effective mitigationsof these emerging threats are currently lacking. By raising awareness of thesevulnerabilities and providing key insights into their implications, we aim topromote the safe and responsible deployment of these powerful models and thedevelopment of robust defenses that protect users and systems from potentialattacks.$$$$$http://arxiv.org/pdf/2302.12173v2
Automated Code generation for Information Technology Tasks in YAML  through Large Language Models$  The recent improvement in code generation capabilities due to the use oflarge language models has mainly benefited general purpose programminglanguages. Domain specific languages, such as the ones used for IT Automation,have received far less attention, despite involving many active developers andbeing an essential component of modern cloud platforms. This work focuses onthe generation of Ansible-YAML, a widely used markup language for ITAutomation. We present Ansible Wisdom, a natural-language to Ansible-YAML codegeneration tool, aimed at improving IT automation productivity. Ansible Wisdomis a transformer-based model, extended by training with a new datasetcontaining Ansible-YAML. We also develop two novel performance metrics for YAMLand Ansible to capture the specific characteristics of this domain. Resultsshow that Ansible Wisdom can accurately generate Ansible script from naturallanguage prompts with performance comparable or better than existing state ofthe art code generation models.$$$$$http://arxiv.org/pdf/2305.02783v2
Gpt-4: A Review on Advancements and Opportunities in Natural Language  Processing$  Generative Pre-trained Transformer 4 (GPT-4) is the fourth-generationlanguage model in the GPT series, developed by OpenAI, which promisessignificant advancements in the field of natural language processing (NLP). Inthis research article, we have discussed the features of GPT-4, its potentialapplications, and the challenges that it might face. We have also comparedGPT-4 with its predecessor, GPT-3. GPT-4 has a larger model size (more than onetrillion), better multilingual capabilities, improved contextual understanding,and reasoning capabilities than GPT-3. Some of the potential applications ofGPT-4 include chatbots, personal assistants, language translation, textsummarization, and question-answering. However, GPT-4 poses several challengesand limitations such as computational requirements, data requirements, andethical concerns.$$$$$http://arxiv.org/pdf/2305.03195v1
QCRI at SemEval-2023 Task 3: News Genre, Framing and Persuasion  Techniques Detection using Multilingual Models$  Misinformation spreading in mainstream and social media has been misleadingusers in different ways. Manual detection and verification efforts byjournalists and fact-checkers can no longer cope with the great scale and quickspread of misleading information. This motivated research and industry effortsto develop systems for analyzing and verifying news spreading online. TheSemEval-2023 Task 3 is an attempt to address several subtasks under thisoverarching problem, targeting writing techniques used in news articles toaffect readers\' opinions. The task addressed three subtasks with six languages,in addition to three ``surprise\'\' test languages, resulting in 27 differenttest setups. This paper describes our participating system to this task. Ourteam is one of the 6 teams that successfully submitted runs for all setups. Theofficial results show that our system is ranked among the top 3 systems for 10out of the 27 setups.$$$$$http://arxiv.org/pdf/2305.03336v1
Bayesian Reinforcement Learning with Limited Cognitive Load$  All biological and artificial agents must learn and make decisions givenlimits on their ability to process information. As such, a general theory ofadaptive behavior should be able to account for the complex interactionsbetween an agent\'s learning history, decisions, and capacity constraints.Recent work in computer science has begun to clarify the principles that shapethese dynamics by bridging ideas from reinforcement learning, Bayesiandecision-making, and rate-distortion theory. This body of work provides anaccount of capacity-limited Bayesian reinforcement learning, a unifyingnormative framework for modeling the effect of processing constraints onlearning and action selection. Here, we provide an accessible review of recentalgorithms and theoretical results in this setting, paying special attention tohow these ideas can be applied to studying questions in the cognitive andbehavioral sciences.$$$$$http://arxiv.org/pdf/2305.03263v1
A Comprehensive Study on Dataset Distillation: Performance, Privacy,  Robustness and Fairness$  The aim of dataset distillation is to encode the rich features of an originaldataset into a tiny dataset. It is a promising approach to accelerate neuralnetwork training and related studies. Different approaches have been proposedto improve the informativeness and generalization performance of distilledimages. However, no work has comprehensively analyzed this technique from asecurity perspective and there is a lack of systematic understanding ofpotential risks. In this work, we conduct extensive experiments to evaluatecurrent state-of-the-art dataset distillation methods. We successfully usemembership inference attacks to show that privacy risks still remain. Our workalso demonstrates that dataset distillation can cause varying degrees of impacton model robustness and amplify model unfairness across classes when makingpredictions. This work offers a large-scale benchmarking framework for datasetdistillation evaluation.$$$$$http://arxiv.org/pdf/2305.03355v1
Adaptive Graph Convolutional Subspace Clustering$  Spectral-type subspace clustering algorithms have shown excellent performancein many subspace clustering applications. The existing spectral-type subspaceclustering algorithms either focus on designing constraints for thereconstruction coefficient matrix or feature extraction methods for findinglatent features of original data samples. In this paper, inspired by graphconvolutional networks, we use the graph convolution technique to develop afeature extraction method and a coefficient matrix constraint simultaneously.And the graph-convolutional operator is updated iteratively and adaptively inour proposed algorithm. Hence, we call the proposed method adaptive graphconvolutional subspace clustering (AGCSC). We claim that by using AGCSC, theaggregated feature representation of original data samples is suitable forsubspace clustering, and the coefficient matrix could reveal the subspacestructure of the original data set more faithfully. Finally, plenty of subspaceclustering experiments prove our conclusions and show that AGCSC outperformssome related methods as well as some deep models.$$$$$http://arxiv.org/pdf/2305.03414v1
Learning Decision Trees with Gradient Descent$  Decision Trees (DTs) are commonly used for many machine learning tasks due totheir high degree of interpretability. However, learning a DT from data is adifficult optimization problem, as it is non-convex and non-differentiable.Therefore, common approaches learn DTs using a greedy growth algorithm thatminimizes the impurity locally at each internal node. Unfortunately, thisgreedy procedure can lead to suboptimal trees. In this paper, we present anovel approach for learning hard, axis-aligned DTs with gradient descent. Theproposed method uses backpropagation with a straight-through operator on adense DT representation to jointly optimize all tree parameters. Our approachoutperforms existing methods on binary classification benchmarks and achievescompetitive results for multi-class tasks.$$$$$http://arxiv.org/pdf/2305.03515v1
Differentiable Gaussianization Layers for Inverse Problems Regularized  by Deep Generative Models$  Deep generative models such as GANs, normalizing flows, and diffusion modelsare powerful regularizers for inverse problems. They exhibit great potentialfor helping reduce ill-posedness and attain high-quality results. However, thelatent tensors of such deep generative models can fall out of the desiredhigh-dimensional standard Gaussian distribution during inversion, particularlyin the presence of data noise and inaccurate forward models, leading tolow-fidelity solutions. To address this issue, we propose to reparameterize andGaussianize the latent tensors using novel differentiable data-dependent layerswherein custom operators are defined by solving optimization problems. Theseproposed layers constrain inverse problems to obtain high-fidelityin-distribution solutions. We validate our technique on three inversion tasks:compressive-sensing MRI, image deblurring, and eikonal tomography (a nonlinearPDE-constrained inverse problem) using two representative deep generativemodels: StyleGAN2 and Glow. Our approach achieves state-of-the-art performancein terms of accuracy and consistency.$$$$$http://arxiv.org/pdf/2112.03860v4
Deep Multi-View Semi-Supervised Clustering with Sample Pairwise  Constraints$  Multi-view clustering has attracted much attention thanks to the capacity ofmulti-source information integration. Although numerous advanced methods havebeen proposed in past decades, most of them generally overlook the significanceof weakly-supervised information and fail to preserve the feature properties ofmultiple views, thus resulting in unsatisfactory clustering performance. Toaddress these issues, in this paper, we propose a novel Deep Multi-viewSemi-supervised Clustering (DMSC) method, which jointly optimizes three kindsof losses during networks finetuning, including multi-view clustering loss,semi-supervised pairwise constraint loss and multiple autoencodersreconstruction loss. Specifically, a KL divergence based multi-view clusteringloss is imposed on the common representation of multi-view data to performheterogeneous feature optimization, multi-view weighting and clusteringprediction simultaneously. Then, we innovatively propose to integrate pairwiseconstraints into the process of multi-view clustering by enforcing the learnedmulti-view representation of must-link samples (cannot-link samples) to besimilar (dissimilar), such that the formed clustering architecture can be morecredible. Moreover, unlike existing rivals that only preserve the encoders foreach heterogeneous branch during networks finetuning, we further propose totune the intact autoencoders frame that contains both encoders and decoders. Inthis way, the issue of serious corruption of view-specific and view-sharedfeature space could be alleviated, making the whole training procedure morestable. Through comprehensive experiments on eight popular image datasets, wedemonstrate that our proposed approach performs better than thestate-of-the-art multi-view and single-view competitors.$Multi-view clustering, Deep clustering, Semi-supervised clustering, Pairwise constraints, Feature properties protection.$aCollege of Information Science and Technology, Hainan University, Haikou, 570208, China；bResearch Center of Precision Sensing and Control, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China；cState Key Laboratory of Marine Resource Utilization in South China Sea, Hainan University, Haikou, 570208, China$提出了一种名为“DMSC”的方法，该方法通过联合优化三种损失（包括多视图聚类损失、半监督配对约束损失和多个自动编码器重构损失），实现异构特征优化，多视图权重和聚类预测，从而解决了当前多视图聚类中存在的弱监督信息忽略和多视图特征信息损坏等问题。此外，与现有方法只保留每个异构支路的编码器不同，在对网络进行微调时，本文提出同时调整包含编码器和解码器的完整自动编码器框架，以缓解视图特定和视图共享特征空间的严重破坏，增强整个训练过程的稳定性。通过对八个流行的图像数据集的全面实验，证明提出的方法优于现有的多视图和单视图竞争对手。$多视图聚类，深度聚类，半监督聚类，样本配对约束，特征属性保护。$http://arxiv.org/pdf/2206.04949v2
Tree species classification from hyperspectral data using  graph-regularized neural networks$  We propose a novel graph-regularized neural network (GRNN) algorithm for treespecies classification. The proposed algorithm encompasses superpixel-basedsegmentation for graph construction, a pixel-wise neural network classifier,and the label propagation technique to generate an accurate and realistic(emulating tree crowns) classification map on a sparsely annotated data set.GRNN outperforms several state-of-the-art techniques not only for the standardIndian Pines HSI but also achieves a high classification accuracy (approx. 92%)on a new HSI data set collected over the heterogeneous forests of French Guiana(FG) when less than 1% of the pixels are labeled. We further show that GRNN iscompetitive with the state-of-the-art semi-supervised methods and exhibits asmall deviation in accuracy for different numbers of training samples and overrepeated trials with randomly sampled labeled pixels for training.$$$$$http://arxiv.org/pdf/2208.08675v2
FLamby: Datasets and Benchmarks for Cross-Silo Federated Learning in  Realistic Healthcare Settings$"  Federated Learning (FL) is a novel approach enabling several clients holdingsensitive data to collaboratively train machine learning models, withoutcentralizing data. The cross-silo FL setting corresponds to the case of few($2$--$50$) reliable clients, each holding medium to large datasets, and istypically found in applications such as healthcare, finance, or industry. Whileprevious works have proposed representative datasets for cross-device FL, fewrealistic healthcare cross-silo FL datasets exist, thereby slowing algorithmicresearch in this critical application. In this work, we propose a novelcross-silo dataset suite focused on healthcare, FLamby (Federated LearningAMple Benchmark of Your cross-silo strategies), to bridge the gap betweentheory and practice of cross-silo FL. FLamby encompasses 7 healthcare datasetswith natural splits, covering multiple tasks, modalities, and data volumes,each accompanied with baseline training code. As an illustration, weadditionally benchmark standard FL algorithms on all datasets. Our flexible andmodular suite allows researchers to easily download datasets, reproduce resultsand re-use the different components for their research. FLamby is availableat~\\url{www.github.com/owkin/flamby}."$$$$$http://arxiv.org/pdf/2210.04620v3
Towards Effective Collaborative Learning in Long-Tailed Recognition$  Real-world data usually suffers from severe class imbalance and long-taileddistributions, where minority classes are significantly underrepresentedcompared to the majority ones. Recent research prefers to utilize multi-expertarchitectures to mitigate the model uncertainty on the minority, wherecollaborative learning is employed to aggregate the knowledge of experts, i.e.,online distillation. In this paper, we observe that the knowledge transferbetween experts is imbalanced in terms of class distribution, which results inlimited performance improvement of the minority classes. To address it, wepropose a re-weighted distillation loss by comparing two classifiers\'predictions, which are supervised by online distillation and label annotations,respectively. We also emphasize that feature-level distillation willsignificantly improve model performance and increase feature robustness.Finally, we propose an Effective Collaborative Learning (ECL) framework thatintegrates a contrastive proxy task branch to further improve feature quality.Quantitative and qualitative experiments on four standard datasets demonstratethat ECL achieves state-of-the-art performance and the detailed ablationstudies manifest the effectiveness of each component in ECL.$$$$$http://arxiv.org/pdf/2305.03378v1
Fine-Grained Product Classification on Leaflet Advertisements$"  In this paper, we describe a first publicly available fine-grained productrecognition dataset based on leaflet images. Using advertisement leaflets,collected over several years from different European retailers, we provide atotal of 41.6k manually annotated product images in 832 classes. Further, weinvestigate three different approaches for this fine-grained productclassification task, Classification by Image, by Text, as well as by Image andText. The approach ""Classification by Text"" uses the text extracted directlyfrom the leaflet product images. We show, that the combination of image andtext as input improves the classification of visual difficult to distinguishproducts. The final model leads to an accuracy of 96.4% with a Top-3 score of99.2%. We release our code athttps://github.com/ladwigd/Leaflet-Product-Classification."$fine-grained product classification, leaflet advertisements, computer vision$IMLA, Offenburg University；Markant Services International GmbH$本文提供了第一个基于广告传单图像的细粒度产品识别数据集，并研究了三种不同的分类方法。文章旨在解决计算机视觉中的细粒度产品分类问题，以实现商品价格监测。通过图像和文本相结合的方法，最终实现了96.4％的精度和99.2％的Top-3得分。$细粒度产品分类、广告传单、计算机视觉$http://arxiv.org/pdf/2305.03706v1
Rethinking the Event Coding Pipeline with Prompt Entailment$"  For monitoring crises, political events are extracted from the news. Thelarge amount of unstructured full-text event descriptions makes a case-by-caseanalysis unmanageable, particularly for low-resource humanitarian aidorganizations. This creates a demand to classify events into event types, atask referred to as event coding. Typically, domain experts craft an event typeontology, annotators label a large dataset and technical experts develop asupervised coding system. In this work, we propose PR-ENT, a new event codingapproach that is more flexible and resource-efficient, while maintainingcompetitive accuracy: first, we extend an event description such as ""Militaryinjured two civilians\'\' by a template, e.g. ""People were [Z]"" and prompt apre-trained (cloze) language model to fill the slot Z. Second, we select answercandidates Z* = {""injured\'\', ""hurt""...} by treating the event description aspremise and the filled templates as hypothesis in a textual entailment task.This allows domain experts to draft the codebook directly as labeled promptsand interpretable answer candidates. This human-in-the-loop process is guidedby our interactive codebook design tool. We evaluate PR-ENT in severalrobustness checks: perturbing the event description and prompt template,restricting the vocabulary and removing contextual information."$$$$$http://arxiv.org/pdf/2210.05257v2
White-Box Multi-Objective Adversarial Attack on Dialogue Generation$  Pre-trained transformers are popular in state-of-the-art dialogue generation(DG) systems. Such language models are, however, vulnerable to variousadversarial samples as studied in traditional tasks such as textclassification, which inspires our curiosity about their robustness in DGsystems. One main challenge of attacking DG models is that perturbations on thecurrent sentence can hardly degrade the response accuracy because the unchangedchat histories are also considered for decision-making. Instead of merelypursuing pitfalls of performance metrics such as BLEU, ROUGE, we observe thatcrafting adversarial samples to force longer generation outputs benefits attackeffectiveness -- the generated responses are typically irrelevant, lengthy, andrepetitive. To this end, we propose a white-box multi-objective attack methodcalled DGSlow. Specifically, DGSlow balances two objectives -- generationaccuracy and length, via a gradient-based multi-objective optimizer and appliesan adaptive searching mechanism to iteratively craft adversarial samples withonly a few modifications. Comprehensive experiments on four benchmark datasetsdemonstrate that DGSlow could significantly degrade state-of-the-art DG modelswith a higher success rate than traditional accuracy-based methods. Besides,our crafted sentences also exhibit strong transferability in attacking othermodels.$$$$$http://arxiv.org/pdf/2305.03655v1
DualCross: Cross-Modality Cross-Domain Adaptation for Monocular BEV  Perception$  Closing the domain gap between training and deployment and incorporatingmultiple sensor modalities are two challenging yet critical topics forself-driving. Existing work only focuses on single one of the above topics,overlooking the simultaneous domain and modality shift which pervasively existsin real-world scenarios. A model trained with multi-sensor data collected inEurope may need to run in Asia with a subset of input sensors available. Inthis work, we propose DualCross, a cross-modality cross-domain adaptationframework to facilitate the learning of a more robust monocular bird\'s-eye-view(BEV) perception model, which transfers the point cloud knowledge from a LiDARsensor in one domain during the training phase to the camera-only testingscenario in a different domain. This work results in the first open analysis ofcross-domain cross-sensor perception and adaptation for monocular 3D tasks inthe wild. We benchmark our approach on large-scale datasets under a wide rangeof domain shifts and show state-of-the-art results against various baselines.$$$$$http://arxiv.org/pdf/2305.03724v1
Posterior Regularization on Bayesian Hierarchical Mixture Clustering$  Bayesian hierarchical mixture clustering (BHMC) improves traditionalBayesianhierarchical clustering by replacing conventional Gaussian-to-Gaussian kernelswith a Hierarchical Dirichlet Process Mixture Model(HDPMM) for parent-to-childdiffusion in the generative process. However,BHMC may produce trees with highnodal variance, indicating weak separation between nodes at higher levels. Toaddress this issue, we employ Posterior Regularization, which imposesmax-margin constraints on nodes at every level to enhance cluster separation.We illustrate how to apply PR toBHMC and demonstrate its effectiveness inimproving the BHMC model.$$$$$http://arxiv.org/pdf/2105.06903v7
PredProp: Bidirectional Stochastic Optimization with Precision Weighted  Predictive Coding$  We present PredProp, a method for optimization of weights and states inpredictive coding networks (PCNs) based on the precision of propagated errorsand neural activity. PredProp jointly addresses inference and learning viastochastic gradient descent and adaptively weights parameter updates byapproximate curvature. Due to the relation between propagated error covarianceand the Fisher information matrix, PredProp implements approximate NaturalGradient Descent. We demonstrate PredProp\'s effectiveness in the context ofdense decoder networks and simple image benchmark datasets. We found thatPredProp performs favorably over Adam, a widely used adaptive learning rateoptimizer in the tested configurations. Furthermore, available optimizationmethods for weight parameters benefit from using PredProp\'s error precisionduring inference. Since hierarchical predictive coding layers are optimisedindividually using local errors, the required precisions factorize overhierarchical layers. Extending beyond classical PCNs with a single set ofdecoder layers per hierarchical layer, we also generalize PredProp to deepneural networks in each PCN layer by additionally factorizing over the weightsin each PCN layer.$$$$$http://arxiv.org/pdf/2111.08792v2
Diagnostics for Deep Neural Networks with Automated Copy/Paste Attacks$  This paper considers the problem of helping humans exercise scalableoversight over deep neural networks (DNNs). Adversarial examples can be usefulby helping to reveal weaknesses in DNNs, but they can be difficult to interpretor draw actionable conclusions from. Some previous works have proposed usinghuman-interpretable adversarial attacks including copy/paste attacks in whichone natural image pasted into another causes an unexpected misclassification.We build on these with two contributions. First, we introduce Search forNatural Adversarial Features Using Embeddings (SNAFUE) which offers a fullyautomated method for finding copy/paste attacks. Second, we use SNAFUE to redteam an ImageNet classifier. We reproduce copy/paste attacks from previousworks and find hundreds of other easily-describable vulnerabilities, allwithout a human in the loop. Code is available athttps://github.com/thestephencasper/snafue$$$$$http://arxiv.org/pdf/2211.10024v3
A Comprehensive Survey on Enterprise Financial Risk Analysis from Big  Data Perspective$  Enterprise financial risk analysis aims at predicting the future financialrisk of enterprises. Due to its wide and significant application, enterprisefinancial risk analysis has always been the core research topic in the fieldsof Finance and Management. Based on advanced computer science and artificialintelligence technologies, enterprise risk analysis research is experiencingrapid developments and making significant progress. Therefore, it is bothnecessary and challenging to comprehensively review the relevant studies.Although there are already some valuable and impressive surveys on enterpriserisk analysis from the perspective of Finance and Management, these surveysintroduce approaches in a relatively isolated way and lack recent advances inenterprise financial risk analysis. In contrast, this paper attempts to providea systematic literature survey of enterprise risk analysis approaches from BigData perspective, which reviews more than 250 representative articles in thepast almost 50 years (from 1968 to 2023). To the best of our knowledge, this isthe first and only survey work on enterprise financial risk from Big Dataperspective. Specifically, this survey connects and systematizes the existingenterprise financial risk studies, i.e. to summarize and interpret theproblems, methods, and spotlights in a comprehensive way. In particular, wefirst introduce the issues of enterprise financial risks in terms of theirtypes,granularity, intelligence, and evaluation metrics, and summarize thecorresponding representative works. Then, we compare the analysis methods usedto learn enterprise financial risk, and finally summarize the spotlights of themost representative works. Our goal is to clarify current cutting-edge researchand its possible future directions to model enterprise risk, aiming to fullyunderstand the mechanisms of enterprise risk generation and contagion.$$$$$http://arxiv.org/pdf/2211.14997v3
LSTM-based Preceding Vehicle Behaviour Prediction during Aggressive Lane  Change for ACC Application$  The development of Adaptive Cruise Control (ACC) systems aims to enhance thesafety and comfort of vehicles by automatically regulating the speed of thevehicle to ensure a safe gap from the preceding vehicle. However, conventionalACC systems are unable to adapt themselves to changing driving conditions anddrivers\' behavior. To address this limitation, we propose a Long Short-TermMemory (LSTM) based ACC system that can learn from past driving experiences andadapt and predict new situations in real time. The model is constructed basedon the real-world highD dataset, acquired from German highways with theassistance of camera-equipped drones. We evaluated the ACC system underaggressive lane changes when the side lane preceding vehicle cut off, forcingthe targeted driver to reduce speed. To this end, the proposed system wasassessed on a simulated driving environment and compared with a feedforwardArtificial Neural Network (ANN) model and Model Predictive Control (MPC) model.The results show that the LSTM-based system is 19.25% more accurate than theANN model and 5.9% more accurate than the MPC model in terms of predictingfuture values of subject vehicle acceleration. The simulation is done inMatlab/Simulink environment.$$$$$http://arxiv.org/pdf/2305.01095v2
Multiplicity Boost Of Transit Signal Classifiers: Validation of 69 New  Exoplanets Using The Multiplicity Boost of ExoMiner$  Most existing exoplanets are discovered using validation techniques ratherthan being confirmed by complementary observations. These techniques generate ascore that is typically the probability of the transit signal being anexoplanet (y(x)=exoplanet) given some information related to that signal(represented by x). Except for the validation technique in Rowe et al. (2014)that uses multiplicity information to generate these probability scores, theexisting validation techniques ignore the multiplicity boost information. Inthis work, we introduce a framework with the following premise: given anexisting transit signal vetter (classifier), improve its performance usingmultiplicity information. We apply this framework to several existingclassifiers, which include vespa (Morton et al. 2016), Robovetter (Coughlin etal. 2017), AstroNet (Shallue &amp; Vanderburg 2018), ExoNet (Ansdel et al. 2018),GPC and RFC (Armstrong et al. 2020), and ExoMiner (Valizadegan et al. 2022), tosupport our claim that this framework is able to improve the performance of agiven classifier. We then use the proposed multiplicity boost framework forExoMiner V1.2, which addresses some of the shortcomings of the originalExoMiner classifier (Valizadegan et al. 2022), and validate 69 new exoplanetsfor systems with multiple KOIs from the Kepler catalog.$$$$$http://arxiv.org/pdf/2305.02470v2
Hierarchical Transformer for Scalable Graph Learning$  Graph Transformer is gaining increasing attention in the field of machinelearning and has demonstrated state-of-the-art performance on benchmarks forgraph representation learning. However, as current implementations of GraphTransformer primarily focus on learning representations of small-scale graphs,the quadratic complexity of the global self-attention mechanism presents achallenge for full-batch training when applied to larger graphs. Additionally,conventional sampling-based methods fail to capture necessary high-levelcontextual information, resulting in a significant loss of performance. In thispaper, we introduce the Hierarchical Scalable Graph Transformer (HSGT) as asolution to these challenges. HSGT successfully scales the Transformerarchitecture to node representation learning tasks on large-scale graphs, whilemaintaining high performance. By utilizing graph hierarchies constructedthrough coarsening techniques, HSGT efficiently updates and stores multi-scaleinformation in node embeddings at different levels. Together withsampling-based training methods, HSGT effectively captures and aggregatesmulti-level information on the hierarchical graph using only Transformerblocks. Empirical evaluations demonstrate that HSGT achieves state-of-the-artperformance on large-scale benchmarks with graphs containing millions of nodeswith high efficiency.$$$$$http://arxiv.org/pdf/2305.02866v2
Leveraging gradient-derived metrics for data selection and valuation in  differentially private training$  Obtaining high-quality data for collaborative training of machine learningmodels can be a challenging task due to A) the regulatory concerns and B) lackof incentive to participate. The first issue can be addressed through the useof privacy enhancing technologies (PET), one of the most frequently used onebeing differentially private (DP) training. The second challenge can beaddressed by identifying which data points can be beneficial for model trainingand rewarding data owners for sharing this data. However, DP in deep learningtypically adversely affects atypical (often informative) data samples, makingit difficult to assess the usefulness of individual contributions. In this workwe investigate how to leverage gradient information to identify trainingsamples of interest in private training settings. We show that there existtechniques which are able to provide the clients with the tools for principleddata selection even in strictest privacy settings.$$$$$http://arxiv.org/pdf/2305.02942v2
Composite Motion Learning with Task Control$  We present a deep learning method for composite and task-driven motioncontrol for physically simulated characters. In contrast to existingdata-driven approaches using reinforcement learning that imitate full-bodymotions, we learn decoupled motions for specific body parts from multiplereference motions simultaneously and directly by leveraging the use of multiplediscriminators in a GAN-like setup. In this process, there is no need of anymanual work to produce composite reference motions for learning. Instead, thecontrol policy explores by itself how the composite motions can be combinedautomatically. We further account for multiple task-specific rewards and traina single, multi-objective control policy. To this end, we propose a novelframework for multi-objective learning that adaptively balances the learning ofdisparate motions from multiple sources and multiple goal-directed controlobjectives. In addition, as composite motions are typically augmentations ofsimpler behaviors, we introduce a sample-efficient method for trainingcomposite control policies in an incremental manner, where we reuse apre-trained policy as the meta policy and train a cooperative policy thatadapts the meta one for new composite tasks. We show the applicability of ourapproach on a variety of challenging multi-objective tasks involving bothcomposite motion imitation and multiple goal-directed control.$Motion Control, Physical Simulation, Deep Learning, Reinforcement Learning, Multi-Objective Learning, Incremental Learning, GAN$Clemson University, USA and Roblox, USA; University of California, Merced, USA$本文提出了一种新的基于深度学习的复合运动学习框架，用于物理仿真角色的控制。与现有的强化学习方法相比，该方法采用多个鉴别器来实现不同部位的解耦运动学习。该框架可以同时引入多种不同的参考运动和目标任务奖励，并提供一个适应性的多目标学习机制来平衡不同来源和目标方向的运动学习，同时支持递增学习策略来提高训练效率。该方法能够有效地处理多个不同任务的合成动作控制，并取得了令人满意的实验结果。$运动控制，物理仿真，深度学习，增强学习，多目标学习，递增学习，GAN$http://arxiv.org/pdf/2305.03286v1
A Survey on Offline Model-Based Reinforcement Learning$  Model-based approaches are becoming increasingly popular in the field ofoffline reinforcement learning, with high potential in real-world applicationsdue to the model\'s capability of thoroughly utilizing the large historicaldatasets available with supervised learning techniques. This paper presents aliterature review of recent work in offline model-based reinforcement learning,a field that utilizes model-based approaches in offline reinforcement learning.The survey provides a brief overview of the concepts and recent developments inboth offline reinforcement learning and model-based reinforcement learning, anddiscuss the intersection of the two fields. We then presents key relevantpapers in the field of offline model-based reinforcement learning and discusstheir methods, particularly their approaches in solving the issue ofdistributional shift, the main problem faced by all current offline model-basedreinforcement learning methods. We further discuss key challenges faced by thefield, and suggest possible directions for future work.$$$$$http://arxiv.org/pdf/2305.03360v1
Deep Learning for Classification of Thyroid Nodules on Ultrasound:  Validation on an Independent Dataset$  Objectives: The purpose is to apply a previously validated deep learningalgorithm to a new thyroid nodule ultrasound image dataset and compare itsperformances with radiologists. Methods: Prior study presented an algorithmwhich is able to detect thyroid nodules and then make malignancyclassifications with two ultrasound images. A multi-task deep convolutionalneural network was trained from 1278 nodules and originally tested with 99separate nodules. The results were comparable with that of radiologists. Thealgorithm was further tested with 378 nodules imaged with ultrasound machinesfrom different manufacturers and product types than the training cases. Fourexperienced radiologists were requested to evaluate the nodules for comparisonwith deep learning. Results: The Area Under Curve (AUC) of the deep learningalgorithm and four radiologists were calculated with parametric, binormalestimation. For the deep learning algorithm, the AUC was 0.69 (95% CI: 0.64 -0.75). The AUC of radiologists were 0.63 (95% CI: 0.59 - 0.67), 0.66 (95%CI:0.61 - 0.71), 0.65 (95% CI: 0.60 - 0.70), and 0.63 (95%CI: 0.58 - 0.67).Conclusion: In the new testing dataset, the deep learning algorithm achievedsimilar performances with all four radiologists. The relative performancedifference between the algorithm and the radiologists is not significantlyaffected by the difference of ultrasound scanner.$$$$$http://arxiv.org/pdf/2207.13765v2
Predicting air quality via multimodal AI and satellite imagery$"  Climate change may be classified as the most important environmental problemthat the Earth is currently facing, and affects all living species on Earth.Given that air-quality monitoring stations are typically ground-based theirabilities to detect pollutant distributions are often restricted to wide areas.Satellites however have the potential for studying the atmosphere at large; theEuropean Space Agency (ESA) Copernicus project satellite, ""Sentinel-5P"" is anewly launched satellite capable of measuring a variety of pollutantinformation with publicly available data outputs. This paper seeks to create amulti-modal machine learning model for predicting air-quality metrics wheremonitoring stations do not exist. The inputs of this model will include afusion of ground measurements and satellite data with the goal of highlightingpollutant distribution and motivating change in societal and industrialbehaviors. A new dataset of European pollution monitoring station measurementsis created with features including $\\textit{altitude, population, etc.}$ fromthe ESA Copernicus project. This dataset is used to train a multi-modal MLmodel, Air Quality Network (AQNet) capable of fusing these various types ofdata sources to output predictions of various pollutants. These predictions arethen aggregated to create an ""air-quality index"" that could be used to compareair quality over different regions. Three pollutants, NO$_2$, O$_3$, andPM$_{10}$, are predicted successfully by AQNet and the network was found to beuseful compared to a model only using satellite imagery. It was also found thatthe addition of supporting data improves predictions. When testing thedeveloped AQNet on out-of-sample data of the UK and Ireland, we obtainsatisfactory estimates though on average pollution metrics were roughlyoverestimated by around 20\\%."$$$$$http://arxiv.org/pdf/2211.00780v2
Exploring the Connection between Robust and Generative Models$  We offer a study that connects robust discriminative classifiers trained withadversarial training (AT) with generative modeling in the form of Energy-basedModels (EBM). We do so by decomposing the loss of a discriminative classifierand showing that the discriminative model is also aware of the input datadensity. Though a common assumption is that adversarial points leave themanifold of the input data, our study finds out that, surprisingly, untargetedadversarial points in the input space are very likely under the generativemodel hidden inside the discriminative classifier -- have low energy in theEBM. We present two evidence: untargeted attacks are even more likely than thenatural data and their likelihood increases as the attack strength increases.This allows us to easily detect them and craft a novel attack calledHigh-Energy PGD that fools the classifier yet has energy similar to the dataset.$$$$$http://arxiv.org/pdf/2304.04033v2
Reconstructing Training Data from Multiclass Neural Networks$"  Reconstructing samples from the training set of trained neural networks is amajor privacy concern. Haim et al. (2022) recently showed that it is possibleto reconstruct training samples from neural network binary classifiers, basedon theoretical results about the implicit bias of gradient methods. In thiswork, we present several improvements and new insights over this previous work.As our main improvement, we show that training-data reconstruction is possiblein the multi-class setting and that the reconstruction quality is even higherthan in the case of binary classification. Moreover, we show that usingweight-decay during training increases the vulnerability to samplereconstruction. Finally, while in the previous work the training set was ofsize at most $1000$ from $10$ classes, we show preliminary evidence of theability to reconstruct from a model trained on $5000$ samples from $100$classes."$$$$$http://arxiv.org/pdf/2305.03350v1
Domain-agnostic segmentation of thalamic nuclei from joint structural  and diffusion MRI$  The human thalamus is a highly connected subcortical grey-matter structurewithin the brain. It comprises dozens of nuclei with different function andconnectivity, which are affected differently by disease. For this reason, thereis growing interest in studying the thalamic nuclei in vivo with MRI. Tools areavailable to segment the thalamus from 1 mm T1 scans, but the contrast of thelateral and internal boundaries is too faint to produce reliable segmentations.Some tools have attempted to incorporate information from diffusion MRI in thesegmentation to refine these boundaries, but do not generalise well acrossdiffusion MRI acquisitions. Here we present the first CNN that can segmentthalamic nuclei from T1 and diffusion data of any resolution without retrainingor fine tuning. Our method builds on a public histological atlas of thethalamic nuclei and silver standard segmentations on high-quality diffusiondata obtained with a recent Bayesian adaptive segmentation tool. We combinethese with an approximate degradation model for fast domain randomisationduring training. Our CNN produces a segmentation at 0.7 mm isotropicresolution, irrespective of the resolution of the input. Moreover, it uses aparsimonious model of the diffusion signal at each voxel (fractional anisotropyand principal eigenvector) that is compatible with virtually any set ofdirections and b-values, including huge amounts of legacy data. We show resultsof our proposed method on three heterogeneous datasets acquired on dozens ofdifferent scanners. An implementation of the method is publicly available athttps://freesurfer.net/fswiki/ThalamicNucleiDTI.$$$$$http://arxiv.org/pdf/2305.03413v1
Segmentation of fundus vascular images based on a dual-attention  mechanism$  Accurately segmenting blood vessels in retinal fundus images is crucial inthe early screening, diagnosing, and evaluating some ocular diseases. However,significant light variations and non-uniform contrast in these images makesegmentation quite challenging. Thus, this paper employ an attention fusionmechanism that combines the channel attention and spatial attention mechanismsconstructed by Transformer to extract information from retinal fundus images inboth spatial and channel dimensions. To eliminate noise from the encoder image,a spatial attention mechanism is introduced in the skip connection. Moreover, aDropout layer is employed to randomly discard some neurons, which can preventoverfitting of the neural network and improve its generalization performance.Experiments were conducted on publicly available datasets DERIVE, STARE, andCHASEDB1. The results demonstrate that our method produces satisfactory resultscompared to some recent retinal fundus image segmentation algorithms.$$$$$http://arxiv.org/pdf/2305.03617v1
G-MATT: Single-step Retrosynthesis Prediction using Molecular Grammar  Tree Transformer$  In recent years, several reaction templates-based and template-freeapproaches have been reported for single-step retrosynthesis prediction. Eventhough many of these approaches perform well from traditional data-drivenmetrics standpoint, there is a disconnect between model architectures used andunderlying chemistry principles governing retrosynthesis. Here, we propose anovel chemistry-aware retrosynthesis prediction framework that combinespowerful data-driven models with chemistry knowledge. We report atree-to-sequence transformer architecture based on hierarchical SMILES grammartrees as input containing underlying chemistry information that is otherwiseignored by models based on purely SMILES-based representations. The proposedframework, grammar-based molecular attention tree transformer (G-MATT),achieves significant performance improvements compared to baselineretrosynthesis models. G-MATT achieves a top-1 accuracy of 51% (top-10 accuracyof 79.1%), invalid rate of 1.5%, and bioactive similarity rate of 74.8%.Further analyses based on attention maps demonstrate G-MATT\'s ability topreserve chemistry knowledge without having to use extremely complex modelarchitectures.$$$$$http://arxiv.org/pdf/2305.03153v1
Learning Discriminative Representations and Decision Boundaries for Open  Intent Detection$  Open intent detection is a significant problem in natural languageunderstanding, which aims to identify the unseen open intent while ensuringknown intent identification performance. However, current methods face twomajor challenges. Firstly, they struggle to learn friendly representations todetect the open intent with prior knowledge of only known intents. Secondly,there is a lack of an effective approach to obtaining specific and compactdecision boundaries for known intents. To address these issues, this paperpresents an original framework called DA-ADB, which successively learnsdistance-aware intent representations and adaptive decision boundaries for openintent detection. Specifically, we first leverage distance information toenhance the distinguishing capability of the intent representations. Then, wedesign a novel loss function to obtain appropriate decision boundaries bybalancing both empirical and open space risks. Extensive experimentsdemonstrate the effectiveness of the proposed distance-aware and boundarylearning strategies. Compared to state-of-the-art methods, our frameworkachieves substantial improvements on three benchmark datasets. Furthermore, ityields robust performance with varying proportions of labeled data and knowncategories.$$$$$http://arxiv.org/pdf/2203.05823v3
Don\'t Lose Yourself! Empathetic Response Generation via Explicit  Self-Other Awareness$  As a critical step to achieve human-like chatbots, empathetic responsegeneration has attained increasing interests. Previous attempts are incompleteand not sufficient enough to elicit empathy because they only focus on theinitial aspect of empathy to automatically mimic the feelings and thoughts ofthe user via other-awareness. However, they ignore to maintain and take the ownviews of the system into account, which is a crucial process to achieve theempathy called self-other awareness. To this end, we propose to generateEmpathetic response with explicit Self-Other Awareness (EmpSOA). Specifically,three stages, self-other differentiation, self-other modulation and self-othergeneration, are devised to clearly maintain, regulate and inject the self-otheraware information into the process of empathetic response generation. Bothautomatic and human evaluations on the benchmark dataset demonstrate thesuperiority of EmpSOA to generate more empathetic responses.$$$$$http://arxiv.org/pdf/2210.03884v2
PVGRU: Generating Diverse and Relevant Dialogue Responses via  Pseudo-Variational Mechanism$  We investigate response generation for multi-turn dialogue ingenerative-based chatbots. Existing generative models based on RNNs (RecurrentNeural Networks) usually employ the last hidden state to summarize thesequences, which makes models unable to capture the subtle variability observedin different dialogues and cannot distinguish the differences between dialoguesthat are similar in composition. In this paper, we propose a Pseudo-VariationalGated Recurrent Unit (PVGRU) component without posterior knowledge throughintroducing a recurrent summarizing variable into the GRU, which can aggregatethe accumulated distribution variations of subsequences. PVGRU can perceive thesubtle semantic variability through summarizing variables that are optimized bythe devised distribution consistency and reconstruction objectives. Inaddition, we build a Pseudo-Variational Hierarchical Dialogue (PVHD) modelbased on PVGRU. Experimental results demonstrate that PVGRU can broadly improvethe diversity and relevance of responses on two benchmark datasets.$$$$$http://arxiv.org/pdf/2212.09086v2
On the Blind Spots of Model-Based Evaluation Metrics for Text Generation$  In this work, we explore a useful but often neglected methodology forrobustness analysis of text generation evaluation metrics: stress tests withsynthetic data. Basically, we design and synthesize a wide range of potentialerrors and check whether they result in a commensurate drop in the metricscores. We examine a range of recently proposed evaluation metrics based onpretrained language models, for the tasks of open-ended generation,translation, and summarization. Our experiments reveal interestinginsensitivities, biases, or even loopholes in existing metrics. For example, wefind that BERTScore is confused by truncation errors in summarization, andMAUVE (built on top of GPT-2) is insensitive to errors at the beginning ormiddle of generations. Further, we investigate the reasons behind these blindspots and suggest practical workarounds for a more reliable evaluation of textgeneration. We have released our code and data athttps://github.com/cloudygoose/blindspot_nlg.$Text Generation, Evaluation Metrics, Synthetic Data, Language Models$Univ. of Washington, Johns Hopkins Univ., Shanghai Jiao Tong Univ., Carnegie Mellon Univ., New York Univ., Mass. Institute of Technology, Univ. of Washington$本文探讨了基于语言模型的文本生成评估指标的盲点，提出了使用合成数据的压力测试方法来检验评估指标的鲁棒性。作者对开放式文本生成、翻译和摘要等任务的评估指标进行了实验，发现了指标存在一些盲点，提出了可行的解决方案来提高文本生成的可靠性。$文本生成、评估指标、合成数据、语言模型$http://arxiv.org/pdf/2212.10020v2
Search-in-the-Chain: Towards Accurate, Credible and Traceable Large  Language Models for Knowledge-intensive Tasks$  With the wide application of Large Language Models (LLMs) such as ChatGPT,how to make the contents generated by LLM accurate and credible becomes veryimportant, especially in complex knowledge-intensive tasks. In this paper, wepropose a novel framework called Search-in-the-Chain (SearChain) to improve theaccuracy, credibility and traceability of LLM-generated content for multi-hopquestion answering, which is a typical complex knowledge-intensive task.SearChain is a framework that deeply integrates LLM and information retrieval(IR). In SearChain, LLM constructs a chain-of-query, which is the decompositionof the multi-hop question. Each node of the chain is a query-answer pairconsisting of an IR-oriented query and the answer generated by LLM for thisquery. IR verifies, completes, and traces the information of each node of thechain, so as to guide LLM to construct the correct chain-of-query, and finallyanswer the multi-hop question. SearChain makes LLM change from trying to give aanswer to trying to construct the chain-of-query when faced with the multi-hopquestion, which can stimulate the knowledge-reasoning ability and provides theinterface for IR to be deeply involved in reasoning process of LLM. IRinteracts with each node of chain-of-query of LLM. It verifies the informationof the node and provides the unknown knowledge to LLM, which ensures theaccuracy of the whole chain in the process of LLM generating the answer.Besides, the contents returned by LLM to the user include not only the finalanswer but also the reasoning process for the question, that is, thechain-of-query and the supporting documents retrieved by IR for each node ofthe chain, which improves the credibility and traceability of the contentsgenerated by LLM. Experimental results show SearChain outperforms relatedbaselines on four multi-hop question-answering datasets.$$$$$http://arxiv.org/pdf/2304.14732v3
UNTER: A Unified Knowledge Interface for Enhancing Pre-trained Language  Models$  Recent research demonstrates that external knowledge injection can advancepre-trained language models (PLMs) in a variety of downstream NLP tasks.However, existing knowledge injection methods are either applicable tostructured knowledge or unstructured knowledge, lacking a unified usage. Inthis paper, we propose a UNified knowledge inTERface, UNTER, to provide aunified perspective to exploit both structured knowledge and unstructuredknowledge. In UNTER, we adopt the decoder as a unified knowledge interface,aligning span representations obtained from the encoder with theircorresponding knowledge. This approach enables the encoder to uniformly invokespan-related knowledge from its parameters for downstream applications.Experimental results show that, with both forms of knowledge injected, UNTERgains continuous improvements on a series of knowledge-driven NLP tasks,including entity typing, named entity recognition and relation extraction,especially in low-resource scenarios.$$$$$http://arxiv.org/pdf/2305.01624v2
A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from  Linguistically Complex Text$  Pretrained Vision-Language Models (VLMs) have achieved remarkable performancein image retrieval from text. However, their performance drops drastically whenconfronted with linguistically complex texts that they struggle to comprehend.Inspired by the Divide-and-Conquer algorithm and dual-process theory, in thispaper, we regard linguistically complex texts as compound proposition textscomposed of multiple simple proposition sentences and propose an end-to-endNeural Divide-and-Conquer Reasoning framework, dubbed NDCR. It contains threemain components: 1) Divide: a proposition generator divides the compoundproposition text into simple proposition sentences and produces theircorresponding representations, 2) Conquer: a pretrained VLMs-basedvisual-linguistic interactor achieves the interaction between decomposedproposition sentences and images, 3) Combine: a neural-symbolic reasonercombines the above reasoning states to obtain the final solution via a neurallogic reasoning approach. According to the dual-process theory, thevisual-linguistic interactor and neural-symbolic reasoner could be regarded asanalogical reasoning System 1 and logical reasoning System 2. We conductextensive experiments on a challenging image retrieval from contextualdescriptions data set. Experimental results and analyses indicate NDCRsignificantly improves performance in the complex image-text reasoning problem.Code link: https://github.com/YunxinLi/NDCR.$$$$$http://arxiv.org/pdf/2305.02265v2
Chain-of-Skills: A Configurable Model for Open-domain Question Answering$  The retrieval model is an indispensable component for real-worldknowledge-intensive tasks, e.g., open-domain question answering (ODQA). Asseparate retrieval skills are annotated for different datasets, recent workfocuses on customized methods, limiting the model transferability andscalability. In this work, we propose a modular retriever where individualmodules correspond to key skills that can be reused across datasets. Ourapproach supports flexible skill configurations based on the target domain toboost performance. To mitigate task interference, we design a novelmodularization parameterization inspired by sparse Transformer. We demonstratethat our model can benefit from self-supervised pretraining on Wikipedia andfine-tuning using multiple ODQA datasets, both in a multi-task fashion. Ourapproach outperforms recent self-supervised retrievers in zero-shot evaluationsand achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQAand OTT-QA.$$$$$http://arxiv.org/pdf/2305.03130v1
The Role of Global and Local Context in Named Entity Recognition$  Pre-trained transformer-based models have recently shown great performancewhen applied to Named Entity Recognition (NER). As the complexity of theirself-attention mechanism prevents them from processing long documents at once,these models are usually applied in a sequential fashion. Such an approachunfortunately only incorporates local context and prevents leveraging globaldocument context in long documents such as novels, which might hinderperformance. In this article, we explore the impact of global document context,and its relationships with local context. We find that correctly retrievingglobal document context has a greater impact on performance than onlyleveraging local context, prompting for further research on how to betterretrieve that context.$$$$$http://arxiv.org/pdf/2305.03132v1
Neuromodulation Gated Transformer$  We introduce a novel architecture, the Neuromodulation Gated Transformer(NGT), which is a simple implementation of neuromodulation in transformers viaa multiplicative effect. We compare it to baselines and show that it results inthe best average performance on the SuperGLUE benchmark validation sets.$$$$$http://arxiv.org/pdf/2305.03232v1
VicunaNER: Zero/Few-shot Named Entity Recognition using Vicuna$  Large Language Models (LLMs, e.g., ChatGPT) have shown impressive zero- andfew-shot capabilities in Named Entity Recognition (NER). However, these modelscan only be accessed via online APIs, which may cause data leak andnon-reproducible problems. In this paper, we propose VicunaNER, a zero/few-shotNER framework based on the newly released open-source LLM -- Vicuna. VicunaNERis a two-phase framework, where each phase leverages multi-turn dialogues withVicuna to recognize entities from texts. We name the second phase asRe-Recognition, which recognizes those entities not recognized in the firstphase (a.k.a. Recognition). Moreover, we set entity correctness check dialoguesin each phase to filter out wrong entities. We evaluate VicunaNER\'s zero-shotcapacity on 10 datasets crossing 5 domains and few-shot capacity on Few-NERD.Experimental results demonstrate that VicunaNER achieves superior performancein both shot settings. Additionally, we conduct comprehensive investigations onVicuna from multiple perspectives.$$$$$http://arxiv.org/pdf/2305.03253v1
Stylized Data-to-Text Generation: A Case Study in the E-Commerce Domain$  Existing data-to-text generation efforts mainly focus on generating acoherent text from non-linguistic input data, such as tables andattribute-value pairs, but overlook that different application scenarios mayrequire texts of different styles. Inspired by this, we define a new task,namely stylized data-to-text generation, whose aim is to generate coherent textfor the given non-linguistic data according to a specific style. This task isnon-trivial, due to three challenges: the logic of the generated text,unstructured style reference, and biased training samples. To address thesechallenges, we propose a novel stylized data-to-text generation model, namedStyleD2T, comprising three components: logic planning-enhanced data embedding,mask-based style embedding, and unbiased stylized text generation. In the firstcomponent, we introduce a graph-guided logic planner for attribute organizationto ensure the logic of generated text. In the second component, we devisefeature-level mask-based style embedding to extract the essential style signalfrom the given unstructured style reference. In the last one, pseudo tripletaugmentation is utilized to achieve unbiased text generation, and amulti-condition based confidence assignment function is designed to ensure thequality of pseudo samples. Extensive experiments on a newly collected datasetfrom Taobao have been conducted, and the results show the superiority of ourmodel over existing methods.$$$$$http://arxiv.org/pdf/2305.03256v1
Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework$  As large language models (LLMs) have become the norm in NLP, demonstratinggood performance in generation and reasoning tasks, one of its most fataldisadvantages is the lack of factual correctness. Generating unfactual textsnot only leads to lower performances but also degrades the trust and validityof their applications. Chain-of-Thought (CoT) prompting improves trust andmodel performance on complex reasoning tasks by generating interpretablereasoning chains, but still suffers from factuality concerns inknowledge-intensive tasks. In this paper, we propose the Verify-and-Editframework for CoT prompting, which seeks to increase prediction factuality bypost-editing reasoning chains according to external knowledge. Building on topof GPT-3, our framework lead to accuracy improvements in multiple open-domainquestion-answering tasks.$$$$$http://arxiv.org/pdf/2305.03268v1
Expository Text Generation: Imitate, Retrieve, Paraphrase$  Expository documents are vital resources for conveying complex information toreaders. Despite their usefulness, writing expository documents by hand is atime-consuming and labor-intensive process that requires knowledge of thedomain of interest, careful content planning, and the ability to synthesizeinformation from multiple sources. To ease these burdens, we introduce the taskof expository text generation, which seeks to automatically generate anaccurate and informative expository document from a knowledge source. We solveour task by developing IRP, an iterative framework that overcomes thelimitations of language models and separately tackles the steps of contentplanning, fact selection, and rephrasing. Through experiments on three diversedatasets, we demonstrate that IRP produces high-quality expository documentsthat accurately inform readers.$$$$$http://arxiv.org/pdf/2305.03276v1
LLM-RM at SemEval-2023 Task 2: Multilingual Complex NER using  XLM-RoBERTa$  Named Entity Recognition(NER) is a task of recognizing entities at a tokenlevel in a sentence. This paper focuses on solving NER tasks in a multilingualsetting for complex named entities. Our team, LLM-RM participated in therecently organized SemEval 2023 task, Task 2: MultiCoNER II,MultilingualComplex Named Entity Recognition. We approach the problem by leveragingcross-lingual representation provided by fine-tuning XLM-Roberta base model ondatasets of all of the 12 languages provided -- Bangla, Chinese, English,Farsi, French, German, Hindi, Italian, Portuguese, Spanish, Swedish andUkrainian$$$$$http://arxiv.org/pdf/2305.03300v1
Block the Label and Noise: An N-Gram Masked Speller for Chinese Spell  Checking$  Recently, Chinese Spell Checking(CSC), a task to detect erroneous charactersin a sentence and correct them, has attracted extensive interest because of itswide applications in various NLP tasks. Most of the existing methods haveutilized BERT to extract semantic information for CSC task. However, thesemethods directly take sentences with only a few errors as inputs, where thecorrect characters may leak answers to the model and dampen its ability tocapture distant context; while the erroneous characters may disturb thesemantic encoding process and result in poor representations. Based on suchobservations, this paper proposes an n-gram masking layer that masks currentand/or surrounding tokens to avoid label leakage and error disturbance.Moreover, considering that the mask strategy may ignore multi-modal informationindicated by errors, a novel dot-product gating mechanism is proposed tointegrate the phonological and morphological information with semanticrepresentation. Extensive experiments on SIGHAN datasets have demonstrated thatthe pluggable n-gram masking mechanism can improve the performance of prevalentCSC models and the proposed methods in this paper outperform multiple powerfulstate-of-the-art models.$$$$$http://arxiv.org/pdf/2305.03314v1
HiPool: Modeling Long Documents Using Graph Neural Networks$  Encoding long sequences in Natural Language Processing (NLP) is a challengingproblem. Though recent pretraining language models achieve satisfyingperformances in many NLP tasks, they are still restricted by a pre-definedmaximum length, making them challenging to be extended to longer sequences. Sosome recent works utilize hierarchies to model long sequences. However, most ofthem apply sequential models for upper hierarchies, suffering from longdependency issues. In this paper, we alleviate these issues through agraph-based method. We first chunk the sequence with a fixed length to modelthe sentence-level information. We then leverage graphs to model intra- andcross-sentence correlations with a new attention mechanism. Additionally, dueto limited standard benchmarks for long document classification (LDC), wepropose a new challenging benchmark, totaling six datasets with up to 53ksamples and 4034 average tokens\' length. Evaluation shows our model surpassescompetitive baselines by 2.6% in F1 score, and 4.8% on the longest sequencedataset. Our method is shown to outperform hierarchical sequential models withbetter performance and scalability, especially for longer sequences.$$$$$http://arxiv.org/pdf/2305.03319v1
Online Gesture Recognition using Transformer and Natural Language  Processing$  The Transformer architecture is shown to provide a powerful machinetransduction framework for online handwritten gestures corresponding to glyphstrokes of natural language sentences. The attention mechanism is successfullyused to create latent representations of an end-to-end encoder-decoder model,solving multi-level segmentation while also learning some language features andsyntax rules. The additional use of a large decoding space with some learnedByte-Pair-Encoding (BPE) is shown to provide robustness to ablated inputs andsyntax rules. The encoder stack was directly fed with spatio-temporal datatokens potentially forming an infinitely large input vocabulary, an approachthat finds applications beyond that of this work. Encoder transfer learningcapabilities is also demonstrated on several languages resulting in fasteroptimisation and shared parameters. A new supervised dataset of onlinehandwriting gestures suitable for generic handwriting recognition tasks wasused to successfully train a small transformer model to an average normalisedLevenshtein accuracy of 96% on English or German sentences and 94% in French.$$$$$http://arxiv.org/pdf/2305.03407v1
Using ChatGPT for Entity Matching$  Entity Matching is the task of deciding if two entity descriptions refer tothe same real-world entity. State-of-the-art entity matching methods often relyon fine-tuning Transformer models such as BERT or RoBERTa. Two major drawbacksof using these models for entity matching are that (i) the models requiresignificant amounts of fine-tuning data for reaching a good performance and(ii) the fine-tuned models are not robust concerning out-of-distributionentities. In this paper, we investigate using ChatGPT for entity matching as amore robust, training data-efficient alternative to traditional Transformermodels. We perform experiments along three dimensions: (i) general promptdesign, (ii) in-context learning, and (iii) provision of higher-level matchingknowledge. We show that ChatGPT is competitive with a fine-tuned RoBERTa model,reaching an average zero-shot performance of 83% F1 on a challenging matchingtask on which RoBERTa requires 2000 training examples for reaching a similarperformance. Adding in-context demonstrations to the prompts further improvesthe F1 by up to 5% even using only a small set of 20 handpicked examples.Finally, we show that guiding the zero-shot model by stating higher-levelmatching rules leads to similar gains as providing in-context examples.$$$$$http://arxiv.org/pdf/2305.03423v1
Simulating H.P. Lovecraft horror literature with the ChatGPT large  language model$  In this paper, we present a novel approach to simulating H.P. Lovecraft\'shorror literature using the ChatGPT large language model, specifically theGPT-4 architecture. Our study aims to generate text that emulates Lovecraft\'sunique writing style and themes, while also examining the effectiveness ofprompt engineering techniques in guiding the model\'s output. To achieve this,we curated a prompt containing several specialized literature references andemployed advanced prompt engineering methods. We conducted an empiricalevaluation of the generated text by administering a survey to a sample ofundergraduate students. Utilizing statistical hypothesis testing, we assessedthe students ability to distinguish between genuine Lovecraft works and thosegenerated by our model. Our findings demonstrate that the participants wereunable to reliably differentiate between the two, indicating the effectivenessof the GPT-4 model and our prompt engineering techniques in emulatingLovecraft\'s literary style. In addition to presenting the GPT model\'scapabilities, this paper provides a comprehensive description of its underlyingarchitecture and offers a comparative analysis with related work that simulatesother notable authors and philosophers, such as Dennett. By exploring thepotential of large language models in the context of literary emulation, ourstudy contributes to the body of research on the applications and limitationsof these models in various creative domains.$$$$$http://arxiv.org/pdf/2305.03429v1
LMs stand their Ground: Investigating the Effect of Embodiment in  Figurative Language Interpretation by Language Models$  Figurative language is a challenge for language models since itsinterpretation is based on the use of words in a way that deviates from theirconventional order and meaning. Yet, humans can easily understand and interpretmetaphors, similes or idioms as they can be derived from embodied metaphors.Language is a proxy for embodiment and if a metaphor is conventional andlexicalised, it becomes easier for a system without a body to make sense ofembodied concepts. Yet, the intricate relation between embodiment and featuressuch as concreteness or age of acquisition has not been studied in the contextof figurative language interpretation concerning language models. Hence, thepresented study shows how larger language models perform better at interpretingmetaphoric sentences when the action of the metaphorical sentence is moreembodied. The analysis rules out multicollinearity with other features (e.g.word length or concreteness) and provides initial evidence that larger languagemodels conceptualise embodied concepts to a degree that facilitates figurativelanguage understanding.$$$$$http://arxiv.org/pdf/2305.03445v1
T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large  Language Model Signals for Science Question Answering$  Large Language Models (LLMs) have recently demonstrated exceptionalperformance in various Natural Language Processing (NLP) tasks. They have alsoshown the ability to perform chain-of-thought (CoT) reasoning to solve complexproblems. Recent studies have explored CoT reasoning in complex multimodalscenarios, such as the science question answering task, by fine-tuningmultimodal models with high-quality human-annotated CoT rationales. However,collecting high-quality COT rationales is usually time-consuming and costly.Besides, the annotated rationales are hardly accurate due to the redundantinformation involved or the essential information missed. To address theseissues, we propose a novel method termed \\emph{T-SciQ} that aims at teachingscience question answering with LLM signals. The T-SciQ approach generateshigh-quality CoT rationales as teaching signals and is advanced to train muchsmaller models to perform CoT reasoning in complex modalities. Additionally, weintroduce a novel data mixing strategy to produce more effective teaching datasamples for simple and complex science question answer problems. Extensiveexperimental results show that our T-SciQ method achieves a newstate-of-the-art performance on the ScienceQA benchmark, with an accuracy of96.18%. Moreover, our approach outperforms the most powerful fine-tunedbaseline by 4.5%.$$$$$http://arxiv.org/pdf/2305.03453v1
Interactive Acquisition of Fine-grained Visual Concepts by Exploiting  Semantics of Generic Characterizations in Discourse$"  Interactive Task Learning (ITL) concerns learning about unforeseen domainconcepts via natural interactions with human users. The learner faces a numberof significant constraints: learning should be online, incremental andfew-shot, as it is expected to perform tangible belief updates right afternovel words denoting unforeseen concepts are introduced. In this work, weexplore a challenging symbol grounding task--discriminating among objectclasses that look very similar--within the constraints imposed by ITL. Wedemonstrate empirically that more data-efficient grounding results fromexploiting the truth-conditions of the teacher\'s generic statements (e.g., ""Xshave attribute Z."") and their implicatures in context (e.g., as an answer to""How are Xs and Ys different?"", one infers Y lacks attribute Z)."$Interactive Task Learning, Symbol Grounding, Natural Language Processing, Contextual Inference, Fine-grained Visual Categorization$School of Informatics, University of Edinburgh，英国爱丁堡EH8 9AB Crichton街10号$本文提出了一种交互式符号接地框架，利用自然语言描述和语境推断来实现领域内细粒度视觉对比。$交互式任务学习、符号接地、自然语言处理、语境推断、视觉子类别识别$http://arxiv.org/pdf/2305.03461v1
DAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented System  for Multilingual Named Entity Recognition$  The MultiCoNER \\RNum{2} shared task aims to tackle multilingual named entityrecognition (NER) in fine-grained and noisy scenarios, and it inherits thesemantic ambiguity and low-context setting of the MultiCoNER \\RNum{1} task. Tocope with these problems, the previous top systems in the MultiCoNER \\RNum{1}either incorporate the knowledge bases or gazetteers. However, they stillsuffer from insufficient knowledge, limited context length, single retrievalstrategy. In this paper, our team \\textbf{DAMO-NLP} proposes a unifiedretrieval-augmented system (U-RaNER) for fine-grained multilingual NER. Weperform error analysis on the previous top systems and reveal that theirperformance bottleneck lies in insufficient knowledge. Also, we discover thatthe limited context length causes the retrieval knowledge to be invisible tothe model. To enhance the retrieval context, we incorporate the entity-centricWikidata knowledge base, while utilizing the infusion approach to broaden thecontextual scope of the model. Also, we explore various search strategies andrefine the quality of retrieval knowledge. Our system\\footnote{We will releasethe dataset, code, and scripts of our system at {\\small\\url{https://github.com/modelscope/AdaSeq/tree/master/examples/U-RaNER}}.} wins9 out of 13 tracks in the MultiCoNER \\RNum{2} shared task. Additionally, wecompared our system with ChatGPT, one of the large language models which haveunlocked strong capabilities on many tasks. The results show that there isstill much room for improvement for ChatGPT on the extraction task.$$$$$http://arxiv.org/pdf/2305.03688v1
Large Language Models in Ambulatory Devices for Home Health Diagnostics:  A case study of Sickle Cell Anemia Management$  This study investigates the potential of an ambulatory device thatincorporates Large Language Models (LLMs) in cadence with other specialized MLmodels to assess anemia severity in sickle cell patients in real time. Thedevice would rely on sensor data that measures angiogenic material levels toassess anemia severity, providing real-time information to patients andclinicians to reduce the frequency of vaso-occlusive crises because of theearly detection of anemia severity, allowing for timely interventions andpotentially reducing the likelihood of serious complications. The mainchallenges in developing such a device are the creation of a reliablenon-invasive tool for angiogenic level assessment, a biophysics model and thepractical consideration of an LLM communicating with emergency personnel onbehalf of an incapacitated patient. A possible system is proposed, and thelimitations of this approach are discussed.$$$$$http://arxiv.org/pdf/2305.03715v1
A Systematic Review of Green AI$  With the ever-growing adoption of AI-based systems, the carbon footprint ofAI is no longer negligible. AI researchers and practitioners are thereforeurged to hold themselves accountable for the carbon emissions of the AI modelsthey design and use. This led in recent years to the appearance of researchestackling AI environmental sustainability, a field referred to as Green AI.Despite the rapid growth of interest in the topic, a comprehensive overview ofGreen AI research is to date still missing. To address this gap, in this paper,we present a systematic review of the Green AI literature. From the analysis of98 primary studies, different patterns emerge. The topic experienced aconsiderable growth from 2020 onward. Most studies consider monitoring AI modelfootprint, tuning hyperparameters to improve model sustainability, orbenchmarking models. A mix of position papers, observational studies, andsolution papers are present. Most papers focus on the training phase, arealgorithm-agnostic or study neural networks, and use image data. Laboratoryexperiments are the most common research strategy. Reported Green AI energysavings go up to 115%, with savings over 50% being rather common. Industrialparties are involved in Green AI studies, albeit most target academic readers.Green AI tool provisioning is scarce. As a conclusion, the Green AI researchfield results to have reached a considerable level of maturity. Therefore, fromthis review emerges that the time is suitable to adopt other Green AI researchstrategies, and port the numerous promising academic results to industrialpractice.$$$$$http://arxiv.org/pdf/2301.11047v3
GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware  Automated Feature Engineering$  As the field of automated machine learning (AutoML) advances, it becomesincreasingly important to include domain knowledge within these systems. Wepresent an approach for doing so by harnessing the power of large languagemodels (LLMs). Specifically, we introduce Context-Aware Automated FeatureEngineering (CAAFE), a feature engineering method for tabular datasets thatutilizes an LLM to generate additional semantically meaningful features fortabular datasets based on their descriptions. The method produces both Pythoncode for creating new features and explanations for the utility of thegenerated features.  Despite being methodologically simple, CAAFE enhances performance on 11 outof 14 datasets, ties on 2 and looses on 1 - boosting mean ROC AUC performancefrom 0.798 to 0.822 across all datasets. On the evaluated datasets, thisimprovement is similar to the average improvement achieved by using a randomforest (AUC 0.782) instead of logistic regression (AUC 0.754).  Furthermore, our method offers valuable insights into the rationale behindthe generated features by providing a textual explanation for each generatedfeature. CAAFE paves the way for more extensive (semi-)automation in datascience tasks and emphasizes the significance of context-aware solutions thatcan extend the scope of AutoML systems. For reproducability, we release ourcode and a simple demo.$$$$$http://arxiv.org/pdf/2305.03403v1
Assessing Trustworthiness of Autonomous Systems$  As Autonomous Systems (AS) become more ubiquitous in society, moreresponsible for our safety and our interaction with them more frequent, it isessential that they are trustworthy. Assessing the trustworthiness of AS is amandatory challenge for the verification and development community. This willrequire appropriate standards and suitable metrics that may serve toobjectively and comparatively judge trustworthiness of AS across the broadrange of current and future applications. The meta-expression `trustworthiness\'is examined in the context of AS capturing the relevant qualities that comprisethis term in the literature. Recent developments in standards and frameworksthat support assurance of autonomous systems are reviewed. A list of keychallenges are identified for the community and we present an outline of aprocess that can be used as a trustworthiness assessment framework for AS.$$$$$http://arxiv.org/pdf/2305.03411v1
Towards Applying Powerful Large AI Models in Classroom Teaching:  Opportunities, Challenges and Prospects$  This perspective paper proposes a series of interactive scenarios thatutilize Artificial Intelligence (AI) to enhance classroom teaching, such asdialogue auto-completion, knowledge and style transfer, and assessment ofAI-generated content. By leveraging recent developments in Large LanguageModels (LLMs), we explore the potential of AI to augment and enrichteacher-student dialogues and improve the quality of teaching. Our goal is toproduce innovative and meaningful conversations between teachers and students,create standards for evaluation, and improve the efficacy of AI-for-Educationinitiatives. In Section 3, we discuss the challenges of utilizing existing LLMsto effectively complete the educated tasks and present a unified framework foraddressing diverse education dataset, processing lengthy conversations, andcondensing information to better accomplish more downstream tasks. In Section4, we summarize the pivoting tasks including Teacher-Student DialogueAuto-Completion, Expert Teaching Knowledge and Style Transfer, and Assessmentof AI-Generated Content (AIGC), providing a clear path for future research. InSection 5, we also explore the use of external and adjustable LLMs to improvethe generated content through human-in-the-loop supervision and reinforcementlearning. Ultimately, this paper seeks to highlight the potential for AI to aidthe field of education and promote its further exploration.$$$$$http://arxiv.org/pdf/2305.03433v1
Causal Discovery with Stage Variables for Health Time Series$  Using observational data to learn causal relationships is essential whenrandomized experiments are not possible, such as in healthcare. Discoveringcausal relationships in time-series health data is even more challenging whenrelationships change over the course of a disease, such as medications that aremost effective early on or for individuals with severe disease. Stage variablessuch as weeks of pregnancy, disease stages, or biomarkers like HbA1c, caninfluence what causal relationships are true for a patient. However, causalinference within each stage is often not possible due to limited amounts ofdata, and combining all data risks incorrect or missed inferences. To addressthis, we propose Causal Discovery with Stage Variables (CDSV), which uses stagevariables to reweight data from multiple time-series while accounting fordifferent causal relationships in each stage. In simulated data, CDSV discoversmore causes with fewer false discoveries compared to baselines, in eICU it hasa lower FDR than baselines, and in MIMIC-III it discovers more clinicallyrelevant causes of high blood pressure.$$$$$http://arxiv.org/pdf/2305.03662v1
GAN-generated Faces Detection: A Survey and New Perspectives$  Generative Adversarial Networks (GAN) have led to the generation of veryrealistic face images, which have been used in fake social media accounts andother disinformation matters that can generate profound impacts. Therefore, thecorresponding GAN-face detection techniques are under active development thatcan examine and expose such fake faces. In this work, we aim to provide acomprehensive review of recent progress in GAN-face detection. We focus onmethods that can detect face images that are generated or synthesized from GANmodels. We classify the existing detection works into four categories: (1) deeplearning-based, (2) physical-based, (3) physiological-based methods, and (4)evaluation and comparison against human visual performance. For each category,we summarize the key ideas and connect them with method implementations. Wealso discuss open problems and suggest future research directions.$$$$$http://arxiv.org/pdf/2202.07145v5
Mates2Motion: Learning How Mechanical CAD Assemblies Work$  We describe our work on inferring the degrees of freedom between mated partsin mechanical assemblies using deep learning on CAD representations. We trainour model using a large dataset of real-world mechanical assemblies consistingof CAD parts and mates joining them together. We present methods forre-defining these mates to make them better reflect the motion of the assembly,as well as narrowing down the possible axes of motion. We also conduct a userstudy to create a motion-annotated test set with more reliable labels.$$$$$http://arxiv.org/pdf/2208.01779v2
OASIS: Automated Assessment of Urban Pedestrian Paths at Scale$  The inspection of the Public Right of Way (PROW) for accessibility barriersis necessary for monitoring and maintaining the built environment forcommunities\' walkability, rollability, safety, active transportation, andsustainability. However, an inspection of the PROW, by surveyors or crowds, islaborious, inconsistent, costly, and unscalable. The core of smart citydevelopments involves the application of information technologies towardmunicipal assets assessment and management. Sidewalks, in comparison toautomobile roads, have not been regularly integrated into information systemsto optimize or inform civic services. We develop an Open Automated SidewalksInspection System (OASIS), a free and open-source automated mapping system, toextract sidewalk network data using mobile physical devices. OASIS leveragesadvances in neural networks, image sensing, location-based methods, and compacthardware to perform sidewalk segmentation and mapping along with theidentification of barriers to generate a GIS pedestrian transportation layerthat is available for routing as well as analytic and operational reports. Wedescribe a prototype system trained and tested with imagery collected inreal-world settings, alongside human surveyors who are part of the localtransit pathway review team. Pilots show promising precision and recall forpath mapping (0.94, 0.98 respectively). Moreover, surveyor teams\' functionalefficiency increased in the field. By design, OASIS takes adoption aspects intoconsideration to ensure the system could be easily integrated with governmentalpathway review teams\' workflows, and that the outcome data would beinteroperable with public data commons.$$$$$http://arxiv.org/pdf/2303.02287v2
BaDLAD: A Large Multi-Domain Bengali Document Layout Analysis Dataset$  While strides have been made in deep learning based Bengali Optical CharacterRecognition (OCR) in the past decade, the absence of large Document LayoutAnalysis (DLA) datasets has hindered the application of OCR in documenttranscription, e.g., transcribing historical documents and newspapers.Moreover, rule-based DLA systems that are currently being employed in practiceare not robust to domain variations and out-of-distribution layouts. To thisend, we present the first multidomain large Bengali Document Layout AnalysisDataset: BaDLAD. This dataset contains 33,695 human annotated document samplesfrom six domains - i) books and magazines, ii) public domain govt. documents,iii) liberation war documents, iv) newspapers, v) historical newspapers, andvi) property deeds, with 710K polygon annotations for four unit types:text-box, paragraph, image, and table. Through preliminary experimentsbenchmarking the performance of existing state-of-the-art deep learningarchitectures for English DLA, we demonstrate the efficacy of our dataset intraining deep learning based Bengali document digitization models.$$$$$http://arxiv.org/pdf/2303.05325v3
Smart-Tree: Neural Medial Axis Approximation of Point Clouds for 3D Tree  Skeletonization$  This paper introduces Smart-Tree, a supervised method for approximating themedial axes of branch skeletons from a tree point cloud. Smart-Tree uses asparse voxel convolutional neural network to extract the radius and directiontowards the medial axis of each input point. A greedy algorithm performs robustskeletonization using the estimated medial axis. Our proposed method providesrobustness to complex tree structures and improves fidelity when dealing withself-occlusions, complex geometry, touching branches, and varying pointdensities. We evaluate Smart-Tree using a multi-species synthetic tree datasetand perform qualitative analysis on a real-world tree point cloud. Ourexperimentation with synthetic and real-world datasets demonstrates therobustness of our approach over the current state-of-the-art method. Thedataset and source code are publicly available.$Tree Skeletonization, Point Cloud, Metric Extraction, Neural Network$UC Vision Research Lab, University of Canterbury, Christchurch 8041, New Zealand$本文介绍了一种基于神经网络的方法Smart-Tree，用于从树木点云中近似提取枝干骨架。该方法使用稀疏体素卷积神经网络提取每个输入点的半径和指向近似中轴线的方向。通过构建一个贪心算法，使用估计的中轴执行骨架化。该方法对于复杂的树形结构具有鲁棒性，处理自遮挡、复杂几何形状、接触的枝干和不同的点密度时提高了准确性。作者在合成和实际的树木点云上进行了实验验证，证明了该方法的鲁棒性。$树木骨架化、点云、度量提取、神经网络$http://arxiv.org/pdf/2303.11560v2
ViT-Calibrator: Decision Stream Calibration for Vision Transformer$  A surge of interest has emerged in utilizing Transformers in diverse visiontasks owing to its formidable performance. However, existing approachesprimarily focus on optimizing internal model architecture designs that oftenentail significant trial and error with high burdens. In this work, we proposea new paradigm dubbed Decision Stream Calibration that boosts the performanceof general Vision Transformers. To achieve this, we shed light on theinformation propagation mechanism in the learning procedure by exploring thecorrelation between different tokens and the relevance coefficient of multipledimensions. Upon further analysis, it was discovered that 1) the final decisionis associated with tokens of foreground targets, while token features offoreground target will be transmitted into the next layer as much as possible,and the useless token features of background area will be eliminated graduallyin the forward propagation. 2) Each category is solely associated with specificsparse dimensions in the tokens. Based on the discoveries mentioned above, wedesigned a two-stage calibration scheme, namely ViT-Calibrator, including tokenpropagation calibration stage and dimension propagation calibration stage.Extensive experiments on commonly used datasets show that the proposed approachcan achieve promising results. The source codes are given in the supplements.$$$$$http://arxiv.org/pdf/2304.04354v2
DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion$  We present DreamPose, a diffusion-based method for generating animatedfashion videos from still images. Given an image and a sequence of human bodyposes, our method synthesizes a video containing both human and fabric motion.To achieve this, we transform a pretrained text-to-image model (StableDiffusion) into a pose-and-image guided video synthesis model, using a novelfinetuning strategy, a set of architectural changes to support the addedconditioning signals, and techniques to encourage temporal consistency. Wefine-tune on a collection of fashion videos from the UBC Fashion dataset. Weevaluate our method on a variety of clothing styles and poses, and demonstratethat our method produces state-of-the-art results on fashion video animation.Video results are available on our project page.$$$$$http://arxiv.org/pdf/2304.06025v3
Towards Precise Weakly Supervised Object Detection via Interactive  Contrastive Learning of Context Information$  Weakly supervised object detection (WSOD) aims at learning precise objectdetectors with only image-level tags. In spite of intensive research on deeplearning (DL) approaches over the past few years, there is still a significantperformance gap between WSOD and fully supervised object detection. In fact,most existing WSOD methods only consider the visual appearance of each regionproposal but ignore employing the useful context information in the image. Tothis end, this paper proposes an interactive end-to-end WSDO framework calledJLWSOD with two innovations: i) two types of WSOD-specific context information(i.e., instance-wise correlation andsemantic-wise correlation) are proposed andintroduced into WSOD framework; ii) an interactive graph contrastive learning(iGCL) mechanism is designed to jointly optimize the visual appearance andcontext information for better WSOD performance. Specifically, the iGCLmechanism takes full advantage of the complementary interpretations of theWSOD, namely instance-wise detection and semantic-wise prediction tasks,forming a more comprehensive solution. Extensive experiments on the widely usedPASCAL VOC and MS COCO benchmarks verify the superiority of JLWSOD overalternative state-of-the-art approaches and baseline models (improvement of3.6%~23.3% on mAP and 3.4%~19.7% on CorLoc, respectively).$$$$$http://arxiv.org/pdf/2304.14114v2
Searching from Area to Point: A Hierarchical Framework for  Semantic-Geometric Combined Feature Matching$  Feature matching is a crucial technique in computer vision. Essentially, itcan be considered as a searching problem to establish correspondences betweenimages. The key challenge in this task lies in the lack of a well-definedsearch space, leading to inaccurate point matching of current methods. Inpursuit of a reasonable matching search space, this paper introduces ahierarchical feature matching framework: Area to Point Matching (A2PM), tofirst find semantic area matches between images, and then perform pointmatching on area matches, thus setting the search space as the area matcheswith salient features to achieve high matching precision. This proper searchspace of A2PM framework also alleviates the accuracy limitation instate-of-the-art Transformer-based matching methods. To realize this framework,we further propose Semantic and Geometry Area Matching (SGAM) method, whichutilizes semantic prior and geometry consistency to establish accurate areamatches between images. By integrating SGAM with off-the-shelfTransformer-based matchers, our feature matching methods, adopting the A2PMframework, achieve encouraging precision improvements in massive point matchingand pose estimation experiments for present arts.$$$$$http://arxiv.org/pdf/2305.00194v3
"""Glitch in the Matrix!"": A Large Scale Benchmark for Content Driven  Audio-Visual Forgery Detection and Localization"$  Most deepfake detection methods focus on detecting spatial and/orspatio-temporal changes in facial attributes. This is because availablebenchmark datasets contain mostly visual-only modifications. However, asophisticated deepfake may include small segments of audio or audio-visualmanipulations that can completely change the meaning of the content. Toaddresses this gap, we propose and benchmark a new dataset, Localized AudioVisual DeepFake (LAV-DF), consisting of strategic content-driven audio, visualand audio-visual manipulations. The proposed baseline method, Boundary AwareTemporal Forgery Detection (BA-TFD), is a 3D Convolutional Neural Network-basedarchitecture which efficiently captures multimodal manipulations. We furtherimprove (i.e. BA-TFD+) the baseline method by replacing the backbone with aMultiscale Vision Transformer and guide the training process with contrastive,frame classification, boundary matching and multimodal boundary matching lossfunctions. The quantitative analysis demonstrates the superiority of BA- TFD+on temporal forgery localization and deepfake detection tasks using severalbenchmark datasets including our newly proposed dataset. The dataset, modelsand code are available at https://github.com/ControlNet/LAV-DF.$$$$$http://arxiv.org/pdf/2305.01979v2
Transforming Visual Scene Graphs to Image Captions$  We propose to Transform Scene Graphs (TSG) into more descriptive captions. InTSG, we apply multi-head attention (MHA) to design the Graph Neural Network(GNN) for embedding scene graphs. After embedding, different graph embeddingscontain diverse specific knowledge for generating the words with differentpart-of-speech, e.g., object/attribute embedding is good for generatingnouns/adjectives. Motivated by this, we design a Mixture-of-Expert (MOE)-baseddecoder, where each expert is built on MHA, for discriminating the graphembeddings to generate different kinds of words. Since both the encoder anddecoder are built based on the MHA, as a result, we construct a homogeneousencoder-decoder unlike the previous heterogeneous ones which usually applyFully-Connected-based GNN and LSTM-based decoder. The homogeneous architectureenables us to unify the training configuration of the whole model instead ofspecifying different training strategies for diverse sub-networks as in theheterogeneous pipeline, which releases the training difficulty. Extensiveexperiments on the MS-COCO captioning benchmark validate the effectiveness ofour TSG. The code is in: https://anonymous.4open.science/r/ACL23_TSG.$$$$$http://arxiv.org/pdf/2305.02177v3
Scanpath Prediction in Panoramic Videos via Expected Code Length  Minimization$"  Predicting human scanpaths when exploring panoramic videos is a challengingtask due to the spherical geometry and the multimodality of the input, and theinherent uncertainty and diversity of the output. Most previous methods fail togive a complete treatment of these characteristics, and thus are prone toerrors. In this paper, we present a simple new criterion for scanpathprediction based on principles from lossy data compression. This criterionsuggests minimizing the expected code length of quantized scanpaths in atraining set, which corresponds to fitting a discrete conditional probabilitymodel via maximum likelihood. Specifically, the probability model isconditioned on two modalities: a viewport sequence as the deformation-reducedvisual input and a set of relative historical scanpaths projected ontorespective viewports as the aligned path input. The probability model isparameterized by a product of discretized Gaussian mixture models to capturethe uncertainty and the diversity of scanpaths from different users. Mostimportantly, the training of the probability model does not rely on thespecification of ""ground-truth"" scanpaths for imitation learning. We alsointroduce a proportional-integral-derivative (PID) controller-based sampler togenerate realistic human-like scanpaths from the learned probability model.Experimental results demonstrate that our method consistently produces betterquantitative scanpath results in terms of prediction accuracy (by comparing tothe assumed ""ground-truths"") and perceptual realism (through machinediscrimination) over a wide range of prediction horizons. We additionallyverify the perceptual realism improvement via a formal psychophysicalexperiment and the generalization improvement on several unseen panoramic videodatasets."$$$$$http://arxiv.org/pdf/2305.02536v2
Generating Virtual On-body Accelerometer Data from Virtual Textual  Descriptions for Human Activity Recognition$  The development of robust, generalized models in human activity recognition(HAR) has been hindered by the scarcity of large-scale, labeled data sets.Recent work has shown that virtual IMU data extracted from videos usingcomputer vision techniques can lead to substantial performance improvementswhen training HAR models combined with small portions of real IMU data.Inspired by recent advances in motion synthesis from textual descriptions andconnecting Large Language Models (LLMs) to various AI models, we introduce anautomated pipeline that first uses ChatGPT to generate diverse textualdescriptions of activities. These textual descriptions are then used togenerate 3D human motion sequences via a motion synthesis model, T2M-GPT, andlater converted to streams of virtual IMU data. We benchmarked our approach onthree HAR datasets (RealWorld, PAMAP2, and USC-HAD) and demonstrate that theuse of virtual IMU training data generated using our new approach leads tosignificantly improved HAR model performance compared to only using real IMUdata. Our approach contributes to the growing field of cross-modality transfermethods and illustrate how HAR models can be improved through the generation ofvirtual training data that do not require any manual effort.$$$$$http://arxiv.org/pdf/2305.03187v1
Robust Face Morphing Attack Detection Using Fusion of Multiple Features  and Classification Techniques$"  Face Recognition System (FRS) are shown to be vulnerable to morphed images ofnewborns. Detecting morphing attacks stemming from face images of newborn isimportant to avoid unwanted consequences, both for security and society. Inthis paper, we present a new reference-based/Differential Morphing AttackDetection (MAD) method to detect newborn morphing images using WaveletScattering Network (WSN). We propose a two-layer WSN with 250 $\\times$ 250pixels and six rotations of wavelets per layer, resulting in 577 paths. Theproposed approach is validated on a dataset of 852 bona fide images and 2460morphing images constructed using face images of 42 unique newborns. Theobtained results indicate a gain of over 10\\% in detection accuracy over otherexisting D-MAD techniques."$$$$$http://arxiv.org/pdf/2305.03264v1
FM-ViT: Flexible Modal Vision Transformers for Face Anti-Spoofing$  The availability of handy multi-modal (i.e., RGB-D) sensors has brought abouta surge of face anti-spoofing research. However, the current multi-modal facepresentation attack detection (PAD) has two defects: (1) The framework based onmulti-modal fusion requires providing modalities consistent with the traininginput, which seriously limits the deployment scenario. (2) The performance ofConvNet-based model on high fidelity datasets is increasingly limited. In thiswork, we present a pure transformer-based framework, dubbed the Flexible ModalVision Transformer (FM-ViT), for face anti-spoofing to flexibly target anysingle-modal (i.e., RGB) attack scenarios with the help of availablemulti-modal data. Specifically, FM-ViT retains a specific branch for eachmodality to capture different modal information and introduces the Cross-ModalTransformer Block (CMTB), which consists of two cascaded attentions namedMulti-headed Mutual-Attention (MMA) and Fusion-Attention (MFA) to guide eachmodal branch to mine potential features from informative patch tokens, and tolearn modality-agnostic liveness features by enriching the modal information ofown CLS token, respectively. Experiments demonstrate that the single modeltrained based on FM-ViT can not only flexibly evaluate different modal samples,but also outperforms existing single-modal frameworks by a large margin, andapproaches the multi-modal frameworks introduced with smaller FLOPs and modelparameters.$$$$$http://arxiv.org/pdf/2305.03277v1
High-Fidelity 3D Face Generation from Natural Language Descriptions$  Synthesizing high-quality 3D face models from natural language descriptionsis very valuable for many applications, including avatar creation, virtualreality, and telepresence. However, little research ever tapped into this task.We argue the major obstacle lies in 1) the lack of high-quality 3D face datawith descriptive text annotation, and 2) the complex mapping relationshipbetween descriptive language space and shape/appearance space. To solve theseproblems, we build Describe3D dataset, the first large-scale dataset withfine-grained text descriptions for text-to-3D face generation task. Then wepropose a two-stage framework to first generate a 3D face that matches theconcrete descriptions, then optimize the parameters in the 3D shape and texturespace with abstract description to refine the 3D face model. Extensiveexperimental results show that our method can produce a faithful 3D face thatconforms to the input descriptions with higher accuracy and quality thanprevious methods. The code and Describe3D dataset are released athttps://github.com/zhuhao-nju/describe3d .$Text generation, 3D face modeling, Face recognition$南京大学$本文提出了一种从自然语言描述生成高保真3D人脸的方法，建立了第一个大规模的具有细粒度文本描述的3D面孔数据集DESCRIBE 3D，并使用两阶段框架解决了文本与形状/外观空间之间的复杂映射关系问题。实验结果显示，方法能够比先前的方法更准确、更高质量地生成符合输入文本描述的3D人脸模型。$文本生成、3D脸部建模、人脸识别$http://arxiv.org/pdf/2305.03302v1
FlowText: Synthesizing Realistic Scene Text Video with Optical Flow  Estimation$  Current video text spotting methods can achieve preferable performance,powered with sufficient labeled training data. However, labeling data manuallyis time-consuming and labor-intensive. To overcome this, using low-costsynthetic data is a promising alternative. This paper introduces a novel videotext synthesis technique called FlowText, which utilizes optical flowestimation to synthesize a large amount of text video data at a low cost fortraining robust video text spotters. Unlike existing methods that focus onimage-level synthesis, FlowText concentrates on synthesizing temporalinformation of text instances across consecutive frames using optical flow.This temporal information is crucial for accurately tracking and spotting textin video sequences, including text movement, distortion, appearance,disappearance, shelter, and blur. Experiments show that combining generaldetectors like TransDETR with the proposed FlowText produces remarkable resultson various datasets, such as ICDAR2015video and ICDAR2013video. Code isavailable at https://github.com/callsys/FlowText.$$$$$http://arxiv.org/pdf/2305.03327v1
A Large Cross-Modal Video Retrieval Dataset with Reading Comprehension$  Most existing cross-modal language-to-video retrieval (VR) research focuseson single-modal input from video, i.e., visual representation, while the textis omnipresent in human environments and frequently critical to understandvideo. To study how to retrieve video with both modal inputs, i.e., visual andtext semantic representations, we first introduce a large-scale and cross-modalVideo Retrieval dataset with text reading comprehension, TextVR, which contains42.2k sentence queries for 10.5k videos of 8 scenario domains, i.e., StreetView (indoor), Street View (outdoor), Games, Sports, Driving, Activity, TVShow, and Cooking. The proposed TextVR requires one unified cross-modal modelto recognize and comprehend texts, relate them to the visual context, anddecide what text semantic information is vital for the video retrieval task.Besides, we present a detailed analysis of TextVR compared to the existingdatasets and design a novel multimodal video retrieval baseline for thetext-based video retrieval task. The dataset analysis and extensive experimentsshow that our TextVR benchmark provides many new technical challenges andinsights from previous datasets for the video-and-language community. Theproject website and GitHub repo can be found athttps://sites.google.com/view/loveucvpr23/guest-track andhttps://github.com/callsys/TextVR, respectively.$$$$$http://arxiv.org/pdf/2305.03347v1
Leaf Cultivar Identification via Prototype-enhanced Learning$  Plant leaf identification is crucial for biodiversity protection andconservation and has gradually attracted the attention of academia in recentyears. Due to the high similarity among different varieties, leaf cultivarrecognition is also considered to be an ultra-fine-grained visualclassification (UFGVC) task, which is facing a huge challenge. In practice, aninstance may be related to multiple varieties to varying degrees, especially inthe UFGVC datasets. However, deep learning methods trained on one-hot labelsfail to reflect patterns shared across categories and thus perform poorly onthis task. To address this issue, we generate soft targets integrated withinter-class similarity information. Specifically, we continuously update theprototypical features for each category and then capture the similarity scoresbetween instances and prototypes accordingly. Original one-hot labels and thesimilarity scores are incorporated to yield enhanced labels. Prototype-enhancedsoft labels not only contain original one-hot label information, but alsointroduce rich inter-category semantic association information, thus providingmore effective supervision for deep model training. Extensive experimentalresults on public datasets show that our method can significantly improve theperformance on the UFGVC task of leaf cultivar identification.$$$$$http://arxiv.org/pdf/2305.03351v1
DisenBooth: Disentangled Parameter-Efficient Tuning for Subject-Driven  Text-to-Image Generation$  Given a small set of images of a specific subject, subject-driventext-to-image generation aims to generate customized images of the subjectaccording to new text descriptions, which has attracted increasing attention inthe community recently. Current subject-driven text-to-image generation methodsare mainly based on finetuning a pretrained large-scale text-to-imagegeneration model. However, these finetuning methods map the images of thesubject into an embedding highly entangled with subject-identity-unrelatedinformation, which may result in the inconsistency between the generated imagesand the text descriptions and the changes in the subject identity. To tacklethe problem, we propose DisenBooth, a disentangled parameter-efficient tuningframework for subject-driven text-to-image generation. DisenBooth enablesgenerating new images that simultaneously preserve the subject identity andconform to the text descriptions, by disentangling the embedding into anidentity-related and an identity-unrelated part. Specifically, DisenBooth isbased on the pretrained diffusion models and conducts finetuning in thediffusion denoising process, where a shared identity embedding and animage-specific identity-unrelated embedding are utilized jointly for denoisingeach image. To make the two embeddings disentangled, two auxiliary objectivesare proposed. Additionally, to improve the finetuning efficiency, aparameter-efficient finetuning strategy is adopted. Extensive experiments showthat our DisenBooth can faithfully learn well-disentangled identity-related andidentity-unrelated embeddings. With the shared identity embedding, DisenBoothdemonstrates superior subject-driven text-to-image generation ability.Additionally, DisenBooth provides a more flexible and controllable frameworkwith different combinations of the disentangled embeddings.$$$$$http://arxiv.org/pdf/2305.03374v1
Guided Image Synthesis via Initial Image Editing in Diffusion Model$  Diffusion models have the ability to generate high quality images bydenoising pure Gaussian noise images. While previous research has primarilyfocused on improving the control of image generation through adjusting thedenoising process, we propose a novel direction of manipulating the initialnoise to control the generated image. Through experiments on stable diffusion,we show that blocks of pixels in the initial latent images have a preferencefor generating specific content, and that modifying these blocks cansignificantly influence the generated image. In particular, we show thatmodifying a part of the initial image affects the corresponding region of thegenerated image while leaving other regions unaffected, which is useful forrepainting tasks. Furthermore, we find that the generation preferences of pixelblocks are primarily determined by their values, rather than their position. Bymoving pixel blocks with a tendency to generate user-desired content touser-specified regions, our approach achieves state-of-the-art performance inlayout-to-image generation. Our results highlight the flexibility and power ofinitial image manipulation in controlling the generated image.$$$$$http://arxiv.org/pdf/2305.03382v1
Optimized Table Tokenization for Table Structure Recognition$  Extracting tables from documents is a crucial task in any document conversionpipeline. Recently, transformer-based models have demonstrated thattable-structure can be recognized with impressive accuracy usingImage-to-Markup-Sequence (Im2Seq) approaches. Taking only the image of a table,such models predict a sequence of tokens (e.g. in HTML, LaTeX) which representthe structure of the table. Since the token representation of the tablestructure has a significant impact on the accuracy and run-time performance ofany Im2Seq model, we investigate in this paper how table-structurerepresentation can be optimised. We propose a new, optimised table-structurelanguage (OTSL) with a minimized vocabulary and specific rules. The benefits ofOTSL are that it reduces the number of tokens to 5 (HTML needs 28+) andshortens the sequence length to half of HTML on average. Consequently, modelaccuracy improves significantly, inference time is halved compared toHTML-based models, and the predicted table structures are always syntacticallycorrect. This in turn eliminates most post-processing needs.$$$$$http://arxiv.org/pdf/2305.03393v1
GAANet: Ghost Auto Anchor Network for Detecting Varying Size Drones in  Dark$  The usage of drones has tremendously increased in different sectors spanningfrom military to industrial applications. Despite all the benefits they offer,their misuse can lead to mishaps, and tackling them becomes more challengingparticularly at night due to their small size and low visibility conditions. Toovercome those limitations and improve the detection accuracy at night, wepropose an object detector called Ghost Auto Anchor Network (GAANet) forinfrared (IR) images. The detector uses a YOLOv5 core to address challenges inobject detection for IR images, such as poor accuracy and a high false alarmrate caused by extended altitudes, poor lighting, and low image resolution. Toimprove performance, we implemented auto anchor calculation, modified theconventional convolution block to ghost-convolution, adjusted the input channelsize, and used the AdamW optimizer. To enhance the precision of multiscale tinyobject recognition, we also introduced an additional extra-small object featureextractor and detector. Experimental results in a custom IR dataset withmultiple classes (birds, drones, planes, and helicopters) demonstrate thatGAANet shows improvement compared to state-of-the-art detectors. In comparisonto GhostNet-YOLOv5, GAANet has higher overall mean average precision (mAP@50),recall, and precision around 2.5\\%, 2.3\\%, and 1.4\\%, respectively. The datasetand code for this paper are available as open source athttps://github.com/ZeeshanKaleem/GhostAutoAnchorNet.$$$$$http://arxiv.org/pdf/2305.03425v1
Next-generation Surgical Navigation: Multi-view Marker-less 6DoF Pose  Estimation of Surgical Instruments$  State-of-the-art research of traditional computer vision is increasinglyleveraged in the surgical domain. A particular focus in computer-assistedsurgery is to replace marker-based tracking systems for instrument localizationwith pure image-based 6DoF pose estimation. However, the state of the art hasnot yet met the accuracy required for surgical navigation. In this context, wepropose a high-fidelity marker-less optical tracking system for surgicalinstrument localization. We developed a multi-view camera setup consisting ofstatic and mobile cameras and collected a large-scale RGB-D video dataset withdedicated synchronization and data fusions methods. Different state-of-the-artpose estimation methods were integrated into a deep learning pipeline andevaluated on multiple camera configurations. Furthermore, the performanceimpacts of different input modalities and camera positions, as well as trainingon purely synthetic data, were compared. The best model achieved an averageposition and orientation error of 1.3 mm and 1.0{\\deg} for a surgical drill aswell as 3.8 mm and 5.2{\\deg} for a screwdriver. These results significantlyoutperform related methods in the literature and are close to clinical-gradeaccuracy, demonstrating that marker-less tracking of surgical instruments isbecoming a feasible alternative to existing marker-based systems.$$$$$http://arxiv.org/pdf/2305.03535v1
HSCNet++: Hierarchical Scene Coordinate Classification and Regression  for Visual Localization with Transformer$  Visual localization is critical to many applications in computer vision androbotics. To address single-image RGB localization, state-of-the-artfeature-based methods match local descriptors between a query image and apre-built 3D model. Recently, deep neural networks have been exploited toregress the mapping between raw pixels and 3D coordinates in the scene, andthus the matching is implicitly performed by the forward pass through thenetwork. However, in a large and ambiguous environment, learning such aregression task directly can be difficult for a single network. In this work,we present a new hierarchical scene coordinate network to predict pixel scenecoordinates in a coarse-to-fine manner from a single RGB image. The proposedmethod, which is an extension of HSCNet, allows us to train compact modelswhich scale robustly to large environments. It sets a new state-of-the-art forsingle-image localization on the 7-Scenes, 12 Scenes, Cambridge Landmarksdatasets, and the combined indoor scenes.$$$$$http://arxiv.org/pdf/2305.03595v1
Conditional Diffusion Feature Refinement for Continuous Sign Language  Recognition$  In this work, we are dedicated to leveraging the denoising diffusion models\'success and formulating feature refinement as the autoencoder-formed diffusionprocess. The state-of-the-art CSLR framework consists of a spatial module, avisual module, a sequence module, and a sequence learning function. However,this framework has faced sequence module overfitting caused by the objectivefunction and small-scale available benchmarks, resulting in insufficient modeltraining. To overcome the overfitting problem, some CSLR studies enforce thesequence module to learn more visual temporal information or be guided by moreinformative supervision to refine its representations. In this work, we proposea novel autoencoder-formed conditional diffusion feature refinement~(ACDR) torefine the sequence representations to equip desired properties by learning theencoding-decoding optimization process in an end-to-end way. Specifically, forthe ACDR, a noising Encoder is proposed to progressively add noise equippedwith semantic conditions to the sequence representations. And a denoisingDecoder is proposed to progressively denoise the noisy sequence representationswith semantic conditions. Therefore, the sequence representations can be imbuedwith the semantics of provided semantic conditions. Further, a semanticconstraint is employed to prevent the denoised sequence representations fromsemantic corruption. Extensive experiments are conducted to validate theeffectiveness of our ACDR, benefiting state-of-the-art methods and achieving anotable gain on three benchmarks.$$$$$http://arxiv.org/pdf/2305.03614v1
Asynchronous Events-based Panoptic Segmentation using Graph Mixer Neural  Network$  In the context of robotic grasping, object segmentation encounters severaldifficulties when faced with dynamic conditions such as real-time operation,occlusion, low lighting, motion blur, and object size variability. In responseto these challenges, we propose the Graph Mixer Neural Network that includes anovel collaborative contextual mixing layer, applied to 3D event graphs formedon asynchronous events. The proposed layer is designed to spread spatiotemporalcorrelation within an event graph at four nearest neighbor levels parallelly.We evaluate the effectiveness of our proposed method on the Event-basedSegmentation (ESD) Dataset, which includes five unique image degradationchallenges, including occlusion, blur, brightness, trajectory, scale variance,and segmentation of known and unknown objects. The results show that ourproposed approach outperforms state-of-the-art methods in terms of meanintersection over the union and pixel accuracy. Code available at:https://github.com/sanket0707/GNN-Mixer.git$$$$$http://arxiv.org/pdf/2305.03640v1
Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head  Videos$  Modern generators render talking-head videos with impressive levels ofphotorealism, ushering in new user experiences such as videoconferencing underconstrained bandwidth budgets. Their safe adoption, however, requires amechanism to verify if the rendered video is trustworthy. For instance, forvideoconferencing we must identify cases in which a synthetic video portraituses the appearance of an individual without their consent. We term this taskavatar fingerprinting. We propose to tackle it by leveraging facial motionsignatures unique to each person. Specifically, we learn an embedding in whichthe motion signatures of one identity are grouped together, and pushed awayfrom those of other identities, regardless of the appearance in the syntheticvideo. Avatar fingerprinting algorithms will be critical as talking headgenerators become more ubiquitous, and yet no large scale datasets exist forthis new task. Therefore, we contribute a large dataset of people deliveringscripted and improvised short monologues, accompanied by synthetic videos inwhich we render videos of one person using the facial appearance of another.Project page: https://research.nvidia.com/labs/nxp/avatar-fingerprinting/.$virtual avatars, identity verification, facial motion signatures$NVIDIA$本文提出了一个名为Avatar指纹识别的任务，旨在识别合成视频肖像是否可信。我们利用面部运动签名中的身份感知信号，学习了一个embedding，其中单个身份的运动签名紧密地聚在一起，并远离其他身份，而不考虑合成视频肖像的外观。我们为这个新任务贡献了一个大型数据集，其中包括人们口述和即兴短篇独白的视频。这个工作对未来越来越流行的交互式虚拟形象技术有着至关重要的意义。$虚拟形象，身份验证，面部运动签名$http://arxiv.org/pdf/2305.03713v1
DSPDet3D: Dynamic Spatial Pruning for 3D Small Object Detection$  In this paper, we propose a new detection framework for 3D small objectdetection. Although deep learning-based 3D object detection methods haveachieved great success in recent years, current methods still struggle on smallobjects due to weak geometric information. With in-depth study, we findincreasing the spatial resolution of the feature maps significantly boosts theperformance of 3D small object detection. And more interestingly, though thecomputational overhead increases dramatically with resolution, the growthmainly comes from the upsampling operation of the decoder. Inspired by this, wepresent a high-resolution multi-level detector with dynamic spatial pruningnamed DSPDet3D, which detects objects from large to small by iterativeupsampling and meanwhile prunes the spatial representation of the scene atregions where there is no smaller object to be detected in higher resolution.As the 3D detector only needs to predict sparse bounding boxes, pruning a largeamount of uninformative features does not degrade the detection performance butsignificantly reduces the computational cost of upsampling. In this way, ourDSPDet3D achieves high accuracy on small object detection while requiring evenless memory footprint and inference time. On ScanNet and TO-SCENE dataset, ourmethod improves the detection performance of small objects to a new level whileachieving leading inference speed among all mainstream indoor 3D objectdetection methods.$$$$$http://arxiv.org/pdf/2305.03716v1
AVATAR: A Parallel Corpus for Java-Python Program Translation$  Program translation refers to migrating source code from one programminglanguage to another. It has tremendous practical value in software development,as porting software across languages is time-consuming and costly. Automatingprogram translation is of paramount importance in software migration, andrecently researchers explored unsupervised approaches due to the unavailabilityof parallel corpora. However, the availability of pre-trained language modelsfor programming languages enables supervised fine-tuning with a small number oflabeled examples. Therefore, we present AVATAR, a collection of 9,515programming problems and their solutions written in two popular languages, Javaand Python. AVATAR is collected from competitive programming sites, onlineplatforms, and open-source repositories. Furthermore, AVATAR includes unittests for 250 examples to facilitate functional correctness evaluation. Webenchmark several pre-trained language models fine-tuned on AVATAR. Experimentresults show that the models lack in generating functionally accurate code.$$$$$http://arxiv.org/pdf/2108.11590v2
Rescue Conversations from Dead-ends: Efficient Exploration for  Task-oriented Dialogue Policy Optimization$  Training a dialogue policy using deep reinforcement learning requires a lotof exploration of the environment. The amount of wasted invalid explorationmakes their learning inefficient. In this paper, we find and define animportant reason for the invalid exploration: dead-ends. When a conversationenters a dead-end state, regardless of the actions taken afterward, it willcontinue in a dead-end trajectory until the agent reaches a termination stateor maximum turn. We propose a dead-end resurrection (DDR) algorithm thatdetects the initial dead-end state in a timely and efficient manner andprovides a rescue action to guide and correct the exploration direction. Toprevent dialogue policies from repeatedly making the same mistake, DDR alsoperforms dialogue data augmentation by adding relevant experiences containingdead-end states. We first validate the dead-end detection reliability and thendemonstrate the effectiveness and generality of the method by reportingexperimental results on several dialogue datasets from different domains.$$$$$http://arxiv.org/pdf/2305.03262v1
Generating Symbolic Reasoning Problems with Transformer GANs$  We study the capabilities of GANs and Wasserstein GANs equipped withTransformer encoders to generate sensible and challenging training data forsymbolic reasoning domains. We conduct experiments on two problem domains whereTransformers have been successfully applied recently: symbolic mathematics andtemporal specifications in verification. Even without autoregression, our GANmodels produce syntactically correct instances. We show that the generated datacan be used as a substitute for real training data when training a classifier,and, especially, that training data can be generated from a dataset that is toosmall to be trained on directly. Using a GAN setting also allows us to alterthe target distribution: We show that by adding a classifier uncertainty partto the generator objective, we obtain a dataset that is even harder to solvefor a temporal logic classifier than our original dataset.$$$$$http://arxiv.org/pdf/2110.10054v3
ADATIME: A Benchmarking Suite for Domain Adaptation on Time Series Data$  Unsupervised domain adaptation methods aim to generalize well on unlabeledtest data that may have a different (shifted) distribution from the trainingdata. Such methods are typically developed on image data, and their applicationto time series data is less explored. Existing works on time series domainadaptation suffer from inconsistencies in evaluation schemes, datasets, andbackbone neural network architectures. Moreover, labeled target data are oftenused for model selection, which violates the fundamental assumption ofunsupervised domain adaptation. To address these issues, we develop abenchmarking evaluation suite (AdaTime) to systematically and fairly evaluatedifferent domain adaptation methods on time series data. Specifically, westandardize the backbone neural network architectures and benchmarkingdatasets, while also exploring more realistic model selection approaches thatcan work with no labeled data or just a few labeled samples. Our evaluationincludes adapting state-of-the-art visual domain adaptation methods to timeseries data as well as the recent methods specifically developed for timeseries data. We conduct extensive experiments to evaluate 11 state-of-the-artmethods on five representative datasets spanning 50 cross-domain scenarios. Ourresults suggest that with careful selection of hyper-parameters, visual domainadaptation methods are competitive with methods proposed for time series domainadaptation. In addition, we find that hyper-parameters could be selected basedon realistic model selection approaches. Our work unveils practical insightsfor applying domain adaptation methods on time series data and builds a solidfoundation for future works in the field. The code is available at\\href{https://github.com/emadeldeen24/AdaTime}{github.com/emadeldeen24/AdaTime}.$$$$$http://arxiv.org/pdf/2203.08321v2
Investigating the Properties of Neural Network Representations in  Reinforcement Learning$  In this paper we investigate the properties of representations learned bydeep reinforcement learning systems. Much of the early work on representationsfor reinforcement learning focused on designing fixed-basis architectures toachieve properties thought to be desirable, such as orthogonality and sparsity.In contrast, the idea behind deep reinforcement learning methods is that theagent designer should not encode representational properties, but rather thatthe data stream should determine the properties of the representation -- goodrepresentations emerge under appropriate training schemes. In this paper webring these two perspectives together, empirically investigating the propertiesof representations that support transfer in reinforcement learning. Weintroduce and measure six representational properties over more than 25thousand agent-task settings. We consider Deep Q-learning agents with differentauxiliary losses in a pixel-based navigation environment, with source andtransfer tasks corresponding to different goal locations. We develop a methodto better understand why some representations work better for transfer, througha systematic approach varying task similarity and measuring and correlatingrepresentation properties with transfer performance. We demonstrate thegenerality of the methodology by investigating representations learned by aRainbow agent that successfully transfer across games modes in Atari 2600.$$$$$http://arxiv.org/pdf/2203.15955v3
Improving Graph Neural Networks with Learnable Propagation Operators$"  Graph Neural Networks (GNNs) are limited in their propagation operators. Inmany cases, these operators often contain non-negative elements only and areshared across channels, limiting the expressiveness of GNNs. Moreover, someGNNs suffer from over-smoothing, limiting their depth. On the other hand,Convolutional Neural Networks (CNNs) can learn diverse propagation filters, andphenomena like over-smoothing are typically not apparent in CNNs. In thispaper, we bridge these gaps by incorporating trainable channel-wise weightingfactors $\\omega$ to learn and mix multiple smoothing and sharpening propagationoperators at each layer. Our generic method is called $\\omega$GNN, and is easyto implement. We study two variants: $\\omega$GCN and $\\omega$GAT. For$\\omega$GCN, we theoretically analyse its behaviour and the impact of $\\omega$on the obtained node features. Our experiments confirm these findings,demonstrating and explaining how both variants do not over-smooth.Additionally, we experiment with 15 real-world datasets on node- andgraph-classification tasks, where our $\\omega$GCN and $\\omega$GAT perform onpar with state-of-the-art methods."$$$$$http://arxiv.org/pdf/2210.17224v2
Cuttlefish: Low-Rank Model Training without All the Tuning$  Recent research has shown that training low-rank neural networks caneffectively reduce the total number of trainable parameters without sacrificingpredictive accuracy, resulting in end-to-end speedups. However, low-rank modeltraining necessitates adjusting several additional factorizationhyperparameters, such as the rank of the factorization at each layer. In thispaper, we tackle this challenge by introducing Cuttlefish, an automatedlow-rank training approach that eliminates the need for tuning factorizationhyperparameters. Cuttlefish leverages the observation that after a few epochsof full-rank training, the stable rank (i.e., an approximation of the truerank) of each layer stabilizes at a constant value. Cuttlefish switches fromfull-rank to low-rank training once the stable ranks of all layers haveconverged, setting the dimension of each factorization to its correspondingstable rank. Our results show that Cuttlefish generates models up to 5.6 timessmaller than full-rank models, and attains up to a 1.2 times faster end-to-endtraining process while preserving comparable accuracy. Moreover, Cuttlefishoutperforms state-of-the-art low-rank model training methods and otherprominent baselines. The source code for our implementation can be found at:https://github.com/hwang595/Cuttlefish.$$$$$http://arxiv.org/pdf/2305.02538v2
On the Effectiveness of Equivariant Regularization for Robust Online  Continual Learning$  Humans can learn incrementally, whereas neural networks forget previouslyacquired information catastrophically. Continual Learning (CL) approaches seekto bridge this gap by facilitating the transfer of knowledge to both previoustasks (backward transfer) and future ones (forward transfer) during training.  Recent research has shown that self-supervision can produce versatile modelsthat can generalize well to diverse downstream tasks. However, contrastiveself-supervised learning (CSSL), a popular self-supervision technique, haslimited effectiveness in online CL (OCL). OCL only permits one iteration of theinput dataset, and CSSL\'s low sample efficiency hinders its use on the inputdata-stream.  In this work, we propose Continual Learning via Equivariant Regularization(CLER), an OCL approach that leverages equivariant tasks for self-supervision,avoiding CSSL\'s limitations. Our method represents the first attempt atcombining equivariant knowledge with CL and can be easily integrated withexisting OCL methods. Extensive ablations shed light on how equivariant pretexttasks affect the network\'s information flow and its impact on CL dynamics.$$$$$http://arxiv.org/pdf/2305.03648v1
LLM+P: Empowering Large Language Models with Optimal Planning  Proficiency$  Large language models (LLMs) have demonstrated remarkable zero-shotgeneralization abilities: state-of-the-art chatbots can provide plausibleanswers to many common questions that arise in daily life. However, so far,LLMs cannot reliably solve long-horizon planning problems. By contrast,classical planners, once a problem is given in a formatted way, can useefficient search algorithms to quickly identify correct, or even optimal,plans. In an effort to get the best of both worlds, this paper introducesLLM+P, the first framework that incorporates the strengths of classicalplanners into LLMs. LLM+P takes in a natural language description of a planningproblem, then returns a correct (or optimal) plan for solving that problem innatural language. LLM+P does so by first converting the language descriptioninto a file written in the planning domain definition language (PDDL), thenleveraging classical planners to quickly find a solution, and then translatingthe found solution back into natural language. Along with LLM+P, we define adiverse set of different benchmark problems taken from common planningscenarios. Via a comprehensive set of experiments on these benchmark problems,we find that LLM+P is able to provide optimal solutions for most problems,while LLMs fail to provide even feasible plans for most problems.\\footnote{Thecode and results are publicly available athttps://github.com/Cranial-XIX/llm-pddl.git.$$$$$http://arxiv.org/pdf/2304.11477v2
Unsupervised Domain Transfer for Science: Exploring Deep Learning  Methods for Translation between LArTPC Detector Simulations with Differing  Response Models$"  Deep learning (DL) techniques have broad applications in science, especiallyin seeking to streamline the pathway to potential solutions and discoveries.Frequently, however, DL models are trained on the results of simulation yetapplied to real experimental data. As such, any systematic differences betweenthe simulated and real data may degrade the model\'s performance -- an effectknown as ""domain shift."" This work studies a toy model of the systematicdifferences between simulated and real data. It presents a fully unsupervised,task-agnostic method to reduce differences between two systematically differentsamples. The method is based on the recent advances in unpaired image-to-imagetranslation techniques and is validated on two sets of samples of simulatedLiquid Argon Time Projection Chamber (LArTPC) detector events, created toillustrate common systematic differences between the simulated and real data ina controlled way. LArTPC-based detectors represent the next-generation particledetectors, producing unique high-resolution particle track data. This workopen-sources the generated LArTPC data set, called Simple Liquid-Argon TrackSamples (or SLATS), allowing researchers from diverse domains to study theLArTPC-like data for the first time. The code and trained models are availableat https://github.com/LS4GAN/uvcgan4slats."$Deep learning, domain transfer, simulated data, experimental data, LArTPC detector$Brookhaven National Laboratory, Upton, NY, USA$本文旨在研究不同响应模型下基于LArTPC探测器模拟数据和实验数据之间的“领域偏移”问题，并提出一种全面无监督的方法以减小这种差异。同时，本文公开了生成的LArTPC数据集(Simple Liquid-Argon Track Samples，简称SLATS)，并提供了代码和训练模型供研究人员使用。$深度学习，领域转移，模拟数据，实验数据，LArTPC探测器$http://arxiv.org/pdf/2304.12858v2
Near-realtime Facial Animation by Deep 3D Simulation Super-Resolution$  We present a neural network-based simulation super-resolution framework thatcan efficiently and realistically enhance a facial performance produced by alow-cost, realtime physics-based simulation to a level of detail that closelyapproximates that of a reference-quality off-line simulator with much higherresolution (26x element count in our examples) and accurate physical modeling.Our approach is rooted in our ability to construct - via simulation - atraining set of paired frames, from the low- and high-resolution simulatorsrespectively, that are in semantic correspondence with each other. We use faceanimation as an exemplar of such a simulation domain, where creating thissemantic congruence is achieved by simply dialing in the same muscle actuationcontrols and skeletal pose in the two simulators. Our proposed neural networksuper-resolution framework generalizes from this training set to unseenexpressions, compensates for modeling discrepancies between the two simulationsdue to limited resolution or cost-cutting approximations in the real-timevariant, and does not require any semantic descriptors or parameters to beprovided as input, other than the result of the real-time simulation. Weevaluate the efficacy of our pipeline on a variety of expressive performancesand provide comparisons and ablation experiments for plausible variations andalternatives to our proposed scheme.$$$$$http://arxiv.org/pdf/2305.03216v1
Repairing Deep Neural Networks Based on Behavior Imitation$  The increasing use of deep neural networks (DNNs) in safety-critical systemshas raised concerns about their potential for exhibiting ill-behaviors. WhileDNN verification and testing provide post hoc conclusions regarding unexpectedbehaviors, they do not prevent the erroneous behaviors from occurring. Toaddress this issue, DNN repair/patch aims to eliminate unexpected predictionsgenerated by defective DNNs. Two typical DNN repair paradigms are retrainingand fine-tuning. However, existing methods focus on the high-level abstractinterpretation or inference of state spaces, ignoring the underlying neurons\'outputs. This renders patch processes computationally prohibitive and limitedto piecewise linear (PWL) activation functions to great extent. To addressthese shortcomings, we propose a behavior-imitation based repair framework,BIRDNN, which integrates the two repair paradigms for the first time. BIRDNNcorrects incorrect predictions of negative samples by imitating the closestexpected behaviors of positive samples during the retraining repair procedure.For the fine-tuning repair process, BIRDNN analyzes the behavior differences ofneurons on positive and negative samples to identify the most responsibleneurons for the erroneous behaviors. To tackle more challenging domain-wiserepair problems (DRPs), we synthesize BIRDNN with a domain behaviorcharacterization technique to repair buggy DNNs in a probably approximatedcorrect style. We also implement a prototype tool based on BIRDNN and evaluateit on ACAS Xu DNNs. Our experimental results show that BIRDNN can successfullyrepair buggy DNNs with significantly higher efficiency than state-of-the-artrepair tools. Additionally, BIRDNN is highly compatible with differentactivation functions.$$$$$http://arxiv.org/pdf/2305.03365v1
Towards Feminist Intersectional XAI: From Explainability to  Response-Ability$  This paper follows calls for critical approaches to computing andconceptualisations of intersectional, feminist, decolonial HCI and AI designand asks what a feminist intersectional perspective in HCXAI research anddesign might look like. Sketching out initial research directions andimplications for explainable AI design, it suggests that explainability from afeminist perspective would include the fostering of response-ability - thecapacity to critically evaluate and respond to AI systems - and would centremarginalised perspectives.$$$$$http://arxiv.org/pdf/2305.03375v1
Explaining the ghosts: Feminist intersectional XAI and cartography as  methods to account for invisible labour$  Contemporary automation through AI entails a substantial amount ofbehind-the-scenes human labour, which is often both invisibilised andunderpaid. Since invisible labour, including labelling and maintenance work, isan integral part of contemporary AI systems, it remains important to sensitiseusers to its role. We suggest that this could be done through explainable AI(XAI) design, particularly feminist intersectional XAI. We propose the methodof cartography, which stems from feminist intersectional research, to draw outa systemic perspective of AI and include dimensions of AI that pertain toinvisible labour.$$$$$http://arxiv.org/pdf/2305.03376v1
Advances on the classification of radio image cubes$  Modern radio telescopes will daily generate data sets on the scale ofexabytes for systems like the Square Kilometre Array (SKA). Massive data setsare a source of unknown and rare astrophysical phenomena that lead todiscoveries. Nonetheless, this is only plausible with the exploitation ofintensive machine intelligence to complement human-aided and traditionalstatistical techniques. Recently, there has been a surge in scientificpublications focusing on the use of artificial intelligence in radio astronomy,addressing challenges such as source extraction, morphological classification,and anomaly detection. This study presents a succinct, but comprehensive reviewof the application of machine intelligence techniques on radio images withemphasis on the morphological classification of radio galaxies. It aims topresent a detailed synthesis of the relevant papers summarizing the literaturebased on data complexity, data pre-processing, and methodological novelty inradio astronomy. The rapid advancement and application of computer intelligencein radio astronomy has resulted in a revolution and a new paradigm shift in theautomation of daunting data processes. However, the optimal exploitation ofartificial intelligence in radio astronomy, calls for continued collaborativeefforts in the creation of annotated data sets. Additionally, in order toquickly locate radio galaxies with similar or dissimilar physicalcharacteristics, it is necessary to index the identified radio sources.Nonetheless, this issue has not been adequately addressed in the literature,making it an open area for further study.$$$$$http://arxiv.org/pdf/2305.03435v1
Employing Hybrid Deep Neural Networks on Dari Speech$  This paper is an extension of our previous conference paper. In recent years,there has been a growing interest among researchers in developing and improvingspeech recognition systems to facilitate and enhance human-computerinteraction. Today, Automatic Speech Recognition (ASR) systems have becomeubiquitous, used in everything from games to translation systems, robots, andmore. However, much research is still needed on speech recognition systems forlow-resource languages. This article focuses on the recognition of individualwords in the Dari language using the Mel-frequency cepstral coefficients(MFCCs) feature extraction method and three different deep neural networkmodels: Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), andMultilayer Perceptron (MLP), as well as two hybrid models combining CNN andRNN. We evaluate these models using an isolated Dari word corpus that we havecreated, consisting of 1000 utterances for 20 short Dari terms. Our studyachieved an impressive average accuracy of 98.365%.$$$$$http://arxiv.org/pdf/2305.03200v1
GAN-Based Multi-View Video Coding with Spatio-Temporal EPI  Reconstruction$  The introduction of multiple viewpoints in video scenes inevitably increasesthe bitrates required for storage and transmission. To reduce bitrates,researchers have developed methods to skip intermediate viewpoints duringcompression and delivery, and ultimately reconstruct them using SideInformation (SI). Typically, depth maps are used to construct SI. However,their methods suffer from inaccuracies in reconstruction and inherently highbitrates. In this paper, we propose a novel multi-view video coding method thatleverages the image generation capabilities of Generative Adversarial Network(GAN) to improve the reconstruction accuracy of SI. Additionally, we considerincorporating information from adjacent temporal and spatial viewpoints tofurther reduce SI redundancy. At the encoder, we construct a spatio-temporalEpipolar Plane Image (EPI) and further utilize a convolutional network toextract the latent code of a GAN as SI. At the decoder side, we combine the SIand adjacent viewpoints to reconstruct intermediate views using the GANgenerator. Specifically, we establish a joint encoder constraint forreconstruction cost and SI entropy to achieve an optimal trade-off betweenreconstruction quality and bitrates overhead. Experiments demonstratesignificantly improved Rate-Distortion (RD) performance compared withstate-of-the-art methods.$$$$$http://arxiv.org/pdf/2205.03599v2
Boosting Adversarial Transferability via Fusing Logits of Top-1  Decomposed Feature$  Recent research has shown that Deep Neural Networks (DNNs) are highlyvulnerable to adversarial samples, which are highly transferable and can beused to attack other unknown black-box models. To improve the transferabilityof adversarial samples, several feature-based adversarial attack methods havebeen proposed to disrupt neuron activation in the middle layers. However,current state-of-the-art feature-based attack methods typically requireadditional computation costs for estimating the importance of neurons. Toaddress this challenge, we propose a Singular Value Decomposition (SVD)-basedfeature-level attack method. Our approach is inspired by the discovery thateigenvectors associated with the larger singular values decomposed from themiddle layer features exhibit superior generalization and attention properties.Specifically, we conduct the attack by retaining the decomposed Top-1 singularvalue-associated feature for computing the output logits, which are thencombined with the original logits to optimize adversarial examples. Ourextensive experimental results verify the effectiveness of our proposed method,which can be easily integrated into various baselines to significantly enhancethe transferability of adversarial samples for disturbing normally trained CNNsand advanced defense strategies. The source code of this study is available at\\textcolor{blue}{\\href{https://anonymous.4open.science/r/SVD-SSA-13BF/README.md}{Link}}.$$$$$http://arxiv.org/pdf/2305.01361v2
HeteroEdge: Addressing Asymmetry in Heterogeneous Collaborative  Autonomous Systems$  Gathering knowledge about surroundings and generating situational awarenessfor IoT devices is of utmost importance for systems developed for smart urbanand uncontested environments. For example, a large-area surveillance system istypically equipped with multi-modal sensors such as cameras and LIDARs and isrequired to execute deep learning algorithms for action, face, behavior, andobject recognition. However, these systems face power and memory constraintsdue to their ubiquitous nature, making it crucial to optimize data processing,deep learning algorithm input, and model inference communication. In thispaper, we propose a self-adaptive optimization framework for a testbedcomprising two Unmanned Ground Vehicles (UGVs) and two NVIDIA Jetson devices.This framework efficiently manages multiple tasks (storage, processing,computation, transmission, inference) on heterogeneous nodes concurrently. Itinvolves compressing and masking input image frames, identifying similarframes, and profiling devices to obtain boundary conditions for optimization..Finally, we propose and optimize a novel parameter split-ratio, which indicatesthe proportion of the data required to be offloaded to another device whileconsidering the networking bandwidth, busy factor, memory (CPU, GPU, RAM), andpower constraints of the devices in the testbed. Our evaluations captured whileexecuting multiple tasks (e.g., PoseNet, SegNet, ImageNet, DetectNet, DepthNet)simultaneously, reveal that executing 70% (split-ratio=70%) of the data on theauxiliary node minimizes the offloading latency by approx. 33% (18.7 ms/imageto 12.5 ms/image) and the total operation time by approx. 47% (69.32s to36.43s) compared to the baseline configuration (executing on the primary node).$$$$$http://arxiv.org/pdf/2305.03252v1
WWFedCBMIR: World-Wide Federated Content-Based Medical Image Retrieval$  The paper proposes a Federated Content-Based Medical Image Retrieval(FedCBMIR) platform that utilizes Federated Learning (FL) to address thechallenges of acquiring a diverse medical data set for training CBMIR models.CBMIR assists pathologists in diagnosing breast cancer more rapidly byidentifying similar medical images and relevant patches in prior cases comparedto traditional cancer detection methods. However, CBMIR in histopathologynecessitates a pool of Whole Slide Images (WSIs) to train to extract an optimalembedding vector that leverages search engine performance, which may not beavailable in all centers. The strict regulations surrounding data sharing inmedical data sets also hinder research and model development, making itdifficult to collect a rich data set. The proposed FedCBMIR distributes themodel to collaborative centers for training without sharing the data set,resulting in shorter training times than local training. FedCBMIR was evaluatedin two experiments with three scenarios on BreaKHis and Camelyon17 (CAM17). Thestudy shows that the FedCBMIR method increases the F1-Score (F1S) of eachclient to 98%, 96%, 94%, and 97% in the BreaKHis experiment with a generalizedmodel of four magnifications and does so in 6.30 hours less time than totallocal training. FedCBMIR also achieves 98% accuracy with CAM17 in 2.49 hoursless training time than local training, demonstrating that our FedCBMIR is bothfast and accurate for both pathologists and engineers. In addition, ourFedCBMIR provides similar images with higher magnification for non-developedcountries where participate in the worldwide FedCBMIR with developed countriesto facilitate mitosis measuring in breast cancer diagnosis. We evaluate thisscenario by scattering BreaKHis into four centers with differentmagnifications.$$$$$http://arxiv.org/pdf/2305.03383v1
AsConvSR: Fast and Lightweight Super-Resolution Network with Assembled  Convolutions$"  In recent years, videos and images in 720p (HD), 1080p (FHD) and 4K (UHD)resolution have become more popular for display devices such as TVs, mobilephones and VR. However, these high resolution images cannot achieve theexpected visual effect due to the limitation of the internet bandwidth, andbring a great challenge for super-resolution networks to achieve real-timeperformance. Following this challenge, we explore multiple efficient networkdesigns, such as pixel-unshuffle, repeat upscaling, and local skip connectionremoval, and propose a fast and lightweight super-resolution network.Furthermore, by analyzing the applications of the idea of divide-and-conquer insuper-resolution, we propose assembled convolutions which can adapt convolutionkernels according to the input features. Experiments suggest that our methodoutperforms all the state-of-the-art efficient super-resolution models, andachieves optimal results in terms of runtime and quality. In addition, ourmethod also wins the first place in NTIRE 2023 Real-Time Super-Resolution -Track 1 ($\\times$2). The code will be available athttps://gitee.com/mindspore/models/tree/master/research/cv/AsConvSR"$Super-resolution; Convolution; Real-time performance$Huawei Noah’s Ark Lab, HiSilicon (Shanghai) Technologies Co., Ltd.$这篇文章介绍了一种快速、轻量级的超分辨率网络AsConvSR，通过像素分离、重复升频和局部跳跃连接的去除等多种有效的网络设计，实现了对高分辨率图像的快速处理。作者还提出了装配卷积的概念，该卷积可以根据输入特征自适应卷积核，最终网络在实时性能和质量方面超越了所有最先进的超分辨率模型。$超分辨率；卷积；实时性能$http://arxiv.org/pdf/2305.03387v1
Evolution under Length Constraints for CNN Architecture design$  In recent years, the CNN architectures designed by evolution algorithms haveproven to be competitive with handcrafted architectures designed by experts.However, these algorithms need a lot of computational power, which is beyondthe capabilities of most researchers and engineers. To overcome this problem,we propose an evolution architecture under length constraints. It consists oftwo algorithms: a search length strategy to find an optimal space and a searcharchitecture strategy based on genetic algorithm to find the best individual inthe optimal space. Our algorithms reduce drastically resource cost and alsokeep good performance. On the Cifar-10 dataset, our framework presentsoutstanding performance with an error rate of 5.12% and only 4.6 GPU a day toconverge to the optimal individual -22 GPU a day less than the lowest costautomatic evolutionary algorithm in the peer competition.$$$$$http://arxiv.org/pdf/2305.03416v1
General Neural Gauge Fields$  The recent advance of neural fields, such as neural radiance fields, hassignificantly pushed the boundary of scene representation learning. Aiming toboost the computation efficiency and rendering quality of 3D scenes, a popularline of research maps the 3D coordinate system to another measuring system,e.g., 2D manifolds and hash tables, for modeling neural fields. The conversionof coordinate systems can be typically dubbed as gauge transformation, which isusually a pre-defined mapping function, e.g., orthogonal projection or spatialhash function. This begs a question: can we directly learn a desired gaugetransformation along with the neural field in an end-to-end manner? In thiswork, we extend this problem to a general paradigm with a taxonomy of discrete&amp; continuous cases, and develop an end-to-end learning framework to jointlyoptimize the gauge transformation and neural fields. To counter the problemthat the learning of gauge transformations can collapse easily, we derive ageneral regularization mechanism from the principle of information conservationduring the gauge transformation. To circumvent the high computation cost ingauge learning with regularization, we directly derive an information-invariantgauge transformation which allows to preserve scene information inherently andyield superior performance.$$$$$http://arxiv.org/pdf/2305.03462v1
High-Level Context Representation for Emotion Recognition in Images$  Emotion recognition is the task of classifying perceived emotions in people.Previous works have utilized various nonverbal cues to extract features fromimages and correlate them to emotions. Of these cues, situational context isparticularly crucial in emotion perception since it can directly influence theemotion of a person. In this paper, we propose an approach for high-levelcontext representation extraction from images. The model relies on a single cueand a single encoding stream to correlate this representation with emotions.Our model competes with the state-of-the-art, achieving an mAP of 0.3002 on theEMOTIC dataset while also being capable of execution on consumer-grade hardwareat approximately 90 frames per second. Overall, our approach is more efficientthan previous models and can be easily deployed to address real-world problemsrelated to emotion recognition.$$$$$http://arxiv.org/pdf/2305.03500v1
Breast Cancer Immunohistochemical Image Generation: a Benchmark Dataset  and Challenge Review$  For invasive breast cancer, immunohistochemical (IHC) techniques are oftenused to detect the expression level of human epidermal growth factor receptor-2(HER2) in breast tissue to formulate a precise treatment plan. From theperspective of saving manpower, material and time costs, directly generatingIHC-stained images from hematoxylin and eosin (H&amp;E) stained images is avaluable research direction. Therefore, we held the breast cancerimmunohistochemical image generation challenge, aiming to explore novel ideasof deep learning technology in pathological image generation and promoteresearch in this field. The challenge provided registered H&amp;E and IHC-stainedimage pairs, and participants were required to use these images to train amodel that can directly generate IHC-stained images from correspondingH&amp;E-stained images. We selected and reviewed the five highest-ranking methodsbased on their PSNR and SSIM metrics, while also providing overviews of thecorresponding pipelines and implementations. In this paper, we further analyzethe current limitations in the field of breast cancer immunohistochemical imagegeneration and forecast the future development of this field. We hope that thereleased dataset and the challenge will inspire more scholars to jointly studyhigher-quality IHC-stained image generation.$$$$$http://arxiv.org/pdf/2305.03546v1
How Segment Anything Model (SAM) Boost Medical Image Segmentation?$  Due to the flexibility of prompting, foundation models have become thedominant force in the domains of natural language processing and imagegeneration. With the recent introduction of the Segment Anything Model (SAM),the prompt-driven paradigm has entered the realm of image segmentation,bringing with a range of previously unexplored capabilities. However, itremains unclear whether it can be applicable to medical image segmentation dueto the significant differences between natural images and medical images. Inthis report, we summarize recent efforts to extend the success of SAM tomedical image segmentation tasks, including both empirical benchmarking andmethodological adaptations, and discuss potential future directions for SAM inmedical image segmentation. We also set up a collection of literature reviewsto boost the research on this topic at https://github.com/YichiZhang98/SAM4MIS.$$$$$http://arxiv.org/pdf/2305.03678v1
Human-centered trust framework: An HCI perspective$  The rationale of this work is based on the current user trust discourse ofArtificial Intelligence (AI). We aim to produce novel HCI approaches that usetrust as a facilitator for the uptake (or appropriation) of currenttechnologies. We propose a framework (HCTFrame) to guide non-experts to unlockthe full potential of user trust in AI design. Results derived from a datatriangulation of findings from three literature reviews demystify somemisconceptions of user trust in computer science and AI discourse, and threecase studies are conducted to assess the effectiveness of a psychometric scalein mapping potential users\' trust breakdowns and concerns. This work primarilycontributes to the fight against the tendency to design technical-centeredvulnerable interactions, which can eventually lead to additional real andperceived breaches of trust. The proposed framework can be used to guide systemdesigners on how to map and define user trust and the socioethical andorganisational needs and characteristics of AI system design. It can also guideAI system designers on how to develop a prototype and operationalise a solutionthat meets user trust requirements. The article ends by providing some userresearch tools that can be employed to measure users\' trust intentions andbehaviours towards a proposed solution.$$$$$http://arxiv.org/pdf/2305.03306v1
Compressing audio CNNs with graph centrality based filter pruning$"  Convolutional neural networks (CNNs) are commonplace in high-performingsolutions to many real-world problems, such as audio classification. CNNs havemany parameters and filters, with some having a larger impact on theperformance than others. This means that networks may contain many unnecessaryfilters, increasing a CNN\'s computation and memory requirements while providinglimited performance benefits. To make CNNs more efficient, we propose a pruningframework that eliminates filters with the highest ""commonality"". We measurethis commonality using the graph-theoretic concept of ""centrality"". Wehypothesise that a filter with a high centrality should be eliminated as itrepresents commonality and can be replaced by other filters without affectingthe performance of a network much. An experimental evaluation of the proposedframework is performed on acoustic scene classification and audio tagging. Onthe DCASE 2021 Task 1A baseline network, our proposed method reducescomputations per inference by 71\\% with 50\\% fewer parameters at less than atwo percentage point drop in accuracy compared to the original network. Forlarge-scale CNNs such as PANNs designed for audio tagging, our method reduces24\\% computations per inference with 41\\% fewer parameters at a slightimprovement in performance."$$$$$http://arxiv.org/pdf/2305.03391v1
Improving LaCAM for Scalable Eventually Optimal Multi-Agent Pathfinding$  This study extends the recently-developed LaCAM algorithm for multi-agentpathfinding (MAPF). LaCAM is a sub-optimal search-based algorithm that useslazy successor generation to dramatically reduce the planning effort. Wepresent two enhancements. First, we propose its anytime version, called LaCAM*,which eventually converges to optima, provided that solution costs areaccumulated transition costs. Second, we improve the successor generation toquickly obtain initial solutions. Exhaustive experiments demonstrate theirutility. For instance, LaCAM* sub-optimally solved 99% of the instancesretrieved from the MAPF benchmark, where the number of agents varied up to athousand, within ten seconds on a standard desktop PC, while ensuring eventualconvergence to optima; developing a new horizon of MAPF algorithms.$$$$$http://arxiv.org/pdf/2305.03632v1
Biophysical Cybernetics of Directed Evolution and Eco-evolutionary  Dynamics$"  Many major questions in the theory of evolutionary dynamics can in ameaningful sense be mapped to analyses of stochastic trajectories in gametheoretic contexts. Often the approach is to analyze small numbers of distinctpopulations and/or to assume dynamics occur within a regime of population sizeslarge enough that deterministic trajectories are an excellent approximation ofreality. The addition of ecological factors, termed ""eco-evolutionarydynamics"", further complicates the dynamics and results in many problems whichare intractable or impractically messy for current theoretical methods.However, an analogous but underexplored approach is to analyze these systemswith an eye primarily towards uncertainty in the models themselves. In thelanguage of researchers in Reinforcement Learning and adjacent fields, aPartially Observable Markov Process. Here we introduce a duality which maps thecomplexity of accounting for both ecology and individual genotypic/phenotypictypes onto a problem of accounting solely for underlying information-theoreticcomputations rather than drawing physical boundaries which do not change thecomputations. Armed with this equivalence between computation and the relevantbiophysics, which we term Taak-duality, we attack the problem of ""directedevolution"" in the form of a Partially Observable Markov Decision Process. Thisprovides a tractable case of studying eco-evolutionary trajectories of a highlygeneral type, and of analyzing questions of potential limits on the efficiencyof evolution in the directed case."$$$$$http://arxiv.org/pdf/2305.03340v1
Fast and Robust Rank Aggregation against Model Misspecification$  In rank aggregation (RA), a collection of preferences from different usersare summarized into a total order under the assumption of homogeneity of users.Model misspecification in RA arises since the homogeneity assumption fails tobe satisfied in the complex real-world situation. Existing robust RAs usuallyresort to an augmentation of the ranking model to account for additionalnoises, where the collected preferences can be treated as a noisy perturbationof idealized preferences. Since the majority of robust RAs rely on certainperturbation assumptions, they cannot generalize well to agnosticnoise-corrupted preferences in the real world. In this paper, we proposeCoarsenRank, which possesses robustness against model misspecification.Specifically, the properties of our CoarsenRank are summarized as follows: (1)CoarsenRank is designed for mild model misspecification, which assumes thereexist the ideal preferences (consistent with model assumption) that locates ina neighborhood of the actual preferences. (2) CoarsenRank then performs regularRAs over a neighborhood of the preferences instead of the original datasetdirectly. Therefore, CoarsenRank enjoys robustness against modelmisspecification within a neighborhood. (3) The neighborhood of the dataset isdefined via their empirical data distributions. Further, we put an exponentialprior on the unknown size of the neighborhood, and derive a much-simplifiedposterior formula for CoarsenRank under particular divergence measures. (4)CoarsenRank is further instantiated to Coarsened Thurstone, CoarsenedBradly-Terry, and Coarsened Plackett-Luce with three popular probabilityranking models. Meanwhile, tractable optimization strategies are introducedwith regards to each instantiation respectively. In the end, we applyCoarsenRank on four real-world datasets.$$$$$http://arxiv.org/pdf/1905.12341v2
Learning Node Representations against Perturbations$  Recent graph neural networks (GNN) has achieved remarkable performance innode representation learning. One key factor of GNN\'s success is the\\emph{smoothness} property on node representations. Despite this, most GNNmodels are fragile to the perturbations on graph inputs and could learnunreliable node representations. In this paper, we study how to learn noderepresentations against perturbations in GNN. Specifically, we consider that anode representation should remain stable under slight perturbations on theinput, and node representations from different structures should beidentifiable, which two are termed as the \\emph{stability} and\\emph{identifiability} on node representations, respectively. To this end, wepropose a novel model called Stability-Identifiability GNN AgainstPerturbations (SIGNNAP) that learns reliable node representations in anunsupervised manner. SIGNNAP formalizes the \\emph{stability} and\\emph{identifiability} by a contrastive objective and preserves the\\emph{smoothness} with existing GNN backbones. The proposed method is a genericframework that can be equipped with many other backbone models (e.g. GCN,GraphSage and GAT). Extensive experiments on six benchmarks under bothtransductive and inductive learning setups of node classification demonstratethe effectiveness of our method. Codes and data are availableonline:~\\url{https://github.com/xuChenSJTU/SIGNNAP-master-online}$graph neural networks, node representation learning, perturbation robustness$aShanghai Jiao Tong University, Shanghai, China；bUniversity of Technology Sydney, Sydney, Australia；cCenter for Frontier AI Research Research, Agency for Science Technology, Singapore$本文针对图输入的微小扰动会导致节点表示不可靠的问题，提出了一种稳定性、可识别性 GNN 抗扰动学习模型。该模型以对比目标形式化稳定性和可识别性，同时保持平滑性。该方法是一种通用框架，可以与许多其他骨干模型配备。$图神经网络、节点表示学习、抗扰动学习$http://arxiv.org/pdf/2008.11416v3
Multi-scale Sinusoidal Embeddings Enable Learning on High Resolution  Mass Spectrometry Data$"  Small molecules in biological samples are studied to provide informationabout disease states, environmental toxins, natural product drug discovery, andmany other applications. The primary window into the composition of smallmolecule mixtures is tandem mass spectrometry (MS2), which produces data thatare of high sensitivity and part per million resolution. We adopt multi-scalesinusoidal embeddings of the mass data in MS2 designed to meet the challenge oflearning from the full resolution of MS2 data. Using these embeddings, weprovide a new state of the art model for spectral library search, the standardtask for initial evaluation of MS2 data. We also introduce a new task, chemicalproperty prediction from MS2 data, that has natural applications inhigh-throughput MS2 experiments and show that an average $R^2$ of 80\\% fornovel compounds can be achieved across 10 chemical properties prioritized bymedicinal chemists. We use dimensionality reduction techniques and experimentswith different floating point resolutions to show the essential rolemulti-scale sinusoidal embeddings play in learning from MS2 data."$$$$$http://arxiv.org/pdf/2207.02980v2
BigIssue: A Realistic Bug Localization Benchmark$  As machine learning tools progress, the inevitable question arises: How canmachine learning help us write better code? With significant progress beingachieved in natural language processing with models like GPT-3 and Bert, theapplications of natural language processing techniques to code are starting tobe explored. Most of the research has been focused on automatic program repair(APR), and while the results on synthetic or highly filtered datasets arepromising, such models are hard to apply in real-world scenarios because ofinadequate bug localization. We propose BigIssue: a benchmark for realistic buglocalization. The goal of the benchmark is two-fold. We provide (1) a generalbenchmark with a diversity of real and synthetic Java bugs and (2) a motivationto improve bug localization capabilities of models through attention to thefull repository context. With the introduction of BigIssue, we hope to advancethe state of the art in bug localization, in turn improving APR performance andincreasing its applicability to the modern development cycle.$$$$$http://arxiv.org/pdf/2207.10739v2
Multi-Step Short-Term Wind Speed Prediction with Rank Pooling and Fast  Fourier Transformation$  Short-term wind speed prediction is essential for economical wind powerutilization. The real-world wind speed data is typically intermittent andfluctuating, presenting great challenges to existing shallow models. In thispaper, we present a novel deep hybrid model for multi-step wind speedprediction, namely LR-FFT-RP-MLP/LSTM (Linear Fast Fourier Transformation RankPooling Multiple-Layer Perception/Long Short-Term Memory). Our hybrid modelprocesses the local and global input features simultaneously. We leverage RankPooling (RP) for the local feature extraction to capture the temporal structurewhile maintaining the temporal order. Besides, to understand the wind periodicpatterns, we exploit Fast Fourier Transformation (FFT) to extract globalfeatures and relevant frequency components in the wind speed data. Theresulting local and global features are respectively integrated with theoriginal data and are fed into an MLP/LSTM layer for the initial wind speedpredictions. Finally, we leverage a linear regression layer to collaboratethese initial predictions to produce the final wind speed prediction. Theproposed hybrid model is evaluated using real wind speed data collected from2010 to 2020, demonstrating superior forecasting capabilities when compared tostate-of-the-art single and hybrid models. Overall, this study presents apromising approach for improving the accuracy of wind speed forecasting.$$$$$http://arxiv.org/pdf/2211.14434v2
Autothrottle: A Practical Bi-Level Approach to Resource Management for  SLO-Targeted Microservices$  Achieving resource efficiency while preserving end-user experience isnon-trivial for cloud application operators. As cloud applicationsprogressively adopt microservices, resource managers are faced with twodistinct levels of system behavior: the end-to-end application latency andper-service resource usage. Translation between these two levels, however, ischallenging because user requests traverse heterogeneous services thatcollectively (but unevenly) contribute to the end-to-end latency. This paperpresents Autothrottle, a bi-level learning-assisted resource managementframework for SLO-targeted microservices. It architecturally decouplesmechanisms of application SLO feedback and service resource control, andbridges them with the notion of performance targets. This decoupling enablestargeted control policies for these two mechanisms, where we combinelightweight heuristics and learning techniques. We evaluate Autothrottle onthree microservice applications, with workload traces from productionscenarios. Results show its superior CPU resource saving, up to 26.21% over thebest-performing baseline, and up to 93.84% over all baselines.$$$$$http://arxiv.org/pdf/2212.12180v3
Towards Multi-User Activity Recognition through Facilitated Training  Data and Deep Learning for Human-Robot Collaboration Applications$  Human-robot interaction (HRI) research is progressively addressingmulti-party scenarios, where a robot interacts with more than one human user atthe same time. Conversely, research is still at an early stage for human-robotcollaboration. The use of machine learning techniques to handle such type ofcollaboration requires data that are less feasible to produce than in a typicalHRC setup. This work outlines scenarios of concurrent tasks for non-dyadic HRCapplications. Based upon these concepts, this study also proposes analternative way of gathering data regarding multi-user activity, by collectingdata related to single users and merging them in post-processing, to reduce theeffort involved in producing recordings of pair settings. To validate thisstatement, 3D skeleton poses of activity of single users were collected andmerged in pairs. After this, such datapoints were used to separately train along short-term memory (LSTM) network and a variational autoencoder (VAE)composed of spatio-temporal graph convolutional networks (STGCN) to recognisethe joint activities of the pairs of people. The results showed that it ispossible to make use of data collected in this way for pair HRC settings andget similar performances compared to using training data regarding groups ofusers recorded under the same settings, relieving from the technicaldifficulties involved in producing these data.  The related code and collected data are publicly available.$$$$$http://arxiv.org/pdf/2302.05763v2
On the Implicit Bias of Linear Equivariant Steerable Networks$  We study the implicit bias of gradient flow on linear equivariant steerablenetworks in group-invariant binary classification. Our findings reveal that theparameterized predictor converges in direction to the unique group-invariantclassifier with a maximum margin defined by the input group action. Under aunitary assumption on the input representation, we establish the equivalencebetween steerable networks and data augmentation. Furthermore, we demonstratethe improved margin and generalization bound of steerable networks over theirnon-invariant counterparts.$$$$$http://arxiv.org/pdf/2303.04198v2
Directed Chain Generative Adversarial Networks$  Real-world data can be multimodal distributed, e.g., data describing theopinion divergence in a community, the interspike interval distribution ofneurons, and the oscillators natural frequencies. Generating multimodaldistributed real-world data has become a challenge to existing generativeadversarial networks (GANs). For example, neural stochastic differentialequations (Neural SDEs), treated as infinite-dimensional GANs, havedemonstrated successful performance mainly in generating unimodal time seriesdata. In this paper, we propose a novel time series generator, named directedchain GANs (DC-GANs), which inserts a time series dataset (called aneighborhood process of the directed chain or input) into the drift anddiffusion coefficients of the directed chain SDEs with distributionalconstraints. DC-GANs can generate new time series of the same distribution asthe neighborhood process, and the neighborhood process will provide the keystep in learning and generating multimodal distributed time series. Theproposed DC-GANs are examined on four datasets, including two stochastic modelsfrom social sciences and computational neuroscience, and two real-worlddatasets on stock prices and energy consumption. To our best knowledge, DC-GANsare the first work that can generate multimodal time series data andconsistently outperforms state-of-the-art benchmarks with respect to measuresof distribution, data similarity, and predictive ability.$$$$$http://arxiv.org/pdf/2304.13131v2
Enhancing Robustness of Gradient-Boosted Decision Trees through One-Hot  Encoding and Regularization$"  Gradient-boosted decision trees (GBDT) are widely used and highly effectivemachine learning approach for tabular data modeling. However, their complexstructure may lead to low robustness against small covariate perturbation inunseen data. In this study, we apply one-hot encoding to convert a GBDT modelinto a linear framework, through encoding of each tree leaf to one dummyvariable. This allows for the use of linear regression techniques, plus a novelrisk decomposition for assessing the robustness of a GBDT model againstcovariate perturbations. We propose to enhance the robustness of GBDT models byrefitting their linear regression forms with $L_1$ or $L_2$ regularization.Theoretical results are obtained about the effect of regularization on themodel performance and robustness. It is demonstrated through numericalexperiments that the proposed regularization approach can enhance therobustness of the one-hot-encoded GBDT models."$$$$$http://arxiv.org/pdf/2304.13761v2
Contrastive losses as generalized models of global epistasis$  Fitness functions map large combinatorial spaces of biological sequences toproperties of interest. Inferring these multimodal functions from experimentaldata is a central task in modern protein engineering. Global epistasis modelsare an effective and physically-grounded class of models for estimating fitnessfunctions from observed data. These models assume that a sparse latent functionis transformed by a monotonic nonlinearity to emit measurable fitness. Here wedemonstrate that minimizing contrastive loss functions, such as theBradley-Terry loss, is a simple and flexible technique for extracting thesparse latent function implied by global epistasis. We argue by way of afitness-epistasis uncertainty principle that the nonlinearities in globalepistasis models can produce observed fitness functions that do not admitsparse representations, and thus may be inefficient to learn from observationswhen using a Mean Squared Error (MSE) loss (a common practice). We show thatcontrastive losses are able to accurately estimate a ranking function fromlimited data even in regimes where MSE is ineffective. We validate thepractical utility of this insight by showing contrastive loss functions resultin consistently improved performance on benchmark tasks.$$$$$http://arxiv.org/pdf/2305.03136v1
Contrastive Learning for Sleep Staging based on Inter Subject  Correlation$  In recent years, multitudes of researches have applied deep learning toautomatic sleep stage classification. Whereas actually, these works have paidless attention to the issue of cross-subject in sleep staging. At the sametime, emerging neuroscience theories on inter-subject correlations can providenew insights for cross-subject analysis. This paper presents the MViTime modelthat have been used in sleep staging study. And we implement the inter-subjectcorrelation theory through contrastive learning, providing a feasible solutionto address the cross-subject problem in sleep stage classification. Finally,experimental results and conclusions are presented, demonstrating that thedeveloped method has achieved state-of-the-art performance on sleep staging.The results of the ablation experiment also demonstrate the effectiveness ofthe cross-subject approach based on contrastive learning.$$$$$http://arxiv.org/pdf/2305.03178v1
A Generative Modeling Framework for Inferring Families of Biomechanical  Constitutive Laws in Data-Sparse Regimes$  Quantifying biomechanical properties of the human vasculature could deepenour understanding of cardiovascular diseases. Standard nonlinear regression inconstitutive modeling requires considerable high-quality data and an explicitform of the constitutive model as prior knowledge. By contrast, we propose anovel approach that combines generative deep learning with Bayesian inferenceto efficiently infer families of constitutive relationships in data-sparseregimes. Inspired by the concept of functional priors, we develop a generativeadversarial network (GAN) that incorporates a neural operator as the generatorand a fully-connected neural network as the discriminator. The generator takesa vector of noise conditioned on measurement data as input and yields thepredicted constitutive relationship, which is scrutinized by the discriminatorin the following step. We demonstrate that this framework can accuratelyestimate means and standard deviations of the constitutive relationships of themurine aorta using data collected either from model-generated synthetic data orex vivo experiments for mice with genetic deficiencies. In addition, theframework learns priors of constitutive models without explicitly knowing theirfunctional form, providing a new model-agnostic approach to learning hiddenconstitutive behaviors from data.$$$$$http://arxiv.org/pdf/2305.03184v1
All models are local: time to replace external validation with recurrent  local validation$  External validation is often recommended to ensure the generalizability of MLmodels. However, it neither guarantees generalizability nor equates to amodel\'s clinical usefulness (the ultimate goal of any clinical decision-supporttool). External validation is misaligned with current healthcare ML needs.First, patient data changes across time, geography, and facilities. Thesechanges create significant volatility in the performance of a single fixedmodel (especially for deep learning models, which dominate clinical ML).Second, newer ML techniques, current market forces, and updated regulatoryframeworks are enabling frequent updating and monitoring of individual deployedmodel instances. We submit that external validation is insufficient toestablish ML models\' safety or utility. Proposals to fix the externalvalidation paradigm do not go far enough. Continued reliance on it as theultimate test is likely to lead us astray. We propose the MLOps-inspiredparadigm of recurring local validation as an alternative that ensures thevalidity of models while protecting against performance-disruptive datavariability. This paradigm relies on site-specific reliability tests beforeevery deployment, followed by regular and recurrent checks throughout the lifecycle of the deployed algorithm. Initial and recurrent reliability testsprotect against performance-disruptive distribution shifts, and concept driftsthat jeopardize patient safety.$$$$$http://arxiv.org/pdf/2305.03219v1
Algorithms for Social Justice: Affirmative Action in Social Networks$  Link recommendation algorithms contribute to shaping human relations ofbillions of users worldwide in social networks. To maximize relevance, theytypically propose connecting users that are similar to each other. This hasbeen found to create information silos, exacerbating the isolation suffered byvulnerable salient groups and perpetuating societal stereotypes. To mitigatethese limitations, a significant body of work has been devoted to theimplementation of fair link recommendation methods. However, most approaches donot question the ultimate goal of link recommendation algorithms, namely themonetization of users\' engagement in intricate business models of data trade.This paper advocates for a diversification of players and purposes of socialnetwork platforms, aligned with the pursue of social justice. To illustratethis conceptual goal, we present ERA-Link, a novel link recommendationalgorithm based on spectral graph theory that counteracts the systemic societaldiscrimination suffered by vulnerable groups by explicitly implementingaffirmative action. We propose four principled evaluation measures, derivedfrom effective resistance, to quantitatively analyze the behavior of theproposed method and compare it to three alternative approaches. Experimentswith synthetic and real-world networks illustrate how ERA-Link generates betteroutcomes according to all evaluation measures, not only for the vulnerablegroup but for the whole network. In other words, ERA-Link recommendsconnections that mitigate the structural discrimination of a vulnerable group,improves social cohesion and increases the social capital of all network users.Furthermore, by promoting the access to a diversity of users, ERA-Linkfacilitates innovation opportunities.$$$社交网络、链路推荐算法、公平性、边缘群体、社会正义$1University of Notre Dame, USA；2ELLIS Alicante, Spain$http://arxiv.org/pdf/2305.03223v1
Carbon Price Forecasting with Quantile Regression and Feature Selection$  Carbon futures has recently emerged as a novel financial asset in the tradingmarkets such as the European Union and China. Monitoring the trend of thecarbon price has become critical for both national policy-making as well asindustrial manufacturing planning. However, various geopolitical, social, andeconomic factors can impose substantial influence on the carbon price. Due toits volatility and non-linearity, predicting accurate carbon prices isgenerally a difficult task. In this study, we propose to improve carbon priceforecasting with several novel practices. First, we collect various influencingfactors, including commodity prices, export volumes such as oil and naturalgas, and prosperity indices. Then we select the most significant factors anddisclose their optimal grouping for explainability. Finally, we use the SparseQuantile Group Lasso and Adaptive Sparse Quantile Group Lasso for robust pricepredictions. We demonstrate through extensive experimental studies that ourproposed methods outperform existing ones. Also, our quantile predictionsprovide a complete profile of future prices at different levels, which betterdescribes the distributions of the carbon market.$$$$$http://arxiv.org/pdf/2305.03224v1
PMP: Learning to Physically Interact with Environments using Part-wise  Motion Priors$  We present a method to animate a character incorporating multiple part-wisemotion priors (PMP). While previous works allow creating realistic articulatedmotions from reference data, the range of motion is largely limited by theavailable samples. Especially for the interaction-rich scenarios, it isimpractical to attempt acquiring every possible interacting motion, as thecombination of physical parameters increases exponentially. The proposed PMPallows us to assemble multiple part skills to animate a character, creating adiverse set of motions with different combinations of existing data. In ourpipeline, we can train an agent with a wide range of part-wise priors.Therefore, each body part can obtain a kinematic insight of the style from themotion captures, or at the same time extract dynamics-related information fromthe additional part-specific simulation. For example, we can first train ageneral interaction skill, e.g. grasping, only for the dexterous part, and thencombine the expert trajectories from the pre-trained agent with the kinematicpriors of other limbs. Eventually, our whole-body agent learns a novel physicalinteraction skill even with the absence of the object trajectories in thereference motion sequence.$$$$$http://arxiv.org/pdf/2305.03249v1
Tiny-PPG: A Lightweight Deep Neural Network for Real-Time Detection of  Motion Artifacts in Photoplethysmogram Signals on Edge Devices$  Photoplethysmogram (PPG) signals are easily contaminated by motion artifactsin real-world settings, despite their widespread use in Internet-of-Things(IoT) based wearable and smart health devices for cardiovascular healthmonitoring. This study proposed a lightweight deep neural network, calledTiny-PPG, for accurate and real-time PPG artifact segmentation on IoT edgedevices. The model was trained and tested on a public dataset, PPG DaLiA, whichfeatured complex artifacts with diverse lengths and morphologies during variousdaily activities of 15 subjects using a watch-type device (Empatica E4). Themodel structure, training method and loss function were specifically designedto balance detection accuracy and speed for real-time PPG artifact detection inresource-constrained embedded devices. To optimize the model size andcapability in multi-scale feature representation, the model employed deepseparable convolution and atrous spatial pyramid pooling modules, respectively.Additionally, the contrastive loss was also utilized to further optimize thefeature embeddings. With additional model pruning, Tiny-PPG achievedstate-of-the-art detection accuracy of 87.8% while only having 19,726 modelparameters (0.15 megabytes), and was successfully deployed on an STM32 embeddedsystem for real-time PPG artifact detection. Therefore, this study provides aneffective solution for resource-constraint IoT smart health devices in PPGartifact detection.$$$$$http://arxiv.org/pdf/2305.03308v1
Random Smoothing Regularization in Kernel Gradient Descent Learning$"  Random smoothing data augmentation is a unique form of regularization thatcan prevent overfitting by introducing noise to the input data, encouraging themodel to learn more generalized features. Despite its success in variousapplications, there has been a lack of systematic study on the regularizationability of random smoothing. In this paper, we aim to bridge this gap bypresenting a framework for random smoothing regularization that can adaptivelyand effectively learn a wide range of ground truth functions belonging to theclassical Sobolev spaces. Specifically, we investigate two underlying functionspaces: the Sobolev space of low intrinsic dimension, which includes theSobolev space in $D$-dimensional Euclidean space or low-dimensionalsub-manifolds as special cases, and the mixed smooth Sobolev space with atensor structure. By using random smoothing regularization as novelconvolution-based smoothing kernels, we can attain optimal convergence rates inthese cases using a kernel gradient descent algorithm, either with earlystopping or weight decay. It is noteworthy that our estimator can adapt to thestructural assumptions of the underlying data and avoid the curse ofdimensionality. This is achieved through various choices of injected noisedistributions such as Gaussian, Laplace, or general polynomial noises, allowingfor broad adaptation to the aforementioned structural assumptions of theunderlying data. The convergence rate depends only on the effective dimension,which may be significantly smaller than the actual data dimension. We conductnumerical experiments on simulated data to validate our theoretical results."$$$$$http://arxiv.org/pdf/2305.03531v1
Contrastive Graph Clustering in Curvature Spaces$  Graph clustering is a longstanding research topic, and has achievedremarkable success with the deep learning methods in recent years.Nevertheless, we observe that several important issues largely remain open. Onthe one hand, graph clustering from the geometric perspective is appealing buthas rarely been touched before, as it lacks a promising space for geometricclustering. On the other hand, contrastive learning boosts the deep graphclustering but usually struggles in either graph augmentation or hard samplemining. To bridge this gap, we rethink the problem of graph clustering fromgeometric perspective and, to the best of our knowledge, make the first attemptto introduce a heterogeneous curvature space to graph clustering problem.Correspondingly, we present a novel end-to-end contrastive graph clusteringmodel named CONGREGATE, addressing geometric graph clustering with Riccicurvatures. To support geometric clustering, we construct a theoreticallygrounded Heterogeneous Curvature Space where deep representations are generatedvia the product of the proposed fully Riemannian graph convolutional nets.Thereafter, we train the graph clusters by an augmentation-free reweightedcontrastive approach where we pay more attention to both hard negatives andhard positives in our curvature space. Empirical results on real-world graphsshow that our model outperforms the state-of-the-art competitors.$$$$$http://arxiv.org/pdf/2305.03555v1
Scope Restriction for Scalable Real-Time Railway Rescheduling: An  Exploratory Study$  With the aim to stimulate future research, we describe an exploratory studyof a railway rescheduling problem. A widely used approach in practice and stateof the art is to decompose these complex problems by geographical scope.Instead, we propose defining a core problem that restricts a reschedulingproblem in response to a disturbance to only trains that need to berescheduled, hence restricting the scope in both time and space. In thiscontext, the difficulty resides in defining a scoper that can predict a subsetof train services that will be affected by a given disturbance. We reportpreliminary results using the Flatland simulation environment that highlightsthe potential and challenges of this idea. We provide an extensible playgroundopen-source implementation based on the Flatland railway environment andAnswer-Set Programming.$$$$$http://arxiv.org/pdf/2305.03574v1
Optimizing Hyperparameters with Conformal Quantile Regression$  Many state-of-the-art hyperparameter optimization (HPO) algorithms rely onmodel-based optimizers that learn surrogate models of the target function toguide the search. Gaussian processes are the de facto surrogate model due totheir ability to capture uncertainty but they make strong assumptions about theobservation noise, which might not be warranted in practice. In this work, wepropose to leverage conformalized quantile regression which makes minimalassumptions about the observation noise and, as a result, models the targetfunction in a more realistic and robust fashion which translates to quicker HPOconvergence on empirical benchmarks. To apply our method in a multi-fidelitysetting, we propose a simple, yet effective, technique that aggregates observedresults across different resource levels and outperforms conventional methodsacross many empirical tasks.$$$$$http://arxiv.org/pdf/2305.03623v1
Data Encoding For Healthcare Data Democratisation and Information  Leakage Prevention$  The lack of data democratization and information leakage from trained modelshinder the development and acceptance of robust deep learning-based healthcaresolutions. This paper argues that irreversible data encoding can provide aneffective solution to achieve data democratization without violating theprivacy constraints imposed on healthcare data and clinical models. An idealencoding framework transforms the data into a new space where it isimperceptible to a manual or computational inspection. However, encoded datashould preserve the semantics of the original data such that deep learningmodels can be trained effectively. This paper hypothesizes the characteristicsof the desired encoding framework and then exploits random projections andrandom quantum encoding to realize this framework for dense and longitudinal ortime-series data. Experimental evaluation highlights that models trained onencoded time-series data effectively uphold the information bottleneckprinciple and hence, exhibit lesser information leakage from trained models.$$$$$http://arxiv.org/pdf/2305.03710v1
Is dataset condensation a silver bullet for healthcare data sharing?$  Safeguarding personal information is paramount for healthcare data sharing, achallenging issue without any silver bullet thus far. We study the prospect ofa recent deep-learning advent, dataset condensation (DC), in sharing healthcaredata for AI research, and the results are promising. The condensed dataabstracts original records and irreversibly conceals individual-level knowledgeto achieve a bona fide de-identification, which permits free sharing. Moreover,the original deep-learning utilities are well preserved in the condensed datawith compressed volume and accelerated model convergences. In PhysioNet-2012, acondensed dataset of 20 samples can orient deep models attaining 80.3% test AUCof mortality prediction (versus 85.8% of 5120 original records), an inspiringdiscovery generalised to MIMIC-III and Coswara datasets. We also interpret theinhere privacy protections of DC through theoretical analysis and empiricalevidence. Dataset condensation opens a new gate to sharing healthcare data forAI research with multiple desirable traits.$$$$$http://arxiv.org/pdf/2305.03711v1
COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM$  Collaborative SLAM is at the core of perception in multi-robot systems as itenables the co-localization of the team of robots in a common reference frame,which is of vital importance for any coordination amongst them. The paradigm ofa centralized architecture is well established, with the robots (i.e. agents)running Visual-Inertial Odometry (VIO) onboard while communicating relevantdata, such as e.g. Keyframes (KFs), to a central back-end (i.e. server), whichthen merges and optimizes the joint maps of the agents. While these frameworkshave proven to be successful, their capability and performance are highlydependent on the choice of the VIO front-end, thus limiting their flexibility.In this work, we present COVINS-G, a generalized back-end building upon theCOVINS framework, enabling the compatibility of the server-back-end with anyarbitrary VIO front-end, including, for example, off-the-shelf cameras withodometry capabilities, such as the Realsense T265. The COVINS-G back-enddeploys a multi-camera relative pose estimation algorithm for computing theloop-closure constraints allowing the system to work purely on 2D image data.In the experimental evaluation, we show on-par accuracy with state-of-the-artmulti-session and collaborative SLAM systems, while demonstrating theflexibility and generality of our approach by employing different front-endsonboard collaborating agents within the same mission. The COVINS-G codebasealong with a generalized front-end wrapper to allow any existing VIO front-endto be readily used in combination with the proposed collaborative back-end isopen-sourced. Video: https://youtu.be/FoJfXCfaYDw$$$$$http://arxiv.org/pdf/2301.07147v3
Solution existence, uniqueness, and stability of discrete basis  sinograms in multispectral CT$  This work investigates conditions for quantitative image reconstruction inmultispectral computed tomography (MSCT), which remains a topic of activeresearch. In MSCT, one seeks to obtain from data the spatial distribution oflinear attenuation coefficient, referred to as a virtual monochromatic image(VMI), at a given X-ray energy, within the subject imaged. As a VMI isdecomposed often into a linear combination of basis images with knowndecomposition coefficients, the reconstruction of a VMI is thus tantamount tothat of the basis images. An empirical, but highly effective, two-stepdata-domain-decomposition (DDD) method has been developed and used widely forquantitative image reconstruction in MSCT. In the two-step DDD method, step (1)estimates the so-called basis sinogram from data through solving a nonlineartransform, whereas step (2) reconstructs basis images from their basissinograms estimated. Subsequently, a VMI can readily be obtained from thelinear combination of basis images reconstructed. As step (2) involves theinversion of a straightforward linear system, step (1) is the key component ofthe DDD method in which a nonlinear system needs to be inverted for estimatingthe basis sinograms from data. In this work, we consider a {\\it discrete} formof the nonlinear system in step (1), and then carry out theoretical andnumerical analyses of conditions on the existence, uniqueness, and stability ofa solution to the discrete nonlinear system for accurately estimating thediscrete basis sinograms, leading to quantitative reconstruction of VMIs inMSCT.$$$$$http://arxiv.org/pdf/2305.03330v1
Prevalence and major risk factors of non-communicable diseases: A  Hospital-based Cross-Sectional Study in Dhaka, Bangladesh$  Objective: The study aimed to determine the prevalence of severalnon-communicable diseases (NCD) and analyze risk factors among adult patientsseeking nutritional guidance in Dhaka, Bangladesh. Result: Our study observedthe relationships between gender, age groups, obesity, and NCDs (DM, CKD, IBS,CVD, CRD, thyroid). The most frequently reported NCD was cardiovascular issues(CVD), which was present in 83.56% of all participants. CVD was more common inmale participants. Consequently, male participants had a higher blood pressuredistribution than females. Diabetes mellitus (DM), on the other hand, did nothave a gender-based inclination. Both CVD and DM had an age-based progression.Our study showed that chronic respiratory illness was more frequent inmiddle-aged participants than in younger or elderly individuals. Based on thedata, every one in five hospitalized patients was obese. We analyzed theco-morbidities and found that 31.5% of the population has only one NCD, 30.1%has two NCDs, and 38.3% has more than two NCDs. Besides, 86.25% of all diabeticpatients had cardiovascular issues. All thyroid patients in our study had CVD.Using a t-test, we found a relationship between CKD and thyroid (p-value0.061). Males under 35 years have a statistically significant relationshipbetween thyroid and chronic respiratory diseases (p-value 0.018). We also foundan association between DM and CKD among patients over 65 (p-value 0.038).Moreover, there has been a statistically significant relationship between CKDand Thyroid (P &lt; 0.05) for those below 35 and 35-65. We used a two-way ANOVAtest to find the statistically significant interaction of heart issues andchronic respiratory illness, in combination with diabetes. The combination ofDM and RTI also affected CKD in male patients over 65 years old.$$$$$http://arxiv.org/pdf/2303.04808v2
Transferability of coVariance Neural Networks and Application to  Interpretable Brain Age Prediction using Anatomical Features$"  Graph convolutional networks (GCN) leverage topology-driven graphconvolutional operations to combine information across the graph for inferencetasks. In our recent work, we have studied GCNs with covariance matrices asgraphs in the form of coVariance neural networks (VNNs) that draw similaritieswith traditional PCA-driven data analysis approaches while offering significantadvantages over them. In this paper, we first focus on theoreticallycharacterizing the transferability of VNNs. The notion of transferability ismotivated from the intuitive expectation that learning models could generalizeto ""compatible"" datasets (possibly of different dimensionalities) with minimaleffort. VNNs inherit the scale-free data processing architecture from GCNs andhere, we show that VNNs exhibit transferability of performance over datasetswhose covariance matrices converge to a limit object. Multi-scale neuroimagingdatasets enable the study of the brain at multiple scales and hence, canvalidate the theoretical results on the transferability of VNNs. To gauge theadvantages offered by VNNs in neuroimaging data analysis, we focus on the taskof ""brain age"" prediction using cortical thickness features. In clinicalneuroscience, there has been an increased interest in machine learningalgorithms which provide estimates of ""brain age"" that deviate fromchronological age. We leverage the architecture of VNNs to extend beyond thecoarse metric of brain age gap in Alzheimer\'s disease (AD) and make twoimportant observations: (i) VNNs can assign anatomical interpretability toelevated brain age gap in AD, and (ii) the interpretability offered by VNNs iscontingent on their ability to exploit specific principal components of theanatomical covariance matrix. We further leverage the transferability of VNNsto cross validate the above observations across different datasets."$$$$$http://arxiv.org/pdf/2305.01807v2
Unsupervised Improvement of Audio-Text Cross-Modal Representations$  Recent advances in using language models to obtain cross-modal audio-textrepresentations have overcome the limitations of conventional trainingapproaches that use predefined labels. This has allowed the community to makeprogress in tasks like zero-shot classification, which would otherwise not bepossible. However, learning such representations requires a large amount ofhuman-annotated audio-text pairs. In this paper, we study unsupervisedapproaches to improve the learning framework of such representations withunpaired text and audio. We explore domain-unspecific and domain-specificcuration methods to create audio-text pairs that we use to further improve themodel. We also show that when domain-specific curation is used in conjunctionwith a soft-labeled contrastive loss, we are able to obtain significantimprovement in terms of zero-shot classification performance on downstreamsound event classification or acoustic scene classification tasks.$audio-text representation learning, data augmentation, contrastive learning, sound event classification, acoustic scene classification$University of Illinois at Urbana-Champaign, Université Laval, Concordia University, Mila-Quebec AI Institute, Insper, Amazon Web Services$本篇文章旨在研究无监督方法提高音频和文本跨模态表示的学习框架。他们探讨了领域无关和领域特异性的管理方法，以创建音频文本对，并用此来进一步完善模型。通过在领域特定图片分类任务中使用软标记的对比损失，还显示了性能的显着提高。$信号处理、音频、声学、语言模型、无监督改进$http://arxiv.org/pdf/2305.01864v2
CAMEL: Co-Designing AI Models and Embedded DRAMs for Efficient On-Device  Learning$"  The emergence of the Internet of Things (IoT) has resulted in a remarkableamount of data generated on edge devices, which are often processed using AIalgorithms. On-device learning enables edge platforms to continually adapt theAI models to user personal data and further allows for a better servicequality. However, AI training on resource-limited devices is extremelydifficult because of the intensive computing workload and the significantamount of on-chip memory consumption exacted by deep neural networks (DNNs). Tomitigate this, we propose to use embedded dynamic random-access memory (eDRAM)as the main storage medium of training data. Compared with static random-accessmemory (SRAM), eDRAM introduces more than $2\\times$ improvement on storagedensity, enabling reduced off-chip memory traffic. However, to keep the storeddata intact, eDRAM is required to perform the power-hungry data refreshoperations.  eDRAM refresh can be eliminated if the data is stored for a period of timethat is shorter than the eDRAM retention time. To achieve this, we design anovel reversible DNN architecture that enables a significantly reduced datalifetime during the training process and removes the need for eDRAM refresh. Wefurther design an efficient on-device training engine, termed~\\textit{CAMEL},that uses eDRAM as the main on-chip memory. CAMEL enables the intermediateresults during training to fit fully in on-chip eDRAM arrays and completelyeliminates the off-chip DRAM traffic during the training process. We evaluateour CAMEL system on multiple DNNs with different datasets, demonstrating a morethan $3\\times$ saving on total DNN training energy consumption than the otherbaselines, while achieving a similar (even better) performance in validationaccuracy."$$$$$http://arxiv.org/pdf/2305.03148v1
Communication-Efficient Graph Neural Networks with Probabilistic  Neighborhood Expansion Analysis and Caching$  Training and inference with graph neural networks (GNNs) on massive graphshas been actively studied since the inception of GNNs, owing to the widespreaduse and success of GNNs in applications such as recommendation systems andfinancial forensics. This paper is concerned with minibatch training andinference with GNNs that employ node-wise sampling in distributed settings,where the necessary partitioning of vertex features across distributed storagecauses feature communication to become a major bottleneck that hampersscalability. To significantly reduce the communication volume withoutcompromising prediction accuracy, we propose a policy for caching dataassociated with frequently accessed vertices in remote partitions. The proposedpolicy is based on an analysis of vertex-wise inclusion probabilities (VIP)during multi-hop neighborhood sampling, which may expand the neighborhood farbeyond the partition boundaries of the graph. VIP analysis not only enables theelimination of the communication bottleneck, but it also offers a means toorganize in-memory data by prioritizing GPU storage for the most frequentlyaccessed vertex features. We present SALIENT++, which extends the priorstate-of-the-art SALIENT system to work with partitioned feature data andleverages the VIP-driven caching policy. SALIENT++ retains the local trainingefficiency and scalability of SALIENT by using a deep pipeline and drasticallyreducing communication volume while consuming only a fraction of the storagerequired by SALIENT. We provide experimental results with the Open GraphBenchmark data sets and demonstrate that training a 3-layer GraphSAGE modelwith SALIENT++ on 8 single-GPU machines is 7.1 faster than with SALIENT on 1single-GPU machine, and 12.7 faster than with DistDGL on 8 single-GPU machines.$$$$$http://arxiv.org/pdf/2305.03152v1
Data-driven and Physics Informed Modelling of Chinese Hamster Ovary Cell  Bioreactors$"  Fed-batch culture is an established operation mode for the production ofbiologics using mammalian cell cultures. Quantitative modeling integrates bothkinetics for some key reaction steps and optimization-driven metabolic fluxallocation, using flux balance analysis; this is known to lead to certainmathematical inconsistencies. Here, we propose a physically-informeddata-driven hybrid model (a ""gray box"") to learn models of the dynamicalevolution of Chinese Hamster Ovary (CHO) cell bioreactors from process data.The approach incorporates physical laws (e.g. mass balances) as well as kineticexpressions for metabolic fluxes. Machine learning (ML) is then used to (a)directly learn evolution equations (black-box modelling); (b) recover unknownphysical parameters (""white-box"" parameter fitting) or -- importantly -- (c)learn partially unknown kinetic expressions (gray-box modelling). We encode theconvex optimization step of the overdetermined metabolic biophysical system asa differentiable, feed-forward layer into our architectures, connecting partialphysical knowledge with data-driven machine learning."$$$$$http://arxiv.org/pdf/2305.03257v1
Demystifying Softmax Gating in Gaussian Mixture of Experts$  Understanding parameter estimation of softmax gating Gaussian mixture ofexperts has remained a long-standing open problem in the literature. It ismainly due to three fundamental theoretical challenges associated with thesoftmax gating: (i) the identifiability only up to the translation of theparameters; (ii) the intrinsic interaction via partial differential equationbetween the softmax gating and the expert functions in Gaussian distribution;(iii) the complex dependence between the numerator and denominator of theconditional density of softmax gating Gaussian mixture of experts. We resolvethese challenges by proposing novel Vononoi loss functions among parameters andestablishing the convergence rates of the maximum likelihood estimator (MLE)for solving parameter estimation in these models. When the number of experts isunknown and over-specified, our findings show a connection between the rate ofMLE and a solvability problem of a system of polynomial equations.$$$$$http://arxiv.org/pdf/2305.03288v1
FedNC: A Secure and Efficient Federated Learning Method Inspired by  Network Coding$  Federated Learning (FL) is a promising distributed learning mechanism whichstill faces two major challenges, namely privacy breaches and systemefficiency. In this work, we reconceptualize the FL system from the perspectiveof network information theory, and formulate an original FL communicationframework, FedNC, which is inspired by Network Coding (NC). The main idea ofFedNC is mixing the information of the local models by making random linearcombinations of the original packets, before uploading for further aggregation.Due to the benefits of the coding scheme, both theoretical and experimentalanalysis indicate that FedNC improves the performance of traditional FL inseveral important ways, including security, throughput, and robustness. To thebest of our knowledge, this is the first framework where NC is introduced inFL. As FL continues to evolve within practical network frameworks, moreapplications and variants can be further designed based on FedNC.$$$$$http://arxiv.org/pdf/2305.03292v1
Decentralized diffusion-based learning under non-parametric limited  prior knowledge$"  We study the problem of diffusion-based network learning of a nonlinearphenomenon, $m$, from local agents\' measurements collected in a noisyenvironment. For a decentralized network and information spreading merelybetween directly neighboring nodes, we propose a non-parametric learningalgorithm, that avoids raw data exchange and requires only mild \\textit{apriori} knowledge about $m$. Non-asymptotic estimation error bounds are derivedfor the proposed method. Its potential applications are illustrated throughsimulation experiments."$$$$$http://arxiv.org/pdf/2305.03295v1
Generic and Robust Root Cause Localization for Multi-Dimensional Data in  Online Service Systems$  Localizing root causes for multi-dimensional data is critical to ensureonline service systems\' reliability. When a fault occurs, only the measurevalues within specific attribute combinations are abnormal. Such attributecombinations are substantial clues to the underlying root causes and thus arecalled root causes of multidimensional data. This paper proposes a generic androbust root cause localization approach for multi-dimensional data, PSqueeze.We propose a generic property of root cause for multi-dimensional data,generalized ripple effect (GRE). Based on it, we propose a novel probabilisticcluster method and a robust heuristic search method. Moreover, we identify theimportance of determining external root causes and propose an effective methodfor the first time in literature. Our experiments on two real-world datasetswith 5400 faults show that the F1-score of PSqueeze outperforms baselines by32.89%, while the localization time is around 10 seconds across all cases. TheF1-score in determining external root causes of PSqueeze achieves 0.90.Furthermore, case studies in several production systems demonstrate thatPSqueeze is helpful to fault diagnosis in the real world.$$$$$http://arxiv.org/pdf/2305.03331v1
Sparsifying Bayesian neural networks with latent binary variables and  normalizing flows$  Artificial neural networks (ANNs) are powerful machine learning methods usedin many modern applications such as facial recognition, machine translation,and cancer diagnostics. A common issue with ANNs is that they usually havemillions or billions of trainable parameters, and therefore tend to overfit tothe training data. This is especially problematic in applications where it isimportant to have reliable uncertainty estimates. Bayesian neural networks(BNN) can improve on this, since they incorporate parameter uncertainty. Inaddition, latent binary Bayesian neural networks (LBBNN) also take into accountstructural uncertainty by allowing the weights to be turned on or off, enablinginference in the joint space of weights and structures. In this paper, we willconsider two extensions to the LBBNN method: Firstly, by using the localreparametrization trick (LRT) to sample the hidden units directly, we get amore computationally efficient algorithm. More importantly, by usingnormalizing flows on the variational posterior distribution of the LBBNNparameters, the network learns a more flexible variational posteriordistribution than the mean field Gaussian. Experimental results show that thisimproves predictive power compared to the LBBNN method, while also obtainingmore sparse networks. We perform two simulation studies. In the first study, weconsider variable selection in a logistic regression setting, where the moreflexible variational distribution leads to improved results. In the secondstudy, we compare predictive uncertainty based on data generated fromtwo-dimensional Gaussian distributions. Here, we argue that our Bayesianmethods lead to more realistic estimates of predictive uncertainty.$$$$$http://arxiv.org/pdf/2305.03395v1
Zoo Guide to Network Embedding$  Networks have provided extremely successful models of data and complexsystems. Yet, as combinatorial objects, networks do not have in generalintrinsic coordinates and do not typically lie in an ambient space. The processof assigning an embedding space to a network has attracted lots of interest inthe past few decades, and has been efficiently applied to fundamental problemsin network inference, such as link prediction, node classification, andcommunity detection. In this review, we provide a user-friendly guide to thenetwork embedding literature and current trends in this field which will allowthe reader to navigate through the complex landscape of methods and approachesemerging from the vibrant research activity on these subjects.$$$$$http://arxiv.org/pdf/2305.03474v1
Exploring Softly Masked Language Modelling for Controllable Symbolic  Music Generation$  This document presents some early explorations of applying Softly MaskedLanguage Modelling (SMLM) to symbolic music generation. SMLM can be seen as ageneralisation of masked language modelling (MLM), where instead of eachelement of the input set being either known or unknown, elements can be partlyknown. We demonstrate some results of applying SMLM to constrained symbolicmusic generation using a transformer encoder architecture. Several audioexamples are available at https://erl-j.github.io/smlm-web-supplement/$$$$$http://arxiv.org/pdf/2305.03530v1
Over-the-Air Federated Averaging with Limited Power and Privacy Budgets$  To jointly overcome the communication bottleneck and privacy leakage ofwireless federated learning (FL), this paper studies a differentially privateover-the-air federated averaging (DP-OTA-FedAvg) system with a limited sumpower budget. With DP-OTA-FedAvg, the gradients are aligned by an alignmentcoefficient and aggregated over the air, and channel noise is employed toprotect privacy. We aim to improve the learning performance by jointlydesigning the device scheduling, alignment coefficient, and the number ofaggregation rounds of federated averaging (FedAvg) subject to sum power andprivacy constraints. We first present the privacy analysis based ondifferential privacy (DP) to quantify the impact of the alignment coefficienton privacy preservation in each communication round. Furthermore, to study howthe device scheduling, alignment coefficient, and the number of the globalaggregation affect the learning process, we conduct the convergence analysis ofDP-OTA-FedAvg in the cases of convex and non-convex loss functions. Based onthese analytical results, we formulate an optimization problem to minimize theoptimality gap of the DP-OTA-FedAvg subject to limited sum power and privacybudgets. The problem is solved by decoupling it into two sub-problems. Giventhe number of communication rounds, we conclude the relationship between thenumber of scheduled devices and the alignment coefficient, which offers a setof potential optimal solution pairs of device scheduling and the alignmentcoefficient. Thanks to the reduced search space, the optimal solution can beefficiently obtained. The effectiveness of the proposed policy is validatedthrough simulations.$$$$$http://arxiv.org/pdf/2305.03547v1
Verifiable Learning for Robust Tree Ensembles$  Verifying the robustness of machine learning models against evasion attacksat test time is an important research problem. Unfortunately, prior workestablished that this problem is NP-hard for decision tree ensembles, hencebound to be intractable for specific inputs. In this paper, we identify arestricted class of decision tree ensembles, called large-spread ensembles,which admit a security verification algorithm running in polynomial time. Wethen propose a new approach called verifiable learning, which advocates thetraining of such restricted model classes which are amenable for efficientverification. We show the benefits of this idea by designing a new trainingalgorithm that automatically learns a large-spread decision tree ensemble fromlabelled data, thus enabling its security verification in polynomial time.Experimental results on publicly available datasets confirm that large-spreadensembles trained using our algorithm can be verified in a matter of seconds,using standard commercial hardware. Moreover, large-spread ensembles are morerobust than traditional ensembles against evasion attacks, while incurring injust a relatively small loss of accuracy in the non-adversarial setting.$$$$$http://arxiv.org/pdf/2305.03626v1
On Preimage Approximation for Neural Networks$  Neural network verification mainly focuses on local robustness properties.However, often it is important to know whether a given property holds globallyfor the whole input domain, and if not then for what proportion of the inputthe property is true. While exact preimage generation can construct anequivalent representation of neural networks that can aid such (quantitative)global robustness verification, it is intractable at scale. In this work, wepropose an efficient and practical anytime algorithm for generating symbolicunder-approximations of the preimage of neural networks based on linearrelaxation. Our algorithm iteratively minimizes the volume approximation errorby partitioning the input region into subregions, where the neural networkrelaxation bounds become tighter. We further employ sampling and differentiableapproximations to the volume in order to prioritize regions to split andoptimize the parameters of the relaxation, leading to faster improvement andmore compact under-approximations. Evaluation results demonstrate that ourapproach is able to generate preimage approximations significantly faster thanexact methods and scales to neural network controllers for which exact preimagegeneration is intractable. We also demonstrate an application of our approachto quantitative global verification.$$$$$http://arxiv.org/pdf/2305.03686v1
Statistical Inference for Fairness Auditing$"  Before deploying a black-box model in high-stakes problems, it is importantto evaluate the model\'s performance on sensitive subpopulations. For example,in a recidivism prediction task, we may wish to identify demographic groups forwhich our prediction model has unacceptably high false positive rates orcertify that no such groups exist. In this paper, we frame this task, oftenreferred to as ""fairness auditing,"" in terms of multiple hypothesis testing. Weshow how the bootstrap can be used to simultaneously bound performancedisparities over a collection of groups with statistical guarantees. Ourmethods can be used to flag subpopulations affected by model underperformance,and certify subpopulations for which the model performs adequately. Crucially,our audit is model-agnostic and applicable to nearly any performance metric orgroup fairness criterion. Our methods also accommodate extremely rich -- eveninfinite -- collections of subpopulations. Further, we generalize beyondsubpopulations by showing how to assess performance over certain distributionshifts. We test the proposed methods on benchmark datasets in predictiveinference and algorithmic fairness and find that our audits can provideinterpretable and trustworthy guarantees."$$$$$http://arxiv.org/pdf/2305.03712v1
CaloFlow II: Even Faster and Still Accurate Generation of Calorimeter  Showers with Normalizing Flows$"  Recently, we introduced CaloFlow, a high-fidelity generative model for GEANT4calorimeter shower emulation based on normalizing flows. Here, we presentCaloFlow v2, an improvement on our original framework that speeds up showergeneration by a further factor of 500 relative to the original. The improvementis based on a technique called Probability Density Distillation, originallydeveloped for speech synthesis in the ML literature, and which we developfurther by introducing a set of powerful new loss terms. We demonstrate thatCaloFlow v2 preserves the same high fidelity of the original using qualitative(average images, histograms of high level features) and quantitative(classifier metric between GEANT4 and generated samples) measures. The resultis a generative model for calorimeter showers that matches the state-of-the-artin speed (a factor of $10^4$ faster than GEANT4) and greatly surpasses theprevious state-of-the-art in fidelity."$$$$$http://arxiv.org/pdf/2110.11377v2
Interpretable Machine Learning for Science with PySR and  SymbolicRegression.jl$"  PySR is an open-source library for practical symbolic regression, a type ofmachine learning which aims to discover human-interpretable symbolic models.PySR was developed to democratize and popularize symbolic regression for thesciences, and is built on a high-performance distributed back-end, a flexiblesearch algorithm, and interfaces with several deep learning packages. PySR\'sinternal search algorithm is a multi-population evolutionary algorithm, whichconsists of a unique evolve-simplify-optimize loop, designed for optimizationof unknown scalar constants in newly-discovered empirical expressions. PySR\'sbackend is the extremely optimized Julia library SymbolicRegression.jl, whichcan be used directly from Julia. It is capable of fusing user-defined operatorsinto SIMD kernels at runtime, performing automatic differentiation, anddistributing populations of expressions to thousands of cores across a cluster.In describing this software, we also introduce a new benchmark,""EmpiricalBench,"" to quantify the applicability of symbolic regressionalgorithms in science. This benchmark measures recovery of historical empiricalequations from original and synthetic datasets."$$$$$http://arxiv.org/pdf/2305.01582v3
The geometry of financial institutions -- Wasserstein clustering of  financial data$  The increasing availability of granular and big data on various objects ofinterest has made it necessary to develop methods for condensing thisinformation into a representative and intelligible map. Financial regulation isa field that exemplifies this need, as regulators require diverse and oftenhighly granular data from financial institutions to monitor and assess theiractivities. However, processing and analyzing such data can be a daunting task,especially given the challenges of dealing with missing values and identifyingclusters based on specific features.  To address these challenges, we propose a variant of Lloyd\'s algorithm thatapplies to probability distributions and uses generalized Wassersteinbarycenters to construct a metric space which represents given data on variousobjects in condensed form. By applying our method to the financial regulationcontext, we demonstrate its usefulness in dealing with the specific challengesfaced by regulators in this domain. We believe that our approach can also beapplied more generally to other fields where large and complex data sets needto be represented in concise form.$$$$$http://arxiv.org/pdf/2305.03565v1
Model-free Reinforcement Learning of Semantic Communication by  Stochastic Policy Gradient$  Motivated by the recent success of Machine Learning tools in wirelesscommunications, the idea of semantic communication by Weaver from 1949 hasgained attention. It breaks with Shannon\'s classic design paradigm by aiming totransmit the meaning, i.e., semantics, of a message instead of its exactversion, allowing for information rate savings. In this work, we apply theStochastic Policy Gradient (SPG) to design a semantic communication system byreinforcement learning, not requiring a known or differentiable channel model -a crucial step towards deployment in practice. Further, we motivate the use ofSPG for both classic and semantic communication from the maximization of themutual information between received and target variables. Numerical resultsshow that our approach achieves comparable performance to a model-awareapproach based on the reparametrization trick, albeit with a decreasedconvergence rate.$Semantic communication, wireless networks, information bottleneck, machine learning, reinforcement learning, stochastic policy gradient$Department of Communications Engineering, University of Bremen, Bremen, Germany$本文研究将语义信息进行数据压缩以提高通信效率的语义通信系统，通过强化学习中的随机策略梯度来优化系统性能。文章提出了一种新的解决方法，并在数值结果中证明了性能的可行性。$语义通信、无线网络、信息瓶颈、机器学习、强化学习、随机策略梯度$http://arxiv.org/pdf/2305.03571v1
On the Optimality, Stability, and Feasibility of Control Barrier  Functions: An Adaptive Learning-Based Approach$"  Safety has been a critical issue for the deployment of learning-basedapproaches in real-world applications. To address this issue, control barrierfunction (CBF) and its variants have attracted extensive attention forsafety-critical control. However, due to the myopic one-step nature of CBF andthe lack of principled methods to design the class-$\\mathcal{K}$ functions,there are still fundamental limitations of current CBFs: optimality, stability,and feasibility. In this paper, we proposed a novel and unified approach toaddress these limitations with Adaptive Multi-step Control Barrier Function(AM-CBF), where we parameterize the class-$\\mathcal{K}$ function by a neuralnetwork and train it together with the reinforcement learning policy. Moreover,to mitigate the myopic nature, we propose a novel \\textit{multi-step trainingand single-step execution} paradigm to make CBF farsighted while the executionremains solving a single-step convex quadratic program. Our method is evaluatedon the first and second-order systems in various scenarios, where our approachoutperforms the conventional CBF both qualitatively and quantitatively."$$$$$http://arxiv.org/pdf/2305.03608v1
Differentially Private Topological Data Analysis$"  This paper is the first to attempt differentially private (DP) topologicaldata analysis (TDA), producing near-optimal private persistence diagrams. Weanalyze the sensitivity of persistence diagrams in terms of the bottleneckdistance, and we show that the commonly used \\v{C}ech complex has sensitivitythat does not decrease as the sample size $n$ increases. This makes itchallenging for the persistence diagrams of \\v{C}ech complexes to beprivatized. As an alternative, we show that the persistence diagram obtained bythe $L^1$-distance to measure (DTM) has sensitivity $O(1/n)$. Based on thesensitivity analysis, we propose using the exponential mechanism whose utilityfunction is defined in terms of the bottleneck distance of the $L^1$-DTMpersistence diagrams. We also derive upper and lower bounds of the accuracy ofour privacy mechanism; the obtained bounds indicate that the privacy error ofour mechanism is near-optimal. We demonstrate the performance of our privatizedpersistence diagrams through simulations as well as on a real dataset trackinghuman movement."$$$$$http://arxiv.org/pdf/2305.03609v1
