title$summary$tag$affiliation$summary_zh$tag_zh$url
FedBoosting: Federated Learning with Gradient Protected Boosting for  Text Recognition$  Typical machine learning approaches require centralized data for modeltraining, which may not be possible where restrictions on data sharing are inplace due to, for instance, privacy and gradient protection. The recentlyproposed Federated Learning (FL) framework allows learning a shared modelcollaboratively without data being centralized or shared among data owners.However, we show in this paper that the generalization ability of the jointmodel is poor on Non-Independent and Non-Identically Distributed (Non-IID)data, particularly when the Federated Averaging (FedAvg) strategy is used dueto the weight divergence phenomenon. Hence, we propose a novel boostingalgorithm for FL to address both the generalization and gradient leakageissues, as well as achieve faster convergence in gradient-based optimization.In addition, a secure gradient sharing protocol using Homomorphic Encryption(HE) and Differential Privacy (DP) is introduced to defend against gradientleakage attack and avoid pairwise encryption that is not scalable. Wedemonstrate the proposed Federated Boosting (FedBoosting) method achievesnoticeable improvements in both prediction accuracy and run-time efficiency ina visual text recognition task on public benchmark.$Federated Learning, Text Recognition, Gradient Protection$Swansea University, Durham University, Xidian University, Xi'an University of Technology$本文研究了在数据受到联邦学习限制时，使用传统Federated Averaging策略时的缺陷，提出了一种新的boosting算法（FedBoosting）以解决这些问题，并通过可靠的实验验证了该算法在公共基准数据集上文本识别任务中的表现。$联邦学习，文本识别，梯度保护$http://arxiv.org/pdf/2007.07296v5
Semi-supervised Medical Image Segmentation through Dual-task Consistency$  Deep learning-based semi-supervised learning (SSL) algorithms have led topromising results in medical images segmentation and can alleviate doctors\'expensive annotations by leveraging unlabeled data. However, most of theexisting SSL algorithms in literature tend to regularize the model training byperturbing networks and/or data. Observing that multi/dual-task learningattends to various levels of information which have inherent predictionperturbation, we ask the question in this work: can we explicitly buildtask-level regularization rather than implicitly constructing networks- and/ordata-level perturbation-and-transformation for SSL? To answer this question, wepropose a novel dual-task-consistency semi-supervised framework for the firsttime. Concretely, we use a dual-task deep network that jointly predicts apixel-wise segmentation map and a geometry-aware level set representation ofthe target. The level set representation is converted to an approximatedsegmentation map through a differentiable task transform layer. Simultaneously,we introduce a dual-task consistency regularization between the levelset-derived segmentation maps and directly predicted segmentation maps for bothlabeled and unlabeled data. Extensive experiments on two public datasets showthat our method can largely improve the performance by incorporating theunlabeled data. Meanwhile, our framework outperforms the state-of-the-artsemi-supervised medical image segmentation methods. Code is available at:https://github.com/Luoxd1996/DTC$Medical Image Segmentation, Semi-supervised Learning, Deep Learning, Dual-task Learning, Regularization$1 University of Electronic Science and Technology of China, 2 SenseTime Research, 3 Tongji University$本文提出了一种新的双任务一致性半监督学习框架，将任务级别的正则化引入医学图像分割中。通过双任务深度网络，同时预测像素级分割地图和几何感知的水平集表示，并通过可微分的任务转换层将水平集表示转换为近似的分割地图。同时，我们引入了对于有标签和无标签数据的双任务一致性正则化。在两个公共数据集上的实验表明，我们的方法可以通过结合无标签数据大大提高性能，并且优于现有的半监督学习方法。$医学图像分割，半监督学习，深度学习，双任务学习，正则化$http://arxiv.org/pdf/2009.04448v3
Implicit Integration of Superpixel Segmentation into Fully Convolutional  Networks$  Superpixels are a useful representation to reduce the complexity of imagedata. However, to combine superpixels with convolutional neural networks (CNNs)in an end-to-end fashion, one requires extra models to generate superpixels andspecial operations such as graph convolution. In this paper, we propose a wayto implicitly integrate a superpixel scheme into CNNs, which makes it easy touse superpixels with CNNs in an end-to-end fashion. Our proposed methodhierarchically groups pixels at downsampling layers and generates superpixels.Our method can be plugged into many existing architectures without a change intheir feed-forward path because our method does not use superpixels in thefeed-forward path but use them to recover the lost resolution instead ofbilinear upsampling. As a result, our method preserves detailed informationsuch as object boundaries in the form of superpixels even when the modelcontains downsampling layers. We evaluate our method on several tasks such assemantic segmentation, superpixel segmentation, and monocular depth estimation,and confirm that it speeds up modern architectures and/or improves theirprediction accuracy in these tasks.$Superpixel Segmentation, Fully Convolutional Networks, Pixel Clustering$Denso IT Laboratory, Inc.$本文提出了一种将超像素分割隐式集成到全卷积网络中的方法，通过对像素进行层次式聚类生成超像素，并在恢复失去的分辨率时使用超像素，有效地保留图像细节信息。作者在多个任务中验证了该方法的优越性能。$超像素分割、全卷积网络、像素聚类$http://arxiv.org/pdf/2103.03435v2
MSN: Multi-Style Network for Trajectory Prediction$  Trajectory prediction aims to forecast agents\' possible future locationsconsidering their observations along with the video context. It is stronglyneeded by many autonomous platforms like tracking, detection, robot navigation,and self-driving cars. Whether it is agents\' internal personality factors,interactive behaviors with the neighborhood, or the influence of surroundings,they all impact agents\' future planning. However, many previous methods modeland predict agents\' behaviors with the same strategy or feature distribution,making them challenging to make predictions with sufficient style differences.This paper proposes the Multi-Style Network (MSN), which utilizes styleproposal and stylized prediction using two sub-networks, to provide multi-stylepredictions in a novel categorical way adaptively. The proposed networkcontains a series of style channels, and each channel is bound to a unique andspecific behavior style. We use agents\' end-point plannings and theirinteraction context as the basis for the behavior classification, so as toadaptively learn multiple diverse behavior styles through these channels. Then,we assume that the target agents may plan their future behaviors according toeach of these categorized styles, thus utilizing different style channels tomake predictions with significant style differences in parallel. Experimentsshow that the proposed MSN outperforms current state-of-the-art methods up to10% quantitatively on two widely used datasets, and presents better multi-stylecharacteristics qualitatively.$Trajectory prediction, Multi-style, Hidden behavior category, Transformer$Huazhong University of Science and Technology$本文提出了一种基于多样化风格的轨迹预测方法，通过两个子网络的风格提取和预测，提供不同的预测风格，并以分类方式进行自适应预测。该方法采用终点预测和互动环境作为行为分类的基础，并通过多个风格通道来自适应地学习多种不同的行为风格。实验表明，该方法在两个广泛使用的数据集上的表现优于现有最先进方法，具有更好的多样化特征。$轨迹预测，多样化风格，隐藏的行为类别$http://arxiv.org/pdf/2107.00932v5
Reuse your features: unifying retrieval and feature-metric alignment$  We propose a compact pipeline to unify all the steps of Visual Localization:image retrieval, candidate re-ranking and initial pose estimation, and camerapose refinement. Our key assumption is that the deep features used for theseindividual tasks share common characteristics, so we should reuse them in allthe procedures of the pipeline. Our DRAN (Deep Retrieval and image AlignmentNetwork) is able to extract global descriptors for efficient image retrieval,use intermediate hierarchical features to re-rank the retrieval list andproduce an initial pose guess, which is finally refined by means of afeature-metric optimization based on learned deep multi-scale dense features.DRAN is the first single network able to produce the features for the threesteps of visual localization. DRAN achieves competitive performance in terms ofrobustness and accuracy under challenging conditions in public benchmarks,outperforming other unified approaches and consuming lower computational andmemory cost than its counterparts using multiple networks. Code and models willbe publicly available at https://github.com/jmorlana/DRAN.$Visual localization, Feature extraction, Deep learning$Instituto de Investigación en Ingeniería de Aragón (I3A), Universidad de Zaragoza, Spain$图像检索、候选重排和初始位姿估计以及相机姿势细化。作者们认为，这些步骤中使用的深度特征具有共同的特征，因此应在流水线中的所有过程中重复使用它们。作者们的DRAN（Deep Retrieval and image Alignment Network）能够提取全局描述符，以实现高效的图像检索，并使用中间层级特征对检索列表进行重新排序和产生初始位姿猜测，在经过基于学习的深度多尺度密集特征的特征度量优化之后，最终细化姿态。 DRAN是首个能够为三个视觉定位步骤生成特征的单个网络。 DRAN在公共基准测试中，在具有挑战性的条件下取得了具有竞争力的健壮性和准确性表现，优于其他统一方法，并且比使用多个网络的相应方法消耗更低的计算和内存成本。$视觉定位，特征提取，深度学习$http://arxiv.org/pdf/2204.06292v2
End-to-end Weakly-supervised Single-stage Multiple 3D Hand Mesh  Reconstruction from a Single RGB Image$  In this paper, we consider the challenging task of simultaneously locatingand recovering multiple hands from a single 2D image. Previous studies eitherfocus on single hand reconstruction or solve this problem in a multi-stage way.Moreover, the conventional two-stage pipeline firstly detects hand areas, andthen estimates 3D hand pose from each cropped patch. To reduce thecomputational redundancy in preprocessing and feature extraction, for the firsttime, we propose a concise but efficient single-stage pipeline for multi-handreconstruction. Specifically, we design a multi-head auto-encoder structure,where each head network shares the same feature map and outputs the handcenter, pose and texture, respectively. Besides, we adopt a weakly-supervisedscheme to alleviate the burden of expensive 3D real-world data annotations. Tothis end, we propose a series of losses optimized by a stage-wise trainingscheme, where a multi-hand dataset with 2D annotations is generated based onthe publicly available single hand datasets. In order to further improve theaccuracy of the weakly supervised model, we adopt several feature consistencyconstraints in both single and multiple hand settings. Specifically, thekeypoints of each hand estimated from local features should be consistent withthe re-projected points predicted from global features. Extensive experimentson public benchmarks including FreiHAND, HO3D, InterHand2.6M and RHDdemonstrate that our method outperforms the state-of-the-art model-basedmethods in both weakly-supervised and fully-supervised manners. The code andmodels are available at {https://github.com/zijinxuxu/SMHR}.$$$$$http://arxiv.org/pdf/2204.08154v3
SImProv: Scalable Image Provenance Framework for Robust Content  Attribution$  We present SImProv - a scalable image provenance framework to match a queryimage back to a trusted database of originals and identify possiblemanipulations on the query. SImProv consists of three stages: a scalable searchstage for retrieving top-k most similar images; a re-ranking andnear-duplicated detection stage for identifying the original among thecandidates; and finally a manipulation detection and visualization stage forlocalizing regions within the query that may have been manipulated to differfrom the original. SImProv is robust to benign image transformations thatcommonly occur during online redistribution, such as artifacts due to noise andrecompression degradation, as well as out-of-place transformations due to imagepadding, warping, and changes in size and shape. Robustness towardsout-of-place transformations is achieved via the end-to-end training of adifferentiable warping module within the comparator architecture. Wedemonstrate effective retrieval and manipulation detection over a dataset of100 million images.$Keywords: Text classification, Machine learning, Deep learning$Author affiliation: Unknown$本文介绍了文本分类中机器学习和深度学习的应用方法。$文本分类，机器学习，深度学习$http://arxiv.org/pdf/2206.14245v2
Unified Object Detector for Different Modalities based on Vision  Transformers$  Traditional systems typically require different models for processingdifferent modalities, such as one model for RGB images and another for depthimages. Recent research has demonstrated that a single model for one modalitycan be adapted for another using cross-modality transfer learning. In thispaper, we extend this approach by combining cross/inter-modality transferlearning with a vision transformer to develop a unified detector that achievessuperior performance across diverse modalities. Our research envisions anapplication scenario for robotics, where the unified system seamlessly switchesbetween RGB cameras and depth sensors in varying lighting conditions.Importantly, the system requires no model architecture or weight updates toenable this smooth transition. Specifically, the system uses the depth sensorduring low-lighting conditions (night time) and both the RGB camera and depthsensor or RGB caemra only in well-lit environments. We evaluate our unifiedmodel on the SUN RGB-D dataset, and demonstrate that it achieves similar orbetter performance in terms of mAP50 compared to state-of-the-art methods inthe SUNRGBD16 category, and comparable performance in point cloud only mode. Wealso introduce a novel inter-modality mixing method that enables our model toachieve significantly better results than previous methods. We provide ourcode, including training/inference logs and model checkpoints, to facilitatereproducibility and further research.\\url{https://github.com/liketheflower/UODDM}$$$$$http://arxiv.org/pdf/2207.01071v2
InterTrack: Interaction Transformer for 3D Multi-Object Tracking$  3D multi-object tracking (MOT) is a key problem for autonomous vehicles,required to perform well-informed motion planning in dynamic environments.Particularly for densely occupied scenes, associating existing tracks to newdetections remains challenging as existing systems tend to omit criticalcontextual information. Our proposed solution, InterTrack, introduces theInteraction Transformer for 3D MOT to generate discriminative objectrepresentations for data association. We extract state and shape features foreach track and detection, and efficiently aggregate global information viaattention. We then perform a learned regression on each track/detection featurepair to estimate affinities, and use a robust two-stage data association andtrack management approach to produce the final tracks. We validate our approachon the nuScenes 3D MOT benchmark, where we observe significant improvements,particularly on classes with small physical sizes and clustered objects. As ofsubmission, InterTrack ranks 1st in overall AMOTA among methods usingCenterPoint detections.$3D multi-object tracking$University of Toronto Robotics Institute$提出了一种称为InterTrack的新方法，引入交互变换器来生成数据关联的判别式对象表示。通过建立全局关注机制进行有效的全局信息聚合，并采用两阶段数据关联和轨迹管理方法。在nuScenes 3D MOT基准测试中，InterTrack在各种类别中实现了显着的改进。$3D多目标跟踪（3D multi-object tracking）$http://arxiv.org/pdf/2208.08041v2
FS-BAN: Born-Again Networks for Domain Generalization Few-Shot  Classification$  Conventional Few-shot classification (FSC) aims to recognize samples fromnovel classes given limited labeled data. Recently, domain generalization FSC(DG-FSC) has been proposed with the goal to recognize novel class samples fromunseen domains. DG-FSC poses considerable challenges to many models due to thedomain shift between base classes (used in training) and novel classes(encountered in evaluation). In this work, we make two novel contributions totackle DG-FSC. Our first contribution is to propose Born-Again Network (BAN)episodic training and comprehensively investigate its effectiveness for DG-FSC.As a specific form of knowledge distillation, BAN has been shown to achieveimproved generalization in conventional supervised classification with aclosed-set setup. This improved generalization motivates us to study BAN forDG-FSC, and we show that BAN is promising to address the domain shiftencountered in DG-FSC. Building on the encouraging findings, our second (major)contribution is to propose Few-Shot BAN (FS-BAN), a novel BAN approach forDG-FSC. Our proposed FS-BAN includes novel multi-task learning objectives:Mutual Regularization, Mismatched Teacher, and Meta-Control Temperature, eachof these is specifically designed to overcome central and unique challenges inDG-FSC, namely overfitting and domain discrepancy. We analyze different designchoices of these techniques. We conduct comprehensive quantitative andqualitative analysis and evaluation over six datasets and three baselinemodels. The results suggest that our proposed FS-BAN consistently improves thegeneralization performance of baseline models and achieves state-of-the-artaccuracy for DG-FSC. Project Page: https://yunqing-me.github.io/Born-Again-FS/.$Few-shot classification, 领域泛化，再生网络，偏置训练，元学习$Singapore University of Technology and Design$本文提出Born-Again Networks (BANs)和Few-Shot BAN (FS-BAN)方法，用于解决领域泛化Few-shot分类任务中的域偏移问题，通过综合定量和定性分析，证明了其在多个数据集上均取得了最先进的性能。$Few-shot classiﬁcation, domain generalization, born-again network, episodic training, meta-learning$http://arxiv.org/pdf/2208.10930v4
SwinFIR: Revisiting the SwinIR with Fast Fourier Convolution and  Improved Training for Image Super-Resolution$  Transformer-based methods have achieved impressive image restorationperformance due to their capacities to model long-range dependency compared toCNN-based methods. However, advances like SwinIR adopts the window-based andlocal attention strategy to balance the performance and computational overhead,which restricts employing large receptive fields to capture global informationand establish long dependencies in the early layers. To further improve theefficiency of capturing global information, in this work, we propose SwinFIR toextend SwinIR by replacing Fast Fourier Convolution (FFC) components, whichhave the image-wide receptive field. We also revisit other advanced techniques,i.e, data augmentation, pre-training, and feature ensemble to improve theeffect of image reconstruction. And our feature ensemble method enables theperformance of the model to be considerably enhanced without increasing thetraining and testing time. We applied our algorithm on multiple popularlarge-scale benchmarks and achieved state-of-the-art performance comparing tothe existing methods. For example, our SwinFIR achieves the PSNR of 32.83 dB onManga109 dataset, which is 0.8 dB higher than the state-of-the-art SwinIRmethod.$$$$$http://arxiv.org/pdf/2208.11247v2
Correlation Information Bottleneck: Towards Adapting Pretrained  Multimodal Models for Robust Visual Question Answering$  Benefiting from large-scale pretrained vision language models (VLMs), theperformance of visual question answering (VQA) has approached human oracles.However, finetuning such models on limited data often suffers from overfittingand poor generalization issues, leading to a lack of model robustness. In thispaper, we aim to improve input robustness from an information bottleneckperspective when adapting pretrained VLMs to the downstream VQA task. Inputrobustness refers to the ability of models to defend against visual andlinguistic input variations, as well as shortcut learning involved in inputs.Generally, the representations obtained by pretrained VLMs inevitably containirrelevant and redundant information for a specific downstream task, resultingin statistically spurious correlations and insensitivity to input variations.To encourage representations to converge to a minimal sufficient statistic inmultimodal learning, we propose Correlation Information Bottleneck (CIB), whichseeks a tradeoff between compression and redundancy in representations byminimizing the mutual information (MI) between inputs and representations whilemaximizing the MI between outputs and representations. Moreover, we derive atight theoretical upper bound for the mutual information between multimodalinputs and representations, incorporating different internal correlations thatguide models to learn more robust representations and facilitate modalityalignment. Extensive experiments consistently demonstrate the effectiveness andsuperiority of the proposed CIB in terms of input robustness and accuracy.$$$$$http://arxiv.org/pdf/2209.06954v3
Understanding the Tricks of Deep Learning in Medical Image Segmentation:  Challenges and Future Directions$  Over the past few years, the rapid development of deep learning technologiesfor computer vision has significantly improved the performance of medical imagesegmentation (MedISeg). However, the diverse implementation strategies ofvarious models have led to an extremely complex MedISeg system, resulting in apotential problem of unfair result comparisons. In this paper, we collect aseries of MedISeg tricks for different model implementation phases (i.e.,pre-training model, data pre-processing, data augmentation, modelimplementation, model inference, and result post-processing), andexperimentally explore the effectiveness of these tricks on consistentbaselines. With the extensive experimental results on both the representative2D and 3D medical image datasets, we explicitly clarify the effect of thesetricks. Moreover, based on the surveyed tricks, we also open-sourced a strongMedISeg repository, where each component has the advantage of plug-and-play. Webelieve that this milestone work not only completes a comprehensive andcomplementary survey of the state-of-the-art MedISeg approaches, but alsooffers a practical guide for addressing the future medical image processingchallenges including but not limited to small dataset, class imbalancelearning, multi-modality learning, and domain adaptation. The code and trainingweights have been released at: https://github.com/hust-linyi/seg_trick.$$$$$http://arxiv.org/pdf/2209.10307v2
Make-A-Story: Visual Memory Conditioned Consistent Story Generation$  There has been a recent explosion of impressive generative models that canproduce high quality images (or videos) conditioned on text descriptions.However, all such approaches rely on conditional sentences that containunambiguous descriptions of scenes and main actors in them. Therefore employingsuch models for more complex task of story visualization, where naturallyreferences and co-references exist, and one requires to reason about when tomaintain consistency of actors and backgrounds across frames/scenes, and whennot to, based on story progression, remains a challenge. In this work, weaddress the aforementioned challenges and propose a novel autoregressivediffusion-based framework with a visual memory module that implicitly capturesthe actor and background context across the generated frames.Sentence-conditioned soft attention over the memories enables effectivereference resolution and learns to maintain scene and actor consistency whenneeded. To validate the effectiveness of our approach, we extend the MUGENdataset and introduce additional characters, backgrounds and referencing inmulti-sentence storylines. Our experiments for story generation on the MUGEN,the PororoSV and the FlintstonesSV dataset show that our method not onlyoutperforms prior state-of-the-art in generating frames with high visualquality, which are consistent with the story, but also models appropriatecorrespondences between the characters and the background.$$$$$http://arxiv.org/pdf/2211.13319v3
Pose-disentangled Contrastive Learning for Self-supervised Facial  Representation$  Self-supervised facial representation has recently attracted increasingattention due to its ability to perform face understanding without relying onlarge-scale annotated datasets heavily. However, analytically, currentcontrastive-based self-supervised learning (SSL) still performsunsatisfactorily for learning facial representation. More specifically,existing contrastive learning (CL) tends to learn pose-invariant features thatcannot depict the pose details of faces, compromising the learning performance.To conquer the above limitation of CL, we propose a novel Pose-disentangledContrastive Learning (PCL) method for general self-supervised facialrepresentation. Our PCL first devises a pose-disentangled decoder (PDD) with adelicately designed orthogonalizing regulation, which disentangles thepose-related features from the face-aware features; therefore, pose-related andother pose-unrelated facial information could be performed in individualsubnetworks and do not affect each other\'s training. Furthermore, we introducea pose-related contrastive learning scheme that learns pose-related informationbased on data augmentation of the same image, which would deliver moreeffective face-aware representation for various downstream tasks. We conductedlinear evaluation on four challenging downstream facial understanding tasks,ie, facial expression recognition, face recognition, AU detection and head poseestimation. Experimental results demonstrate that our method significantlyoutperforms state-of-the-art SSL methods. Code is available athttps://github.com/DreamMr/PCL}{https://github.com/DreamMr/PCL$$$$$http://arxiv.org/pdf/2211.13490v2
CLIPVG: Text-Guided Image Manipulation Using Differentiable Vector  Graphics$  Considerable progress has recently been made in leveraging CLIP (ContrastiveLanguage-Image Pre-Training) models for text-guided image manipulation.However, all existing works rely on additional generative models to ensure thequality of results, because CLIP alone cannot provide enough guidanceinformation for fine-scale pixel-level changes. In this paper, we introduceCLIPVG, a text-guided image manipulation framework using differentiable vectorgraphics, which is also the first CLIP-based general image manipulationframework that does not require any additional generative models. Wedemonstrate that CLIPVG can not only achieve state-of-art performance in bothsemantic correctness and synthesis quality, but also is flexible enough tosupport various applications far beyond the capability of all existing methods.$$$$$http://arxiv.org/pdf/2212.02122v2
Neural Volume Super-Resolution$  Neural volumetric representations have become a widely adopted model forradiance fields in 3D scenes. These representations are fully implicit orhybrid function approximators of the instantaneous volumetric radiance in ascene, which are typically learned from multi-view captures of the scene. Weinvestigate the new task of neural volume super-resolution - renderinghigh-resolution views corresponding to a scene captured at low resolution. Tothis end, we propose a neural super-resolution network that operates directlyon the volumetric representation of the scene. This approach allows us toexploit an advantage of operating in the volumetric domain, namely the abilityto guarantee consistent super-resolution across different viewing directions.To realize our method, we devise a novel 3D representation that hinges onmultiple 2D feature planes. This allows us to super-resolve the 3D scenerepresentation by applying 2D convolutional networks on the 2D feature planes.We validate the proposed method by super-resolving multi-view consistent viewson a diverse set of unseen 3D scenes, confirming qualitative and quantitativelyfavorable quality over existing approaches.$neural volume super-resolution$Princeton University, University of Siegen$本文提出了一种神经超分辨率网络，通过直接对场景的体积表示进行操作，实现从低分辨率图像中渲染高分辨率视图的任务。通过在体积域操作，能够保证不同观察方向下的一致性超分辨率，提出了一种多2D特征平面的新型三维表示方法。在多个3D场景上进行了实验验证，并与现有方法进行了比较。$神经体积超分辨率$http://arxiv.org/pdf/2212.04666v2
Diff-Font: Diffusion Model for Robust One-Shot Font Generation$  Font generation is a difficult and time-consuming task, especially in thoselanguages using ideograms that have complicated structures with a large numberof characters, such as Chinese. To solve this problem, few-shot font generationand even one-shot font generation have attracted a lot of attention. However,most existing font generation methods may still suffer from (i) largecross-font gap challenge; (ii) subtle cross-font variation problem; and (iii)incorrect generation of complicated characters. In this paper, we propose anovel one-shot font generation method based on a diffusion model, namedDiff-Font, which can be stably trained on large datasets. The proposed modelaims to generate the entire font library by giving only one sample as thereference. Specifically, a large stroke-wise dataset is constructed, and astroke-wise diffusion model is proposed to preserve the structure and thecompletion of each generated character. To our best knowledge, the proposedDiff-Font is the first work that developed diffusion models to handle the fontgeneration task. The well-trained Diff-Font is not only robust to font gap andfont variation, but also achieved promising performance on difficult charactergeneration. Compared to previous font generation methods, our model reachesstate-of-the-art performance both qualitatively and quantitatively.$$$$$http://arxiv.org/pdf/2212.05895v3
Mask-FPAN: Semi-Supervised Face Parsing in the Wild With De-Occlusion  and UV GAN$  Fine-grained semantic segmentation of a person\'s face and head, includingfacial parts and head components, has progressed a great deal in recent years.However, it remains a challenging task, whereby considering ambiguousocclusions and large pose variations are particularly difficult. To overcomethese difficulties, we propose a novel framework termed Mask-FPAN. It uses ade-occlusion module that learns to parse occluded faces in a semi-supervisedway. In particular, face landmark localization, face occlusionstimations, anddetected head poses are taken into account. A 3D morphable face model combinedwith the UV GAN improves the robustness of 2D face parsing. In addition, weintroduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for faceparing work. The proposed Mask-FPAN framework addresses the face parsingproblem in the wild and shows significant performance improvements with MIOUfrom 0.7353 to 0.9013 compared to the state-of-the-art on challenging facedatasets.$Face analysis, face parsing, face landmark, 3D face, generative adversarial network$Department of Computer Science, University of Copenhagen, Denmark；School of Information and Communication Engineering, University of Electronic Science and Technology of China$本文提出了一种新的框架 Mask-FPAN，旨在解决在野外场景中的人脸解析问题。该框架将遮挡人脸进行半监督学习，同时采用3D可形变人脸模型和UV GAN增强人脸分割的鲁棒性。作者还提出了两个新的数据集，FaceOccMask-HQ和CelebAMaskOcc-HQ，以帮助人脸解析工作。实验结果表明，作者提出的 Mask-FPAN 框架在大幅度超越现有最先进的人脸解析算法的同时，有效解决了野外场景中的人脸解析难题。$人脸分析、人脸解析、人脸地标、3D人脸、生成对抗网络$http://arxiv.org/pdf/2212.09098v4
A Large-scale Film Style Dataset for Learning Multi-frequency Driven  Film Enhancement$  Film, a classic image style, is culturally significant to the wholephotographic industry since it marks the birth of photography. However, filmphotography is time-consuming and expensive, necessitating a more efficientmethod for collecting film-style photographs. Numerous datasets that haveemerged in the field of image enhancement so far are not film-specific. Inorder to facilitate film-based image stylization research, we constructFilmSet, a large-scale and high-quality film style dataset. Our datasetincludes three different film types and more than 5000 in-the-wild highresolution images. Inspired by the features of FilmSet images, we propose anovel framework called FilmNet based on Laplacian Pyramid for stylizing imagesacross frequency bands and achieving film style outcomes. Experiments revealthat the performance of our model is superior than state-of-the-art techniques.The link of code and data is \\url{https://github.com/CXH-Research/FilmNet}.$Film-style image enhancement, Dataset, Image processing, Deep learning$University of Macau, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences$本研究构建了一个大规模高质量的电影风格数据集FilmSet，旨在帮助电影风格图像风格化研究。该数据集包含三种不同的电影类型和5000多张高分辨率图片。通过FilmSet图像的特征，提出了基于Laplacian金字塔的新颖框架FilmNet，用于在频率带间对图像进行风格化并实现电影风格的效果。实验证明，我们的模型的性能优于现有技术。$电影风格图像增强、数据集、图像处理、深度学习$http://arxiv.org/pdf/2301.08880v2
MDPose: Real-Time Multi-Person Pose Estimation via Mixture Density Model$  One of the major challenges in multi-person pose estimation is instance-awarekeypoint estimation. Previous methods address this problem by leveraging anoff-the-shelf detector, heuristic post-grouping process or explicit instanceidentification process, hindering further improvements in the inference speedwhich is an important factor for practical applications. From the statisticalpoint of view, those additional processes for identifying instances arenecessary to bypass learning the high-dimensional joint distribution of humankeypoints, which is a critical factor for another major challenge, theocclusion scenario. In this work, we propose a novel framework of single-stageinstance-aware pose estimation by modeling the joint distribution of humankeypoints with a mixture density model, termed as MDPose. Our MDPose estimatesthe distribution of human keypoints\' coordinates using a mixture density modelwith an instance-aware keypoint head consisting simply of 8 convolutionallayers. It is trained by minimizing the negative log-likelihood of the groundtruth keypoints. Also, we propose a simple yet effective training strategy,Random Keypoint Grouping (RKG), which significantly alleviates the underflowproblem leading to successful learning of relations between keypoints. OnOCHuman dataset, which consists of images with highly occluded people, ourMDPose achieves state-of-the-art performance by successfully learning thehigh-dimensional joint distribution of human keypoints. Furthermore, our MDPoseshows significant improvement in inference speed with a competitive accuracy onMS COCO, a widely-used human keypoint dataset, thanks to the proposed muchsimpler single-stage pipeline.$$$$$http://arxiv.org/pdf/2302.08751v2
StyleAdv: Meta Style Adversarial Training for Cross-Domain Few-Shot  Learning$  Cross-Domain Few-Shot Learning (CD-FSL) is a recently emerging task thattackles few-shot learning across different domains. It aims at transferringprior knowledge learned on the source dataset to novel target datasets. TheCD-FSL task is especially challenged by the huge domain gap between differentdatasets. Critically, such a domain gap actually comes from the changes ofvisual styles, and wave-SAN empirically shows that spanning the styledistribution of the source data helps alleviate this issue. However, wave-SANsimply swaps styles of two images. Such a vanilla operation makes the generatedstyles ``real\'\' and ``easy\'\', which still fall into the original set of thesource styles. Thus, inspired by vanilla adversarial learning, a novelmodel-agnostic meta Style Adversarial training (StyleAdv) method together witha novel style adversarial attack method is proposed for CD-FSL. Particularly,our style attack method synthesizes both ``virtual\'\' and ``hard\'\' adversarialstyles for model training. This is achieved by perturbing the original stylewith the signed style gradients. By continually attacking styles and forcingthe model to recognize these challenging adversarial styles, our model isgradually robust to the visual styles, thus boosting the generalization abilityfor novel target datasets. Besides the typical CNN-based backbone, we alsoemploy our StyleAdv method on large-scale pretrained vision transformer.Extensive experiments conducted on eight various target datasets show theeffectiveness of our method. Whether built upon ResNet or ViT, we achieve thenew state of the art for CD-FSL. Code is available athttps://github.com/lovelyqian/StyleAdv-CDFSL.$$$$$http://arxiv.org/pdf/2302.09309v2
One-class Damage Detector Using Deeper Fully-Convolutional Data  Descriptions for Civil Application$  Infrastructure managers must maintain high standards to ensure usersatisfaction during the lifecycle of infrastructures. Surveillance cameras andvisual inspections have enabled progress in automating the detection ofanomalous features and assessing the occurrence of deterioration. However,collecting damage data is typically time consuming and requires repeatedinspections. The one-class damage detection approach has an advantage in thatnormal images can be used to optimize model parameters. Additionally, visualevaluation of heatmaps enables us to understand localized anomalous features.The authors highlight damage vision applications utilized in the robustproperty and localized damage explainability. First, we propose a civil-purposeapplication for automating one-class damage detection reproducing a fullyconvolutional data description (FCDD) as a baseline model. We have obtainedaccurate and explainable results demonstrating experimental studies on concretedamage and steel corrosion in civil engineering. Additionally, to develop amore robust application, we applied our method to another outdoor domain thatcontains complex and noisy backgrounds using natural disaster datasetscollected using various devices. Furthermore, we propose a valuable solution ofdeeper FCDDs focusing on other powerful backbones to improve the performance ofdamage detection and implement ablation studies on disaster datasets. The keyresults indicate that the deeper FCDDs outperformed the baseline FCDD ondatasets representing natural disaster damage caused by hurricanes, typhoons,earthquakes, and four-event disasters.$Anomaly detection, Civil inspection, Damage explanation, Natural disasters, One-class classification$Research Institute for Infrastructure Paradigm Shift, Japan$本文介绍了一种基于深度学习的单类损伤检测器，在损伤数据缺乏的情况下，利用正常数据进行参数优化，并提供可解释的结果，同时进一步深入应用到自然灾害的数据中。该方法表现出了较好的性能。$基于深度全卷积数据描述的单类损伤检测器应用于土木工程，关键词包括异常检测、土木检测、损伤解释、自然灾害、单类分类。$http://arxiv.org/pdf/2303.01732v3
SILOP: An Automated Framework for Semantic Segmentation Using Image  Labels Based on Object Perimeters$  Achieving high-quality semantic segmentation predictions using onlyimage-level labels enables a new level of real-world applicability. Althoughstate-of-the-art networks deliver reliable predictions, the amount ofhandcrafted pixel-wise annotations to enable these results are not feasible inmany real-world applications. Hence, several works have already targeted thisbottleneck, using classifier-based networks like Class ActivationMaps~\\cite{CAM} (CAMs) as a base. Addressing CAM\'s weaknesses of fuzzy bordersand incomplete predictions, state-of-the-art approaches rely only on addingregulations to the classifier loss or using pixel-similarity-based refinementafter the fact. We propose a framework that introduces an additional moduleusing object perimeters for improved saliency. We define object perimeterinformation as the line separating the object and background. Our newPerimeterFit module will be applied to pre-refine the CAM predictions beforeusing the pixel-similarity-based network. In this way, our PerimeterFitincreases the quality of the CAM prediction while simultaneously improving thefalse negative rate. We investigated a wide range of state-of-the-artunsupervised semantic segmentation networks and edge detection techniques tocreate useful perimeter maps, which enable our framework to predict objectlocations with sharper perimeters. We achieved up to 1.5% improvement overframeworks without our PerimeterFit module. We conduct an exhaustive analysisto illustrate that SILOP enhances existing state-of-the-art frameworks forimage-level-based semantic segmentation. The framework is open-source andaccessible online at https://github.com/ErikOstrowski/SILOP.$$$$$http://arxiv.org/pdf/2303.07892v3
Mutual Information-Based Temporal Difference Learning for Human Pose  Estimation in Video$  Temporal modeling is crucial for multi-frame human pose estimation. Mostexisting methods directly employ optical flow or deformable convolution topredict full-spectrum motion fields, which might incur numerous irrelevantcues, such as a nearby person or background. Without further efforts toexcavate meaningful motion priors, their results are suboptimal, especially incomplicated spatiotemporal interactions. On the other hand, the temporaldifference has the ability to encode representative motion information whichcan potentially be valuable for pose estimation but has not been fullyexploited. In this paper, we present a novel multi-frame human pose estimationframework, which employs temporal differences across frames to model dynamiccontexts and engages mutual information objectively to facilitate useful motioninformation disentanglement. To be specific, we design a multi-stage TemporalDifference Encoder that performs incremental cascaded learning conditioned onmulti-stage feature difference sequences to derive informative motionrepresentation. We further propose a Representation Disentanglement module fromthe mutual information perspective, which can grasp discriminativetask-relevant motion signals by explicitly defining useful and noisyconstituents of the raw motion features and minimizing their mutualinformation. These place us to rank No.1 in the Crowd Pose Estimation inComplex Events Challenge on benchmark dataset HiEve, and achievestate-of-the-art performance on three benchmarks PoseTrack2017, PoseTrack2018,and PoseTrack21.$$$$$http://arxiv.org/pdf/2303.08475v2
Equiangular Basis Vectors$"  We propose Equiangular Basis Vectors (EBVs) for classification tasks. In deepneural networks, models usually end with a k-way fully connected layer withsoftmax to handle different classification tasks. The learning objective ofthese methods can be summarized as mapping the learned feature representationsto the samples\' label space. While in metric learning approaches, the mainobjective is to learn a transformation function that maps training data pointsfrom the original space to a new space where similar points are closer whiledissimilar points become farther apart. Different from previous methods, ourEBVs generate normalized vector embeddings as ""predefined classifiers"" whichare required to not only be with the equal status between each other, but alsobe as orthogonal as possible. By minimizing the spherical distance of theembedding of an input between its categorical EBV in training, the predictionscan be obtained by identifying the categorical EBV with the smallest distanceduring inference. Various experiments on the ImageNet-1K dataset and otherdownstream tasks demonstrate that our method outperforms the general fullyconnected classifier while it does not introduce huge additional computationcompared with classical metric learning methods. Our EBVs won the first placein the 2022 DIGIX Global AI Challenge, and our code is open-source andavailable at https://github.com/NJUST-VIPGroup/Equiangular-Basis-Vectors."$$$$$http://arxiv.org/pdf/2303.11637v2
Towards Efficient Task-Driven Model Reprogramming with Foundation Models$  Vision foundation models exhibit impressive power, benefiting from theextremely large model capacity and broad training data. However, in practice,downstream scenarios may only support a small model due to the limitedcomputational resources or efficiency considerations. Moreover, the data usedfor pretraining foundation models are usually invisible and very different fromthe target data of downstream tasks. This brings a critical challenge for thereal-world application of foundation models: one has to transfer the knowledgeof a foundation model to the downstream task that has a quite differentarchitecture with only downstream target data. Existing transfer learning orknowledge distillation methods depend on either the same model structure orfinetuning of the foundation model. Thus, naively introducing these methods canbe either infeasible or very inefficient. To address this, we propose aTask-Driven Model Reprogramming (TDMR) framework. Specifically, we reprogramthe foundation model to project the knowledge into a proxy space, whichalleviates the adverse effect of task mismatch and domain inconsistency. Then,we reprogram the target model via progressive distillation from the proxy spaceto efficiently learn the knowledge from the reprogrammed foundation model. TDMRis compatible with different pre-trained model types (CNN, transformer or theirmix) and limited target data, and promotes the wide applications of visionfoundation models to downstream tasks in a cost-effective manner. Extensiveexperiments on different downstream classification tasks and target modelstructures demonstrate the effectiveness of our methods with both CNNs andtransformer foundation models.$$$$$http://arxiv.org/pdf/2304.02263v2
hist2RNA: An efficient deep learning architecture to predict gene  expression from breast cancer histopathology images$  Gene expression can be used to subtype breast cancer with improved predictionof risk of recurrence and treatment responsiveness over that obtained usingroutine immunohistochemistry (IHC). However, in the clinic, molecular profilingis primarily used for ER+ breast cancer, which is costly, tissue destructive,requires specialized platforms and takes several weeks to obtain a result. Deeplearning algorithms can effectively extract morphological patterns in digitalhistopathology images to predict molecular phenotypes quickly andcost-effectively. We propose a new, computationally efficient approach calledhist2RNA inspired by bulk RNA-sequencing techniques to predict the expressionof 138 genes (incorporated from six commercially available molecular profilingtests), including luminal PAM50 subtype, from hematoxylin and eosin (H&amp;E)stained whole slide images (WSIs). The training phase involves the aggregationof extracted features for each patient from a pretrained model to predict geneexpression at the patient level using annotated H&amp;E images from The CancerGenome Atlas (TCGA, n=335). We demonstrate successful gene prediction on aheld-out test set (n = 160, corr = 0.82 across patients, corr = 0.29 acrossgenes) and perform exploratory analysis on an external tissue microarray (TMA)dataset (n = 498) with known IHC and survival information. Our model is able topredict gene expression and luminal PAM50 subtype (Luminal A versus Luminal B)on the TMA dataset with prognostic significance for overall survival inunivariate analysis (c-index = 0.56, hazard ratio = 2.16 (95% CI 1.12-3.06), p&lt; 5 x 10-3), and independent significance in multivariate analysisincorporating standard clinicopathological variables (c-index = 0.65, hazardratio = 1.85 (95% CI 1.30-2.68), p &lt; 5 x 10-3).$$$$$http://arxiv.org/pdf/2304.04507v4
Self-Supervised Learning from Non-Object Centric Images with a Geometric  Transformation Sensitive Architecture$"  Most invariance-based self-supervised methods rely on single object-centricimages (e.g., ImageNet images) for pretraining, learning invariantrepresentations from geometric transformations. However, when images are notobject-centric, the semantics of the image can be significantly altered due tocropping. Furthermore, as the model becomes insensitive to geometrictransformations, it may struggle to capture location information. For thisreason, we propose a Geometric Transformation Sensitive Architecture designedto learn features that are sensitive to geometric transformations, specificallyfocusing on four-fold rotation, random crop, and multi-crop. Our methodencourages the student to be sensitive by using targets that are sensitive tothose transforms via pooling and rotating of the teacher feature map andpredicting rotation. Additionally, as training insensitively to multi-cropencourages local-to-global correspondence, the model can capture long-termdependencies. We use patch correspondence loss to encourage correspondencebetween patches with similar features, instead of enforcing correspondencebetween views of the image. This approach allows us to capture long-termdependencies in a more appropriate way. Our approach demonstrates improvedperformance when using non-object-centric images as pretraining data comparedto other methods that learn geometric transformation-insensitiverepresentations. We surpass the DINO baseline in tasks including imageclassification, semantic segmentation, detection, and instance segmentationwith improvements of 4.9 $Top-1 Acc$, 3.3 $mIoU$, 3.4 $AP^b$, and 2.7 $AP^m$.Code and pretrained models are publicly available at:https://github.com/bok3948/GTSA"$$$$$http://arxiv.org/pdf/2304.08014v3
Learning Situation Hyper-Graphs for Video Question Answering$  Answering questions about complex situations in videos requires not onlycapturing the presence of actors, objects, and their relations but also theevolution of these relationships over time. A situation hyper-graph is arepresentation that describes situations as scene sub-graphs for video framesand hyper-edges for connected sub-graphs and has been proposed to capture allsuch information in a compact structured form. In this work, we propose anarchitecture for Video Question Answering (VQA) that enables answeringquestions related to video content by predicting situation hyper-graphs, coinedSituation Hyper-Graph based Video Question Answering (SHG-VQA). To this end, wetrain a situation hyper-graph decoder to implicitly identify graphrepresentations with actions and object/human-object relationships from theinput video clip. and to use cross-attention between the predicted situationhyper-graphs and the question embedding to predict the correct answer. Theproposed method is trained in an end-to-end manner and optimized by a VQA losswith the cross-entropy function and a Hungarian matching loss for the situationgraph prediction. The effectiveness of the proposed architecture is extensivelyevaluated on two challenging benchmarks: AGQA and STAR. Our results show thatlearning the underlying situation hyper-graphs helps the system tosignificantly improve its performance for novel challenges of videoquestion-answering tasks.$$$$$http://arxiv.org/pdf/2304.08682v2
Improving Post-Training Quantization on Object Detection with Task  Loss-Guided Lp Metric$  Efficient inference for object detection networks is a major challenge onedge devices. Post-Training Quantization (PTQ), which transforms afull-precision model into low bit-width directly, is an effective andconvenient approach to reduce model inference complexity. But it suffers severeaccuracy drop when applied to complex tasks such as object detection. PTQoptimizes the quantization parameters by different metrics to minimize theperturbation of quantization. The p-norm distance of feature maps before andafter quantization, Lp, is widely used as the metric to evaluate perturbation.For the specialty of object detection network, we observe that the parameter pin Lp metric will significantly influence its quantization performance. Weindicate that using a fixed hyper-parameter p does not achieve optimalquantization performance. To mitigate this problem, we propose a framework,DetPTQ, to assign different p values for quantizing different layers using anObject Detection Output Loss (ODOL), which represents the task loss of objectdetection. DetPTQ employs the ODOL-based adaptive Lp metric to select theoptimal quantization parameters. Experiments show that our DetPTQ outperformsthe state-of-the-art PTQ methods by a significant margin on both 2D and 3Dobject detectors. For example, we achieve31.1/31.7(quantization/full-precision) mAP on RetinaNet-ResNet18 with 4-bitweight and 4-bit activation.$$$$$http://arxiv.org/pdf/2304.09785v3
Universal Domain Adaptation via Compressive Attention Matching$  Universal domain adaptation (UniDA) aims to transfer knowledge from thesource domain to the target domain without any prior knowledge about the labelset. The challenge lies in how to determine whether the target samples belongto common categories. The mainstream methods make judgments based on the samplefeatures, which overemphasizes global information while ignoring the mostcrucial local objects in the image, resulting in limited accuracy. To addressthis issue, we propose a Universal Attention Matching (UniAM) framework byexploiting the self-attention mechanism in vision transformer to capture thecrucial object information. The proposed framework introduces a novelCompressive Attention Matching (CAM) approach to explore the core informationby compressively representing attentions. Furthermore, CAM incorporates aresidual-based measurement to determine the sample commonness. By utilizing themeasurement, UniAM achieves domain-wise and category-wise Common FeatureAlignment (CFA) and Target Class Separation (TCS). Notably, UniAM is the firstmethod utilizing the attention in vision transformer directly to performclassification tasks. Extensive experiments show that UniAM outperforms thecurrent state-of-the-art methods on various benchmark datasets.$$$$$http://arxiv.org/pdf/2304.11862v2
AVATAR: Adversarial self-superVised domain Adaptation network for TARget  domain$  This paper presents an unsupervised domain adaptation (UDA) method forpredicting unlabeled target domain data, specific to complex UDA tasks wherethe domain gap is significant. Mainstream UDA models aim to learn from bothdomains and improve target discrimination by utilizing labeled source domaindata. However, the performance boost may be limited when the discrepancybetween the source and target domains is large or the target domain containsoutliers. To explicitly address this issue, we propose the Adversarialself-superVised domain Adaptation network for the TARget domain (AVATAR)algorithm. It outperforms state-of-the-art UDA models by concurrently reducingdomain discrepancy while enhancing discrimination through domain adversariallearning, self-supervised learning, and sample selection strategy for thetarget domain, all guided by deep clustering. Our proposed model significantlyoutperforms state-of-the-art methods on three UDA benchmarks, and extensiveablation studies and experiments demonstrate the effectiveness of our approachfor addressing complex UDA tasks.$Deep learning, Unsupervised domain adaptation, Deep clustering, Adversarial learning, Self-supervised learning$aThe State University of New York at Binghamton, Department of Systems Science and Industrial Engineering, Engineering Bldg, L2, 13905, NY, United States bYonsei University, Department of Industrial Engineering, 50 Yonsei-ro, Seodaemun-gu, 03722, Seoul, Republic of Korea$本研究介绍了一种无监督的自适应领域适应方法，针对复杂的自适应领域适应任务，其中领域差距显着。通过利用有标签的源域数据进行学习来提高目标区分能力是主流的自适应领域适应模型的目标。然而，当源域和目标域之间的差异很大或者目标域包含异常值时，性能提升可能会受到限制。为了显式地解决这个问题，我们提出了适用于TAR get领域（A V ATAR）算法的对抗自自监督领域适应网络。它通过深度聚类指导全部指导域对抗学习、自监督学习和目标域样本选择策略，同时降低域差异和增强区分能力。我们的模型在三个UDA基准测试中显著优于现有的最先进方法，大量剖析研究和实验证明我们的方法对于解决复杂的UDA任务是有效的。$深度学习、自适应领域适应、深度聚类、对抗式学习、自监督学习$http://arxiv.org/pdf/2305.00082v2
Discriminative Co-Saliency and Background Mining Transformer for  Co-Salient Object Detection$  Most previous co-salient object detection works mainly focus on extractingco-salient cues via mining the consistency relations across images whileignoring explicit exploration of background regions. In this paper, we proposea Discriminative co-saliency and background Mining Transformer framework (DMT)based on several economical multi-grained correlation modules to explicitlymine both co-saliency and background information and effectively model theirdiscrimination. Specifically, we first propose a region-to-region correlationmodule for introducing inter-image relations to pixel-wise segmentationfeatures while maintaining computational efficiency. Then, we use two types ofpre-defined tokens to mine co-saliency and background information via ourproposed contrast-induced pixel-to-token correlation and co-saliencytoken-to-token correlation modules. We also design a token-guided featurerefinement module to enhance the discriminability of the segmentation featuresunder the guidance of the learned tokens. We perform iterative mutual promotionfor the segmentation feature extraction and token construction. Experimentalresults on three benchmark datasets demonstrate the effectiveness of ourproposed method. The source code is available at:https://github.com/dragonlee258079/DMT.$$$$$http://arxiv.org/pdf/2305.00514v2
Caption Anything: Interactive Image Description with Diverse Multimodal  Controls$"  Controllable image captioning is an emerging multimodal topic that aims todescribe the image with natural language following human purpose,$\\textit{e.g.}$, looking at the specified regions or telling in a particulartext style. State-of-the-art methods are trained on annotated pairs of inputcontrols and output captions. However, the scarcity of such well-annotatedmultimodal data largely limits their usability and scalability for interactiveAI systems. Leveraging unimodal instruction-following foundation models is apromising alternative that benefits from broader sources of data. In thispaper, we present Caption AnyThing (CAT), a foundation model augmented imagecaptioning framework supporting a wide range of multimodel controls: 1) visualcontrols, including points, boxes, and trajectories; 2) language controls, suchas sentiment, length, language, and factuality. Powered by Segment AnythingModel (SAM) and ChatGPT, we unify the visual and language prompts into amodularized framework, enabling the flexible combination between differentcontrols. Extensive case studies demonstrate the user intention alignmentcapabilities of our framework, shedding light on effective user interactionmodeling in vision-language applications. Our code is publicly available athttps://github.com/ttengwang/Caption-Anything."$$$$$http://arxiv.org/pdf/2305.02677v2
Incremental 3D Semantic Scene Graph Prediction from RGB Sequences$  3D semantic scene graphs are a powerful holistic representation as theydescribe the individual objects and depict the relation between them. They arecompact high-level graphs that enable many tasks requiring scene reasoning. Inreal-world settings, existing 3D estimation methods produce robust predictionsthat mostly rely on dense inputs. In this work, we propose a real-timeframework that incrementally builds a consistent 3D semantic scene graph of ascene given an RGB image sequence. Our method consists of a novel incrementalentity estimation pipeline and a scene graph prediction network. The proposedpipeline simultaneously reconstructs a sparse point map and fuses entityestimation from the input images. The proposed network estimates 3D semanticscene graphs with iterative message passing using multi-view and geometricfeatures extracted from the scene entities. Extensive experiments on the 3RScandataset show the effectiveness of the proposed method in this challenging task,outperforming state-of-the-art approaches.$$$$$http://arxiv.org/pdf/2305.02743v2
Towards End-to-End Semi-Supervised Table Detection with Deformable  Transformer$  Table detection is the task of classifying and localizing table objectswithin document images. With the recent development in deep learning methods,we observe remarkable success in table detection. However, a significant amountof labeled data is required to train these models effectively. Manysemi-supervised approaches are introduced to mitigate the need for asubstantial amount of label data. These approaches use CNN-based detectors thatrely on anchor proposals and post-processing stages such as NMS. To tacklethese limitations, this paper presents a novel end-to-end semi-supervised tabledetection method that employs the deformable transformer for detecting tableobjects. We evaluate our semi-supervised method on PubLayNet, DocBank, ICADR-19and TableBank datasets, and it achieves superior performance compared toprevious methods. It outperforms the fully supervised method (Deformabletransformer) by +3.4 points on 10\\% labels of TableBank-both dataset and theprevious CNN-based semi-supervised approach (Soft Teacher) by +1.8 points on10\\% labels of PubLayNet dataset. We hope this work opens new possibilitiestowards semi-supervised and unsupervised table detection methods.$Semi-Supervised Learning、Deformable Transformer、Table Analysis、Table Detection$1Department of Computer Science, Technical University of Kaiserslautern, 67663 Kaiserslautern, Germany；2Mindgarage, Technical University of Kaiserslautern, 67663 Kaiserslautern, Germany；3Department of Computer Science, Luleå University of Technology, 971 87 Luleå, Sweden；4German Research Institute for Artiﬁcial Intelligence (DFKI), 67663 Kaiserslautern, Germany$本文提出了一种新的半监督方法，使用变形Transformer来检测表格对象，该方法在PubLayNet，DocBank，ICADR-19和TableBank数据集上进行了评估，并达到了比以前的方法更好的性能。我们希望这项工作能够为半监督和无监督的表格检测方法开辟新的可能性。$半监督学习、变形Transformer、表格分析、表格检测$http://arxiv.org/pdf/2305.02769v2
APR: Online Distant Point Cloud Registration Through Aggregated Point  Cloud Reconstruction$  For many driving safety applications, it is of great importance to accuratelyregister LiDAR point clouds generated on distant moving vehicles. However, suchpoint clouds have extremely different point density and sensor perspective onthe same object, making registration on such point clouds very hard. In thispaper, we propose a novel feature extraction framework, called APR, for onlinedistant point cloud registration. Specifically, APR leverages an autoencoderdesign, where the autoencoder reconstructs a denser aggregated point cloud withseveral frames instead of the original single input point cloud. Our designforces the encoder to extract features with rich local geometry informationbased on one single input point cloud. Such features are then used for onlinedistant point cloud registration. We conduct extensive experiments againststate-of-the-art (SOTA) feature extractors on KITTI and nuScenes datasets.Results show that APR outperforms all other extractors by a large margin,increasing average registration recall of SOTA extractors by 7.1% on LoKITTIand 4.6% on LoNuScenes. Code is available at https://github.com/liuQuan98/APR.$$$$$http://arxiv.org/pdf/2305.02893v2
DocDiff: Document Enhancement via Residual Diffusion Models$  Removing degradation from document images not only improves their visualquality and readability, but also enhances the performance of numerousautomated document analysis and recognition tasks. However, existingregression-based methods optimized for pixel-level distortion reduction tend tosuffer from significant loss of high-frequency information, leading todistorted and blurred text edges. To compensate for this major deficiency, wepropose DocDiff, the first diffusion-based framework specifically designed fordiverse challenging document enhancement problems, including documentdeblurring, denoising, and removal of watermarks and seals. DocDiff consists oftwo modules: the Coarse Predictor (CP), which is responsible for recovering theprimary low-frequency content, and the High-Frequency Residual Refinement (HRR)module, which adopts the diffusion models to predict the residual(high-frequency information, including text edges), between the ground-truthand the CP-predicted image. DocDiff is a compact and computationally efficientmodel that benefits from a well-designed network architecture, an optimizedtraining loss objective, and a deterministic sampling process with short timesteps. Extensive experiments demonstrate that DocDiff achieves state-of-the-art(SOTA) performance on multiple benchmark datasets, and can significantlyenhance the readability and recognizability of degraded document images.Furthermore, our proposed HRR module in pre-trained DocDiff is plug-and-playand ready-to-use, with only 4.17M parameters. It greatly sharpens the textedges generated by SOTA deblurring methods without additional joint training.Available codes: https://github.com/Royalvice/DocDiff$$$$$http://arxiv.org/pdf/2305.03892v1
Prompt What You Need: Enhancing Segmentation in Rainy Scenes with  Anchor-based Prompting$  Semantic segmentation in rainy scenes is a challenging task due to thecomplex environment, class distribution imbalance, and limited annotated data.To address these challenges, we propose a novel framework that utilizessemi-supervised learning and pre-trained segmentation foundation model toachieve superior performance. Specifically, our framework leverages thesemi-supervised model as the basis for generating raw semantic segmentationresults, while also serving as a guiding force to prompt pre-trained foundationmodel to compensate for knowledge gaps with entropy-based anchors. In addition,to minimize the impact of irrelevant segmentation masks generated by thepre-trained foundation model, we also propose a mask filtering and fusionmechanism that optimizes raw semantic segmentation results based on theprinciple of minimum risk. The proposed framework achieves superiorsegmentation performance on the Rainy WCity dataset and is awarded the firstprize in the sub-track of STRAIN in ICME 2023 Grand Challenges.$$$$$http://arxiv.org/pdf/2305.03902v1
Listen to Look into the Future: Audio-Visual Egocentric Gaze  Anticipation$  Egocentric gaze anticipation serves as a key building block for the emergingcapability of Augmented Reality. Notably, gaze behavior is driven by bothvisual cues and audio signals during daily activities. Motivated by thisobservation, we introduce the first model that leverages both the video andaudio modalities for egocentric gaze anticipation. Specifically, we propose aContrastive Spatial-Temporal Separable (CSTS) fusion approach that adopts twomodules to separately capture audio-visual correlations in spatial and temporaldimensions, and applies a contrastive loss on the re-weighted audio-visualfeatures from fusion modules for representation learning. We conduct extensiveablation studies and thorough analysis using two egocentric video datasets:Ego4D and Aria, to validate our model design. We also demonstrate improvementsover prior state-of-the-art methods. Moreover, we provide visualizations toshow the gaze anticipation results and provide additional insights intoaudio-visual representation learning.$$$$$http://arxiv.org/pdf/2305.03907v1
DBAT: Dynamic Backward Attention Transformer for Material Segmentation  with Cross-Resolution Patches$  The objective of dense material segmentation is to identify the materialcategories for every image pixel. Recent studies adopt image patches to extractmaterial features. Although the trained networks can improve the segmentationperformance, their methods choose a fixed patch resolution which fails to takeinto account the variation in pixel area covered by each material. In thispaper, we propose the Dynamic Backward Attention Transformer (DBAT) toaggregate cross-resolution features. The DBAT takes cropped image patches asinput and gradually increases the patch resolution by merging adjacent patchesat each transformer stage, instead of fixing the patch resolution duringtraining. We explicitly gather the intermediate features extracted fromcross-resolution patches and merge them dynamically with predicted attentionmasks. Experiments show that our DBAT achieves an accuracy of 86.85%, which isthe best performance among state-of-the-art real-time models. Like othersuccessful deep learning solutions with complex architectures, the DBAT alsosuffers from lack of interpretability. To address this problem, this paperexamines the properties that the DBAT makes use of. By analysing thecross-resolution features and the attention weights, this paper interprets howthe DBAT learns from image patches. We further align features to semanticlabels, performing network dissection, to infer that the proposed model canextract material-related features better than other methods. We show that theDBAT model is more robust to network initialisation, and yields fewer variablepredictions compared to other models. The project code is available athttps://github.com/heng-yuwen/Dynamic-Backward-Attention-Transformer.$$$$$http://arxiv.org/pdf/2305.03919v1
Annotation-efficient learning for OCT segmentation$  Deep learning has been successfully applied to OCT segmentation. However, fordata from different manufacturers and imaging protocols, and for differentregions of interest (ROIs), it requires laborious and time-consuming dataannotation and training, which is undesirable in many scenarios, such assurgical navigation and multi-center clinical trials. Here we propose anannotation-efficient learning method for OCT segmentation that couldsignificantly reduce annotation costs. Leveraging self-supervised generativelearning, we train a Transformer-based model to learn the OCT imagery. Then weconnect the trained Transformer-based encoder to a CNN-based decoder, to learnthe dense pixel-wise prediction in OCT segmentation. These training phases useopen-access data and thus incur no annotation costs, and the pre-trained modelcan be adapted to different data and ROIs without re-training. Based on thegreedy approximation for the k-center problem, we also introduce an algorithmfor the selective annotation of the target data. We verified our method onpublicly-available and private OCT datasets. Compared to the widely-used U-Netmodel with 100% training data, our method only requires ~10% of the data forachieving the same segmentation accuracy, and it speeds the training up to ~3.5times. Furthermore, our proposed method outperforms other potential strategiesthat could improve annotation efficiency. We think this emphasis on learningefficiency may help improve the intelligence and application penetration ofOCT-based technologies. Our code and pre-trained model are publicly availableathttps://github.com/SJTU-Intelligent-Optics-Lab/Annotation-efficient-learning-for-OCT-segmentation.$$$$$http://arxiv.org/pdf/2305.03936v1
Structural and Statistical Texture Knowledge Distillation for Semantic  Segmentation$  Existing knowledge distillation works for semantic segmentation mainly focuson transferring high-level contextual knowledge from teacher to student.However, low-level texture knowledge is also of vital importance forcharacterizing the local structural pattern and global statistical property,such as boundary, smoothness, regularity and color contrast, which may not bewell addressed by high-level deep features. In this paper, we are intended totake full advantage of both structural and statistical texture knowledge andpropose a novel Structural and Statistical Texture Knowledge Distillation(SSTKD) framework for semantic segmentation. Specifically, for structuraltexture knowledge, we introduce a Contourlet Decomposition Module (CDM) thatdecomposes low-level features with iterative Laplacian pyramid and directionalfilter bank to mine the structural texture knowledge. For statisticalknowledge, we propose a Denoised Texture Intensity Equalization Module (DTIEM)to adaptively extract and enhance statistical texture knowledge throughheuristics iterative quantization and denoised operation. Finally, eachknowledge learning is supervised by an individual loss function, forcing thestudent network to mimic the teacher better from a broader perspective.Experiments show that the proposed method achieves state-of-the-art performanceon Cityscapes, Pascal VOC 2012 and ADE20K datasets.$$$语义分割、纹理知识提取、深度神经网络。 $1.阿里云；2.上海交通大学计算机科学与工程系。 $http://arxiv.org/pdf/2305.03944v1
Feature Chirality in Deep Learning Models$  As deep learning applications extensively increase by leaps and bounds, theirinterpretability has become increasingly prominent. As a universal property,chirality exists widely in nature, and applying it to the explanatory researchof deep learning may be helpful to some extent. Inspired by a recent study thatused CNN (convolutional neural network), which applied visual chirality, todistinguish whether an image is flipped or not. In this paper, we study featurechirality innovatively, which shows how the statistics of deep learning models\'feature data are changed by training. We rethink the feature-level chiralityproperty, propose the feature chirality, and give the measure. Our analysis offeature chirality on AlexNet, VGG, and ResNet reveals similar but surprisingresults, including the prevalence of feature chirality in these models, theinitialization methods of the models do not affect feature chirality. Our workshows that feature chirality implies model evaluation, interpretability of themodel, and model parameters optimization.$$$$$http://arxiv.org/pdf/2305.03966v1
Multi-object Video Generation from Single Frame Layouts$  In this paper, we study video synthesis with emphasis on simplifying thegeneration conditions. Most existing video synthesis models or datasets aredesigned to address complex motions of a single object, lacking the ability ofcomprehensively understanding the spatio-temporal relationships among multipleobjects. Besides, current methods are usually conditioned on intricateannotations (e.g. video segmentations) to generate new videos, beingfundamentally less practical. These motivate us to generate multi-object videosconditioning exclusively on object layouts from a single frame. To solve abovechallenges and inspired by recent research on image generation from layouts, wehave proposed a novel video generative framework capable of synthesizing globalscenes with local objects, via implicit neural representations and layoutmotion self-inference. Our framework is a non-trivial adaptation from imagegeneration methods, and is new to this field. In addition, our model has beenevaluated on two widely-used video recognition benchmarks, demonstratingeffectiveness compared to the baseline model.$Video Synthesis, Multi-object Scene, Generative Modeling$Sun Yat-sen University$本文提出了一种新的视频生成框架，可从单帧布局中生成全局场景与局部对象的动态视频，通过隐式神经表征和布局运动自我推断。模型在两个广泛使用的视频识别基准上进行评估，证明了基线模型的有效性。$视频综合，多对象场景，生成建模$http://arxiv.org/pdf/2305.03983v1
LEO: Generative Latent Image Animator for Human Video Synthesis$  Spatio-temporal coherency is a major challenge in synthesizing high qualityvideos, particularly in synthesizing human videos that contain rich global andlocal deformations. To resolve this challenge, previous approaches haveresorted to different features in the generation process aimed at representingappearance and motion. However, in the absence of strict mechanisms toguarantee such disentanglement, a separation of motion from appearance hasremained challenging, resulting in spatial distortions and temporal jitteringthat break the spatio-temporal coherency. Motivated by this, we here proposeLEO, a novel framework for human video synthesis, placing emphasis onspatio-temporal coherency. Our key idea is to represent motion as a sequence offlow maps in the generation process, which inherently isolate motion fromappearance. We implement this idea via a flow-based image animator and a LatentMotion Diffusion Model (LMDM). The former bridges a space of motion codes withthe space of flow maps, and synthesizes video frames in a warp-and-inpaintmanner. LMDM learns to capture motion prior in the training data bysynthesizing sequences of motion codes. Extensive quantitative and qualitativeanalysis suggests that LEO significantly improves coherent synthesis of humanvideos over previous methods on the datasets TaichiHD, FaceForensics andCelebV-HQ. In addition, the effective disentanglement of appearance and motionin LEO allows for two additional tasks, namely infinite-length human videosynthesis, as well as content-preserving video editing.$Video synthesis (英文)$Author Affiliations: 1Shanghai Artiﬁcial Intelligence Laboratory, 2Monash University, 3Inria, Universit ´e Cˆote d’Azur$本文介绍了一种基于生成式模型的方法，可以用于合成高质量的人类视频，包括无条件视频生成、基于单个图像的有条件生成以及从初始图像进行视频编辑。 (中文)$视频合成（中文）$http://arxiv.org/pdf/2305.03989v1
Weighted Point Cloud Normal Estimation$  Existing normal estimation methods for point clouds are often less robust tosevere noise and complex geometric structures. Also, they usually ignore thecontributions of different neighbouring points during normal estimation, whichleads to less accurate results. In this paper, we introduce a weighted normalestimation method for 3D point cloud data. We innovate in two key points: 1) wedevelop a novel weighted normal regression technique that predicts point-wiseweights from local point patches and use them for robust, feature-preservingnormal regression; 2) we propose to conduct contrastive learning between pointpatches and the corresponding ground-truth normals of the patches\' centralpoints as a pre-training process to facilitate normal regression. Comprehensiveexperiments demonstrate that our method can robustly handle noisy and complexpoint clouds, achieving state-of-the-art performance on both synthetic andreal-world datasets.$$$$$http://arxiv.org/pdf/2305.04007v1
Exploring One-shot Semi-supervised Federated Learning with A Pre-trained  Diffusion Model$  Federated learning is a privacy-preserving collaborative learning approach.Recently, some studies have proposed the semi-supervised federated learningsetting to handle the commonly seen real-world scenarios with labeled data onthe server and unlabeled data on the clients. However, existing methods stillface challenges such as high communication costs, training pressure on theclient devices, and distribution differences among the server and the clients.In this paper, we introduce the powerful pre-trained diffusion models intofederated learning and propose FedDISC, a Federated Diffusion InspiredSemi-supervised Co-training method, to address these challenges. Specifically,we first extract prototypes from the labeled data on the server and send themto the clients. The clients then use these prototypes to predict pseudo-labelsof the local data, and compute the cluster centroids and domain-specificfeatures to represent their personalized distributions. After adding noise, theclients send these features and their corresponding pseudo-labels back to theserver, which uses a pre-trained diffusion model to conditionally generatepseudo-samples complying with the client distributions and train an aggregatedmodel on them. Our method does not require local training and only involvesforward inference on the clients. Our extensive experiments on DomainNet,Openimage, and NICO++ demonstrate that the proposed FedDISC method effectivelyaddresses the one-shot semi-supervised problem on Non-IID clients andoutperforms the compared SOTA methods. We also demonstrate throughvisualization that it is of neglectable possibility for FedDISC to leakprivacy-sensitive information of the clients.$$$$$http://arxiv.org/pdf/2305.04063v1
PointCMP: Contrastive Mask Prediction for Self-supervised Learning on  Point Cloud Videos$  Self-supervised learning can extract representations of good quality fromsolely unlabeled data, which is appealing for point cloud videos due to theirhigh labelling cost. In this paper, we propose a contrastive mask prediction(PointCMP) framework for self-supervised learning on point cloud videos.Specifically, our PointCMP employs a two-branch structure to achievesimultaneous learning of both local and global spatio-temporal information. Ontop of this two-branch structure, a mutual similarity based augmentation moduleis developed to synthesize hard samples at the feature level. By maskingdominant tokens and erasing principal channels, we generate hard samples tofacilitate learning representations with better discrimination andgeneralization performance. Extensive experiments show that our PointCMPachieves the state-of-the-art performance on benchmark datasets and outperformsexisting full-supervised counterparts. Transfer learning results demonstratethe superiority of the learned representations across different datasets andtasks.$$$$$http://arxiv.org/pdf/2305.04075v1
Transform-Equivariant Consistency Learning for Temporal Sentence  Grounding$  This paper addresses the temporal sentence grounding (TSG). Although existingmethods have made decent achievements in this task, they not only severely relyon abundant video-query paired data for training, but also easily fail into thedataset distribution bias. To alleviate these limitations, we introduce a novelEquivariant Consistency Regulation Learning (ECRL) framework to learn morediscriminative query-related frame-wise representations for each video, in aself-supervised manner. Our motivation comes from that the temporal boundary ofthe query-guided activity should be consistently predicted under variousvideo-level transformations. Concretely, we first design a series ofspatio-temporal augmentations on both foreground and background video segmentsto generate a set of synthetic video samples. In particular, we devise aself-refine module to enhance the completeness and smoothness of the augmentedvideo. Then, we present a novel self-supervised consistency loss (SSCL) appliedon the original and augmented videos to capture their invariant query-relatedsemantic by minimizing the KL-divergence between the sequence similarity of twovideos and a prior Gaussian distribution of timestamp distance. At last, ashared grounding head is introduced to predict the transform-equivariantquery-guided segment boundaries for both the original and augmented videos.Extensive experiments on three challenging datasets (ActivityNet, TACoS, andCharades-STA) demonstrate both effectiveness and efficiency of our proposedECRL framework.$$$$$http://arxiv.org/pdf/2305.04123v1
Context-Aware Chart Element Detection$  As a prerequisite of chart data extraction, the accurate detection of chartbasic elements is essential and mandatory. In contrast to object detection inthe general image domain, chart element detection relies heavily on contextinformation as charts are highly structured data visualization formats. Toaddress this, we propose a novel method CACHED, which stands for Context-AwareChart Element Detection, by integrating a local-global context fusion moduleconsisting of visual context enhancement and positional context encoding withthe Cascade R-CNN framework. To improve the generalization of our method forbroader applicability, we refine the existing chart element categorization andstandardized 18 classes for chart basic elements, excluding plot elements. OurCACHED method, with the updated category of chart elements, achievesstate-of-the-art performance in our experiments, underscoring the importance ofcontext in chart element detection. Extending our method to the bar plotdetection task, we obtain the best result on the PMC test dataset.$$$$$http://arxiv.org/pdf/2305.04151v1
PhysBench: A Benchmark Framework for Remote Physiological Sensing with  New Dataset and Baseline$  In recent years, due to the widespread use of internet videos, physiologicalremote sensing has gained more and more attention in the fields of affectivecomputing and telemedicine. Recovering physiological signals from facial videosis a challenging task that involves a series of preprocessing, imagealgorithms, and post-processing to finally restore waveforms. We propose acomplete and efficient end-to-end training and testing framework that providesfair comparisons for different algorithms through unified preprocessing andpost-processing. In addition, we introduce a highly synchronized losslessformat dataset along with a lightweight algorithm. The dataset contains over 32hours (3.53M frames) of video from 58 subjects; by training on our collecteddataset both our proposed algorithm as well as existing ones can achieveimprovements.$本文提出了一种完整高效的端到端训练和测试框架，为不同算法提供公平比较，并介绍了一个高度同步的无损格式数据集和轻量级算法，为远程心理生理学信号还原提供了基础。$生理远程感知，视频压缩，数据集，基准，算法比较$$$http://arxiv.org/pdf/2305.04161v1
YOLOCS: Object Detection based on Dense Channel Compression for Feature  Spatial Solidification$  In this study, we examine the associations between channel features andconvolutional kernels during the processes of feature purification and gradientbackpropagation, with a focus on the forward and backward propagation withinthe network. Consequently, we propose a method called Dense Channel Compressionfor Feature Spatial Solidification. Drawing upon the central concept of thismethod, we introduce two innovative modules for backbone and head networks: theDense Channel Compression for Feature Spatial Solidification Structure (DCFS)and the Asymmetric Multi-Level Compression Decoupled Head (ADH). Whenintegrated into the YOLOv5 model, these two modules demonstrate exceptionalperformance, resulting in a modified model referred to as YOLOCS. Evaluated onthe MSCOCO dataset, the large, medium, and small YOLOCS models yield AP of50.1%, 47.6%, and 42.5%, respectively. Maintaining inference speeds remarkablysimilar to those of the YOLOv5 model, the large, medium, and small YOLOCSmodels surpass the YOLOv5 model\'s AP by 1.1%, 2.3%, and 5.2%, respectively.$$$$$http://arxiv.org/pdf/2305.04170v1
Video-Specific Query-Key Attention Modeling for Weakly-Supervised  Temporal Action Localization$  Weakly-supervised temporal action localization aims to identify and localizethe action instances in the untrimmed videos with only video-level actionlabels. When humans watch videos, we can adapt our abstract-level knowledgeabout actions in different video scenarios and detect whether some actions areoccurring. In this paper, we mimic how humans do and bring a new perspectivefor locating and identifying multiple actions in a video. We propose a networknamed VQK-Net with a video-specific query-key attention modeling that learns aunique query for each action category of each input video. The learned queriesnot only contain the actions\' knowledge features at the abstract level but alsohave the ability to fit this knowledge into the target video scenario, and theywill be used to detect the presence of the corresponding action along thetemporal dimension. To better learn these action category queries, we exploitnot only the features of the current input video but also the correlationbetween different videos through a novel video-specific action category querylearner worked with a query similarity loss. Finally, we conduct extensiveexperiments on three commonly used datasets (THUMOS14, ActivityNet1.2, andActivityNet1.3) and achieve state-of-the-art performance.$$$$$http://arxiv.org/pdf/2305.04186v1
Robust Image Ordinal Regression with Controllable Image Generation$  Image ordinal regression has been mainly studied along the line of exploitingthe order of categories. However, the issues of class imbalance and categoryoverlap that are very common in ordinal regression were largely overlooked. Asa result, the performance on minority categories is often unsatisfactory. Inthis paper, we propose a novel framework called CIG based on controllable imagegeneration to directly tackle these two issues. Our main idea is to generateextra training samples with specific labels near category boundaries, and thesample generation is biased toward the less-represented categories. To achievecontrollable image generation, we seek to separate structural and categoricalinformation of images based on structural similarity, categorical similarity,and reconstruction constraints. We evaluate the effectiveness of our new CIGapproach in three different image ordinal regression scenarios. The resultsdemonstrate that CIG can be flexibly integrated with off-the-shelf imageencoders or ordinal regression models to achieve improvement, and further, theimprovement is more significant for minority categories.$$$$$http://arxiv.org/pdf/2305.04213v1
Visual Causal Scene Refinement for Video Question Answering$  Existing methods for video question answering (VideoQA) often suffer fromspurious correlations between different modalities, leading to a failure inidentifying the dominant visual evidence and the intended question. Moreover,these methods function as black boxes, making it difficult to interpret thevisual scene during the QA process. In this paper, to discover critical videosegments and frames that serve as the visual causal scene for generatingreliable answers, we present a causal analysis of VideoQA and propose aframework for cross-modal causal relational reasoning, named Visual CausalScene Refinement (VCSR). Particularly, a set of causal front-door interventionoperations is introduced to explicitly find the visual causal scenes at bothsegment and frame levels. Our VCSR involves two essential modules: i) theQuestion-Guided Refiner (QGR) module, which refines consecutive video framesguided by the question semantics to obtain more representative segment featuresfor causal front-door intervention; ii) the Causal Scene Separator (CSS)module, which discovers a collection of visual causal and non-causal scenesbased on the visual-linguistic causal relevance and estimates the causal effectof the scene-separating intervention in a contrastive learning manner.Extensive experiments on the NExT-QA, Causal-VidQA, and MSRVTT-QA datasetsdemonstrate the superiority of our VCSR in discovering visual causal scene andachieving robust video question answering.$Data Mining Algorithms$Department of Computer Science, University of Illinois at Urbana-Champaign$本文章介绍了数据挖掘算法在数据处理过程中的重要性和应用。$数据挖掘算法$http://arxiv.org/pdf/2305.04224v1
CatFLW: Cat Facial Landmarks in the Wild Dataset$  Animal affective computing is a quickly growing field of research, where onlyrecently first efforts to go beyond animal tracking into recognizing theirinternal states, such as pain and emotions, have emerged. In most mammals,facial expressions are an important channel for communicating information aboutthese states. However, unlike the human domain, there is an acute lack ofdatasets that make automation of facial analysis of animals feasible.  This paper aims to fill this gap by presenting a dataset called Cat FacialLandmarks in the Wild (CatFLW) which contains 2016 images of cat faces indifferent environments and conditions, annotated with 48 facial landmarksspecifically chosen for their relationship with underlying musculature, andrelevance to cat-specific facial Action Units (CatFACS). To the best of ourknowledge, this dataset has the largest amount of cat facial landmarksavailable.  In addition, we describe a semi-supervised (human-in-the-loop) method ofannotating images with landmarks, used for creating this dataset, whichsignificantly reduces the annotation time and could be used for creatingsimilar datasets for other animals.  The dataset is available on request.$animal affective computing, cat, facial expressions, landmark detection$University of Haifa, Israel$本文介绍了一个包含2016张猫脸不同环境及状态下局部关键点注释的数据集CatFLW，描述了对图像标注关键点的一种半监督人机交互方法。此外针对猫疼痛评估的需求，该数据集为后续的猫疼痛识别建立了基础。$动物情感计算、猫、面部表情、局部关键点检测$http://arxiv.org/pdf/2305.04232v1
RFR-WWANet: Weighted Window Attention-Based Recovery Feature Resolution  Network for Unsupervised Image Registration$  The Swin transformer has recently attracted attention in medical imageanalysis due to its computational efficiency and long-range modelingcapability, which enables the establishment of more distant relationshipsbetween corresponding voxels. However, transformer-based models split imagesinto tokens, which results in transformers that can only model and outputcoarse-grained spatial information representations. To address this issue, wepropose Recovery Feature Resolution Network (RFRNet), which enables thetransformer to contribute with fine-grained spatial information and richsemantic correspondences. Furthermore, shifted window partitioning operationsare inflexible, indicating that they cannot perceive the semantic informationover uncertain distances and automatically bridge the global connectionsbetween windows. Therefore, we present a Weighted Window Attention (WWA) toautomatically build global interactions between windows after the regular andcyclic shifted window partitioning operations for Swin transformer blocks. Theproposed unsupervised deformable image registration model, named RFR-WWANet,senses the long-range correlations, thereby facilitating meaningful semanticrelevance of anatomical structures. Qualitative and quantitative results showthat RFR-WWANet achieves significant performance improvements over baselinemethods. Ablation experiments demonstrate the effectiveness of the RFRNet andWWA designs.$$$$$http://arxiv.org/pdf/2305.04236v1
Estimation of control area in badminton doubles with pose information  from top and back view drone videos$  The application of visual tracking to the performance analysis of sportsplayers in dynamic competitions is vital for effective coaching. In racketsports, most previous studies have focused on analyzing and assessing singlesplayers without occlusion in broadcast videos and discrete representations(e.g., stroke) that ignore meaningful spatial distributions. In this work, wepresent the first annotated drone dataset from top and back views in badmintondoubles and propose a framework to estimate the control area probability map,which can be used to evaluate teamwork performance. We present an efficientframework of deep neural networks that enables the calculation of fullprobability surfaces, which utilizes the embedding of a Gaussian mixture map ofplayers\' positions and graph convolution of their poses. In the experiment, weverify our approach by comparing various baselines and discovering thecorrelations between the score and control area. Furthermore, we propose thepractical application of assessing optimal positioning to provide instructionsduring a game. Our approach can visually and quantitatively evaluate players\'movements, providing valuable insights into doubles teamwork.$$$$$http://arxiv.org/pdf/2305.04247v1
Multi-Space Neural Radiance Fields$  Existing Neural Radiance Fields (NeRF) methods suffer from the existence ofreflective objects, often resulting in blurry or distorted rendering. Insteadof calculating a single radiance field, we propose a multi-space neuralradiance field (MS-NeRF) that represents the scene using a group of featurefields in parallel sub-spaces, which leads to a better understanding of theneural network toward the existence of reflective and refractive objects. Ourmulti-space scheme works as an enhancement to existing NeRF methods, with onlysmall computational overheads needed for training and inferring the extra-spaceoutputs. We demonstrate the superiority and compatibility of our approach usingthree representative NeRF-based models, i.e., NeRF, Mip-NeRF, and Mip-NeRF 360.Comparisons are performed on a novelly constructed dataset consisting of 25synthetic scenes and 7 real captured scenes with complex reflection andrefraction, all having 360-degree viewpoints. Extensive experiments show thatour approach significantly outperforms the existing single-space NeRF methodsfor rendering high-quality scenes concerned with complex light paths throughmirror-like objects. Our code and dataset will be publicly available athttps://zx-yin.github.io/msnerf.$$$$$http://arxiv.org/pdf/2305.04268v1
Neural Voting Field for Camera-Space 3D Hand Pose Estimation$  We present a unified framework for camera-space 3D hand pose estimation froma single RGB image based on 3D implicit representation. As opposed to recentworks, most of which first adopt holistic or pixel-level dense regression toobtain relative 3D hand pose and then follow with complex second-stageoperations for 3D global root or scale recovery, we propose a novel unified 3Ddense regression scheme to estimate camera-space 3D hand pose via dense 3Dpoint-wise voting in camera frustum. Through direct dense modeling in 3D domaininspired by Pixel-aligned Implicit Functions for 3D detailed reconstruction,our proposed Neural Voting Field (NVF) fully models 3D dense local evidence andhand global geometry, helping to alleviate common 2D-to-3D ambiguities.Specifically, for a 3D query point in camera frustum and its pixel-alignedimage feature, NVF, represented by a Multi-Layer Perceptron, regresses: (i) itssigned distance to the hand surface; (ii) a set of 4D offset vectors (1D votingweight and 3D directional vector to each hand joint). Following a vote-castingscheme, 4D offset vectors from near-surface points are selected to calculatethe 3D hand joint coordinates by a weighted average. Experiments demonstratethat NVF outperforms existing state-of-the-art algorithms on FreiHAND datasetfor camera-space 3D hand pose estimation. We also adapt NVF to the classic taskof root-relative 3D hand pose estimation, for which NVF also obtainsstate-of-the-art results on HO3D dataset.$3D手部姿态估计、相机空间、神经投票场$1University at Buffalo, 2Microsoft$本文提出了一个基于3D隐式表示的统一框架，用于从单个RGB图像中进行相机空间3D手部姿态估计。作者提出了一种新颖的统一3D密集回归方案，通过在相机视锥体中进行密集3D点投票来估计相机空间3D手部姿态。作者提出的神经投票场（NVF）完全模拟了3D密集局部证据和手部全局几何形状，有助于减轻常见的2D到3D模糊。NVF在FreiHAND数据集上表现优异，并在HO3D数据集上取得了最先进的结果。$3D hand pose estimation, camera-space, neural voting field$http://arxiv.org/pdf/2305.04328v1
Segmentation of the veterinary cytological images for fast neoplastic  tumors diagnosis$  This paper shows the machine learning system which performs instancesegmentation of cytological images in veterinary medicine. Eleven cell typeswere used directly and indirectly in the experiments, including damaged andunrecognized categories. The deep learning models employed in the systemachieve a high score of average precision and recall metrics, i.e. 0.94 and 0.8respectively, for the selected three types of tumors. This variety of labeltypes allowed us to draw a meaningful conclusion that there are relatively fewmistakes for tumor cell types. Additionally, the model learned tumor cellfeatures well enough to avoid misclassification mistakes of one tumor type intoanother. The experiments also revealed that the quality of the results improveswith the dataset size (excluding the damaged cells). It is worth noting thatall the experiments were done using a custom dedicated dataset provided by thecooperating vet doctors.$$$$$http://arxiv.org/pdf/2305.04332v1
Living in a Material World: Learning Material Properties from  Full-Waveform Flash Lidar Data for Semantic Segmentation$  Advances in lidar technology have made the collection of 3D point clouds fastand easy. While most lidar sensors return per-point intensity (or reflectance)values along with range measurements, flash lidar sensors are able to provideinformation about the shape of the return pulse. The shape of the returnwaveform is affected by many factors, including the distance that the lightpulse travels and the angle of incidence with a surface. Importantly, the shapeof the return waveform also depends on the material properties of thereflecting surface. In this paper, we investigate whether the material type orclass can be determined from the full-waveform response. First, as a proof ofconcept, we demonstrate that the extra information about material class, ifknown accurately, can improve performance on scene understanding tasks such assemantic segmentation. Next, we learn two different full-waveform materialclassifiers: a random forest classifier and a temporal convolutional neuralnetwork (TCN) classifier. We find that, in some cases, material types can bedistinguished, and that the TCN generally performs better across a wider rangeof materials. However, factors such as angle of incidence, material colour, andmaterial similarity may hinder overall performance.$$$$$http://arxiv.org/pdf/2305.04334v1
Spatiotemporally Consistent HDR Indoor Lighting Estimation$  We propose a physically-motivated deep learning framework to solve a generalversion of the challenging indoor lighting estimation problem. Given a singleLDR image with a depth map, our method predicts spatially consistent lightingat any given image position. Particularly, when the input is an LDR videosequence, our framework not only progressively refines the lighting predictionas it sees more regions, but also preserves temporal consistency by keeping therefinement smooth. Our framework reconstructs a spherical Gaussian lightingvolume (SGLV) through a tailored 3D encoder-decoder, which enables spatiallyconsistent lighting prediction through volume ray tracing, a hybrid blendingnetwork for detailed environment maps, an in-network Monte-Carlo renderinglayer to enhance photorealism for virtual object insertion, and recurrentneural networks (RNN) to achieve temporally consistent lighting prediction witha video sequence as the input. For training, we significantly enhance theOpenRooms public dataset of photorealistic synthetic indoor scenes with around360K HDR environment maps of much higher resolution and 38K video sequences,rendered with GPU-based path tracing. Experiments show that our frameworkachieves lighting prediction with higher quality compared to state-of-the-artsingle-image or video-based methods, leading to photorealistic AR applicationssuch as object insertion.$computer vision, deep learning, indoor lighting estimation$Meta Reality Labs Research, UC San Diego, USA; Meta Reality Labs, USA$本文提出了一个基于物理学原理的深度学习框架，针对室内光照估计问题，实现了在空间和时间上的高一致性预测，可支持高度真实的AR应用，如虚拟物体插入。框架包括三维编码器-解码器以及体积光线追踪等模块，使用递归神经网络实现时间上的一致性预测。在公共数据集OpenRooms的基础上，通过GPU路径追踪实验说明该框架在单图像或视频的高质量室内光照预测方面具有优越性。$计算机视觉、深度学习、室内光照估计$http://arxiv.org/pdf/2305.04374v1
SegGPT Meets Co-Saliency Scene$  Co-salient object detection targets at detecting co-existed salient objectsamong a group of images. Recently, a generalist model for segmenting everythingin context, called SegGPT, is gaining public attention. In view of itsbreakthrough for segmentation, we can hardly wait to probe into itscontribution to the task of co-salient object detection. In this report, wefirst design a framework to enable SegGPT for the problem of co-salient objectdetection. Proceed to the next step, we evaluate the performance of SegGPT onthe problem of co-salient object detection on three available datasets. Weachieve a finding that co-saliency scenes challenges SegGPT due to contextdiscrepancy within a group of co-saliency images.$Co-Salient object detection, SegGPT, Context $School of Computer Science and Artificial Intelligence, Aliyun School of Big Data, School of Software, Changzhou University, Hefei Comprehensive National Science Center, Institute of Artificial Intelligence, School of Automation, Northwestern Polytechnical University, and Department of Computer Science, The University of Sheffield。$本文研究了将SegGPT应用于共显著目标检测的问题，并在三个公共数据集上进行了实验和分析，发现共显著场景对SegGPT的挑战在于一组共显著图像中的上下文不一致性。$共显著目标检测，SegGPT，上下文$http://arxiv.org/pdf/2305.04396v1
TaLU: A Hybrid Activation Function Combining Tanh and Rectified Linear  Unit to Enhance Neural Networks$  The application of the deep learning model in classification plays animportant role in the accurate detection of the target objects. However, theaccuracy is affected by the activation function in the hidden and output layer.In this paper, an activation function called TaLU, which is a combination ofTanh and Rectified Linear Units (ReLU), is used to improve the prediction. ReLUactivation function is used by many deep learning researchers for itscomputational efficiency, ease of implementation, intuitive nature, etc.However, it suffers from a dying gradient problem. For instance, when the inputis negative, its output is always zero because its gradient is zero. A numberof researchers used different approaches to solve this issue. Some of the mostnotable are LeakyReLU, Softplus, Softsign, Elu, ThresholdedReLU, etc. Thisresearch developed TaLU, a modified activation function combining Tanh andReLU, which mitigates the dying gradient problem of ReLU. The deep learningmodel with the proposed activation function was tested on MNIST and CIFAR-10,and it outperforms ReLU and some other studied activation functions in terms ofaccuracy(from 0\\% upto 6\\% in most cases, when used with Batch Normalizationand a reasonable learning rate).$$$$$http://arxiv.org/pdf/2305.04402v1
Improving 2D face recognition via fine-level facial depth generation and  RGB-D complementary feature learning$  Face recognition in complex scenes suffers severe challenges coming fromperturbations such as pose deformation, ill illumination, partial occlusion.Some methods utilize depth estimation to obtain depth corresponding to RGB toimprove the accuracy of face recognition. However, the depth generated by themsuffer from image blur, which introduces noise in subsequent RGB-D facerecognition tasks. In addition, existing RGB-D face recognition methods areunable to fully extract complementary features. In this paper, we propose afine-grained facial depth generation network and an improved multimodalcomplementary feature learning network. Extensive experiments on the Lock3DFacedataset and the IIIT-D dataset show that the proposed FFDGNet and I MCFLNet canimprove the accuracy of RGB-D face recognition while achieving thestate-of-the-art performance.$2D面部识别、精细面部深度生成、RGB-D互补特征学习$College of Electrical Engineering and Automation, Shandong University of Science and Technology, Qingdao, China$Improving 2D face recognition accuracy by generating fine-grained depth images and learning complementary features using RGB-D modality.$2D face recognition, fine-grained facial depth generation, RGB-D complementary feature learning$http://arxiv.org/pdf/2305.04426v1
Adversarial Examples Detection with Enhanced Image Difference Features  based on Local Histogram Equalization$  Deep Neural Networks (DNNs) have recently made significant progress in manyfields. However, studies have shown that DNNs are vulnerable to adversarialexamples, where imperceptible perturbations can greatly mislead DNNs even ifthe full underlying model parameters are not accessible. Various defensemethods have been proposed, such as feature compression and gradient masking.However, numerous studies have proven that previous methods create detection ordefense against certain attacks, which renders the method ineffective in theface of the latest unknown attack methods. The invisibility of adversarialperturbations is one of the evaluation indicators for adversarial exampleattacks, which also means that the difference in the local correlation ofhigh-frequency information in adversarial examples and normal examples can beused as an effective feature to distinguish the two. Therefore, we propose anadversarial example detection framework based on a high-frequency informationenhancement strategy, which can effectively extract and amplify the featuredifferences between adversarial examples and normal examples. Experimentalresults show that the feature augmentation module can be combined with existingdetection models in a modular way under this framework. Improve the detector\'sperformance and reduce the deployment cost without modifying the existingdetection model.$$$$$http://arxiv.org/pdf/2305.04436v1
Vision Transformer Off-the-Shelf: A Surprising Baseline for Few-Shot  Class-Agnostic Counting$  Class-agnostic counting (CAC) aims to count objects of interest from a queryimage given few exemplars. This task is typically addressed by extracting thefeatures of query image and exemplars respectively with (un)shared featureextractors and by matching their feature similarity, leading to anextract-\\textit{then}-match paradigm. In this work, we show that CAC can besimplified in an extract-\\textit{and}-match manner, particularly using apretrained and plain vision transformer (ViT) where feature extraction andsimilarity matching are executed simultaneously within the self-attention. Wereveal the rationale of such simplification from a decoupled view of theself-attention and point out that the simplification is only made possible ifthe query and exemplar tokens are concatenated as input. The resulting model,termed CACViT, simplifies the CAC pipeline and unifies the feature spacesbetween the query image and exemplars. In addition, we find CACViT naturallyencodes background information within self-attention, which helps reducebackground disturbance. Further, to compensate the loss of the scale and theorder-of-magnitude information due to resizing and normalization in ViT, wepresent two effective strategies for scale and magnitude embedding. Extensiveexperiments on the FSC147 and the CARPK datasets show that CACViT significantlyoutperforms state-of-the-art CAC approaches in both effectiveness (23.60% errorreduction) and generalization, which suggests CACViT provides a concise andstrong baseline for CAC. Code will be available.$$$$$http://arxiv.org/pdf/2305.04440v1
Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion  Models$  Recently large-scale language-image models (e.g., text-guided diffusionmodels) have considerably improved the image generation capabilities togenerate photorealistic images in various domains. Based on this success,current image editing methods use texts to achieve intuitive and versatilemodification of images. To edit a real image using diffusion models, one mustfirst invert the image to a noisy latent from which an edited image is sampledwith a target text prompt. However, most methods lack one of the following:user-friendliness (e.g., additional masks or precise descriptions of the inputimage are required), generalization to larger domains, or high fidelity to theinput image. In this paper, we design an accurate and quick inversiontechnique, Prompt Tuning Inversion, for text-driven image editing.Specifically, our proposed editing method consists of a reconstruction stageand an editing stage. In the first stage, we encode the information of theinput image into a learnable conditional embedding via Prompt Tuning Inversion.In the second stage, we apply classifier-free guidance to sample the editedimage, where the conditional embedding is calculated by linearly interpolatingbetween the target embedding and the optimized one obtained in the first stage.This technique ensures a superior trade-off between editability and highfidelity to the input image of our method. For example, we can change the colorof a specific object while preserving its original shape and background underthe guidance of only a target text prompt. Extensive experiments on ImageNetdemonstrate the superior editing performance of our method compared to thestate-of-the-art baselines.$text-driven image editing, diffusion models, Prompt Tuning Inversion, image generation, deep learning$1Baidu VIS, 2Beihang University$本文提出了一种准确且快速的图像反演技术Prompt Tuning Inversion，用于文本驱动的图像编辑，该技术确保了编辑性和对输入图像的高保真度。在ImageNet上的实验表明，与最先进的基线方法相比，我们的方法具有卓越的编辑性能。$文本驱动图像编辑、扩散模型、Prompt Tuning Inversion、图像生成、深度学习$http://arxiv.org/pdf/2305.04441v1
FashionTex: Controllable Virtual Try-on with Text and Texture$  Virtual try-on attracts increasing research attention as a promising way forenhancing the user experience for online cloth shopping. Though existingmethods can generate impressive results, users need to provide a well-designedreference image containing the target fashion clothes that often do not exist.To support user-friendly fashion customization in full-body portraits, wepropose a multi-modal interactive setting by combining the advantages of bothtext and texture for multi-level fashion manipulation. With the carefullydesigned fashion editing module and loss functions, FashionTex framework cansemantically control cloth types and local texture patterns without annotatedpairwise training data. We further introduce an ID recovery module to maintainthe identity of input portrait. Extensive experiments have demonstrated theeffectiveness of our proposed pipeline.$$$$$http://arxiv.org/pdf/2305.04451v1
Real-World Denoising via Diffusion Model$  Real-world image denoising is an extremely important image processingproblem, which aims to recover clean images from noisy images captured innatural environments. In recent years, diffusion models have achieved verypromising results in the field of image generation, outperforming previousgeneration models. However, it has not been widely used in the field of imagedenoising because it is difficult to control the appropriate position of theadded noise. Inspired by diffusion models, this paper proposes a novel generaldenoising diffusion model that can be used for real-world image denoising. Weintroduce a diffusion process with linear interpolation, and the intermediatenoisy image is interpolated from the original clean image and the correspondingreal-world noisy image, so that this diffusion model can handle the level ofadded noise. In particular, we also introduce two sampling algorithms for thisdiffusion model. The first one is a simple sampling procedure defined accordingto the diffusion process, and the second one targets the problem of the firstone and makes a number of improvements. Our experimental results show that ourproposed method with a simple CNNs Unet achieves comparable results compared tothe Transformer architecture. Both quantitative and qualitative evaluations onreal-world denoising benchmarks show that the proposed general diffusion modelperforms almost as well as against the state-of-the-art methods.$$$$$http://arxiv.org/pdf/2305.04457v1
Generalized Universal Domain Adaptation with Generative Flow Networks$  We introduce a new problem in unsupervised domain adaptation, termed asGeneralized Universal Domain Adaptation (GUDA), which aims to achieve preciseprediction of all target labels including unknown categories. GUDA bridges thegap between label distribution shift-based and label space mismatch-basedvariants, essentially categorizing them as a unified problem, guiding to acomprehensive framework for thoroughly solving all the variants. The keychallenge of GUDA is developing and identifying novel target categories whileestimating the target label distribution. To address this problem, we takeadvantage of the powerful exploration capability of generative flow networksand propose an active domain adaptation algorithm named GFlowDA, which selectsdiverse samples with probabilities proportional to a reward function. Toenhance the exploration capability and effectively perceive the target labeldistribution, we tailor the states and rewards, and introduce an efficientsolution for parent exploration and state transition. We also propose atraining paradigm for GUDA called Generalized Universal Adversarial Network(GUAN), which involves collaborative optimization between GUAN and GFlowNet.Theoretical analysis highlights the importance of exploration, and extensiveexperiments on benchmark datasets demonstrate the superiority of GFlowDA.$$$$$http://arxiv.org/pdf/2305.04466v1
Video Object Segmentation in Panoptic Wild Scenes$  In this paper, we introduce semi-supervised video object segmentation (VOS)to panoptic wild scenes and present a large-scale benchmark as well as abaseline method for it. Previous benchmarks for VOS with sparse annotations arenot sufficient to train or evaluate a model that needs to process all possibleobjects in real-world scenarios. Our new benchmark (VIPOSeg) containsexhaustive object annotations and covers various real-world object categorieswhich are carefully divided into subsets of thing/stuff and seen/unseen classesfor comprehensive evaluation. Considering the challenges in panoptic VOS, wepropose a strong baseline method named panoptic object association withtransformers (PAOT), which uses panoptic identification to associate objectswith a pyramid architecture on multiple scales. Experimental results show thatVIPOSeg can not only boost the performance of VOS models by panoptic trainingbut also evaluate them comprehensively in panoptic scenes. Previous methods forclassic VOS still need to improve in performance and efficiency when dealingwith panoptic scenes, while our PAOT achieves SOTA performance with goodefficiency on VIPOSeg and previous VOS benchmarks. PAOT also ranks 1st in theVOT2022 challenge. Our dataset is available athttps://github.com/yoxu515/VIPOSeg-Benchmark.$$$$$http://arxiv.org/pdf/2305.04470v1
DiffBFR: Bootstrapping Diffusion Model Towards Blind Face Restoration$  Blind face restoration (BFR) is important while challenging. Prior worksprefer to exploit GAN-based frameworks to tackle this task due to the balanceof quality and efficiency. However, these methods suffer from poor stabilityand adaptability to long-tail distribution, failing to simultaneously retainsource identity and restore detail. We propose DiffBFR to introduce DiffusionProbabilistic Model (DPM) for BFR to tackle the above problem, given itssuperiority over GAN in aspects of avoiding training collapse and generatinglong-tail distribution. DiffBFR utilizes a two-step design, that first restoresidentity information from low-quality images and then enhances texture detailsaccording to the distribution of real faces. This design is implemented withtwo key components: 1) Identity Restoration Module (IRM) for preserving theface details in results. Instead of denoising from pure Gaussian randomdistribution with LQ images as the condition during the reverse process, wepropose a novel truncated sampling method which starts from LQ images with partnoise added. We theoretically prove that this change shrinks the evidence lowerbound of DPM and then restores more original details. With theoretical proof,two cascade conditional DPMs with different input sizes are introduced tostrengthen this sampling effect and reduce training difficulty in thehigh-resolution image generated directly. 2) Texture Enhancement Module (TEM)for polishing the texture of the image. Here an unconditional DPM, a LQ-freemodel, is introduced to further force the restorations to appear realistic. Wetheoretically proved that this unconditional DPM trained on pure HQ imagescontributes to justifying the correct distribution of inference images outputfrom IRM in pixel-level space. Truncated sampling with fractional time step isutilized to polish pixel-level textures while preserving identity information.$Diffusion Probabilistic Model、identity restoration、texture enhancement、Blind Face Restoration、Bootstrapping$University of Chinese Academy of Sciences, Beijing, China$本文介绍了一种基于Diffusion Probabilistic Model (DPM)的盲目人脸修复框架DiffBFR，通过两个关键模块（Identity Restoration Module和Texture Enhancement Module）实现对低质量人脸图像恢复原始信息的目标。同时，通过引入截断抽样方法避免了GAN中存在的训练不稳定和适应长尾分布困难的问题。$人脸修复、Diffusion Probabilistic Model、identity restoration、texture enhancement、Blind Face Restoration$http://arxiv.org/pdf/2305.04517v1
Scene Text Recognition with Image-Text Matching-guided Dictionary$  Employing a dictionary can efficiently rectify the deviation between thevisual prediction and the ground truth in scene text recognition methods.However, the independence of the dictionary on the visual features may lead toincorrect rectification of accurate visual predictions. In this paper, wepropose a new dictionary language model leveraging the Scene Image-TextMatching(SITM) network, which avoids the drawbacks of the explicit dictionarylanguage model: 1) the independence of the visual features; 2) noisy choice incandidates etc. The SITM network accomplishes this by using Image-TextContrastive (ITC) Learning to match an image with its corresponding text amongcandidates in the inference stage. ITC is widely used in vision-languagelearning to pull the positive image-text pair closer in feature space. Inspiredby ITC, the SITM network combines the visual features and the text features ofall candidates to identify the candidate with the minimum distance in thefeature space. Our lexicon method achieves better results(93.8\\% accuracy) thanthe ordinary method results(92.1\\% accuracy) on six mainstream benchmarks.Additionally, we integrate our method with ABINet and establish newstate-of-the-art results on several benchmarks.$Dictionary Language Model, Scene Image-Text Matching, Image-Text Contrastive Learning, Scene Text Recognition.$1上海东华大学多维信息加工重点实验室，2中国东华大学重庆学院，3印度统计学研究所CVPR单位$本文提出了一种新的字典语言模型，利用场景图像-文本匹配网络，避免了显式字典语言模型的缺点，并在六个主流基准测试中取得了更好的效果。同时将本方法与ABINet集成，实现了新的最新基准测试结果。$场景文本识别，图像文本匹配，字典语言模型，图像文本对比学习。$http://arxiv.org/pdf/2305.04524v1
SNT: Sharpness-Minimizing Network Transformation for Fast  Compression-friendly Pretraining$  Model compression has become the de-facto approach for optimizing theefficiency of vision models. Recently, the focus of most compression effortshas shifted to post-training scenarios due to the very high cost of large-scalepretraining. This has created the need to build compressible models fromscratch, which can effectively be compressed after training. In this work, wepresent a sharpness-minimizing network transformation (SNT) method appliedduring pretraining that can create models with desirable compressibility andgeneralizability features. We compare our approach to a well-knownsharpness-minimizing optimizer to validate its efficacy in creating a flat losslandscape. To the best of our knowledge, SNT is the first pretraining methodthat uses an architectural transformation to generate compression-friendlynetworks. We find that SNT generalizes across different compression tasks andnetwork backbones, delivering consistent improvements over the ADAM baselinewith up to 2% accuracy improvement on weight pruning and 5.4% accuracyimprovement on quantization. Code to reproduce our results will be madepublicly available.$$$$$http://arxiv.org/pdf/2305.04526v1
Smart Home Device Detection Algorithm Based on FSA-YOLOv5$  Smart home device detection is a critical aspect of human-computerinteraction. However, detecting targets in indoor environments can bechallenging due to interference from ambient light and background noise. Inthis paper, we present a new model called FSA-YOLOv5, which addresses thelimitations of traditional convolutional neural networks by introducing theTransformer to learn long-range dependencies. Additionally, we propose a newattention module, the full-separation attention module, which integratesspatial and channel dimensional information to learn contextual information. Toimprove tiny device detection, we include a prediction head for the indoorsmart home device detection task. We also release the Southeast UniversityIndoor Smart Speaker Dataset (SUSSD) to supplement existing data samples.Through a series of experiments on SUSSD, we demonstrate that our methodoutperforms other methods, highlighting the effectiveness of FSA-YOLOv5.$$$$$http://arxiv.org/pdf/2305.04534v1
LMPT: Prompt Tuning with Class-Specific Embedding Loss for Long-tailed  Multi-Label Visual Recognition$  Long-tailed multi-label visual recognition (LTML) task is a highlychallenging task due to the label co-occurrence and imbalanced datadistribution. In this work, we propose a unified framework for LTML, namelyprompt tuning with class-specific embedding loss (LMPT), capturing the semanticfeature interactions between categories by combining text and image modalitydata and improving the performance synchronously on both head and tail classes.Specifically, LMPT introduces the embedding loss function with class-aware softmargin and re-weighting to learn class-specific contexts with the benefit oftextual descriptions (captions), which could help establish semanticrelationships between classes, especially between the head and tail classes.Furthermore, taking into account the class imbalance, the distribution-balancedloss is adopted as the classification loss function to further improve theperformance on the tail classes without compromising head classes. Extensiveexperiments are conducted on VOC-LT and COCO-LT datasets, which demonstratesthat the proposed method significantly surpasses the previous state-of-the-artmethods and zero-shot CLIP in LTML. Our codes are fully available at\\url{https://github.com/richard-peng-xia/LMPT}.$$$$$http://arxiv.org/pdf/2305.04536v1
High Quality Large-Scale 3-D Urban Mapping with Multi-Master TomoSAR$  Multi-baseline interferometric synthetic aperture radar (InSAR) techniquesare effective approaches for retrieving the 3-D information of urban areas. Inorder to obtain a plausible reconstruction, it is necessary to use large-stackinterferograms. Hence, these methods are commonly not appropriate forlarge-scale 3-D urban mapping using TanDEM-X data where only a few acquisitionsare available in average for each city. This work proposes a new SARtomographic processing framework to work with those extremely small stacks,which integrates the non-local filtering into SAR tomography inversion. Theapplicability of the algorithm is demonstrated using a TanDEM-X multi-baselinestack with 5 bistatic interferograms over the whole city of Munich, Germany.Systematic comparison of our result with airborne LiDAR data shows that therelative height accuracy of two third buildings is within two meters, whichoutperforms the TanDEM-X raw DEM. The promising performance of the proposedalgorithm paved the first step towards high quality large-scale 3-D urbanmapping.$$$$$http://arxiv.org/pdf/2305.04541v1
Privacy-Preserving Representations are not Enough -- Recovering Scene  Content from Camera Poses$  Visual localization is the task of estimating the camera pose from which agiven image was taken and is central to several 3D computer visionapplications. With the rapid growth in the popularity of AR/VR/MR devices andcloud-based applications, privacy issues are becoming a very important aspectof the localization process. Existing work on privacy-preserving localizationaims to defend against an attacker who has access to a cloud-based service. Inthis paper, we show that an attacker can learn about details of a scene withoutany access by simply querying a localization service. The attack is based onthe observation that modern visual localization algorithms are robust tovariations in appearance and geometry. While this is in general a desiredproperty, it also leads to algorithms localizing objects that are similarenough to those present in a scene. An attacker can thus query a server with alarge enough set of images of objects, \\eg, obtained from the Internet, andsome of them will be localized. The attacker can thus learn about objectplacements from the camera poses returned by the service (which is the minimalinformation returned by such a service). In this paper, we develop aproof-of-concept version of this attack and demonstrate its practicalfeasibility. The attack does not place any requirements on the localizationalgorithm used, and thus also applies to privacy-preserving representations.Current work on privacy-preserving representations alone is thus insufficient.$$$$$http://arxiv.org/pdf/2305.04603v1
Target-driven One-Shot Unsupervised Domain Adaptation$  In this paper, we introduce a novel framework for the challenging problem ofOne-Shot Unsupervised Domain Adaptation (OSUDA), which aims to adapt to atarget domain with only a single unlabeled target sample. Unlike existingapproaches that rely on large labeled source and unlabeled target data, ourTarget-driven One-Shot UDA (TOS-UDA) approach employs a learnable augmentationstrategy guided by the target sample\'s style to align the source distributionwith the target distribution. Our method consists of three modules: anaugmentation module, a style alignment module, and a classifier. Unlikeexisting methods, our augmentation module allows for strong transformations ofthe source samples, and the style of the single target sample available isexploited to guide the augmentation by ensuring perceptual similarity.Furthermore, our approach integrates augmentation with style alignment,eliminating the need for separate pre-training on additional datasets. Ourmethod outperforms or performs comparably to existing OS-UDA methods on theDigits and DomainNet benchmarks.$$$$$http://arxiv.org/pdf/2305.04628v1
ReGeneration Learning of Diffusion Models with Rich Prompts for  Zero-Shot Image Translation$  Large-scale text-to-image models have demonstrated amazing ability tosynthesize diverse and high-fidelity images. However, these models are oftenviolated by several limitations. Firstly, they require the user to provideprecise and contextually relevant descriptions for the desired imagemodifications. Secondly, current models can impose significant changes to theoriginal image content during the editing process. In this paper, we exploreReGeneration learning in an image-to-image Diffusion model (ReDiffuser), thatpreserves the content of the original image without human prompting and therequisite editing direction is automatically discovered within the textembedding space. To ensure consistent preservation of the shape during imageediting, we propose cross-attention guidance based on regeneration learning.This novel approach allows for enhanced expression of the target domainfeatures while preserving the original shape of the image. In addition, weintroduce a cooperative update strategy, which allows for efficientpreservation of the original shape of an image, thereby improving the qualityand consistency of shape preservation throughout the editing process. Ourproposed method leverages an existing pre-trained text-image diffusion modelwithout any additional training. Extensive experiments show that the proposedmethod outperforms existing work in both real and synthetic image editing.$Image Editing, Zero-Shot Learning, ReGeneration Learning$1Guangdong University of Technology，2The University of Sydney，3Anhui University$本文提出了一种无需人类提示和自动发现文本嵌入空间内的编辑方向的图像到图像扩散模型（ReDiffuser），以保留原始图像的内容和形状。文章实现了跨注意力引导的重建学习方法，以确保形状在图像编辑过程中得到一致的保留。文章还介绍了一种合作更新策略，允许有效地保护图像的原始形状，从而改善形状保留的质量和一致性。实验证明，该方法在真实图像和合成图像编辑方面优于现有工作。$图像编辑，零样本学习，重建学习$http://arxiv.org/pdf/2305.04651v1
Self-supervised Learning for Pre-Training 3D Point Clouds: A Survey$  Point cloud data has been extensively studied due to its compact form andflexibility in representing complex 3D structures. The ability of point clouddata to accurately capture and represent intricate 3D geometry makes it anideal choice for a wide range of applications, including computer vision,robotics, and autonomous driving, all of which require an understanding of theunderlying spatial structures. Given the challenges associated with annotatinglarge-scale point clouds, self-supervised point cloud representation learninghas attracted increasing attention in recent years. This approach aims to learngeneric and useful point cloud representations from unlabeled data,circumventing the need for extensive manual annotations. In this paper, wepresent a comprehensive survey of self-supervised point cloud representationlearning using DNNs. We begin by presenting the motivation and general trendsin recent research. We then briefly introduce the commonly used datasets andevaluation metrics. Following that, we delve into an extensive exploration ofself-supervised point cloud representation learning methods based on thesetechniques. Finally, we share our thoughts on some of the challenges andpotential issues that future research in self-supervised learning forpre-training 3D point clouds may encounter.$$$$$http://arxiv.org/pdf/2305.04691v1
Learning to Generate Poetic Chinese Landscape Painting with Calligraphy$  In this paper, we present a novel system (denoted as Polaca) to generatepoetic Chinese landscape painting with calligraphy. Unlike previous singleimage-to-image painting generation, Polaca takes the classic poetry as inputand outputs the artistic landscape painting image with the correspondingcalligraphy. It is equipped with three different modules to complete the wholepiece of landscape painting artwork: the first one is a text-to-image module togenerate landscape painting image, the second one is an image-to-image moduleto generate stylistic calligraphy image, and the third one is an image fusionmodule to fuse the two images into a whole piece of aesthetic artwork.$Literary generation, Text-to-image, Artistic generation, Landscape painting, Calligraphy$1JD AI, Beijing, China；2Central Academy of Fine Arts, Beijing, China$第一个是文本到图像模块，用于生成山水画图像；第二个是图像到图像模块，用于生成具有风格的书法图像；第三个是图像融合模块，将两个图像融合成一个整体的艺术品。$文学生成，文本到图像，艺术生成，山水画，书法$http://arxiv.org/pdf/2305.04719v1
Understanding Gaussian Attention Bias of Vision Transformers Using  Effective Receptive Fields$  Vision transformers (ViTs) that model an image as a sequence of partitionedpatches have shown notable performance in diverse vision tasks. Becausepartitioning patches eliminates the image structure, to reflect the order ofpatches, ViTs utilize an explicit component called positional embedding.However, we claim that the use of positional embedding does not simplyguarantee the order-awareness of ViT. To support this claim, we analyze theactual behavior of ViTs using an effective receptive field. We demonstrate thatduring training, ViT acquires an understanding of patch order from thepositional embedding that is trained to be a specific pattern. Based on thisobservation, we propose explicitly adding a Gaussian attention bias that guidesthe positional embedding to have the corresponding pattern from the beginningof training. We evaluated the influence of Gaussian attention bias on theperformance of ViTs in several image classification, object detection, andsemantic segmentation experiments. The results showed that proposed method notonly facilitates ViTs to understand images but also boosts their performance onvarious datasets, including ImageNet, COCO 2017, and ADE20K.$Vision recognition; self-attention mechanism; effective receptive field; positional embedding; Gaussian attention bias$POSTECH$本文通过分析有效感受域，证明了简单地使用位置嵌入不足以保证ViT具备对输入特征顺序的理解，并提出了注入高斯注意偏差的方法以更好地提高ViT的性能。我们在多个图像分类、物体检测和语义分割实验中验证了所提出方法的有效性。$视觉识别；自注意机制；有效感受域；位置嵌入；高斯注意偏差$http://arxiv.org/pdf/2305.04722v1
Large-scale and Efficient Texture Mapping Algorithm via Loopy Belief  Propagation$  Texture mapping as a fundamental task in 3D modeling has been wellestablished for well-acquired aerial assets under consistent illumination, yetit remains a challenge when it is scaled to large datasets with images undervarying views and illuminations. A well-performed texture mapping algorithmmust be able to efficiently select views, fuse and map textures from theseviews to mesh models, at the same time, achieve consistent radiometry over theentire model. Existing approaches achieve efficiency either by limiting thenumber of images to one view per face, or simplifying global inferences to onlyachieve local color consistency. In this paper, we break this tie by proposinga novel and efficient texture mapping framework that allows the use of multipleviews of texture per face, at the same time to achieve global colorconsistency. The proposed method leverages a loopy belief propagation algorithmto perform an efficient and global-level probabilistic inferences to rankcandidate views per face, which enables face-level multi-view texture fusionand blending. The texture fusion algorithm, being non-parametric, bringsanother advantage over typical parametric post color correction methods, due toits improved robustness to non-linear illumination differences. The experimentson three different types of datasets (i.e. satellite dataset, unmanned-aerialvehicle dataset and close-range dataset) show that the proposed method hasproduced visually pleasant and texturally consistent results in all scenarios,with an added advantage of consuming less running time as compared to the stateof the art methods, especially for large-scale dataset such assatellite-derived models.$本文提出了一种新的算法框架来实现大规模和高效的纹理映射，在多个视角的情况下实现全局颜色一致性。经过三种不同类型数据集的实验，结果显示该方法在大规模数据集中比现有方法更加节省时间。$纹理映射、置信传播、多标签、图像融合。 $$$http://arxiv.org/pdf/2305.04763v1
OSTA: One-shot Task-adaptive Channel Selection for Semantic Segmentation  of Multichannel Images$  Semantic segmentation of multichannel images is a fundamental task for manyapplications. Selecting an appropriate channel combination from the originalmultichannel image can improve the accuracy of semantic segmentation and reducethe cost of data storage, processing and future acquisition. Existing channelselection methods typically use a reasonable selection procedure to determine adesirable channel combination, and then train a semantic segmentation networkusing that combination. In this study, the concept of pruning from a supernetis used for the first time to integrate the selection of channel combinationand the training of a semantic segmentation network. Based on this concept, aOne-Shot Task-Adaptive (OSTA) channel selection method is proposed for thesemantic segmentation of multichannel images. OSTA has three stages, namely thesupernet training stage, the pruning stage and the fine-tuning stage. Theoutcomes of six groups of experiments (L7Irish3C, L7Irish2C, L8Biome3C,L8Biome2C, RIT-18 and Semantic3D) demonstrated the effectiveness and efficiencyof OSTA. OSTA achieved the highest segmentation accuracies in all tests (62.49%(mIoU), 75.40% (mIoU), 68.38% (mIoU), 87.63% (mIoU), 66.53% (mA) and 70.86%(mIoU), respectively). It even exceeded the highest accuracies of exhaustivetests (61.54% (mIoU), 74.91% (mIoU), 67.94% (mIoU), 87.32% (mIoU), 65.32% (mA)and 70.27% (mIoU), respectively), where all possible channel combinations weretested. All of this can be accomplished within a predictable and relativelyefficient timeframe, ranging from 101.71% to 298.1% times the time required totrain the segmentation network alone. In addition, there were interestingfindings that were deemed valuable for several fields.$$$$$http://arxiv.org/pdf/2305.04766v1
SignBERT+: Hand-model-aware Self-supervised Pre-training for Sign  Language Understanding$  Hand gesture serves as a crucial role during the expression of sign language.Current deep learning based methods for sign language understanding (SLU) areprone to over-fitting due to insufficient sign data resource and suffer limitedinterpretability. In this paper, we propose the first self-supervisedpre-trainable SignBERT+ framework with model-aware hand prior incorporated. Inour framework, the hand pose is regarded as a visual token, which is derivedfrom an off-the-shelf detector. Each visual token is embedded with gesturestate and spatial-temporal position encoding. To take full advantage of currentsign data resource, we first perform self-supervised learning to model itsstatistics. To this end, we design multi-level masked modeling strategies(joint, frame and clip) to mimic common failure detection cases. Jointly withthese masked modeling strategies, we incorporate model-aware hand prior tobetter capture hierarchical context over the sequence. After the pre-training,we carefully design simple yet effective prediction heads for downstream tasks.To validate the effectiveness of our framework, we perform extensiveexperiments on three main SLU tasks, involving isolated and continuous signlanguage recognition (SLR), and sign language translation (SLT). Experimentalresults demonstrate the effectiveness of our method, achieving newstate-of-the-art performance with a notable gain.$$$$$http://arxiv.org/pdf/2305.04868v1
PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR  Point Clouds$  In order to deal with the sparse and unstructured raw point clouds, LiDARbased 3D object detection research mostly focuses on designing dedicated localpoint aggregators for fine-grained geometrical modeling. In this paper, werevisit the local point aggregators from the perspective of allocatingcomputational resources. We find that the simplest pillar based models performsurprisingly well considering both accuracy and latency. Additionally, we showthat minimal adaptions from the success of 2D object detection, such asenlarging receptive field, significantly boost the performance. Extensiveexperiments reveal that our pillar based networks with modernized designs interms of architecture and training render the state-of-the-art performance onthe two popular benchmarks: Waymo Open Dataset and nuScenes. Our resultschallenge the common intuition that the detailed geometry modeling is essentialto achieve high performance for 3D object detection.$$$$$http://arxiv.org/pdf/2305.04925v1
RelPose++: Recovering 6D Poses from Sparse-view Observations$  We address the task of estimating 6D camera poses from sparse-view image sets(2-8 images). This task is a vital pre-processing stage for nearly allcontemporary (neural) reconstruction algorithms but remains challenging givensparse views, especially for objects with visual symmetries and texture-lesssurfaces. We build on the recent RelPose framework which learns a network thatinfers distributions over relative rotations over image pairs. We extend thisapproach in two key ways; first, we use attentional transformer layers toprocess multiple images jointly, since additional views of an object mayresolve ambiguous symmetries in any given image pair (such as the handle of amug that becomes visible in a third view). Second, we augment this network toalso report camera translations by defining an appropriate coordinate systemthat decouples the ambiguity in rotation estimation from translationprediction. Our final system results in large improvements in 6D poseprediction over prior art on both seen and unseen object categories and alsoenables pose estimation and 3D reconstruction for in-the-wild objects.$6D camera pose recovery, sparse-view, attention mechanism, transformer network$Carnegie Mellon University$文章提出了一个名为RelPose++的框架，可以从稀疏输入图像中恢复相应的6D相机旋转和平移。使用注意力转换网络处理多个图像，解决旋转估计中的歧义问题，还能够实现在野外对象的姿态估计和3D重建。$6D相机姿态恢复，稀疏视图，注意力机制，转换网络$http://arxiv.org/pdf/2305.04926v1
VCGAN: Video Colorization with Hybrid Generative Adversarial Network$  We propose a hybrid recurrent Video Colorization with Hybrid GenerativeAdversarial Network (VCGAN), an improved approach to video colorization usingend-to-end learning. The VCGAN addresses two prevalent issues in the videocolorization domain: Temporal consistency and unification of colorizationnetwork and refinement network into a single architecture. To enhancecolorization quality and spatiotemporal consistency, the mainstream ofgenerator in VCGAN is assisted by two additional networks, i.e., global featureextractor and placeholder feature extractor, respectively. The global featureextractor encodes the global semantics of grayscale input to enhancecolorization quality, whereas the placeholder feature extractor acts as afeedback connection to encode the semantics of the previous colorized frame inorder to maintain spatiotemporal consistency. If changing the input forplaceholder feature extractor as grayscale input, the hybrid VCGAN also has thepotential to perform image colorization. To improve the consistency of farframes, we propose a dense long-term loss that smooths the temporal disparityof every two remote frames. Trained with colorization and temporal lossesjointly, VCGAN strikes a good balance between color vividness and videocontinuity. Experimental results demonstrate that VCGAN produces higher-qualityand temporally more consistent colorful videos than existing approaches.$$$$$http://arxiv.org/pdf/2104.12357v2
Robust Pose Transfer with Dynamic Details using Neural Video Rendering$  Pose transfer of human videos aims to generate a high fidelity video of atarget person imitating actions of a source person. A few studies have madegreat progress either through image translation with deep latent features orneural rendering with explicit 3D features. However, both of them rely on largeamounts of training data to generate realistic results, and the performancedegrades on more accessible internet videos due to insufficient trainingframes. In this paper, we demonstrate that the dynamic details can be preservedeven trained from short monocular videos. Overall, we propose a neural videorendering framework coupled with an image-translation-based dynamic detailsgeneration network (D2G-Net), which fully utilizes both the stability ofexplicit 3D features and the capacity of learning components. To be specific, anovel texture representation is presented to encode both the static andpose-varying appearance characteristics, which is then mapped to the imagespace and rendered as a detail-rich frame in the neural rendering stage.Moreover, we introduce a concise temporal loss in the training stage tosuppress the detail flickering that is made more visible due to high-qualitydynamic details generated by our method. Through extensive comparisons, wedemonstrate that our neural human video renderer is capable of achieving bothclearer dynamic details and more robust performance even on accessible shortvideos with only 2k - 4k frames.$$$人体视频合成，姿态迁移，动态细节生成，深度生成模型，神经渲染$作者机构: Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Xverse; Tencent; School of Computer Science and Informatics, Cardiff University, Wales, UK$http://arxiv.org/pdf/2106.14132v3
TPC: Transformation-Specific Smoothing for Point Cloud Models$"  Point cloud models with neural network architectures have achieved greatsuccess and have been widely used in safety-critical applications, such asLidar-based recognition systems in autonomous vehicles. However, such modelsare shown vulnerable to adversarial attacks which aim to apply stealthysemantic transformations such as rotation and tapering to mislead modelpredictions. In this paper, we propose a transformation-specific smoothingframework TPC, which provides tight and scalable robustness guarantees forpoint cloud models against semantic transformation attacks. We first categorizecommon 3D transformations into three categories: additive (e.g., shearing),composable (e.g., rotation), and indirectly composable (e.g., tapering), and wepresent generic robustness certification strategies for all categoriesrespectively. We then specify unique certification protocols for a range ofspecific semantic transformations and their compositions. Extensive experimentson several common 3D transformations show that TPC significantly outperformsthe state of the art. For example, our framework boosts the certified accuracyagainst twisting transformation along z-axis (within 20$^\\circ$) from 20.3$\\%$to 83.8$\\%$. Codes and models are available athttps://github.com/chuwd19/Point-Cloud-Smoothing."$点云模型，神经网络结构，在自动驾驶车辆等安全关键应用中获得了巨大成功。然而，这类模型容易受到对抗性攻击的影响，这些攻击旨在对点云模型进行秘密的语义转换，如旋转和锥形。因此，文章提出了一种特定转换的平滑框架TPC，为点云模型提供紧密且可扩展的鲁棒性保证。文章首先将常用的3D转换分为三类，并针对不同类别提供了普适的鲁棒性认证策略。随后，针对各种语义转换和它们的组合提出了独特的鲁棒性认证协议。最后，在常见的3D转换上进行的广泛实验表明，TPC明显优于现有技术。$Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, P. R. China & University of Illinois Urbana-Champaign (UIUC), Illinois, USA$$$http://arxiv.org/pdf/2201.12733v5
Blind Image Deconvolution Using Variational Deep Image Prior$  Conventional deconvolution methods utilize hand-crafted image priors toconstrain the optimization. While deep-learning-based methods have simplifiedthe optimization by end-to-end training, they fail to generalize well to blursunseen in the training dataset. Thus, training image-specific models isimportant for higher generalization. Deep image prior (DIP) provides anapproach to optimize the weights of a randomly initialized network with asingle degraded image by maximum a posteriori (MAP), which shows that thearchitecture of a network can serve as the hand-crafted image prior. Differentfrom the conventional hand-crafted image priors that are statisticallyobtained, it is hard to find a proper network architecture because therelationship between images and their corresponding network architectures isunclear. As a result, the network architecture cannot provide enough constraintfor the latent sharp image. This paper proposes a new variational deep imageprior (VDIP) for blind image deconvolution, which exploits additivehand-crafted image priors on latent sharp images and approximates adistribution for each pixel to avoid suboptimal solutions. Our mathematicalanalysis shows that the proposed method can better constrain the optimization.The experimental results further demonstrate that the generated images havebetter quality than that of the original DIP on benchmark datasets. The sourcecode of our VDIP is available athttps://github.com/Dong-Huo/VDIP-Deconvolution.$$$$$http://arxiv.org/pdf/2202.00179v2
Blind2Unblind: Self-Supervised Image Denoising with Visible Blind Spots$  Real noisy-clean pairs on a large scale are costly and difficult to obtain.Meanwhile, supervised denoisers trained on synthetic data perform poorly inpractice. Self-supervised denoisers, which learn only from single noisy images,solve the data collection problem. However, self-supervised denoising methods,especially blindspot-driven ones, suffer sizable information loss during inputor network design. The absence of valuable information dramatically reduces theupper bound of denoising performance. In this paper, we propose a simple yetefficient approach called Blind2Unblind to overcome the information loss inblindspot-driven denoising methods. First, we introduce a global-aware maskmapper that enables global perception and accelerates training. The mask mappersamples all pixels at blind spots on denoised volumes and maps them to the samechannel, allowing the loss function to optimize all blind spots at once.Second, we propose a re-visible loss to train the denoising network and makeblind spots visible. The denoiser can learn directly from raw noise imageswithout losing information or being trapped in identity mapping. We alsotheoretically analyze the convergence of the re-visible loss. Extensiveexperiments on synthetic and real-world datasets demonstrate the superiorperformance of our approach compared to previous work. Code is available athttps://github.com/demonsjin/Blind2Unblind.$$$$$http://arxiv.org/pdf/2203.06967v3
Provable Defense Against Geometric Transformations$"  Geometric image transformations that arise in the real world, such as scalingand rotation, have been shown to easily deceive deep neural networks (DNNs).Hence, training DNNs to be certifiably robust to these perturbations iscritical. However, no prior work has been able to incorporate the objective ofdeterministic certified robustness against geometric transformations into thetraining procedure, as existing verifiers are exceedingly slow. To addressthese challenges, we propose the first provable defense for deterministiccertified geometric robustness. Our framework leverages a novel GPU-optimizedverifier that can certify images between 60$\\times$ to 42,600$\\times$ fasterthan existing geometric robustness verifiers, and thus unlike existing works,is fast enough for use in training. Across multiple datasets, our results showthat networks trained via our framework consistently achieve state-of-the-artdeterministic certified geometric robustness and clean accuracy. Furthermore,for the first time, we verify the geometric robustness of a neural network forthe challenging, real-world setting of autonomous driving."$$$$$http://arxiv.org/pdf/2207.11177v3
A knee cannot have lung disease: out-of-distribution detection with  in-distribution voting using the medical example of chest X-ray  classification$  To investigate the impact of OOD radiographs on existing chest X-rayclassification models and to increase their robustness against OOD data. Thestudy employed the commonly used chest X-ray classification model, CheXnet,trained on the chest X-ray 14 data set, and tested its robustness against OODdata using three public radiography data sets: IRMA, Bone Age, and MURA, andthe ImageNet data set. To detect OOD data for multi-label classification, weproposed in-distribution voting (IDV). The OOD detection performance ismeasured across data sets using the area under the receiver operatingcharacteristic curve (AUC) analysis and compared with Mahalanobis-based OODdetection, MaxLogit, MaxEnergy and self-supervised OOD detection (SS OOD).Without additional OOD detection, the chest X-ray classifier failed to discardany OOD images, with an AUC of 0.5. The proposed IDV approach trained on ID(chest X-ray 14) and OOD data (IRMA and ImageNet) achieved, on average, 0.999OOD AUC across the three data sets, surpassing all other OOD detection methods.Mahalanobis-based OOD detection achieved an average OOD detection AUC of 0.982.IDV trained solely with a few thousand ImageNet images had an AUC 0.913, whichwas higher than MaxLogit (0.726), MaxEnergy (0.724), and SS OOD (0.476). Theperformance of all tested OOD detection methods did not translate well toradiography data sets, except Mahalanobis-based OOD detection and the proposedIDV method. Training solely on ID data led to incorrect classification of OODimages as ID, resulting in increased false positive rates. IDV substantiallyimproved the model\'s ID classification performance, even when trained with datathat will not occur in the intended use case or test set, without additionalinference overhead.$$$$$http://arxiv.org/pdf/2208.01077v2
Persuasion Strategies in Advertisements$  Modeling what makes an advertisement persuasive, i.e., eliciting the desiredresponse from consumer, is critical to the study of propaganda, socialpsychology, and marketing. Despite its importance, computational modeling ofpersuasion in computer vision is still in its infancy, primarily due to thelack of benchmark datasets that can provide persuasion-strategy labelsassociated with ads. Motivated by persuasion literature in social psychologyand marketing, we introduce an extensive vocabulary of persuasion strategiesand build the first ad image corpus annotated with persuasion strategies. Wethen formulate the task of persuasion strategy prediction with multi-modallearning, where we design a multi-task attention fusion model that can leverageother ad-understanding tasks to predict persuasion strategies. Further, weconduct a real-world case study on 1600 advertising campaigns of 30 Fortune-500companies where we use our model\'s predictions to analyze which strategies workwith different demographics (age and gender). The dataset also provides imagesegmentation masks, which labels persuasion strategies in the corresponding adimages on the test split. We publicly release our code and datasethttps://midas-research.github.io/persuasion-advertisements/.$Advertisement, Persuasion, Social Psychology, Marketing, Computer Vision  $1IIIT-Delhi, 2Adobe Media and Data Science Research (MDSR), 3University at Buffalo$本文主要介绍了如何计算广告中的说服策略，从而诱导消费者做出所期待的反应。文章提出了一个广告图像语料库，用来注释说服策略，同时提出了使用多模态学习的多任务注意力融合模型来预测说服策略。此外，数据集还提供了图像分割掩码，用于在测试集中标记相应广告图像中的说服策略。$广告、推销、社会心理学、市场营销、计算机视觉$http://arxiv.org/pdf/2208.09626v2
DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic  Models$  Diffusion probabilistic models (DPMs) have achieved impressive success inhigh-resolution image synthesis, especially in recent large-scale text-to-imagegeneration applications. An essential technique for improving the samplequality of DPMs is guided sampling, which usually needs a large guidance scaleto obtain the best sample quality. The commonly-used fast sampler for guidedsampling is DDIM, a first-order diffusion ODE solver that generally needs 100to 250 steps for high-quality samples. Although recent works propose dedicatedhigh-order solvers and achieve a further speedup for sampling without guidance,their effectiveness for guided sampling has not been well-tested before. Inthis work, we demonstrate that previous high-order fast samplers suffer frominstability issues, and they even become slower than DDIM when the guidancescale grows large. To further speed up guided sampling, we proposeDPM-Solver++, a high-order solver for the guided sampling of DPMs. DPM-Solver++solves the diffusion ODE with the data prediction model and adopts thresholdingmethods to keep the solution matches training data distribution. We furtherpropose a multistep variant of DPM-Solver++ to address the instability issue byreducing the effective step size. Experiments show that DPM-Solver++ cangenerate high-quality samples within only 15 to 20 steps for guided sampling bypixel-space and latent-space DPMs.$$$$$http://arxiv.org/pdf/2211.01095v2
YoloCurvSeg: You Only Label One Noisy Skeleton for Vessel-style  Curvilinear Structure Segmentation$  Weakly-supervised learning (WSL) has been proposed to alleviate the conflictbetween data annotation cost and model performance through employingsparsely-grained (i.e., point-, box-, scribble-wise) supervision and has shownpromising performance, particularly in the image segmentation field. However,it is still a very challenging task due to the limited supervision, especiallywhen only a small number of labeled samples are available. Additionally, almostall existing WSL segmentation methods are designed for star-convex structureswhich are very different from curvilinear structures such as vessels andnerves. In this paper, we propose a novel sparsely annotated segmentationframework for curvilinear structures, named YoloCurvSeg. A very essentialcomponent of YoloCurvSeg is image synthesis. Specifically, a backgroundgenerator delivers image backgrounds that closely match the real distributionsthrough inpainting dilated skeletons. The extracted backgrounds are thencombined with randomly emulated curves generated by a Space ColonizationAlgorithm-based foreground generator and through a multilayer patch-wisecontrastive learning synthesizer. In this way, a synthetic dataset with bothimages and curve segmentation labels is obtained, at the cost of only one or afew noisy skeleton annotations. Finally, a segmenter is trained with thegenerated dataset and possibly an unlabeled dataset. The proposed YoloCurvSegis evaluated on four publicly available datasets (OCTA500, CORN, DRIVE andCHASEDB1) and the results show that YoloCurvSeg outperforms state-of-the-artWSL segmentation methods by large margins. With only one noisy skeletonannotation (respectively 0.14%, 0.03%, 1.40%, and 0.65% of the fullannotation), YoloCurvSeg achieves more than 97% of the fully-supervisedperformance on each dataset. Code and datasets will be released athttps://github.com/llmir/YoloCurvSeg.$$$$$http://arxiv.org/pdf/2212.05566v4
BKinD-3D: Self-Supervised 3D Keypoint Discovery from Multi-View Videos$  Quantifying motion in 3D is important for studying the behavior of humans andother animals, but manual pose annotations are expensive and time-consuming toobtain. Self-supervised keypoint discovery is a promising strategy forestimating 3D poses without annotations. However, current keypoint discoveryapproaches commonly process single 2D views and do not operate in the 3D space.We propose a new method to perform self-supervised keypoint discovery in 3Dfrom multi-view videos of behaving agents, without any keypoint or bounding boxsupervision in 2D or 3D. Our method, BKinD-3D, uses an encoder-decoderarchitecture with a 3D volumetric heatmap, trained to reconstructspatiotemporal differences across multiple views, in addition to joint lengthconstraints on a learned 3D skeleton of the subject. In this way, we discoverkeypoints without requiring manual supervision in videos of humans and rats,demonstrating the potential of 3D keypoint discovery for studying behavior.$$$$$http://arxiv.org/pdf/2212.07401v2
From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language  Models$  Large language models (LLMs) have demonstrated excellent zero-shotgeneralization to new language tasks. However, effective utilization of LLMsfor zero-shot visual question-answering (VQA) remains challenging, primarilydue to the modality disconnection and task disconnection between LLM and VQAtask. End-to-end training on vision and language data may bridge thedisconnections, but is inflexible and computationally expensive. To addressthis issue, we propose \\emph{Img2Prompt}, a plug-and-play module that providesthe prompts that can bridge the aforementioned modality and taskdisconnections, so that LLMs can perform zero-shot VQA tasks without end-to-endtraining. In order to provide such prompts, we further employ LLM-agnosticmodels to provide prompts that can describe image content and self-constructedquestion-answer pairs, which can effectively guide LLM to perform zero-shot VQAtasks. Img2Prompt offers the following benefits: 1) It can flexibly work withvarious LLMs to perform VQA. 2)~Without the needing of end-to-end training, itsignificantly reduces the cost of deploying LLM for zero-shot VQA tasks. 3) Itachieves comparable or better performance than methods relying on end-to-endtraining. For example, we outperform Flamingo \\cite{Deepmind:Flamingo2022} by5.6\\% on VQAv2. On the challenging A-OKVQA dataset, our method even outperformsfew-shot methods by as much as 20\\%.$$$$$http://arxiv.org/pdf/2212.10846v3
Dual PatchNorm$  We propose Dual PatchNorm: two Layer Normalization layers (LayerNorms),before and after the patch embedding layer in Vision Transformers. Wedemonstrate that Dual PatchNorm outperforms the result of exhaustive search foralternative LayerNorm placement strategies in the Transformer block itself. Inour experiments, incorporating this trivial modification, often leads toimproved accuracy over well-tuned Vision Transformers and never hurts.$$$$$http://arxiv.org/pdf/2302.01327v3
2D-Empowered Point Cloud Analytics on the Edge$  3D object detection plays a pivotal role in many applications, most notablyautonomous driving and robotics. These applications are commonly deployed onedge devices to promptly interact with the environment, and often require nearreal-time response. With limited computation power, it is challenging toexecute 3D detection on the edge using highly complex neural networks. Commonapproaches such as offloading to the cloud induce significant latency overheadsdue to the large amount of point cloud data during transmission. To resolve thetension between wimpy edge devices and compute-intensive inference workloads,we explore the possibility of empowering fast 2D detection to extrapolate 3Dbounding boxes. To this end, we present Moby, a novel system that demonstratesthe feasibility and potential of our approach. We design a transformationpipeline for Moby that generates 3D bounding boxes efficiently and accuratelybased on 2D detection results without running 3D detectors. Further, we devisea frame offloading scheduler that decides when to launch the 3D detectorjudiciously in the cloud to avoid the errors from accumulating. Extensiveevaluations on NVIDIA Jetson TX2 with real-world autonomous driving datasetsdemonstrate that Moby offers up to 91.9% latency improvement with modestaccuracy loss over state of the art.$YU MAO, City University of Hong Kong, China$YIK HONG CAI, The Chinese University of Hong Kong, China$CHUN JASON XUE, City University of Hong Kong, China$LIBIN LIU, Zhongguancun Laboratory, China$http://arxiv.org/pdf/2302.09221v2
Mover: Mask and Recovery based Facial Part Consistency Aware Method for  Deepfake Video Detection$  Deepfake techniques have been widely used for malicious purposes, promptingextensive research interest in developing Deepfake detection methods. Deepfakemanipulations typically involve tampering with facial parts, which can resultin inconsistencies across different parts of the face. For instance, Deepfaketechniques may change smiling lips to an upset lip, while the eyes remainsmiling. Existing detection methods depend on specific indicators of forgery,which tend to disappear as the forgery patterns are improved. To address thelimitation, we propose Mover, a new Deepfake detection model that exploitsunspecific facial part inconsistencies, which are inevitable weaknesses ofDeepfake videos. Mover randomly masks regions of interest (ROIs) and recoversfaces to learn unspecific features, which makes it difficult for fake faces tobe recovered, while real faces can be easily recovered. Specifically, given areal face image, we first pretrain a masked autoencoder to learn facial partconsistency by dividing faces into three parts and randomly masking ROIs, whichare then recovered based on the unmasked facial parts. Furthermore, to maximizethe discrepancy between real and fake videos, we propose a novel model withdual networks that utilize the pretrained encoder and masked autoencoder,respectively. 1) The pretrained encoder is finetuned for capturing the encodingof inconsistent information in the given video. 2) The pretrained maskedautoencoder is utilized for mapping faces and distinguishing real and fakevideos. Our extensive experiments on standard benchmarks demonstrate that Moveris highly effective.$$$$$http://arxiv.org/pdf/2303.01740v2
Refined Vision-Language Modeling for Fine-grained Multi-modal  Pre-training$  Fine-grained supervision based on object annotations has been widely used forvision and language pre-training (VLP). However, in real-world applicationscenarios, aligned multi-modal data is usually in the image-caption format,which only provides coarse-grained supervision. It is not only cost-expensivebut also compute-expensive to collect object annotations and build objectannotation pre-extractor for different scenarios. In this paper, we propose afine-grained VLP scheme without object annotations from the linguisticperspective. First, we propose a homonym sentence rewriting (HSR) algorithm toprovide token-level supervision. The algorithm replaces averb/noun/adjective/quantifier word of the caption with its homonyms fromWordNet. Correspondingly, we propose refined vision-language modeling (RVLM)framework to exploit the token-level supervision. Three refined tasks, i.e.,refined image-text contrastive (RITC), refined image-text matching (RITM), andreplace language modeling (RLM) are proposed to learn the fine-grainedalignment. Extensive experiments on several downstream tasks demonstrate thesuperior performance of the proposed method.$$$$$http://arxiv.org/pdf/2303.05313v2
SVD-DIP: Overcoming the Overfitting Problem in DIP-based CT  Reconstruction$"  The deep image prior (DIP) is a well-established unsupervised deep learningmethod for image reconstruction; yet it is far from being flawless. The DIPoverfits to noise if not early stopped, or optimized via a regularizedobjective. We build on the regularized fine-tuning of a pretrained DIP, byadopting a novel strategy that restricts the learning to the adaptation ofsingular values. The proposed SVD-DIP uses ad hoc convolutional layers whosepretrained parameters are decomposed via the singular value decomposition.Optimizing the DIP then solely consists in the fine-tuning of the singularvalues, while keeping the left and right singular vectors fixed. We thoroughlyvalidate the proposed method on real-measured $\\mu$CT data of a lotus root aswell as two medical datasets (LoDoPaB and Mayo). We report significantlyimproved stability of the DIP optimization, by overcoming the overfitting tonoise."$Unknown$Unknown$Unknown$Unknown$http://arxiv.org/pdf/2303.15748v2
Domain Generalization for Mammographic Image Analysis via Contrastive  Learning$  Mammographic image analysis is a fundamental problem in the computer-aideddiagnosis scheme, which has recently made remarkable progress with the advanceof deep learning. However, the construction of a deep learning model requirestraining data that are large and sufficiently diverse in terms of image styleand quality. In particular, the diversity of image style may be majorlyattributed to the vendor factor. However, mammogram collection from vendors asmany as possible is very expensive and sometimes impractical forlaboratory-scale studies. Accordingly, to further augment the generalizationcapability of deep learning models to various vendors with limited resources, anew contrastive learning scheme is developed. Specifically, the backbonenetwork is firstly trained with a multi-style and multi-view unsupervisedself-learning scheme for the embedding of invariant features to various vendorstyles. Afterward, the backbone network is then recalibrated to the downstreamtasks of mass detection, multi-view mass matching, BI-RADS classification andbreast density classification with specific supervised learning. The proposedmethod is evaluated with mammograms from four vendors and two unseen publicdatasets. The experimental results suggest that our approach can effectivelyimprove analysis performance on both seen and unseen domains, and outperformsmany state-of-the-art (SOTA) generalization methods.$$$$$http://arxiv.org/pdf/2304.10226v2
Exploiting the Distortion-Semantic Interaction in Fisheye Data$  In this work, we present a methodology to shape a fisheye-specificrepresentation space that reflects the interaction between distortion andsemantic context present in this data modality. Fisheye data has the widerfield of view advantage over other types of cameras, but this comes at theexpense of high radial distortion. As a result, objects further from the centerexhibit deformations that make it difficult for a model to identify theirsemantic context. While previous work has attempted architectural and trainingaugmentation changes to alleviate this effect, no work has attempted to guidethe model towards learning a representation space that reflects thisinteraction between distortion and semantic context inherent to fisheye data.We introduce an approach to exploit this relationship by first extractingdistortion class labels based on an object\'s distance from the center of theimage. We then shape a backbone\'s representation space with a weightedcontrastive loss that constrains objects of the same semantic class anddistortion class to be close to each other within a lower dimensional embeddingspace. This backbone trained with both semantic and distortion information isthen fine-tuned within an object detection setting to empirically evaluate thequality of the learnt representation. We show this method leads to performanceimprovements by as much as 1.1% mean average precision over standard objectdetection strategies and .6% improvement over other state of the artrepresentation learning approaches.$Distortion, Semantic Interaction, Fisheye Data$Georgia Institute of Technology, USA$本文研究了扭曲数据的语义交互作用并提出了一种新的修复和分析方法。$扭曲数据的修复和分析$http://arxiv.org/pdf/2305.00079v2
Attack-SAM: Towards Attacking Segment Anything Model With Adversarial  Examples$  Segment Anything Model (SAM) has attracted significant attention recently,due to its impressive performance on various downstream tasks in a zero-shortmanner. Computer vision (CV) area might follow the natural language processing(NLP) area to embark on a path from task-specific vision models towardfoundation models. However, deep vision models are widely recognized asvulnerable to adversarial examples, which fool the model to make wrongpredictions with imperceptible perturbation. Such vulnerability to adversarialattacks causes serious concerns when applying deep models to security-sensitiveapplications. Therefore, it is critical to know whether the vision foundationmodel SAM can also be fooled by adversarial attacks. To the best of ourknowledge, our work is the first of its kind to conduct a comprehensiveinvestigation on how to attack SAM with adversarial examples. With the basicattack goal set to mask removal, we investigate the adversarial robustness ofSAM in the full white-box setting and transfer-based black-box settings. Beyondthe basic goal of mask removal, we further investigate and find that it ispossible to generate any desired mask by the adversarial attack.$$$$$http://arxiv.org/pdf/2305.00866v2
Hamming Similarity and Graph Laplacians for Class Partitioning and  Adversarial Image Detection$  Researchers typically investigate neural network representations by examiningactivation outputs for one or more layers of a network. Here, we investigatethe potential for ReLU activation patterns (encoded as bit vectors) to aid inunderstanding and interpreting the behavior of neural networks. We utilizeRepresentational Dissimilarity Matrices (RDMs) to investigate the coherence ofdata within the embedding spaces of a deep neural network. From each layer of anetwork, we extract and utilize bit vectors to construct similarity scoresbetween images. From these similarity scores, we build a similarity matrix fora collection of images drawn from 2 classes. We then apply Fiedler partitioningto the associated Laplacian matrix to separate the classes. Our resultsindicate, through bit vector representations, that the network continues torefine class detectability with the last ReLU layer achieving better than 95\\%separation accuracy. Additionally, we demonstrate that bit vectors aid inadversarial image detection, again achieving over 95\\% accuracy in separatingadversarial and non-adversarial images using a simple classifier.$neural networks, pattern analysis, similarity, graph Laplacian, adversarial attacks$1Computer Science Department, Colorado State University, Fort Collins, CO, USA；2Mathematics Department, Colorado State University, Fort Collins, CO, USA$文章通过研究ReLU激活模式编码的比特向量在理解神经网络行为方面的潜力，使用表示不相似矩阵和Fiedler分区技术来分析深度神经网络中存在的较低维嵌入空间的数据相干性，并探索了使用比特向量来帮助对抗攻击的可行性。$神经网络、模式分析、相似度、图拉普拉斯矩阵、对抗攻击$http://arxiv.org/pdf/2305.01808v2
Using Spatio-Temporal Dual-Stream Network with Self-Supervised Learning  for Lung Tumor Classification on Radial Probe Endobronchial Ultrasound Video$  The purpose of this study is to develop a computer-aided diagnosis system forclassifying benign and malignant lung lesions, and to assist physicians inreal-time analysis of radial probe endobronchial ultrasound (EBUS) videos.During the biopsy process of lung cancer, physicians use real-time ultrasoundimages to find suitable lesion locations for sampling. However, most of theseimages are difficult to classify and contain a lot of noise. Previous studieshave employed 2D convolutional neural networks to effectively differentiatebetween benign and malignant lung lesions, but doctors still need to manuallyselect good-quality images, which can result in additional labor costs. Inaddition, the 2D neural network has no ability to capture the temporalinformation of the ultrasound video, so it is difficult to obtain therelationship between the features of the continuous images. This study designsan automatic diagnosis system based on a 3D neural network, uses the SlowFastarchitecture as the backbone to fuse temporal and spatial features, and usesthe SwAV method of contrastive learning to enhance the noise robustness of themodel. The method we propose includes the following advantages, such as (1)using clinical ultrasound films as model input, thereby reducing the need forhigh-quality image selection by physicians, (2) high-accuracy classification ofbenign and malignant lung lesions can assist doctors in clinical diagnosis andreduce the time and risk of surgery, and (3) the capability to classify welleven in the presence of significant image noise. The AUC, accuracy, precision,recall and specificity of our proposed method on the validation set reached0.87, 83.87%, 86.96%, 90.91% and 66.67%, respectively. The results haveverified the importance of incorporating temporal information and theeffectiveness of using the method of contrastive learning on featureextraction.$$$$$http://arxiv.org/pdf/2305.02719v2
OctFormer: Octree-based Transformers for 3D Point Clouds$  We propose octree-based transformers, named OctFormer, for 3D point cloudlearning. OctFormer can not only serve as a general and effective backbone for3D point cloud segmentation and object detection but also have linearcomplexity and is scalable for large-scale point clouds. The key challenge inapplying transformers to point clouds is reducing the quadratic, thusoverwhelming, computation complexity of attentions. To combat this issue,several works divide point clouds into non-overlapping windows and constrainattentions in each local window. However, the point number in each windowvaries greatly, impeding the efficient execution on GPU. Observing thatattentions are robust to the shapes of local windows, we propose a novel octreeattention, which leverages sorted shuffled keys of octrees to partition pointclouds into local windows containing a fixed number of points while permittingshapes of windows to change freely. And we also introduce dilated octreeattention to expand the receptive field further. Our octree attention can beimplemented in 10 lines of code with open-sourced libraries and runs 17 timesfaster than other point cloud attentions when the point number exceeds 200k.Built upon the octree attention, OctFormer can be easily scaled up and achievesstate-of-the-art performances on a series of 3D segmentation and detectionbenchmarks, surpassing previous sparse-voxel-based CNNs and point cloudtransformers in terms of both efficiency and effectiveness. Notably, on thechallenging ScanNet200 dataset, OctFormer outperforms sparse-voxel-based CNNsby 7.3 in mIoU. Our code and trained models are available athttps://wang-ps.github.io/octformer.$$$$$http://arxiv.org/pdf/2305.03045v2
Clothes Grasping and Unfolding Based on RGB-D Semantic Segmentation$  Clothes grasping and unfolding is a core step in robotic-assisted dressing.Most existing works leverage depth images of clothes to train a deeplearning-based model to recognize suitable grasping points. These methods oftenutilize physics engines to synthesize depth images to reduce the cost of reallabeled data collection. However, the natural domain gap between synthetic andreal images often leads to poor performance of these methods on real data.Furthermore, these approaches often struggle in scenarios where grasping pointsare occluded by the clothing item itself. To address the above challenges, wepropose a novel Bi-directional Fractal Cross Fusion Network (BiFCNet) forsemantic segmentation, enabling recognition of graspable regions in order toprovide more possibilities for grasping. Instead of using depth images only, wealso utilize RGB images with rich color features as input to our network inwhich the Fractal Cross Fusion (FCF) module fuses RGB and depth data byconsidering global complex features based on fractal geometry. To reduce thecost of real data collection, we further propose a data augmentation methodbased on an adversarial strategy, in which the color and geometrictransformations simultaneously process RGB and depth data while maintaining thelabel correspondence. Finally, we present a pipeline for clothes grasping andunfolding from the perspective of semantic segmentation, through the additionof a strategy for grasp point selection from segmentation regions based onclothing flatness measures, while taking into account the grasping direction.We evaluate our BiFCNet on the public dataset NYUDv2 and obtained comparableperformance to current state-of-the-art models. We also deploy our model on aBaxter robot, running extensive grasping and unfolding experiments as part ofour ablation studies, achieving an 84% success rate.$$$$$http://arxiv.org/pdf/2305.03259v2
Physics-based network fine-tuning for robust quantitative susceptibility  mapping from high-pass filtered phase$  Purpose: To improve the generalization ability of convolutional neuralnetwork (CNN) based prediction of quantitative susceptibility mapping (QSM)from high-pass filtered phase (HPFP) image. Methods: The proposed networkaddresses two common generalization issues that arise when using a pre-trainednetwork to predict QSM from HPFP: a) data with unseen voxel sizes, and b) datawith unknown high-pass filter parameters. A network fine-tuning step based on ahigh-pass filtering dipole convolution forward model is proposed to reduce thegeneralization error of the pre-trained network. A progressive Unetarchitecture is proposed to improve prediction accuracy without increasingfine-tuning computational cost. Results: In retrospective studies using RMSE,PSNR, SSIM and HFEN as quality metrics, the performance of both Unet andprogressive Unet was improved after physics-based fine-tuning at all voxelsizes and most high-pass filtering cutoff frequencies tested in the experiment.Progressive Unet slightly outperformed Unet both before and after fine-tuning.In a prospective study, image sharpness was improved after physics-basedfine-tuning for both Unet and progressive Unet. Compared to Unet, progressiveUnet had better agreement of regional susceptibility values with reference QSM.Conclusion: The proposed method shows improved robustness compared to thepre-trained network without fine-tuning when the test dataset deviates fromtraining. Our code is available at https://github.com/Jinwei1209/SWI_to_QSM/$$$$$http://arxiv.org/pdf/2305.03844v1
White Matter Hyperintensities Segmentation Using Probabilistic TransUNet$  White Matter Hyperintensities (WMH) are areas of the brain that have higherintensity than other normal brain regions on Magnetic Resonance Imaging (MRI)scans. WMH is often associated with small vessel disease in the brain, makingearly detection of WMH important. However, there are two common issues in thedetection of WMH: high ambiguity and difficulty in detecting small WMH. In thisstudy, we propose a method called Probabilistic TransUNet to address theprecision of small object segmentation and the high ambiguity of medicalimages. To measure model performance, we conducted a k-fold cross validationand cross dataset robustness experiment. Based on the experiments, the additionof a probabilistic model and the use of a transformer-based approach were ableto achieve better performance.$$$基于概率TransUNet的白质高信号区分割 $Faculty of Computer Science, University of Indonesia, Depok, Indonesia; Brain Image Analysis Unit, RIKEN Center for Brain Science, Wako-shi, Japan $http://arxiv.org/pdf/2305.03912v1
Adaptive loose optimization for robust question answering$  Question answering methods are well-known for leveraging data bias, such asthe language prior in visual question answering and the position bias inmachine reading comprehension (extractive question answering). Currentdebiasing methods often come at the cost of significant in-distributionperformance to achieve favorable out-of-distribution generalizability, whilenon-debiasing methods sacrifice a considerable amount of out-of-distributionperformance in order to obtain high in-distribution performance. Therefore, itis challenging for them to deal with the complicated changing real-worldsituations. In this paper, we propose a simple yet effective novel lossfunction with adaptive loose optimization, which seeks to make the best of bothworlds for question answering. Our main technical contribution is to reduce theloss adaptively according to the ratio between the previous and currentoptimization state on mini-batch training data. This loose optimization can beused to prevent non-debiasing methods from overlearning data bias whileenabling debiasing methods to maintain slight bias learning. Experiments on thevisual question answering datasets, including VQA v2, VQA-CP v1, VQA-CP v2,GQA-OOD, and the extractive question answering dataset SQuAD demonstrate thatour approach enables QA methods to obtain state-of-the-art in- andout-of-distribution performance in most cases. The source code has beenreleased publicly in \\url{https://github.com/reml-group/ALO}.$$$$$http://arxiv.org/pdf/2305.03971v1
Towards Prompt-robust Face Privacy Protection via Adversarial Decoupling  Augmentation Framework$  Denoising diffusion models have shown remarkable potential in variousgeneration tasks. The open-source large-scale text-to-image model, StableDiffusion, becomes prevalent as it can generate realistic artistic or facialimages with personalization through fine-tuning on a limited number of newsamples. However, this has raised privacy concerns as adversaries can acquirefacial images online and fine-tune text-to-image models for malicious editing,leading to baseless scandals, defamation, and disruption to victims\' lives.Prior research efforts have focused on deriving adversarial loss fromconventional training processes for facial privacy protection throughadversarial perturbations. However, existing algorithms face two issues: 1)they neglect the image-text fusion module, which is the vital module oftext-to-image diffusion models, and 2) their defensive performance is unstableagainst different attacker prompts. In this paper, we propose the AdversarialDecoupling Augmentation Framework (ADAF), addressing these issues by targetingthe image-text fusion module to enhance the defensive performance of facialprivacy protection algorithms. ADAF introduces multi-level text-relatedaugmentations for defense stability against various attacker prompts.Concretely, considering the vision, text, and common unit space, we proposeVision-Adversarial Loss, Prompt-Robust Augmentation, and Attention-DecouplingLoss. Extensive experiments on CelebA-HQ and VGGFace2 demonstrate ADAF\'spromising performance, surpassing existing algorithms.$$$$$http://arxiv.org/pdf/2305.03980v1
Unlocking Low-Light-Rainy Image Restoration by Pairwise Degradation  Feature Vector Guidance$"  Rain in the dark is a common natural phenomenon. Photos captured in such acondition significantly impact the performance of various nighttime activities,such as autonomous driving, surveillance systems, and night photography. Whileexisting methods designed for low-light enhancement or deraining show promisingperformance, they have limitations in simultaneously addressing the task ofbrightening low light and removing rain. Furthermore, using a cascade approach,such as ``deraining followed by low-light enhancement\'\' or vice versa, may leadto difficult-to-handle rain patterns or excessively blurred and overexposedimages. To overcome these limitations, we propose an end-to-end network called$L^{2}RIRNet$ which can jointly handle low-light enhancement and deraining. Ournetwork mainly includes a Pairwise Degradation Feature Vector ExtractionNetwork (P-Net) and a Restoration Network (R-Net). P-Net can learn degradationfeature vectors on the dark and light areas separately, using contrastivelearning to guide the image restoration process. The R-Net is responsible forrestoring the image. We also introduce an effective Fast Fourier - ResNetDetail Guidance Module (FFR-DG) that initially guides image restoration usingdetail image that do not contain degradation information but focus on texturedetail information. Additionally, we contribute a dataset containing syntheticand real-world low-light-rainy images. Extensive experiments demonstrate thatour $L^{2}RIRNet$ outperforms existing methods in both synthetic and complexreal-world scenarios."$$$$$http://arxiv.org/pdf/2305.03997v1
AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion$  Recent advances in diffusion models have showcased promising results in thetext-to-video (T2V) synthesis task. However, as these T2V models solely employtext as the guidance, they tend to struggle in modeling detailed temporaldynamics. In this paper, we introduce a novel T2V framework that additionallyemploy audio signals to control the temporal dynamics, empowering anoff-the-shelf T2I diffusion to generate audio-aligned videos. We proposeaudio-based regional editing and signal smoothing to strike a good balancebetween the two contradicting desiderata of video synthesis, i.e., temporalflexibility and coherence. We empirically demonstrate the effectiveness of ourmethod through experiments, and further present practical applications forcontents creation.$$$$$http://arxiv.org/pdf/2305.04001v1
Degradation-Noise-Aware Deep Unfolding Transformer for Hyperspectral  Image Denoising$  Hyperspectral imaging (HI) has emerged as a powerful tool in diverse fieldssuch as medical diagnosis, industrial inspection, and agriculture, owing to itsability to detect subtle differences in physical properties through highspectral resolution. However, hyperspectral images (HSIs) are often quite noisybecause of narrow band spectral filtering. To reduce the noise in HSI datacubes, both model-driven and learning-based denoising algorithms have beenproposed. However, model-based approaches rely on hand-crafted priors andhyperparameters, while learning-based methods are incapable of estimating theinherent degradation patterns and noise distributions in the imaging procedure,which could inform supervised learning. Secondly, learning-based algorithmspredominantly rely on CNN and fail to capture long-range dependencies,resulting in limited interpretability. This paper proposes aDegradation-Noise-Aware Unfolding Network (DNA-Net) that addresses theseissues. Firstly, DNA-Net models sparse noise, Gaussian noise, and explicitlyrepresent image prior using transformer. Then the model is unfolded into anend-to-end network, the hyperparameters within the model are estimated from thenoisy HSI and degradation model and utilizes them to control each iteration.Additionally, we introduce a novel U-Shaped Local-Non-local-SpectralTransformer (U-LNSA) that captures spectral correlation, local contents, andnon-local dependencies simultaneously. By integrating U-LNSA into DNA-Net, wepresent the first Transformer-based deep unfolding HSI denoising method.Experimental results show that DNA-Net outperforms state-of-the-art methods,and the modeling of noise distributions helps in cases with heavy noise.$$$$$http://arxiv.org/pdf/2305.04047v1
SST-ReversibleNet: Reversible-prior-based Spectral-Spatial Transformer  for Efficient Hyperspectral Image Reconstruction$  Spectral image reconstruction is an important task in snapshot compressedimaging. This paper aims to propose a new end-to-end framework with iterativecapabilities similar to a deep unfolding network to improve reconstructionaccuracy, independent of optimization conditions, and to reduce the number ofparameters. A novel framework called the reversible-prior-based method isproposed. Inspired by the reversibility of the optical path, thereversible-prior-based framework projects the reconstructions back into themeasurement space, and then the residuals between the projected data and thereal measurements are fed into the network for iteration. The reconstructionsubnet in the network then learns the mapping of the residuals to the truevalues to improve reconstruction accuracy. In addition, a novelspectral-spatial transformer is proposed to account for the global correlationof spectral data in both spatial and spectral dimensions while balancingnetwork depth and computational complexity, in response to the shortcomings ofexisting transformer-based denoising modules that ignore spatial texturefeatures or learn local spatial features at the expense of global spatialfeatures. Extensive experiments show that our SST-ReversibleNet significantlyoutperforms state-of-the-art methods on simulated and real HSI datasets, whilerequiring lower computational and storage costs.https://github.com/caizeyu1992/SST$$$$$http://arxiv.org/pdf/2305.04054v1
SynthMix: Mixing up Aligned Synthesis for Medical Cross-Modality Domain  Adaptation$  The adversarial methods showed advanced performance by producing syntheticimages to mitigate the domain shift, a common problem due to the hardship ofacquiring labelled data in medical field. Most existing studies focus onmodifying the network architecture, but little has worked on the GAN trainingstrategy. In this work, we propose SynthMix, an add-on module with a naturalyet effective training policy that can promote synthetic quality withoutaltering the network architecture. Following the adversarial philosophy of GAN,we designed a mix-up synthesis scheme termed SynthMix. It coherently mixed upaligned images of real and synthetic samples to stimulate the generation offine-grained features, examined by an associated Inspector for thedomain-specific details. We evaluated our method on two segmentation benchmarksamong three publicly available datasets, where our method showed a significantperformance gain compared with existing state-of-the-art approaches.$$$$$http://arxiv.org/pdf/2305.04156v1
UIT-OpenViIC: A Novel Benchmark for Evaluating Image Captioning in  Vietnamese$  Image Captioning is one of the vision-language tasks that still interest theresearch community worldwide in the 2020s. MS-COCO Caption benchmark iscommonly used to evaluate the performance of advanced captioning models,although it was published in 2015. Recent captioning models trained on theMS-COCO Caption dataset only have good performance in language patterns ofEnglish; they do not have such good performance in contexts captured in Vietnamor fluently caption images using Vietnamese. To contribute to the low-resourcesresearch community as in Vietnam, we introduce a novel image captioning datasetin Vietnamese, the Open-domain Vietnamese Image Captioning dataset(UIT-OpenViIC). The introduced dataset includes complex scenes captured inVietnam and manually annotated by Vietnamese under strict rules andsupervision. In this paper, we present in more detail the dataset creationprocess. From preliminary analysis, we show that our dataset is challenging torecent state-of-the-art (SOTA) Transformer-based baselines, which performedwell on the MS COCO dataset. Then, the modest results prove that UIT-OpenViIChas room to grow, which can be one of the standard benchmarks in Vietnamese forthe research community to evaluate their captioning models. Furthermore, wepresent a CAMO approach that effectively enhances the image representationability by a multi-level encoder output fusion mechanism, which helps improvethe quality of generated captions compared to previous captioning models.$image captioning, Vietnamese image captioning, open-domain image captioning, transformer.$University of Information Technology, Ho Chi Minh city, Vietnam; Vietnam National University, Ho Chi Minh City, Vietnam $本文介绍了一种新的用于评估越南语图像字幕的基准数据集，UIT-OpenViIC。该数据集由越南语注释员手动标注，包括越南生活和文化背景下的图像。文章还提出了一种对图像进行多级编码器输出融合机制的CAMO方法，该方法比以前的图像字幕模型有更好的表现。$图像字幕，越南语图像字幕，开放领域图像字幕，转换器$http://arxiv.org/pdf/2305.04166v1
Cross-Modal Retrieval for Motion and Text via MildTriple Loss$  Cross-modal retrieval has become a prominent research topic in computervision and natural language processing with advances made in image-text andvideo-text retrieval technologies. However, cross-modal retrieval between humanmotion sequences and text has not garnered sufficient attention despite theextensive application value it holds, such as aiding virtual realityapplications in better understanding users\' actions and language. This taskpresents several challenges, including joint modeling of the two modalities,demanding the understanding of person-centered information from text, andlearning behavior features from 3D human motion sequences. Previous work onmotion data modeling mainly relied on autoregressive feature extractors thatmay forget previous information, while we propose an innovative model thatincludes simple yet powerful transformer-based motion and text encoders, whichcan learn representations from the two different modalities and capturelong-term dependencies. Furthermore, the overlap of the same atomic actions ofdifferent human motions can cause semantic conflicts, leading us to explore anew triplet loss function, MildTriple Loss. it leverages the similarity betweensamples in intra-modal space to guide soft-hard negative sample mining in thejoint embedding space to train the triplet loss and reduce the violation causedby false negative samples. We evaluated our model and method on the latestHumanML3D and KIT Motion-Language datasets, achieving a 62.9\\% recall formotion retrieval and a 71.5\\% recall for text retrieval (based on R@10) on theHumanML3D dataset. Our code is available athttps://github.com/eanson023/rehamot.$Cross-modal retrieval, Motion-text retrieval, Triplet loss, Contrastive learning$aSchool of Artiﬁcial Intelligence, Chongqing University of Technology;bKey Laboratory of Machine Perception, Shenzhen Graduate School, Peking University$本文提出了一种用于人类运动序列和文本的跨模态检索方法，旨在通过联合建模两种模态来帮助虚拟现实应用更好地理解用户的动作和语言。文章采用了一种包含基于变压器的运动和文本编码器的创新模型，并探索了一种新的三元损失函数——MildTriple Loss来减少错误负样本带来的模型违反。实验结果表明，本文方法在HumanML3D和KIT Motion-Language数据集上均取得了较好的结果。$跨模态检索、文本-运动检索、三元损失、对比学习$http://arxiv.org/pdf/2305.04195v1
Unlocking the Power of Open Set : A New Perspective for Open-set Noisy  Label Learning$  Learning from noisy data has attracted much attention, where most methodsfocus on closed-set label noise. However, a more common scenario in the realworld is the presence of both open-set and closed-set noise. Existing methodstypically identify and handle these two types of label noise separately bydesigning a specific strategy for each type. However, in many real-worldscenarios, it would be challenging to identify open-set examples, especiallywhen the dataset has been severely corrupted. Unlike the previous works, weexplore how models behave when faced open-set examples, and find that a part ofopen-set examples gradually get integrated into certain known classes, which isbeneficial for the seperation among known classes. Motivated by the phenomenon,in this paper, we propose a novel two-step contrastive learning method calledCECL, which aims to deal with both types of label noise by exploiting theuseful information of open-set examples. Specifically, we incorporate someopen-set examples into closed-set classes to enhance performance while treatingothers as delimiters to improve representative ability. Extensive experimentson synthetic and real-world datasets with diverse label noise demonstrate thatCECL can outperform state-of-the-art methods.$$$$$http://arxiv.org/pdf/2305.04203v1
RATs-NAS: Redirection of Adjacent Trails on GCN for Neural Architecture  Search$  Various hand-designed CNN architectures have been developed, such as VGG,ResNet, DenseNet, etc., and achieve State-of-the-Art (SoTA) levels on differenttasks. Neural Architecture Search (NAS) now focuses on automatically findingthe best CNN architecture to handle the above tasks. However, the verificationof a searched architecture is very time-consuming and makes predictor-basedmethods become an essential and important branch of NAS. Two commonly usedtechniques to build predictors are graph-convolution networks (GCN) andmultilayer perceptron (MLP). In this paper, we consider the difference betweenGCN and MLP on adjacent operation trails and then propose the RedirectedAdjacent Trails NAS (RATs-NAS) to quickly search for the desired neural networkarchitecture. The RATs-NAS consists of two components: the Redirected AdjacentTrails GCN (RATs-GCN) and the Predictor-based Search Space Sampling (P3S)module. RATs-GCN can change trails and their strengths to search for a betterneural network architecture. DSS can rapidly focus on tighter intervals ofFLOPs in the search space. Based on our observations on cell-based NAS, webelieve that architectures with similar FLOPs will perform similarly. Finally,the RATs-NAS consisting of RATs-GCN and DSS beats WeakNAS, Arch-Graph, andothers by a significant margin on three sub-datasets of NASBench-201.$$$$$http://arxiv.org/pdf/2305.04206v1
Segmentation and Vascular Vectorization for Coronary Artery by  Geometry-based Cascaded Neural Network$  Segmentation of the coronary artery is an important task for the quantitativeanalysis of coronary computed tomography angiography (CCTA) images and is beingstimulated by the field of deep learning. However, the complex structures withtiny and narrow branches of the coronary artery bring it a great challenge.Coupled with the medical image limitations of low resolution and poor contrast,fragmentations of segmented vessels frequently occur in the prediction.Therefore, a geometry-based cascaded segmentation method is proposed for thecoronary artery, which has the following innovations: 1) Integrating geometricdeformation networks, we design a cascaded network for segmenting the coronaryartery and vectorizing results. The generated meshes of the coronary artery arecontinuous and accurate for twisted and sophisticated coronary arterystructures, without fragmentations. 2) Different from mesh annotationsgenerated by the traditional marching cube method from voxel-based labels, afiner vectorized mesh of the coronary artery is reconstructed with theregularized morphology. The novel mesh annotation benefits the geometry-basedsegmentation network, avoiding bifurcation adhesion and point cloud dispersionin intricate branches. 3) A dataset named CCA-200 is collected, consisting of200 CCTA images with coronary artery disease. The ground truths of 200 casesare coronary internal diameter annotations by professional radiologists.Extensive experiments verify our method on our collected dataset CCA-200 andpublic ASOCA dataset, with a Dice of 0.778 on CCA-200 and 0.895 on ASOCA,showing superior results. Especially, our geometry-based model generates anaccurate, intact and smooth coronary artery, devoid of any fragmentations ofsegmented vessels.$$$冠状动脉分割、基于几何的分割、网格标注$1 College of Electronics and Information Engineering, Tongji University 2 Shanghai Artificial Intelligence Laboratory 3 Centre for Perceptual and Interactive Intelligence, the Chinese University of Hong Kong 4 Department of Imaging and Interventional Radiology, the Chinese University of Hong Kong 5 SenseTime Research 6 Department of Electronic Engineering, the Chinese University of Hong Kong$http://arxiv.org/pdf/2305.04208v1
Design, Implementation and Evaluation of an External Pose-Tracking  System for Underwater Cameras$  In order to advance underwater computer vision and robotics from labenvironments and clear water scenarios to the deep dark ocean or murky coastalwaters, representative benchmarks and realistic datasets with ground truthinformation are required. In particular, determining the camera pose isessential for many underwater robotic or photogrammetric applications and knownground truth is mandatory to evaluate the performance of e.g., simultaneouslocalization and mapping approaches in such extreme environments. This paperpresents the conception, calibration and implementation of an externalreference system for determining the underwater camera pose in real-time. Theapproach, based on an HTC Vive tracking system in air, calculates theunderwater camera pose by fusing the poses of two controllers tracked above thewater surface of a tank. It is shown that the mean deviation of this approachto an optical marker based reference in air is less than 3 mm and 0.3{\\deg}.Finally, the usability of the system for underwater applications isdemonstrated.$$$$$http://arxiv.org/pdf/2305.04226v1
Instance-Variant Loss with Gaussian RBF Kernel for 3D Cross-modal  Retriveal$  3D cross-modal retrieval is gaining attention in the multimedia community.Central to this topic is learning a joint embedding space to represent datafrom different modalities, such as images, 3D point clouds, and polygon meshes,to extract modality-invariant and discriminative features. Hence, theperformance of cross-modal retrieval methods heavily depends on therepresentational capacity of this embedding space. Existing methods treat allinstances equally, applying the same penalty strength to instances with varyingdegrees of difficulty, ignoring the differences between instances. This canresult in ambiguous convergence or local optima, severely compromising theseparability of the feature space. To address this limitation, we propose anInstance-Variant loss to assign different penalty strengths to differentinstances, improving the space separability. Specifically, we assign differentpenalty weights to instances positively related to their intra-class distance.Simultaneously, we reduce the cross-modal discrepancy between features bylearning a shared weight vector for the same class data from differentmodalities. By leveraging the Gaussian RBF kernel to evaluate samplesimilarity, we further propose an Intra-Class loss function that minimizes theintra-class distance among same-class instances. Extensive experiments on three3D cross-modal datasets show that our proposed method surpasses recentstate-of-the-art approaches.$$$$$http://arxiv.org/pdf/2305.04239v1
Dual Residual Attention Network for Image Denoising$  In image denoising, deep convolutional neural networks (CNNs) can obtainfavorable performance on removing spatially invariant noise. However, many ofthese networks cannot perform well on removing the real noise (i.e. spatiallyvariant noise) generated during image acquisition or transmission, whichseverely sets back their application in practical image denoising tasks.Instead of continuously increasing the network depth, many researchers haverevealed that expanding the width of networks can also be a useful way toimprove model performance. It also has been verified that feature filtering canpromote the learning ability of the models. Therefore, in this paper, wepropose a novel Dual-branch Residual Attention Network (DRANet) for imagedenoising, which has both the merits of a wide model architecture andattention-guided feature learning. The proposed DRANet includes two differentparallel branches, which can capture complementary features to enhance thelearning ability of the model. We designed a new residual attention block (RAB)and a novel hybrid dilated residual attention block (HDRAB) for the upper andthe lower branches, respectively. The RAB and HDRAB can capture rich localfeatures through multiple skip connections between different convolutionallayers, and the unimportant features are dropped by the residual attentionmodules. Meanwhile, the long skip connections in each branch, and the globalfeature fusion between the two parallel branches can capture the globalfeatures as well. Moreover, the proposed DRANet uses downsampling operationsand dilated convolutions to increase the size of the receptive field, which canenable DRANet to capture more image context information. Extensive experimentsdemonstrate that compared with other state-of-the-art denoising methods, ourDRANet can produce competitive denoising performance both on synthetic andreal-world noise removal.$$$$$http://arxiv.org/pdf/2305.04269v1
RSC-VAE: Recoding Semantic Consistency Based VAE for One-Class Novelty  Detection$  In recent years, there is an increasing interests in reconstruction basedgenerative models for image One-Class Novelty Detection, most of which onlyfocus on image-level information. While in this paper, we further exploit thelatent space of Variational Auto-encoder (VAE), a typical reconstruction basedmodel, and we innovatively divide it into three regions:Normal/Anomalous/Unknown-semantic-region. Based on this hypothesis, we proposea new VAE architecture, Recoding Semantic Consistency Based VAE (RSC-VAE),combining VAE with recoding mechanism and constraining the semantic consistencyof two encodings. We come up with three training modes of RSC-VAE: 1. One-ClassTraining Mode, alleviating False Positive problem of normal samples; 2.Distributionally-Shifted Training Mode, alleviating False Negative problem ofanomalous samples; 3. Extremely-Imbalanced Training Mode, introducing a smallnumber of anomalous samples for training to enhance the second mode. Theexperimental results on multiple datasets demonstrate that our mechanismachieves state-of-the-art performance in various baselines including VAE.$$$$$http://arxiv.org/pdf/2305.04275v1
Learning from synthetic data generated with GRADE$  Recently, synthetic data generation and realistic rendering has advancedtasks like target tracking and human pose estimation. Simulations for mostrobotics applications are obtained in (semi)static environments, with specificsensors and low visual fidelity. To solve this, we present a fully customizableframework for generating realistic animated dynamic environments (GRADE) forrobotics research, first introduced in [1]. GRADE supports full simulationcontrol, ROS integration, realistic physics, while being in an engine thatproduces high visual fidelity images and ground truth data. We use GRADE togenerate a dataset focused on indoor dynamic scenes with people and flyingobjects. Using this, we evaluate the performance of YOLO and Mask R-CNN on thetasks of segmenting and detecting people. Our results provide evidence thatusing data generated with GRADE can improve the model performance when used fora pre-training step. We also show that, even training using only syntheticdata, can generalize well to real-world images in the same application domainsuch as the ones from the TUM-RGBD dataset. The code, results, trained models,and the generated data are provided as open-source athttps://eliabntt.github.io/grade-rr.$$$$$http://arxiv.org/pdf/2305.04282v1
PELE scores: Pelvic X-ray Landmark Detection by Pelvis Extraction and  Enhancement$  The pelvis, the lower part of the trunk, supports and balances the trunk.Landmark detection from a pelvic X-ray (PXR) facilitates downstream analysisand computer-assisted diagnosis and treatment of pelvic diseases. Although PXRshave the advantages of low radiation and reduced cost compared to computedtomography (CT) images, their 2D pelvis-tissue superposition of 3D structuresconfuses clinical decision-making. In this paper, we propose a PELvisExtraction (PELE) module that utilizes 3D prior anatomical knowledge in CT toguide and well isolate the pelvis from PXRs, thereby eliminating the influenceof soft tissue. We conduct an extensive evaluation based on two public datasetsand one private dataset, totaling 850 PXRs. The experimental results show thatthe proposed PELE module significantly improves the accuracy of PXRs landmarkdetection and achieves state-of-the-art performances in several benchmarkmetrics, thus better serving downstream tasks.$Bone extraction, landmark detection, pelvic X-rays$Medical Imaging, Robotics, and Analytic Computing, pelvic X-ray landmark detection$本文提出PELvis Extraction（PELE）模块，通过利用CT中的3D先验解剖知识引导和很好地隔离PXRs中的盆骨，从而消除软组织的影响，显著提高了PXRs标志点检测的准确性，为下游任务提供更好的服务。$医学影像，机器人，分析计算，盆骨X线标志点检测$http://arxiv.org/pdf/2305.04294v1
HashCC: Lightweight Method to Improve the Quality of the Camera-less  NeRF Scene Generation$  Neural Radiance Fields has become a prominent method of scene generation viaview synthesis. A critical requirement for the original algorithm to learnmeaningful scene representation is camera pose information for each image in adata set. Current approaches try to circumnavigate this assumption withmoderate success, by learning approximate camera positions alongside learningneural representations of a scene. This requires complicated camera models,causing a long and complicated training process, or results in a lack oftexture and sharp details in rendered scenes. In this work we introduce HashColor Correction (HashCC) -- a lightweight method for improving Neural RadianceFields rendered image quality, applicable also in situations where camerapositions for a given set of images are unknown.$View synthesis, deep learning, image generation$University of Warsaw$本文提出了一种轻量级的方法，名为HashCC，用于提高视图合成中NeRF生成场景的质量，同时也适用于相机位置未知的情况。该方法使用哈希编码和颜色校正网络，并结合球面谐波编码和傅里叶编码来改进预测质量，而不需要使用正则化技术或更改训练计划，并且计算开销可忽略不计。作者的主要目标是开发一种简单的解决方案，可应用于不同的NeRF架构并改善现有方案的颜色预测准确性。$视图合成，深度学习，图像生成$http://arxiv.org/pdf/2305.04296v1
Poses as Queries: Image-to-LiDAR Map Localization with Transformers$  High-precision vehicle localization with commercial setups is a crucialtechnique for high-level autonomous driving tasks. Localization with amonocular camera in LiDAR map is a newly emerged approach that achievespromising balance between cost and accuracy, but estimating pose by findingcorrespondences between such cross-modal sensor data is challenging, therebydamaging the localization accuracy. In this paper, we address the problem byproposing a novel Transformer-based neural network to register 2D images into3D LiDAR map in an end-to-end manner. Poses are implicitly represented ashigh-dimensional feature vectors called pose queries and can be iterativelyupdated by interacting with the retrieved relevant information from cross-modelfeatures using attention mechanism in a proposed POse Estimator Transformer(POET) module. Moreover, we apply a multiple hypotheses aggregation method thatestimates the final poses by performing parallel optimization on multiplerandomly initialized pose queries to reduce the network uncertainty.Comprehensive analysis and experimental results on public benchmark concludethat the proposed image-to-LiDAR map localization network could achievestate-of-the-art performances in challenging cross-modal localization tasks.$$$$$http://arxiv.org/pdf/2305.04298v1
Data Efficient Training with Imbalanced Label Sample Distribution for  Fashion Detection$  Multi-label classification models have a wide range of applications inE-commerce, including visual-based label predictions and language-basedsentiment classifications. A major challenge in achieving satisfactoryperformance for these tasks in the real world is the notable imbalance in datadistribution. For instance, in fashion attribute detection, there may be onlysix \'puff sleeve\' clothes among 1000 products in most E-commerce fashioncatalogs. To address this issue, we explore more data-efficient model trainingtechniques rather than acquiring a huge amount of annotations to collectsufficient samples, which is neither economic nor scalable. In this paper, wepropose a state-of-the-art weighted objective function to boost the performanceof deep neural networks (DNNs) for multi-label classification with long-taileddata distribution. Our experiments involve image-based attribute classificationof fashion apparels, and the results demonstrate favorable performance for thenew weighting method compared to non-weighted and inverse-frequency-basedweighting mechanisms. We further evaluate the robustness of the new weightingmechanism using two popular fashion attribute types in today\'s fashionindustry: sleevetype and archetype.$$$$$http://arxiv.org/pdf/2305.04379v1
Few Shot Learning for Medical Imaging: A Comparative Analysis of  Methodologies and Formal Mathematical Framework$"  Deep learning becomes an elevated context regarding disposing of many machinelearning tasks and has shown a breakthrough upliftment to extract features fromunstructured data. Though this flourishing context is developing in the medicalimage processing sector, scarcity of problem-dependent training data has becomea larger issue in the way of easy application of deep learning in the medicalsector. To unravel the confined data source, researchers have developed a modelthat can solve machine learning problems with fewer data called ``Few shotlearning"". Few hot learning algorithms determine to solve the data limitationproblems by extracting the characteristics from a small dataset throughclassification and segmentation methods. In the medical sector, there isfrequently a shortage of available datasets in respect of some confidentialdiseases. Therefore, Few shot learning gets the limelight in this data scarcitysector. In this chapter, the background and basic overview of a few shots oflearning is represented. Henceforth, the classification of few-shot learning isdescribed also. Even the paper shows a comparison of methodological approachesthat are applied in medical image analysis over time. The current advancementin the implementation of few-shot learning concerning medical imaging isillustrated. The future scope of this domain in the medical imaging sector isfurther described."$$$$$http://arxiv.org/pdf/2305.04401v1
Towards Accurate Human Motion Prediction via Iterative Refinement$  Human motion prediction aims to forecast an upcoming pose sequence given apast human motion trajectory. To address the problem, in this work we proposeFreqMRN, a human motion prediction framework that takes into account both thekinematic structure of the human body and the temporal smoothness nature ofmotion. Specifically, FreqMRN first generates a fixed-size motion historysummary using a motion attention module, which helps avoid inaccurate motionpredictions due to excessively long motion inputs. Then, supervised by theproposed spatial-temporal-aware, velocity-aware and global-smoothness-awarelosses, FreqMRN iteratively refines the predicted motion though the proposedmotion refinement module, which converts motion representations back and forthbetween pose space and frequency space. We evaluate FreqMRN on several standardbenchmark datasets, including Human3.6M, AMASS and 3DPW. Experimental resultsdemonstrate that FreqMRN outperforms previous methods by large margins for bothshort-term and long-term predictions, while demonstrating superior robustness.$human motion prediction, neural networks, motion sequence, frequency transformation, spatial-temporal structure$University of Illinois at Urbana, Champaign, USA$本文提出了一种基于神经网络的人体运动预测框架，通过使用运动关注模块生成固定大小的运动历史摘要，利用空间-时间感知、速度感知和全局平滑度感知等提出的损失函数进行监督，再通过运动细化模块将运动表示在姿势空间和频率空间之间进行转换，从而迭代地进行预测运动的改进。在多个标准基准数据集上的实验结果表明，该方法不仅提高了短期和长期预测的准确性，而且展现了更强的稳健性。$人体运动预测，神经网络，运动序列，频率转换，空间-时间结构$http://arxiv.org/pdf/2305.04443v1
Locally Attentional SDF Diffusion for Controllable 3D Shape Generation$  Although the recent rapid evolution of 3D generative neural networks greatlyimproves 3D shape generation, it is still not convenient for ordinary users tocreate 3D shapes and control the local geometry of generated shapes. To addressthese challenges, we propose a diffusion-based 3D generation framework --locally attentional SDF diffusion, to model plausible 3D shapes, via 2D sketchimage input. Our method is built on a two-stage diffusion model. The firststage, named occupancy-diffusion, aims to generate a low-resolution occupancyfield to approximate the shape shell. The second stage, named SDF-diffusion,synthesizes a high-resolution signed distance field within the occupied voxelsdetermined by the first stage to extract fine geometry. Our model is empoweredby a novel view-aware local attention mechanism for image-conditioned shapegeneration, which takes advantage of 2D image patch features to guide 3D voxelfeature learning, greatly improving local controllability and modelgeneralizability. Through extensive experiments in sketch-conditioned andcategory-conditioned 3D shape generation tasks, we validate and demonstrate theability of our method to provide plausible and diverse 3D shapes, as well asits superior controllability and generalizability over existing work. Our codeand trained models are available athttps://zhengxinyang.github.io/projects/LAS-Diffusion.html$$$$$http://arxiv.org/pdf/2305.04461v1
Vision Lanauge Pre-training by Contrastive Learning with Cross-Modal  Similarity Regulation$  Cross-modal contrastive learning in vision language pretraining (VLP) facesthe challenge of (partial) false negatives. In this paper, we study thisproblem from the perspective of Mutual Information (MI) optimization. It iscommon sense that InfoNCE loss used in contrastive learning will maximize thelower bound of MI between anchors and their positives, while we theoreticallyprove that MI involving negatives also matters when noises commonly exist.Guided by a more general lower bound form for optimization, we propose acontrastive learning strategy regulated by progressively refined cross-modalsimilarity, to more accurately optimize MI between an image/text anchor and itsnegative texts/images instead of improperly minimizing it. Our method performscompetitively on four downstream cross-modal tasks and systematically balancesthe beneficial and harmful effects of (partial) false negative samples undertheoretical guidance.$$$$$http://arxiv.org/pdf/2305.04474v1
IIITD-20K: Dense captioning for Text-Image ReID$  Text-to-Image (T2I) ReID has attracted a lot of attention in the recent past.CUHK-PEDES, RSTPReid and ICFG-PEDES are the three available benchmarks toevaluate T2I ReID methods. RSTPReid and ICFG-PEDES comprise of identities fromMSMT17 but due to limited number of unique persons, the diversity is limited.On the other hand, CUHK-PEDES comprises of 13,003 identities but has relativelyshorter text description on average. Further, these datasets are captured in arestricted environment with limited number of cameras. In order to furtherdiversify the identities and provide dense captions, we propose a novel datasetcalled IIITD-20K. IIITD-20K comprises of 20,000 unique identities captured inthe wild and provides a rich dataset for text-to-image ReID. With a minimum of26 words for a description, each image is densely captioned. We furthersynthetically generate images and fine-grained captions using Stable-diffusionand BLIP models trained on our dataset. We perform elaborate experiments usingstate-of-art text-to-image ReID models and vision-language pre-trained modelsand present a comprehensive analysis of the dataset. Our experiments alsoreveal that synthetically generated data leads to a substantial performanceimprovement in both same dataset as well as cross dataset settings. Our datasetis available at https://bit.ly/3pkA3Rj.$$$$$http://arxiv.org/pdf/2305.04497v1
Building Footprint Extraction with Graph Convolutional Network$  Building footprint information is an essential ingredient for 3-Dreconstruction of urban models. The automatic generation of building footprintsfrom satellite images presents a considerable challenge due to the complexityof building shapes. Recent developments in deep convolutional neural networks(DCNNs) have enabled accurate pixel-level labeling tasks. One central issueremains, which is the precise delineation of boundaries. Deep architecturesgenerally fail to produce fine-grained segmentation with accurate boundariesdue to progressive downsampling. In this work, we have proposed a end-to-endframework to overcome this issue, which uses the graph convolutional network(GCN) for building footprint extraction task. Our proposed frameworkoutperforms state-of-the-art methods.$$$$$http://arxiv.org/pdf/2305.04499v1
Pedestrian Behavior Maps for Safety Advisories: CHAMP Framework and  Real-World Data Analysis$  It is critical for vehicles to prevent any collisions with pedestrians.Current methods for pedestrian collision prevention focus on integrating visualpedestrian detectors with Automatic Emergency Braking (AEB) systems which cantrigger warnings and apply brakes as a pedestrian enters a vehicle\'s path.Unfortunately, pedestrian-detection-based systems can be hindered in certainsituations such as night-time or when pedestrians are occluded. Our systemaddresses such issues using an online, map-based pedestrian detectionaggregation system where common pedestrian locations are learned after repeatedpasses of locations. Using a carefully collected and annotated dataset in LaJolla, CA, we demonstrate the system\'s ability to learn pedestrian zones andgenerate advisory notices when a vehicle is approaching a pedestrian despitechallenges like dark lighting or pedestrian occlusion. Using the number ofcorrect advisories, false advisories, and missed advisories to define precisionand recall performance metrics, we evaluate our system and discuss futurepositive effects with further data collection. We have made our code availableat https://github.com/s7desai/ped-mapping, and a video demonstration of theCHAMP system at https://youtu.be/dxeCrS_Gpkw.$$$$$http://arxiv.org/pdf/2305.04506v1
Multi-Temporal Lip-Audio Memory for Visual Speech Recognition$  Visual Speech Recognition (VSR) is a task to predict a sentence or word fromlip movements. Some works have been recently presented which use audio signalsto supplement visual information. However, existing methods utilize onlylimited information such as phoneme-level features and soft labels of AutomaticSpeech Recognition (ASR) networks. In this paper, we present a Multi-TemporalLip-Audio Memory (MTLAM) that makes the best use of audio signals to complementinsufficient information of lip movements. The proposed method is mainlycomposed of two parts: 1) MTLAM saves multi-temporal audio features producedfrom short- and long-term audio signals, and the MTLAM memorizes avisual-to-audio mapping to load stored multi-temporal audio features fromvisual features at the inference phase. 2) We design an audio temporal model toproduce multi-temporal audio features capturing the context of neighboringwords. In addition, to construct effective visual-to-audio mapping, the audiotemporal models can generate audio features time-aligned with visual features.Through extensive experiments, we validate the effectiveness of the MTLAMachieving state-of-the-art performances on two public VSR datasets.$$$$$http://arxiv.org/pdf/2305.04542v1
SwinDocSegmenter: An End-to-End Unified Domain Adaptive Transformer for  Document Instance Segmentation$  Instance-level segmentation of documents consists in assigning a class-awareand instance-aware label to each pixel of the image. It is a key step indocument parsing for their understanding. In this paper, we present a unifiedtransformer encoder-decoder architecture for en-to-end instance segmentation ofcomplex layouts in document images. The method adapts a contrastive trainingwith a mixed query selection for anchor initialization in the decoder. Lateron, it performs a dot product between the obtained query embeddings and thepixel embedding map (coming from the encoder) for semantic reasoning. Extensiveexperimentation on competitive benchmarks like PubLayNet, PRIMA, HistoricalJapanese (HJ), and TableBank demonstrate that our model with SwinL backboneachieves better segmentation performance than the existing state-of-the-artapproaches with the average precision of \\textbf{93.72}, \\textbf{54.39},\\textbf{84.65} and \\textbf{98.04} respectively under one billion parameters.The code is made publicly available at:\\href{https://github.com/ayanban011/SwinDocSegmenter}{github.com/ayanban011/SwinDocSegmenter}$Document Layout Analysis, Instance-Level Segmentation, Swin Transformer, Contrastive Learning$1Computer Vision Center & Computer Science Department, Universitat Autonoma de Barcelona, Spain；2CVPR Unit, Indian Statistical Institute, India$该文章提出了一种用于文档布局中复杂图片的实例分割的统一变压器编码器-解码器架构，利用对比训练和混合查询选择进行锚初始化。实验结果显示，该模型在PubLayNet、PRIMA、Historical Japanese (HJ)和TableBank等基准测试中均取得比现有最先进方法更好的分割性能。$文档布局分析、实例级分割、Swin Transformer、对比学习$http://arxiv.org/pdf/2305.04609v1
Riesz networks: scale invariant neural networks in a single forward pass$  Scale invariance of an algorithm refers to its ability to treat objectsequally independently of their size. For neural networks, scale invariance istypically achieved by data augmentation. However, when presented with a scalefar outside the range covered by the training set, neural networks may fail togeneralize.  Here, we introduce the Riesz network, a novel scale invariant neural network.Instead of standard 2d or 3d convolutions for combining spatial information,the Riesz network is based on the Riesz transform which is a scale equivariantoperation. As a consequence, this network naturally generalizes to unseen oreven arbitrary scales in a single forward pass. As an application example, weconsider detecting and segmenting cracks in tomographic images of concrete. Inthis context, \'scale\' refers to the crack thickness which may vary stronglyeven within the same sample. To prove its scale invariance, the Riesz networkis trained on one fixed crack width. We then validate its performance insegmenting simulated and real tomographic images featuring a wide range ofcrack widths. An additional experiment is carried out on the MNIST Large Scaledata set.$neural networks, scale invariance, Riesz transform$Mathematics Department, RPTU Kaiserslautern-Landau, Germany；Department Image Processing, Fraunhofer-Institut fur Techno- und Wirtschaftsmathematik (ITWM), Germany$本文介绍了一种新型的尺度不变性神经网络Riesz网络，采用Riesz变换对空间信息进行组合，具有适应未知或任意尺度的能力，在混凝土断裂图像分割等领域具有应用前景。$神经网络，尺度不变性，Riesz变换$http://arxiv.org/pdf/2305.04665v1
ElasticHash: Semantic Image Similarity Search by Deep Hashing with  Elasticsearch$  We present ElasticHash, a novel approach for high-quality, efficient, andlarge-scale semantic image similarity search. It is based on a deep hashingmodel to learn hash codes for fine-grained image similarity search in naturalimages and a two-stage method for efficiently searching binary hash codes usingElasticsearch (ES). In the first stage, a coarse search based on short hashcodes is performed using multi-index hashing and ES terms lookup of neighboringhash codes. In the second stage, the list of results is re-ranked by computingthe Hamming distance on long hash codes. We evaluate the retrieval performanceof \\textit{ElasticHash} for more than 120,000 query images on about 6.9 milliondatabase images of the OpenImages data set. The results show that our approachachieves high-quality retrieval results and low search latencies.$$$$$http://arxiv.org/pdf/2305.04710v1
Strategy for Rapid Diabetic Retinopathy Exposure Based on Enhanced  Feature Extraction Processing$  In the modern world, one of the most severe eye infections brought on bydiabetes is known as diabetic retinopathy, which will result in retinal damage,and, thus, lead to blindness. Diabetic retinopathy can be well treated withearly diagnosis. Retinal fundus images of humans are used to screen for lesionsin the retina. However, detecting DR in the early stages is challenging due tothe minimal symptoms. Furthermore, the occurrence of diseases linked tovascular anomalies brought on by DR aids in diagnosing the condition.Nevertheless, the resources required for manually identifying the lesions arehigh. Similarly, training for Convolutional Neural Networks is moretime-consuming. This proposed research aims to improve diabetic retinopathydiagnosis by developing an enhanced deep learning model for timely DRidentification that is potentially more accurate than existing CNN-basedmodels. The proposed model will detect various lesions from retinal images inthe early stages. First, characteristics are retrieved from the retinal funduspicture and put into the EDLM for classification. For dimensionality reduction,EDLM is used. Additionally, the classification and feature extraction processesare optimized using the stochastic gradient descent optimizer. The EDLMeffectiveness is assessed on the KAG GLE dataset with 3459 retinal images, andresults are compared over VGG16, VGG19, RESNET18, RESNET34, and RESNET50.$$$$$http://arxiv.org/pdf/2305.04724v1
Controllable Light Diffusion for Portraits$  We introduce light diffusion, a novel method to improve lighting inportraits, softening harsh shadows and specular highlights while preservingoverall scene illumination. Inspired by professional photographers\' diffusersand scrims, our method softens lighting given only a single portrait photo.Previous portrait relighting approaches focus on changing the entire lightingenvironment, removing shadows (ignoring strong specular highlights), orremoving shading entirely. In contrast, we propose a learning based method thatallows us to control the amount of light diffusion and apply it on in-the-wildportraits. Additionally, we design a method to synthetically generate plausibleexternal shadows with sub-surface scattering effects while conforming to theshape of the subject\'s face. Finally, we show how our approach can increase therobustness of higher level vision applications, such as albedo estimation,geometry estimation and semantic segmentation.$$$$$http://arxiv.org/pdf/2305.04745v1
Toeplitz Neural Network for Sequence Modeling$  Sequence modeling has important applications in natural language processingand computer vision. Recently, the transformer-based models have shown strongperformance on various sequence modeling tasks, which rely on attention tocapture pairwise token relations, and position embedding to inject positionalinformation. While showing good performance, the transformer models areinefficient to scale to long input sequences, mainly due to the quadraticspace-time complexity of attention. To overcome this inefficiency, we proposeto model sequences with a relative position encoded Toeplitz matrix and use aToeplitz matrix-vector production trick to reduce the space-time complexity ofthe sequence modeling to log linear. A lightweight sub-network called relativeposition encoder is proposed to generate relative position coefficients with afixed budget of parameters, enabling the proposed Toeplitz neural network todeal with varying sequence lengths. In addition, despite being trained on512-token sequences, our model can extrapolate input sequence length up to 14Ktokens in inference with consistent performance. Extensive experiments onautoregressive and bidirectional language modeling, image modeling, and thechallenging Long-Range Arena benchmark show that our method achieves betterperformance than its competitors in most downstream tasks while beingsignificantly faster. The code is available athttps://github.com/OpenNLPLab/Tnn.$Natural Language Processing, Machine Learning, Text Classification$XXX$本文研究了基于机器学习的文本分类方法，通过实验比较了各种方法的性能和优缺点，提出了一种新的文本分类方法并进行了实验验证。$自然语言处理、机器学习、文本分类$http://arxiv.org/pdf/2305.04749v1
AvatarReX: Real-time Expressive Full-body Avatars$  We present AvatarReX, a new method for learning NeRF-based full-body avatarsfrom video data. The learnt avatar not only provides expressive control of thebody, hands and the face together, but also supports real-time animation andrendering. To this end, we propose a compositional avatar representation, wherethe body, hands and the face are separately modeled in a way that thestructural prior from parametric mesh templates is properly utilized withoutcompromising representation flexibility. Furthermore, we disentangle thegeometry and appearance for each part. With these technical designs, we proposea dedicated deferred rendering pipeline, which can be executed in real-timeframerate to synthesize high-quality free-view images. The disentanglement ofgeometry and appearance also allows us to design a two-pass training strategythat combines volume rendering and surface rendering for network training. Inthis way, patch-level supervision can be applied to force the network to learnsharp appearance details on the basis of geometry estimation. Overall, ourmethod enables automatic construction of expressive full-body avatars withreal-time rendering capability, and can generate photo-realistic images withdynamic details for novel body motions and facial expressions.$$$$$http://arxiv.org/pdf/2305.04789v1
MultiModal-GPT: A Vision and Language Model for Dialogue with Humans$  We present a vision and language model named MultiModal-GPT to conductmulti-round dialogue with humans. MultiModal-GPT can follow variousinstructions from humans, such as generating a detailed caption, counting thenumber of interested objects, and answering general questions from users.MultiModal-GPT is parameter-efficiently fine-tuned from OpenFlamingo, withLow-rank Adapter (LoRA) added both in the cross-attention part and theself-attention part of the language model. We first construct instructiontemplates with vision and language data for multi-modality instruction tuningto make the model understand and follow human instructions. We find the qualityof training data is vital for the dialogue performance, where few datacontaining short answers can lead the model to respond shortly to anyinstructions. To further enhance the ability to chat with humans of theMultiModal-GPT, we utilize language-only instruction-following data to trainthe MultiModal-GPT jointly. The joint training of language-only andvisual-language instructions with the \\emph{same} instruction templateeffectively improves dialogue performance. Various demos show the ability ofcontinuous dialogue of MultiModal-GPT with humans. Code and demo are athttps://github.com/open-mmlab/Multimodal-GPT$$$$$http://arxiv.org/pdf/2305.04790v1
Compressed Video Quality Assessment for Super-Resolution: a Benchmark  and a Quality Metric$  We developed a super-resolution (SR) benchmark to analyze SR\'s capacity toupscale compressed videos. Our dataset employed video codecs based on fivecompression standards: H.264, H.265, H.266, AV1, and AVS3. We assessed 17state-ofthe-art SR models using our benchmark and evaluated their ability topreserve scene context and their susceptibility to compression artifacts. Toget an accurate perceptual ranking of SR models, we conducted a crowd-sourcedside-by-side comparison of their outputs. The benchmark is publicly availableathttps://videoprocessing.ai/benchmarks/super-resolutionfor-video-compression.html.We also analyzed benchmark results and developed anobjective-quality-assessment metric based on the current bestperformingobjective metrics. Our metric outperforms others, according to Spearmancorrelation with subjective scores for compressed video upscaling. It ispublicly available athttps://github.com/EvgeneyBogatyrev/super-resolution-metric.$$$$$http://arxiv.org/pdf/2305.04844v1
Learning to Evaluate the Artness of AI-generated Images$  Assessing the artness of AI-generated images continues to be a challengewithin the realm of image generation. Most existing metrics cannot be used toperform instance-level and reference-free artness evaluation. This paperpresents ArtScore, a metric designed to evaluate the degree to which an imageresembles authentic artworks by artists (or conversely photographs), therebyoffering a novel approach to artness assessment. We first blend pre-trainedmodels for photo and artwork generation, resulting in a series of mixed models.Subsequently, we utilize these mixed models to generate images exhibitingvarying degrees of artness with pseudo-annotations. Each photorealistic imagehas a corresponding artistic counterpart and a series of interpolated imagesthat range from realistic to artistic. This dataset is then employed to train aneural network that learns to estimate quantized artness levels of arbitraryimages. Extensive experiments reveal that the artness levels predicted byArtScore align more closely with human artistic evaluation than existingevaluation metrics, such as Gram loss and ArtFID.$Field keywords: Unknown field$Authorship: Unknown$本文未提供具体领域内容，无法总结相关关键词。$未知领域$http://arxiv.org/pdf/2305.04923v1
Multi-Label Classification of Thoracic Diseases using Dense  Convolutional Network on Chest Radiographs$  Traditional methods of identifying pathologies in X-ray images rely heavilyon skilled human interpretation and are often time-consuming. The advent ofdeep learning techniques has enabled the development of automated diseasediagnosis systems, but the performance of such systems is dependent on thequality of the model and the level of interpretability it provides. In thispaper, we propose a multi-label disease diagnosis model for chest X-rays usinga dense convolutional neural network (DenseNet) and model interpretabilityusing GRADCAM. We trained our model using frontal X-rays and evaluated itsperformance using various quantitative metrics, including the area under thereceiver operating characteristic curve (AUC). Our proposed model achieved thehighest AUC score of 0.896 for the condition Cardiomegaly with an accuracy of0.826, while the lowest AUC score was obtained for Nodule, at 0.655 with anaccuracy of 0.66. To promote model interpretability and build trust in decisionmaking, we generated heatmaps on X-rays to visualize the regions where themodel paid attention to make certain predictions. Additionally, we estimatedthe uncertainty in model predictions by presenting the confidence interval ofour measurements. Our proposed automated disease diagnosis model obtained highperformance metrics in multi-label disease diagnosis tasks and providedvisualization of model predictions for model interpretability.$deep learning; disease diagnosis; model explainability; chest x-rays$Department of Electronics and Computer Engineering, Institute of Engineering, Pulchowk Campus, Lalitpur, Nepal$本文提出了一种使用密集卷积神经网络(DenseNet)和GRADCAM进行模型可解释性的胸部X光片多标签疾病诊断模型。为了促进模型可解释性和增强决策信任，我们生成了X光片的热图来可视化模型用来进行预测的区域，并通过提供我们测量的置信区间来估计模型预测的不确定性。我们的模型在多标签疾病诊断任务中获得了高性能指标，并提供模型预测的可视化，具有重要的应用价值。$深度学习、疾病诊断、模型可解释性、胸部X光片$http://arxiv.org/pdf/2202.03583v2
Delving into the Openness of CLIP$  Contrastive Language-Image Pre-training (CLIP) formulates imageclassification as an image-to-text matching task, i.e., matching images to thecorresponding natural language descriptions instead of discrete category IDs.This allows for open-vocabulary visual recognition, where the model canrecognize images from an open class set (also known as an open vocabulary) in azero-shot manner. However, evaluating the openness of CLIP-like models ischallenging, as the models are open to arbitrary vocabulary in theory, buttheir accuracy varies in practice. To address this, we resort to an incrementalperspective to assess the openness through vocabulary expansions, and defineextensibility to measure a model\'s ability to handle novel classes. Ourevaluation shows that CLIP-like models are not truly open, and theirperformance deteriorates as the vocabulary expands. We further dissect thefeature space of CLIP from the perspectives of representation alignment anduniformity. Our investigation reveals that the overestimation of openness isdue to confusion among competing text features, rather than a failure tocapture the similarity between image features and text features of novelclasses. We hope that our investigation and analysis will facilitate futureresearch on the CLIP openness issue.$$$$$http://arxiv.org/pdf/2206.01986v3
An Image Processing approach to identify solar plages observed at 393.37  nm by the Kodaikanal Solar Observatory$  Solar plages, which are bright regions on the Sun\'s surface, are an importantindicator of solar activity. In this study, we propose an automated algorithmfor identifying solar plages in Ca K wavelength solar data obtained from theKodaikanal Solar Observatory. The algorithm successfully annotates all visuallyidentifiable plages in an image and outputs the corresponding calculated plageindex. We perform a time series analysis of the plage index (rolling mean)across multiple solar cycles to test the algorithm\'s reliability androbustness. The results show a strong correlation between the calculated plageindex and those reported in a previous study. The correlation coefficientsobtained for all the solar cycles are higher than 0.90, indicating thereliability of the model. We also suggest that adjusting the hyperparametersappropriately for a specific image using our web-based app can increase themodel\'s efficiency. The algorithm has been deployed on the Streamlit CommunityCloud platform, where users can upload images and customize the hyperparametersfor desired results. The input data used in this study is freely available fromthe KSO data archive, and the code and the generated data are publiclyavailable on our GitHub repository. Our proposed algorithm provides anefficient and reliable method for identifying solar plages, which can aid thestudy of solar activity and its impact on the Earth\'s climate, technology, andspace weather.$$$$$http://arxiv.org/pdf/2209.10631v3
CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features  for a Disentangled, Interpretable, and Controllable Text-Guided Face  Manipulation$  Recently introduced Contrastive Language-Image Pre-Training (CLIP) bridgesimages and text by embedding them into a joint latent space. This opens thedoor to ample literature that aims to manipulate an input image by providing atextual explanation. However, due to the discrepancy between image and textembeddings in the joint space, using text embeddings as the optimization targetoften introduces undesired artifacts in the resulting images. Disentanglement,interpretability, and controllability are also hard to guarantee formanipulation. To alleviate these problems, we propose to define corpussubspaces spanned by relevant prompts to capture specific imagecharacteristics. We introduce CLIP Projection-Augmentation Embedding (PAE) asan optimization target to improve the performance of text-guided imagemanipulation. Our method is a simple and general paradigm that can be easilycomputed and adapted, and smoothly incorporated into any CLIP-based imagemanipulation algorithm. To demonstrate the effectiveness of our method, weconduct several theoretical and empirical studies. As a case study, we utilizethe method for text-guided semantic face editing. We quantitatively andqualitatively demonstrate that PAE facilitates a more disentangled,interpretable, and controllable image manipulation with state-of-the-artquality and accuracy.$$$$$http://arxiv.org/pdf/2210.03919v4
MMRNet: Improving Reliability for Multimodal Object Detection and  Segmentation for Bin Picking via Multimodal Redundancy$  Recently, there has been tremendous interest in industry 4.0 infrastructureto address labor shortages in global supply chains. Deploying artificialintelligence-enabled robotic bin picking systems in real world has becomeparticularly important for reducing stress and physical demands of workerswhile increasing speed and efficiency of warehouses. To this end, artificialintelligence-enabled robotic bin picking systems may be used to automate orderpicking, but with the risk of causing expensive damage during an abnormal eventsuch as sensor failure. As such, reliability becomes a critical factor fortranslating artificial intelligence research to real world applications andproducts. In this paper, we propose a reliable object detection andsegmentation system with MultiModal Redundancy (MMRNet) for tackling objectdetection and segmentation for robotic bin picking using data from differentmodalities. This is the first system that introduces the concept of multimodalredundancy to address sensor failure issues during deployment. In particular,we realize the multimodal redundancy framework with a gate fusion module anddynamic ensemble learning. Finally, we present a new label-free multi-modalconsistency (MC) score that utilizes the output from all modalities to measurethe overall system output reliability and uncertainty. Through experiments, wedemonstrate that in an event of missing modality, our system provides a muchmore reliable performance compared to baseline models. We also demonstrate thatour MC score is a more reliability indicator for outputs during inference timecompared to the model generated confidence scores that are oftenover-confident.$AI, 机器人技术，物体检测，分割，多模态冗余；$1University of Waterloo, Canada；2Karlsruhe Institute of Technology, Germany；$本文提出了一种可靠的物体检测和分割系统，使用多模态数据解决机器人抓取过程中传感器故障导致损坏的问题。该方法利用门控融合模块和动态集成学习实现了多模态冗余框架，并且引入了新的标签无关的多模态一致性（MC）分数来度量系统输出的可靠性和不确定性。通过实验，证明该系统在缺失模态时比基准模型提供更可靠的性能，并且MC分数是一个比模型置信度得分更可靠的输出可靠性指标。$AI, robotics, object detection, segmentation, multimodal redundancy；$http://arxiv.org/pdf/2210.10842v3
Data Models for Dataset Drift Controls in Machine Learning With Optical  Images$  Camera images are ubiquitous in machine learning research. They also play acentral role in the delivery of important services spanning medicine andenvironmental surveying. However, the application of machine learning models inthese domains has been limited because of robustness concerns. A primaryfailure mode are performance drops due to differences between the training anddeployment data. While there are methods to prospectively validate therobustness of machine learning models to such dataset drifts, existingapproaches do not account for explicit models of the primary object ofinterest: the data. This limits our ability to study and understand therelationship between data generation and downstream machine learning modelperformance in a physically accurate manner. In this study, we demonstrate howto overcome this limitation by pairing traditional machine learning withphysical optics to obtain explicit and differentiable data models. Wedemonstrate how such data models can be constructed for image data and used tocontrol downstream machine learning model performance related to dataset drift.The findings are distilled into three applications. First, drift synthesisenables the controlled generation of physically faithful drift test cases topower model selection and targeted generalization. Second, the gradientconnection between machine learning task model and data model allows advanced,precise tolerancing of task model sensitivity to changes in the datageneration. These drift forensics can be used to precisely specify theacceptable data environments in which a task model may be run. Third, driftoptimization opens up the possibility to create drifts that can help the taskmodel learn better faster, effectively optimizing the data generating processitself. A guide to access the open code and datasets is available athttps://github.com/aiaudit-org/raw2logit.$$$$$http://arxiv.org/pdf/2211.02578v3
Hierarchical Dynamic Image Harmonization$"  Image harmonization is a critical task in computer vision, which aims toadjust the foreground to make it compatible with the background. Recent worksmainly focus on using global transformations (i.e., normalization and colorcurve rendering) to achieve visual consistency. However, these models ignorelocal visual consistency and their huge model sizes limit their harmonizationability on edge devices. In this paper, we propose a hierarchical dynamicnetwork (HDNet) to adapt features from local to global view for better featuretransformation in efficient image harmonization. Inspired by the success ofvarious dynamic models, local dynamic (LD) module and mask-aware global dynamic(MGD) module are proposed in this paper. Specifically, LD matches localrepresentations between the foreground and background regions based on semanticsimilarities, then adaptively adjust every foreground local representationaccording to the appearance of its $K$-nearest neighbor background regions. Inthis way, LD can produce more realistic images at a more fine-grained level,and simultaneously enjoy the characteristic of semantic alignment. The MGDeffectively applies distinct convolution to the foreground and background,learning the representations of foreground and background regions as well astheir correlations to the global harmonization, facilitating local visualconsistency for the images much more efficiently. Experimental resultsdemonstrate that the proposed HDNet significantly reduces the total modelparameters by more than 80\\% compared to previous methods, while stillattaining state-of-the-art performance on the popular iHarmony4 dataset.Notably, the HDNet achieves a 4\\% improvement in PSNR and a 19\\% reduction inMSE compared to the prior state-of-the-art methods."$$$$$http://arxiv.org/pdf/2211.08639v3
Twin-S: A Digital Twin for Skull-base Surgery$  Purpose: Digital twins are virtual interactive models of the real world,exhibiting identical behavior and properties. In surgical applications,computational analysis from digital twins can be used, for example, to enhancesituational awareness. Methods: We present a digital twin framework forskull-base surgeries, named Twin-S, which can be integrated within variousimage-guided interventions seamlessly. Twin-S combines high-precision opticaltracking and real-time simulation. We rely on rigorous calibration routines toensure that the digital twin representation precisely mimics all real-worldprocesses. Twin-S models and tracks the critical components of skull-basesurgery, including the surgical tool, patient anatomy, and surgical camera.Significantly, Twin-S updates and reflects real-world drilling of theanatomical model in frame rate. Results: We extensively evaluate the accuracyof Twin-S, which achieves an average 1.39 mm error during the drilling process.We further illustrate how segmentation masks derived from the continuouslyupdated digital twin can augment the surgical microscope view in a mixedreality setting, where bone requiring ablation is highlighted to providesurgeons additional situational awareness. Conclusion: We present Twin-S, adigital twin environment for skull-base surgery. Twin-S tracks and updates thevirtual model in real-time given measurements from modern trackingtechnologies. Future research on complementing optical tracking withhigher-precision vision-based approaches may further increase the accuracy ofTwin-S.$digital twin, skull base surgery, real-time tracking, augmented reality$Johns Hopkins University和Johns Hopkins Medicine$本文介绍了一种数字孪生框架Twin-S，可用于颅底手术的实时跟踪和模拟，并提供增强现实的额外环境感知力。实验证明该框架的平均误差为1.39毫米。$数字孪生、颅底手术、实时跟踪、增强现实$http://arxiv.org/pdf/2211.11863v2
Diffusion-SDF: Text-to-Shape via Voxelized Diffusion$  With the rising industrial attention to 3D virtual modeling technology,generating novel 3D content based on specified conditions (e.g. text) hasbecome a hot issue. In this paper, we propose a new generative 3D modelingframework called Diffusion-SDF for the challenging task of text-to-shapesynthesis. Previous approaches lack flexibility in both 3D data representationand shape generation, thereby failing to generate highly diversified 3D shapesconforming to the given text descriptions. To address this, we propose a SDFautoencoder together with the Voxelized Diffusion model to learn and generaterepresentations for voxelized signed distance fields (SDFs) of 3D shapes.Specifically, we design a novel UinU-Net architecture that implants alocal-focused inner network inside the standard U-Net architecture, whichenables better reconstruction of patch-independent SDF representations. Weextend our approach to further text-to-shape tasks including text-conditionedshape completion and manipulation. Experimental results show that Diffusion-SDFgenerates both higher quality and more diversified 3D shapes that conform wellto given text descriptions when compared to previous approaches. Code isavailable at: https://github.com/ttlmh/Diffusion-SDF$$$$$http://arxiv.org/pdf/2212.03293v2
Self-Attention Amortized Distributional Projection Optimization for  Sliced Wasserstein Point-Cloud Reconstruction$"  Max sliced Wasserstein (Max-SW) distance has been widely known as a solutionfor less discriminative projections of sliced Wasserstein (SW) distance. Inapplications that have various independent pairs of probability measures,amortized projection optimization is utilized to predict the ``max"" projectingdirections given two input measures instead of using projected gradient ascentmultiple times. Despite being efficient, Max-SW and its amortized versioncannot guarantee metricity property due to the sub-optimality of the projectedgradient ascent and the amortization gap. Therefore, we propose to replaceMax-SW with distributional sliced Wasserstein distance with von Mises-Fisher(vMF) projecting distribution (v-DSW). Since v-DSW is a metric with anynon-degenerate vMF distribution, its amortized version can guarantee themetricity when performing amortization. Furthermore, current amortized modelsare not permutation invariant and symmetric. To address the issue, we designamortized models based on self-attention architecture. In particular, we adoptefficient self-attention architectures to make the computation linear in thenumber of supports. With the two improvements, we derive self-attentionamortized distributional projection optimization and show its appealingperformance in point-cloud reconstruction and its downstream applications."$$$$$http://arxiv.org/pdf/2301.04791v2
GlyphDiffusion: Text Generation as Image Generation$  Diffusion models have become a new generative paradigm for text generation.Considering the discrete categorical nature of text, in this paper, we proposeGlyphDiffusion, a novel diffusion approach for text generation via text-guidedimage generation. Our key idea is to render the target text as a glyph imagecontaining visual language content. In this way, conditional text generationcan be cast as a glyph image generation task, and it is then natural to applycontinuous diffusion models to discrete texts. Specially, we utilize a cascadedarchitecture (ie a base and a super-resolution diffusion model) to generatehigh-fidelity glyph images, conditioned on the input text. Furthermore, wedesign a text grounding module to transform and refine the visual languagecontent from generated glyph images into the final texts. In experiments overfour conditional text generation tasks and two classes of metrics (ie qualityand diversity), GlyphDiffusion can achieve comparable or even better resultsthan several baselines, including pretrained language models. Our model alsomakes significant improvements compared to the recent diffusion model.$$$$$http://arxiv.org/pdf/2304.12519v2
From Association to Generation: Text-only Captioning by Unsupervised  Cross-modal Mapping$  With the development of Vision-Language Pre-training Models (VLPMs)represented by CLIP and ALIGN, significant breakthroughs have been achieved forassociation-based visual tasks such as image classification and image-textretrieval by the zero-shot capability of CLIP without fine-tuning. However,CLIP is hard to apply to generation-based tasks. This is due to the lack ofdecoder architecture and pre-training tasks for generation. Although previousworks have created generation capacity for CLIP through additional languagemodels, a modality gap between the CLIP representations of different modalitiesand the inability of CLIP to model the offset of this gap, which fails theconcept to transfer across modalities. To solve the problem, we try to mapimages/videos to the language modality and generate captions from the languagemodality. In this paper, we propose the K-nearest-neighbor Cross-modalityMapping (Knight), a zero-shot method from association to generation. Withtext-only unsupervised training, Knight achieves State-of-the-Art performancein zero-shot methods for image captioning and video captioning. Our code isavailable at https://github.com/junyangwang0410/Knight.$$$$$http://arxiv.org/pdf/2304.13273v3
Extraction of volumetric indices from echocardiography: which deep  learning solution for clinical use?$  Deep learning-based methods have spearheaded the automatic analysis ofechocardiographic images, taking advantage of the publication of multiple openaccess datasets annotated by experts (CAMUS being one of the largest publicdatabases). However, these models are still considered unreliable by cliniciansdue to unresolved issues concerning i) the temporal consistency of theirpredictions, and ii) their ability to generalize across datasets. In thiscontext, we propose a comprehensive comparison between the current bestperforming methods in medical/echocardiographic image segmentation, with aparticular focus on temporal consistency and cross-dataset aspects. Weintroduce a new private dataset, named CARDINAL, of apical two-chamber andapical four-chamber sequences, with reference segmentation over the fullcardiac cycle. We show that the proposed 3D nnU-Net outperforms alternative 2Dand recurrent segmentation methods. We also report that the best models trainedon CARDINAL, when tested on CAMUS without any fine-tuning, still manage toperform competitively with respect to prior methods. Overall, the experimentalresults suggest that with sufficient training data, 3D nnU-Net could become thefirst automated tool to finally meet the standards of an everyday clinicaldevice.$Ultrasound, cardiac segmentation, temporal segmentation, deep learning, CNN$1Univ Lyon, INSA-Lyon, Universit e Claude Bernard Lyon 1, UJM-Saint Etienne, CNRS, Inserm, CREATIS UMR 5220, U1294, F-69621, Lyon, France; 2Dept. of Computer Science, University of Sherbrooke, Sherbrooke, QC, Canada; 3Cardiology Dept., H^ opital Croix-Rousse, Hospices Civils de Lyon, Lyon, France; 4Cardiology Dept., H^ opital Lyon Sud, Hospices Civils de Lyon, Lyon, France$本研究围绕从超声心动图中提取三维指标展开，比较现有的医学/超声心动图图像分割方法，特别关注其时间连续性和跨数据集性能，提出了一个新的私有数据集CARDINAL，展示了3D nnU-Net模型在性能上胜过其他2D和循环分割方法，并证明了在其他数据集上测试时，基于CARDINAL训练的模型仍然可以在不进行任何微调的情况下与之前的方法竞争。实验结果表明，3D nnU-Net有望成为第一个能够满足日常临床设备标准的自动化工具。$超声、心脏分割、时间分割、深度学习、CNN$http://arxiv.org/pdf/2305.01997v2
Fairness in Image Search: A Study of Occupational Stereotyping in Image  Retrieval and its Debiasing$"  Multi-modal search engines have experienced significant growth and widespreaduse in recent years, making them the second most common internet use. Whilesearch engine systems offer a range of services, the image search field hasrecently become a focal point in the information retrieval community, as theadage goes, ""a picture is worth a thousand words"". Although popular searchengines like Google excel at image search accuracy and agility, there is anongoing debate over whether their search results can be biased in terms ofgender, language, demographics, socio-cultural aspects, and stereotypes. Thispotential for bias can have a significant impact on individuals\' perceptionsand influence their perspectives.  In this paper, we present our study on bias and fairness in web search, witha focus on keyword-based image search. We first discuss several kinds of biasesthat exist in search systems and why it is important to mitigate them. Wenarrow down our study to assessing and mitigating occupational stereotypes inimage search, which is a prevalent fairness issue in image retrieval. For theassessment of stereotypes, we take gender as an indicator. We explore variousopen-source and proprietary APIs for gender identification from images. Withthese, we examine the extent of gender bias in top-tanked image search resultsobtained for several occupational keywords. To mitigate the bias, we thenpropose a fairness-aware re-ranking algorithm that optimizes (a) relevance ofthe search result with the keyword and (b) fairness w.r.t genders identified.We experiment on 100 top-ranked images obtained for 10 occupational keywordsand consider random re-ranking and re-ranking based on relevance as baselines.Our experimental results show that the fairness-aware re-ranking algorithmproduces rankings with better fairness scores and competitive relevance scoresthan the baselines."$$$$$http://arxiv.org/pdf/2305.03881v1
NL-CS Net: Deep Learning with Non-Local Prior for Image Compressive  Sensing$"  Deep learning has been applied to compressive sensing (CS) of imagessuccessfully in recent years. However, existing network-based methods are oftentrained as the black box, in which the lack of prior knowledge is often thebottleneck for further performance improvement. To overcome this drawback, thispaper proposes a novel CS method using non-local prior which combines theinterpretability of the traditional optimization methods with the speed ofnetwork-based methods, called NL-CS Net. We unroll each phase from iteration ofthe augmented Lagrangian method solving non-local and sparse regularizedoptimization problem by a network. NL-CS Net is composed of the up-samplingmodule and the recovery module. In the up-sampling module, we use learnableup-sampling matrix instead of a predefined one. In the recovery module,patch-wise non-local network is employed to capture long-range featurecorrespondences. Important parameters involved (e.g. sampling matrix, nonlineartransforms, shrinkage thresholds, step size, $etc.$) are learned end-to-end,rather than hand-crafted. Furthermore, to facilitate practical implementation,orthogonal and binary constraints on the sampling matrix are simultaneouslyadopted. Extensive experiments on natural images and magnetic resonance imaging(MRI) demonstrate that the proposed method outperforms the state-of-the-artmethods while maintaining great interpretability and speed."$$$$$http://arxiv.org/pdf/2305.03899v1
HateMM: A Multi-Modal Dataset for Hate Video Classification$  Hate speech has become one of the most significant issues in modern society,having implications in both the online and the offline world. Due to this, hatespeech research has recently gained a lot of traction. However, most of thework has primarily focused on text media with relatively little work on imagesand even lesser on videos. Thus, early stage automated video moderationtechniques are needed to handle the videos that are being uploaded to keep theplatform safe and healthy. With a view to detect and remove hateful contentfrom the video sharing platforms, our work focuses on hate video detectionusing multi-modalities. To this end, we curate ~43 hours of videos fromBitChute and manually annotate them as hate or non-hate, along with the framespans which could explain the labelling decision. To collect the relevantvideos we harnessed search keywords from hate lexicons. We observe various cuesin images and audio of hateful videos. Further, we build deep learningmulti-modal models to classify the hate videos and observe that using all themodalities of the videos improves the overall hate speech detection performance(accuracy=0.798, macro F1-score=0.790) by ~5.7% compared to the best uni-modalmodel in terms of macro F1 score. In summary, our work takes the first steptoward understanding and modeling hateful videos on video hosting platformssuch as BitChute.$$$$$http://arxiv.org/pdf/2305.03915v1
Beyond the Model: Data Pre-processing Attack to Deep Learning Models in  Android Apps$  The increasing popularity of deep learning (DL) models and the advantages ofcomputing, including low latency and bandwidth savings on smartphones, have ledto the emergence of intelligent mobile applications, also known as DL apps, inrecent years. However, this technological development has also given rise toseveral security concerns, including adversarial examples, model stealing, anddata poisoning issues. Existing works on attacks and countermeasures foron-device DL models have primarily focused on the models themselves. However,scant attention has been paid to the impact of data processing disturbance onthe model inference. This knowledge disparity highlights the need foradditional research to fully comprehend and address security issues related todata processing for on-device models. In this paper, we introduce a dataprocessing-based attacks against real-world DL apps. In particular, our attackcould influence the performance and latency of the model without affecting theoperation of a DL app. To demonstrate the effectiveness of our attack, we carryout an empirical study on 517 real-world DL apps collected from Google Play.Among 320 apps utilizing MLkit, we find that 81.56\\% of them can besuccessfully attacked.  The results emphasize the importance of DL app developers being aware of andtaking actions to secure on-device models from the perspective of dataprocessing.$$$$$http://arxiv.org/pdf/2305.03963v1
A Sea-Land Clutter Classification Framework for Over-the-Horizon-Radar  Based on Weighted Loss Semi-supervised GAN$  Deep convolutional neural network has made great achievements in sea-landclutter classification for over-the-horizon-radar (OTHR). The premise is that alarge number of labeled training samples must be provided for a sea-landclutter classifier. In practical engineering applications, it is relativelyeasy to obtain label-free sea-land clutter samples. However, the labelingprocess is extremely cumbersome and requires expertise in the field of OTHR. Tosolve this problem, we propose an improved generative adversarial network,namely weighted loss semi-supervised generative adversarial network (WL-SSGAN).Specifically, we propose a joint feature matching loss by weighting the middlelayer features of the discriminator of semi-supervised generative adversarialnetwork. Furthermore, we propose the weighted loss of WL-SSGAN by linearlyweighting standard adversarial loss and joint feature matching loss. Thesemi-supervised classification performance of WL-SSGAN is evaluated on asea-land clutter dataset. The experimental results show that WL-SSGAN canimprove the performance of the fully supervised classifier with only a smallnumber of labeled samples by utilizing a large number of unlabeled sea-landclutter samples. Further, the proposed weighted loss is superior to both theadversarial loss and the feature matching loss. Additionally, we compareWL-SSGAN with conventional semi-supervised classification methods anddemonstrate that WL-SSGAN achieves the highest classification accuracy.$$$$$http://arxiv.org/pdf/2305.04021v1
Gradient Leakage Defense with Key-Lock Module for Federated Learning$  Federated Learning (FL) is a widely adopted privacy-preserving machinelearning approach where private data remains local, enabling securecomputations and the exchange of local model gradients between local clientsand third-party parameter servers. However, recent findings reveal that privacymay be compromised and sensitive information potentially recovered from sharedgradients. In this study, we offer detailed analysis and a novel perspective onunderstanding the gradient leakage problem. These theoretical works lead to anew gradient leakage defense technique that secures arbitrary modelarchitectures using a private key-lock module. Only the locked gradient istransmitted to the parameter server for global model aggregation. Our proposedlearning method is resistant to gradient leakage attacks, and the key-lockmodule is designed and trained to ensure that, without the private informationof the key-lock module: a) reconstructing private training data from the sharedgradient is infeasible; and b) the global model\'s inference performance issignificantly compromised. We discuss the theoretical underpinnings of whygradients can leak private information and provide theoretical proof of ourmethod\'s effectiveness. We conducted extensive empirical evaluations with atotal of forty-four models on several popular benchmarks, demonstrating therobustness of our proposed approach in both maintaining model performance anddefending against gradient leakage attacks.$$$$$http://arxiv.org/pdf/2305.04095v1
Transformer-Based Hierarchical Clustering for Brain Network Analysis$  Brain networks, graphical models such as those constructed from MRI, havebeen widely used in pathological prediction and analysis of brain functions.Within the complex brain system, differences in neuronal connection strengthsparcellate the brain into various functional modules (network communities),which are critical for brain analysis. However, identifying such communitieswithin the brain has been a nontrivial issue due to the complexity of neuronalinteractions. In this work, we propose a novel interpretable transformer-basedmodel for joint hierarchical cluster identification and brain networkclassification. Extensive experimental results on real-world brain networkdatasets show that with the help of hierarchical clustering, the model achievesincreased accuracy and reduced runtime complexity while providing plausibleinsight into the functional organization of brain regions. The implementationis available at https://github.com/DDVD233/THC.$$$$$http://arxiv.org/pdf/2305.04142v1
X-LLM: Bootstrapping Advanced Large Language Models by Treating  Multi-Modalities as Foreign Languages$  Large language models (LLMs) have demonstrated remarkable language abilities.GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilitiesbeyond previous visual language models. We attribute this to the use of moreadvanced LLMs compared with previous multimodal models. Unfortunately, themodel architecture and training strategies of GPT-4 are unknown. To endow LLMswith multimodal capabilities, we propose X-LLM, which converts Multi-modalities(images, speech, videos) into foreign languages using X2L interfaces and inputsthem into a large Language model (ChatGLM). Specifically, X-LLM aligns multiplefrozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X\'\'denotes multi-modalities such as image, speech, and videos, and ``L\'\' denoteslanguages. X-LLM\'s training consists of three stages: (1) Converting MultimodalInformation: The first stage trains each X2L interface to align with itsrespective single-modal encoder separately to convert multimodal informationinto languages. (2) Aligning X2L representations with the LLM: single-modalencoders are aligned with the LLM through X2L interfaces independently. (3)Integrating multiple modalities: all single-modal encoders are aligned with theLLM through X2L interfaces to integrate multimodal capabilities into the LLM.Our experiments show that X-LLM demonstrates impressive multimodel chatabilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseenimages/instructions, and yields a 84.5\\% relative score compared with GPT-4 ona synthetic multimodal instruction-following dataset. And we also conductquantitative tests on using LLM for ASR and multimodal ASR, hoping to promotethe era of LLM-based speech recognition.$$$$$http://arxiv.org/pdf/2305.04160v1
Text-to-Image Diffusion Models can be Easily Backdoored through  Multimodal Data Poisoning$  With the help of conditioning mechanisms, the state-of-the-art diffusionmodels have achieved tremendous success in guided image generation,particularly in text-to-image synthesis. To gain a better understanding of thetraining process and potential risks of text-to-image synthesis, we perform asystematic investigation of backdoor attack on text-to-image diffusion modelsand propose BadT2I, a general multimodal backdoor attack framework that tamperswith image synthesis in diverse semantic levels. Specifically, we performbackdoor attacks on three levels of the vision semantics: Pixel-Backdoor,Object-Backdoor and Style-Backdoor. By utilizing a regularization loss, ourmethods efficiently inject backdoors into a large-scale text-to-image diffusionmodel while preserving its utility with benign inputs. We conduct empiricalexperiments on Stable Diffusion, the widely-used text-to-image diffusion model,demonstrating that the large-scale diffusion model can be easily backdooredwithin a few fine-tuning steps. We conduct additional experiments to explorethe impact of different types of textual triggers. Besides, we discuss thebackdoor persistence during further training, the findings of which provideinsights for the development of backdoor defense methods.$$$$$http://arxiv.org/pdf/2305.04175v1
Bi-Mapper: Holistic BEV Semantic Mapping for Autonomous Driving$  A semantic map of the road scene, covering fundamental road elements, is anessential ingredient in autonomous driving systems. It provides importantperception foundations for positioning and planning when rendered in theBird\'s-Eye-View (BEV). Currently, the prior knowledge of hypothetical depth canguide the learning of translating front perspective views into BEV directlywith the help of calibration parameters. However, it suffers from geometricdistortions in the representation of distant objects. In addition, anotherstream of methods without prior knowledge can learn the transformation betweenfront perspective views and BEV implicitly with a global view. Considering thatthe fusion of different learning methods may bring surprising beneficialeffects, we propose a Bi-Mapper framework for top-down road-scene semanticunderstanding, which incorporates a global view and local prior knowledge. Toenhance reliable interaction between them, an asynchronous mutual learningstrategy is proposed. At the same time, an Across-Space Loss (ASL) is designedto mitigate the negative impact of geometric distortions. Extensive results onnuScenes and Cam2BEV datasets verify the consistent effectiveness of eachmodule in the proposed Bi-Mapper framework. Compared with exiting road mappingnetworks, the proposed Bi-Mapper achieves 5.0 higher IoU on the nuScenesdataset. Moreover, we verify the generalization performance of Bi-Mapper in areal-world driving scenario. Code will be available athttps://github.com/lynn-yu/Bi-Mapper.$Autonomous driving, map construction, semantic segmentation, deep learning, inverse perspective mapping$School of Robotics, Hunan University, Zhejiang University, Karlsruhe Institute of Technology, University of Oxford$该文章提出了一种新的Bi-Mapper框架，融合了局部先验知识和全局自学习能力，可以更好地实现前视图像到鸟瞰图的转换以进行道路场景的语义地图构建，提高自动驾驶系统的感知效果和路径规划能力。$自动驾驶，地图构建，语义分割，深度学习，逆透视变换$http://arxiv.org/pdf/2305.04205v1
AdaptiveClick: Clicks-aware Transformer with Adaptive Focal Loss for  Interactive Image Segmentation$  Interactive Image Segmentation (IIS) has emerged as a promising technique fordecreasing annotation time. Substantial progress has been made in pre- andpost-processing for IIS, but the critical issue of interaction ambiguitynotably hindering segmentation quality, has been under-researched. To addressthis, we introduce AdaptiveClick -- a clicks-aware transformer incorporating anadaptive focal loss, which tackles annotation inconsistencies with tools formask- and pixel-level ambiguity resolution. To the best of our knowledge,AdaptiveClick is the first transformer-based, mask-adaptive segmentationframework for IIS. The key ingredient of our method is the Clicks-awareMask-adaptive Transformer Decoder (CAMD), which enhances the interactionbetween clicks and image features. Additionally, AdaptiveClick enablespixel-adaptive differentiation of hard and easy samples in the decision space,independent of their varying distributions. This is primarily achieved byoptimizing a generalized Adaptive Focal Loss (AFL) with a theoreticalguarantee, where two adaptive coefficients control the ratio of gradient valuesfor hard and easy pixels. Our analysis reveals that the commonly used Focal andBCE losses can be considered special cases of the proposed AFL loss. With aplain ViT backbone, extensive experimental results on nine datasets demonstratethe superiority of AdaptiveClick compared to state-of-the-art methods. Codewill be publicly available at https://github.com/lab206/AdaptiveClick.$本文提出了一种交互式图像分割的模型，使用了Clicks-aware attention和Adaptive Focal Loss等新算法，并且在多个数据集和现有方法上进行了实验验证。$交互式图像分割、注意力机制、Focal Loss、ViT、掩膜适应性$$$http://arxiv.org/pdf/2305.04276v1
Performance Gaps of Artificial Intelligence Models Screening Mammography  -- Towards Fair and Interpretable Models$  Purpose: To analyze the demographic and imaging characteristics associatedwith increased risk of failure for abnormality classification in screeningmammograms. Materials and Methods: This retrospective study used data from theEmory BrEast Imaging Dataset (EMBED) which includes mammograms from 115,931patients imaged at Emory University Healthcare between 2013 to 2020. Clinicaland imaging data includes Breast Imaging Reporting and Data System (BI-RADS)assessment, region of interest coordinates for abnormalities, imaging features,pathologic outcomes, and patient demographics. Multiple deep learning modelswere developed to distinguish between patches of abnormal tissue and randomlyselected patches of normal tissue from the screening mammograms. We assessedmodel performance overall and within subgroups defined by age, race, pathologicoutcome, and imaging characteristics to evaluate reasons formisclassifications. Results: On a test set size of 5,810 studies (13,390patches), a ResNet152V2 model trained to classify normal versus abnormal tissuepatches achieved an accuracy of 92.6% (95% CI = 92.0-93.2%), and area under thereceiver operative characteristics curve 0.975 (95% CI = 0.972-0.978). Imagingcharacteristics associated with higher misclassifications of images includehigher tissue densities (risk ratio [RR]=1.649; p=.010, BI-RADS density C andRR=2.026; p=.003, BI-RADS density D), and presence of architectural distortion(RR=1.026; p&lt;.001). Conclusion: Even though deep learning models forabnormality classification can perform well in screening mammography, wedemonstrate certain imaging features that result in worse model performance.This is the first such work to systematically evaluate breast abnormalityclassification by various subgroups and better-informed developers andend-users of population subgroups which are likely to experience biased modelperformance.$$$$$http://arxiv.org/pdf/2305.04422v1
Breaking Through the Haze: An Advanced Non-Homogeneous Dehazing Method  based on Fast Fourier Convolution and ConvNeXt$  Haze usually leads to deteriorated images with low contrast, color shift andstructural distortion. We observe that many deep learning based models exhibitexceptional performance on removing homogeneous haze, but they usually fail toaddress the challenge of non-homogeneous dehazing. Two main factors account forthis situation. Firstly, due to the intricate and non uniform distribution ofdense haze, the recovery of structural and chromatic features with highfidelity is challenging, particularly in regions with heavy haze. Secondly, theexisting small scale datasets for non-homogeneous dehazing are inadequate tosupport reliable learning of feature mappings between hazy images and theircorresponding haze-free counterparts by convolutional neural network(CNN)-based models. To tackle these two challenges, we propose a novel twobranch network that leverages 2D discrete wavelete transform (DWT), fastFourier convolution (FFC) residual block and a pretrained ConvNeXt model.Specifically, in the DWT-FFC frequency branch, our model exploits DWT tocapture more high-frequency features. Moreover, by taking advantage of thelarge receptive field provided by FFC residual blocks, our model is able toeffectively explore global contextual information and produce images withbetter perceptual quality. In the prior knowledge branch, an ImageNetpretrained ConvNeXt as opposed to Res2Net is adopted. This enables our model tolearn more supplementary information and acquire a stronger generalizationability. The feasibility and effectiveness of the proposed method isdemonstrated via extensive experiments and ablation studies. The code isavailable at https://github.com/zhouh115/DWT-FFC.$$$$$http://arxiv.org/pdf/2305.04430v1
Robust Traffic Light Detection Using Salience-Sensitive Loss:  Computational Framework and Evaluations$  One of the most important tasks for ensuring safe autonomous driving systemsis accurately detecting road traffic lights and accurately determining how theyimpact the driver\'s actions. In various real-world driving situations, a scenemay have numerous traffic lights with varying levels of relevance to thedriver, and thus, distinguishing and detecting the lights that are relevant tothe driver and influence the driver\'s actions is a critical safety task. Thispaper proposes a traffic light detection model which focuses on this task byfirst defining salient lights as the lights that affect the driver\'s futuredecisions. We then use this salience property to construct the LAVA SalientLights Dataset, the first US traffic light dataset with an annotated salienceproperty. Subsequently, we train a Deformable DETR object detection transformermodel using Salience-Sensitive Focal Loss to emphasize stronger performance onsalient traffic lights, showing that a model trained with this loss functionhas stronger recall than one trained without.$$$$$http://arxiv.org/pdf/2305.04516v1
Latest Trends in Artificial Intelligence Technology: A Scoping Review$  Artificial intelligence is more ubiquitous in multiple domains. Smartphones,social media platforms, search engines, and autonomous vehicles are just a fewexamples of applications that utilize artificial intelligence technologies toenhance their performance. This study carries out a scoping review of thecurrent state-of-the-art artificial intelligence technologies following thePRISMA framework. The goal was to find the most advanced technologies used indifferent domains of artificial intelligence technology research. Threerecognized journals were used from artificial intelligence and machine learningdomain: Journal of Artificial Intelligence Research, Journal of MachineLearning Research, and Machine Learning, and articles published in 2022 wereobserved. Certain qualifications were laid for the technological solutions: thetechnology must be tested against comparable solutions, commonly approved orotherwise well justified datasets must be used while applying, and results mustshow improvements against comparable solutions. One of the most important partsof the technology development appeared to be how to process and exploit thedata gathered from multiple sources. The data can be highly unstructured andthe technological solution should be able to utilize the data with minimummanual work from humans. The results of this review indicate that creatinglabeled datasets is very laborious, and solutions exploiting unsupervised orsemi-supervised learning technologies are more and more researched. Thelearning algorithms should be able to be updated efficiently, and predictionsshould be interpretable. Using artificial intelligence technologies inreal-world applications, safety and explainable predictions are mandatory toconsider before mass adoption can occur.$$$$$http://arxiv.org/pdf/2305.04532v1
Development of a Vision System to Enhance the Reliability of the  Pick-and-Place Robot for Autonomous Testing of Camera Module used in  Smartphones$  Pick-and-place robots are commonly used in modern industrial manufacturing.For complex devices/parts like camera modules used in smartphones, whichcontain optical parts, electrical components and interfacing connectors, theplacement operation may not absolutely accurate, which may cause damage in thedevice under test during the mechanical movement to make good contact forelectrical functions inspection. In this paper, we proposed an effective visionsystem including hardware and algorithm to enhance the reliability of thepick-and-place robot for autonomous testing memory of camera modules. Withlimited hardware based on camera and raspberry PI and using simplify imageprocessing algorithm based on histogram information, the vision system canconfirm the presence of the camera modules in feeding tray and the placementaccuracy of the camera module in test socket. Through that, the system can workwith more flexibility and avoid damaging the device under test. The system wasexperimentally quantified through testing approximately 2000 camera modules ina stable light condition. Experimental results demonstrate that the systemachieves accuracy of more than 99.92%. With its simplicity and effectiveness,the proposed vision system can be considered as a useful solution for using inpick-and-place systems in industry.$$$$$http://arxiv.org/pdf/2305.04605v1
The Treachery of Images: Bayesian Scene Keypoints for Deep Policy  Learning in Robotic Manipulation$  In policy learning for robotic manipulation, sample efficiency is ofparamount importance. Thus, learning and extracting more compactrepresentations from camera observations is a promising avenue. However,current methods often assume full observability of the scene and struggle withscale invariance. In many tasks and settings, this assumption does not hold asobjects in the scene are often occluded or lie outside the field of view of thecamera, rendering the camera observation ambiguous with regard to theirlocation. To tackle this problem, we present BASK, a Bayesian approach totracking scale-invariant keypoints over time. Our approach successfullyresolves inherent ambiguities in images, enabling keypoint tracking onsymmetrical objects and occluded and out-of-view objects. We employ our methodto learn challenging multi-object robot manipulation tasks from wrist cameraobservations and demonstrate superior utility for policy learning compared toother representation learning techniques. Furthermore, we show outstandingrobustness towards disturbances such as clutter, occlusions, and noisy depthmeasurements, as well as generalization to unseen objects both in simulationand real-world robotic experiments.$$$$$http://arxiv.org/pdf/2305.04718v1
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning$  The ability of deep neural networks to continually learn and adapt to asequence of tasks has remained challenging due to catastrophic forgetting ofpreviously learned tasks. Humans, on the other hand, have a remarkable abilityto acquire, assimilate, and transfer knowledge across tasks throughout theirlifetime without catastrophic forgetting. The versatility of the brain can beattributed to the rehearsal of abstract experiences through a complementarylearning system. However, representation rehearsal in vision transformers lacksdiversity, resulting in overfitting and consequently, performance dropssignificantly compared to raw image rehearsal. Therefore, we propose BiRT, anovel representation rehearsal-based continual learning approach using visiontransformers. Specifically, we introduce constructive noises at various stagesof the vision transformer and enforce consistency in predictions with respectto an exponential moving average of the working model. Our method providesconsistent performance gain over raw image and vanilla representation rehearsalon several challenging CL benchmarks, while being memory efficient and robustto natural and adversarial corruptions.$$$$$http://arxiv.org/pdf/2305.04769v1
Reasoning with Language Model Prompting: A Survey$  Reasoning, as an essential ability for complex problem-solving, can provideback-end support for various real-world applications, such as medicaldiagnosis, negotiation, etc. This paper provides a comprehensive survey ofcutting-edge research on reasoning with language model prompting. We introduceresearch works with comparisons and summaries and provide systematic resourcesto help beginners. We also discuss the potential reasons for emerging suchreasoning abilities and highlight future research directions. Resources areavailable at https://github.com/zjunlp/Prompt4ReasoningPapers (updatedperiodically).$$$$$http://arxiv.org/pdf/2212.09597v3
Energy-based Models are Zero-Shot Planners for Compositional Scene  Rearrangement$  Language is compositional; an instruction can express multiple relationconstraints to hold among objects in a scene that a robot is tasked torearrange. Our focus in this work is an instructable scene-rearrangingframework that generalizes to longer instructions and to spatial conceptcompositions never seen at training time. We propose to representlanguage-instructed spatial concepts with energy functions over relative objectarrangements. A language parser maps instructions to corresponding energyfunctions and an open-vocabulary visual-language model grounds their argumentsto relevant objects in the scene. We generate goal scene configurations bygradient descent on the sum of energy functions, one per language predicate inthe instruction. Local vision-based policies then re-locate objects to theinferred goal locations. We test our model on established instruction-guidedmanipulation benchmarks, as well as benchmarks of compositional instructions weintroduce. We show our model can execute highly compositional instructionszero-shot in simulation and in the real world. It outperformslanguage-to-action reactive policies and Large Language Model planners by alarge margin, especially for long instructions that involve compositions ofmultiple spatial concepts. Simulation and real-world robot execution videos, aswell as our code and datasets are publicly available on our website:https://ebmplanner.github.io.$$$$$http://arxiv.org/pdf/2304.14391v2
A Variational Perspective on Solving Inverse Problems with Diffusion  Models$  Diffusion models have emerged as a key pillar of foundation models in visualdomains. One of their critical applications is to universally solve differentdownstream inverse tasks via a single diffusion prior without re-training foreach task. Most inverse tasks can be formulated as inferring a posteriordistribution over data (e.g., a full image) given a measurement (e.g., a maskedimage). This is however challenging in diffusion models since the nonlinear anditerative nature of the diffusion process renders the posterior intractable. Tocope with this challenge, we propose a variational approach that by designseeks to approximate the true posterior distribution. We show that our approachnaturally leads to regularization by denoising diffusion process (RED-Diff)where denoisers at different timesteps concurrently impose different structuralconstraints over the image. To gauge the contribution of denoisers fromdifferent timesteps, we propose a weighting mechanism based onsignal-to-noise-ratio (SNR). Our approach provides a new variationalperspective for solving inverse problems with diffusion models, allowing us toformulate sampling as stochastic optimization, where one can simply applyoff-the-shelf solvers with lightweight iterates. Our experiments for imagerestoration tasks such as inpainting and superresolution demonstrate thestrengths of our method compared with state-of-the-art sampling-based diffusionmodels.$inverse problems, diffusion models, variational inference, regularization, image restoration$NVIDIA Inc.$本文提出了一种基于变分推断的方法，用于解决扩散模型中的反问题，即推断测量下的数据后验分布，解决极度非线性的过程。本文的方法称为RED-Diff，提供了一种新的视角来解决扩散模型中的反问题，其中采样可以用作随机优化。同时，在图像修复任务中，与基于采样的扩散模型相比，我们的方法表现出更强的能力。$反问题、扩散模型、变分推断、正则化、图像恢复$http://arxiv.org/pdf/2305.04391v1
