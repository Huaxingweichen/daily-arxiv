title$summary$tag$affiliation$summary_zh$tag_zh$url
End-to-end Weakly-supervised Single-stage Multiple 3D Hand Mesh  Reconstruction from a Single RGB Image$  In this paper, we consider the challenging task of simultaneously locatingand recovering multiple hands from a single 2D image. Previous studies eitherfocus on single hand reconstruction or solve this problem in a multi-stage way.Moreover, the conventional two-stage pipeline firstly detects hand areas, andthen estimates 3D hand pose from each cropped patch. To reduce thecomputational redundancy in preprocessing and feature extraction, for the firsttime, we propose a concise but efficient single-stage pipeline for multi-handreconstruction. Specifically, we design a multi-head auto-encoder structure,where each head network shares the same feature map and outputs the handcenter, pose and texture, respectively. Besides, we adopt a weakly-supervisedscheme to alleviate the burden of expensive 3D real-world data annotations. Tothis end, we propose a series of losses optimized by a stage-wise trainingscheme, where a multi-hand dataset with 2D annotations is generated based onthe publicly available single hand datasets. In order to further improve theaccuracy of the weakly supervised model, we adopt several feature consistencyconstraints in both single and multiple hand settings. Specifically, thekeypoints of each hand estimated from local features should be consistent withthe re-projected points predicted from global features. Extensive experimentson public benchmarks including FreiHAND, HO3D, InterHand2.6M and RHDdemonstrate that our method outperforms the state-of-the-art model-basedmethods in both weakly-supervised and fully-supervised manners. The code andmodels are available at {https://github.com/zijinxuxu/SMHR}.$$$$$http://arxiv.org/pdf/2204.08154v3
SImProv: Scalable Image Provenance Framework for Robust Content  Attribution$  We present SImProv - a scalable image provenance framework to match a queryimage back to a trusted database of originals and identify possiblemanipulations on the query. SImProv consists of three stages: a scalable searchstage for retrieving top-k most similar images; a re-ranking andnear-duplicated detection stage for identifying the original among thecandidates; and finally a manipulation detection and visualization stage forlocalizing regions within the query that may have been manipulated to differfrom the original. SImProv is robust to benign image transformations thatcommonly occur during online redistribution, such as artifacts due to noise andrecompression degradation, as well as out-of-place transformations due to imagepadding, warping, and changes in size and shape. Robustness towardsout-of-place transformations is achieved via the end-to-end training of adifferentiable warping module within the comparator architecture. Wedemonstrate effective retrieval and manipulation detection over a dataset of100 million images.$$$$$http://arxiv.org/pdf/2206.14245v2
Unified Object Detector for Different Modalities based on Vision  Transformers$  Traditional systems typically require different models for processingdifferent modalities, such as one model for RGB images and another for depthimages. Recent research has demonstrated that a single model for one modalitycan be adapted for another using cross-modality transfer learning. In thispaper, we extend this approach by combining cross/inter-modality transferlearning with a vision transformer to develop a unified detector that achievessuperior performance across diverse modalities. Our research envisions anapplication scenario for robotics, where the unified system seamlessly switchesbetween RGB cameras and depth sensors in varying lighting conditions.Importantly, the system requires no model architecture or weight updates toenable this smooth transition. Specifically, the system uses the depth sensorduring low-lighting conditions (night time) and both the RGB camera and depthsensor or RGB caemra only in well-lit environments. We evaluate our unifiedmodel on the SUN RGB-D dataset, and demonstrate that it achieves similar orbetter performance in terms of mAP50 compared to state-of-the-art methods inthe SUNRGBD16 category, and comparable performance in point cloud only mode. Wealso introduce a novel inter-modality mixing method that enables our model toachieve significantly better results than previous methods. We provide ourcode, including training/inference logs and model checkpoints, to facilitatereproducibility and further research.\\url{https://github.com/liketheflower/UODDM}$$$$$http://arxiv.org/pdf/2207.01071v2
