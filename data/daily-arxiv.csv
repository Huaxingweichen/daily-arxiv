title,summary,tag,affiliation,summary_zh,tag_zh,url
SImProv: Scalable Image Provenance Framework for Robust Content  Attribution,"  We present SImProv - a scalable image provenance framework to match a queryimage back to a trusted database of originals and identify possiblemanipulations on the query. SImProv consists of three stages: a scalable searchstage for retrieving top-k most similar images; a re-ranking andnear-duplicated detection stage for identifying the original among thecandidates; and finally a manipulation detection and visualization stage forlocalizing regions within the query that may have been manipulated to differfrom the original. SImProv is robust to benign image transformations thatcommonly occur during online redistribution, such as artifacts due to noise andrecompression degradation, as well as out-of-place transformations due to imagepadding, warping, and changes in size and shape. Robustness towardsout-of-place transformations is achieved via the end-to-end training of adifferentiable warping module within the comparator architecture. Wedemonstrate effective retrieval and manipulation detection over a dataset of100 million images.",,,,,http://arxiv.org/pdf/2206.14245v2
Unified Object Detector for Different Modalities based on Vision  Transformers,"  Traditional systems typically require different models for processingdifferent modalities, such as one model for RGB images and another for depthimages. Recent research has demonstrated that a single model for one modalitycan be adapted for another using cross-modality transfer learning. In thispaper, we extend this approach by combining cross/inter-modality transferlearning with a vision transformer to develop a unified detector that achievessuperior performance across diverse modalities. Our research envisions anapplication scenario for robotics, where the unified system seamlessly switchesbetween RGB cameras and depth sensors in varying lighting conditions.Importantly, the system requires no model architecture or weight updates toenable this smooth transition. Specifically, the system uses the depth sensorduring low-lighting conditions (night time) and both the RGB camera and depthsensor or RGB caemra only in well-lit environments. We evaluate our unifiedmodel on the SUN RGB-D dataset, and demonstrate that it achieves similar orbetter performance in terms of mAP50 compared to state-of-the-art methods inthe SUNRGBD16 category, and comparable performance in point cloud only mode. Wealso introduce a novel inter-modality mixing method that enables our model toachieve significantly better results than previous methods. We provide ourcode, including training/inference logs and model checkpoints, to facilitatereproducibility and further research.\\url{https://github.com/liketheflower/UODDM}",,,,,http://arxiv.org/pdf/2207.01071v2
InterTrack: Interaction Transformer for 3D Multi-Object Tracking,"  3D multi-object tracking (MOT) is a key problem for autonomous vehicles,required to perform well-informed motion planning in dynamic environments.Particularly for densely occupied scenes, associating existing tracks to newdetections remains challenging as existing systems tend to omit criticalcontextual information. Our proposed solution, InterTrack, introduces theInteraction Transformer for 3D MOT to generate discriminative objectrepresentations for data association. We extract state and shape features foreach track and detection, and efficiently aggregate global information viaattention. We then perform a learned regression on each track/detection featurepair to estimate affinities, and use a robust two-stage data association andtrack management approach to produce the final tracks. We validate our approachon the nuScenes 3D MOT benchmark, where we observe significant improvements,particularly on classes with small physical sizes and clustered objects. As ofsubmission, InterTrack ranks 1st in overall AMOTA among methods usingCenterPoint detections.",3D multi-object tracking,University of Toronto Robotics Institute,提出一种新的3D MOT解决方案，通过引入交互变换器为数据关联生成判别式目标表示。在nuScenes 3D MOT基准测试中进行验证，取得显著进展。,3D多目标跟踪,http://arxiv.org/pdf/2208.08041v2
FS-BAN: Born-Again Networks for Domain Generalization Few-Shot  Classification,"  Conventional Few-shot classification (FSC) aims to recognize samples fromnovel classes given limited labeled data. Recently, domain generalization FSC(DG-FSC) has been proposed with the goal to recognize novel class samples fromunseen domains. DG-FSC poses considerable challenges to many models due to thedomain shift between base classes (used in training) and novel classes(encountered in evaluation). In this work, we make two novel contributions totackle DG-FSC. Our first contribution is to propose Born-Again Network (BAN)episodic training and comprehensively investigate its effectiveness for DG-FSC.As a specific form of knowledge distillation, BAN has been shown to achieveimproved generalization in conventional supervised classification with aclosed-set setup. This improved generalization motivates us to study BAN forDG-FSC, and we show that BAN is promising to address the domain shiftencountered in DG-FSC. Building on the encouraging findings, our second (major)contribution is to propose Few-Shot BAN (FS-BAN), a novel BAN approach forDG-FSC. Our proposed FS-BAN includes novel multi-task learning objectives:Mutual Regularization, Mismatched Teacher, and Meta-Control Temperature, eachof these is specifically designed to overcome central and unique challenges inDG-FSC, namely overfitting and domain discrepancy. We analyze different designchoices of these techniques. We conduct comprehensive quantitative andqualitative analysis and evaluation over six datasets and three baselinemodels. The results suggest that our proposed FS-BAN consistently improves thegeneralization performance of baseline models and achieves state-of-the-artaccuracy for DG-FSC. Project Page: https://yunqing-me.github.io/Born-Again-FS/.",,,,,http://arxiv.org/pdf/2208.10930v4
SwinFIR: Revisiting the SwinIR with Fast Fourier Convolution and  Improved Training for Image Super-Resolution,"  Transformer-based methods have achieved impressive image restorationperformance due to their capacities to model long-range dependency compared toCNN-based methods. However, advances like SwinIR adopts the window-based andlocal attention strategy to balance the performance and computational overhead,which restricts employing large receptive fields to capture global informationand establish long dependencies in the early layers. To further improve theefficiency of capturing global information, in this work, we propose SwinFIR toextend SwinIR by replacing Fast Fourier Convolution (FFC) components, whichhave the image-wide receptive field. We also revisit other advanced techniques,i.e, data augmentation, pre-training, and feature ensemble to improve theeffect of image reconstruction. And our feature ensemble method enables theperformance of the model to be considerably enhanced without increasing thetraining and testing time. We applied our algorithm on multiple popularlarge-scale benchmarks and achieved state-of-the-art performance comparing tothe existing methods. For example, our SwinFIR achieves the PSNR of 32.83 dB onManga109 dataset, which is 0.8 dB higher than the state-of-the-art SwinIRmethod.",,,,,http://arxiv.org/pdf/2208.11247v2
Understanding the Tricks of Deep Learning in Medical Image Segmentation:  Challenges and Future Directions,"  Over the past few years, the rapid development of deep learning technologiesfor computer vision has significantly improved the performance of medical imagesegmentation (MedISeg). However, the diverse implementation strategies ofvarious models have led to an extremely complex MedISeg system, resulting in apotential problem of unfair result comparisons. In this paper, we collect aseries of MedISeg tricks for different model implementation phases (i.e.,pre-training model, data pre-processing, data augmentation, modelimplementation, model inference, and result post-processing), andexperimentally explore the effectiveness of these tricks on consistentbaselines. With the extensive experimental results on both the representative2D and 3D medical image datasets, we explicitly clarify the effect of thesetricks. Moreover, based on the surveyed tricks, we also open-sourced a strongMedISeg repository, where each component has the advantage of plug-and-play. Webelieve that this milestone work not only completes a comprehensive andcomplementary survey of the state-of-the-art MedISeg approaches, but alsooffers a practical guide for addressing the future medical image processingchallenges including but not limited to small dataset, class imbalancelearning, multi-modality learning, and domain adaptation. The code and trainingweights have been released at: https://github.com/hust-linyi/seg_trick.",,,,,http://arxiv.org/pdf/2209.10307v2
Make-A-Story: Visual Memory Conditioned Consistent Story Generation,"  There has been a recent explosion of impressive generative models that canproduce high quality images (or videos) conditioned on text descriptions.However, all such approaches rely on conditional sentences that containunambiguous descriptions of scenes and main actors in them. Therefore employingsuch models for more complex task of story visualization, where naturallyreferences and co-references exist, and one requires to reason about when tomaintain consistency of actors and backgrounds across frames/scenes, and whennot to, based on story progression, remains a challenge. In this work, weaddress the aforementioned challenges and propose a novel autoregressivediffusion-based framework with a visual memory module that implicitly capturesthe actor and background context across the generated frames.Sentence-conditioned soft attention over the memories enables effectivereference resolution and learns to maintain scene and actor consistency whenneeded. To validate the effectiveness of our approach, we extend the MUGENdataset and introduce additional characters, backgrounds and referencing inmulti-sentence storylines. Our experiments for story generation on the MUGEN,the PororoSV and the FlintstonesSV dataset show that our method not onlyoutperforms prior state-of-the-art in generating frames with high visualquality, which are consistent with the story, but also models appropriatecorrespondences between the characters and the background.",,,,,http://arxiv.org/pdf/2211.13319v3
Pose-disentangled Contrastive Learning for Self-supervised Facial  Representation,"  Self-supervised facial representation has recently attracted increasingattention due to its ability to perform face understanding without relying onlarge-scale annotated datasets heavily. However, analytically, currentcontrastive-based self-supervised learning (SSL) still performsunsatisfactorily for learning facial representation. More specifically,existing contrastive learning (CL) tends to learn pose-invariant features thatcannot depict the pose details of faces, compromising the learning performance.To conquer the above limitation of CL, we propose a novel Pose-disentangledContrastive Learning (PCL) method for general self-supervised facialrepresentation. Our PCL first devises a pose-disentangled decoder (PDD) with adelicately designed orthogonalizing regulation, which disentangles thepose-related features from the face-aware features; therefore, pose-related andother pose-unrelated facial information could be performed in individualsubnetworks and do not affect each other\'s training. Furthermore, we introducea pose-related contrastive learning scheme that learns pose-related informationbased on data augmentation of the same image, which would deliver moreeffective face-aware representation for various downstream tasks. We conductedlinear evaluation on four challenging downstream facial understanding tasks,ie, facial expression recognition, face recognition, AU detection and head poseestimation. Experimental results demonstrate that our method significantlyoutperforms state-of-the-art SSL methods. Code is available athttps://github.com/DreamMr/PCL}{https://github.com/DreamMr/PCL",,,,,http://arxiv.org/pdf/2211.13490v2
Diff-Font: Diffusion Model for Robust One-Shot Font Generation,"  Font generation is a difficult and time-consuming task, especially in thoselanguages using ideograms that have complicated structures with a large numberof characters, such as Chinese. To solve this problem, few-shot font generationand even one-shot font generation have attracted a lot of attention. However,most existing font generation methods may still suffer from (i) largecross-font gap challenge; (ii) subtle cross-font variation problem; and (iii)incorrect generation of complicated characters. In this paper, we propose anovel one-shot font generation method based on a diffusion model, namedDiff-Font, which can be stably trained on large datasets. The proposed modelaims to generate the entire font library by giving only one sample as thereference. Specifically, a large stroke-wise dataset is constructed, and astroke-wise diffusion model is proposed to preserve the structure and thecompletion of each generated character. To our best knowledge, the proposedDiff-Font is the first work that developed diffusion models to handle the fontgeneration task. The well-trained Diff-Font is not only robust to font gap andfont variation, but also achieved promising performance on difficult charactergeneration. Compared to previous font generation methods, our model reachesstate-of-the-art performance both qualitatively and quantitatively.",,,,,http://arxiv.org/pdf/2212.05895v3
Mask-FPAN: Semi-Supervised Face Parsing in the Wild With De-Occlusion  and UV GAN,"  Fine-grained semantic segmentation of a person\'s face and head, includingfacial parts and head components, has progressed a great deal in recent years.However, it remains a challenging task, whereby considering ambiguousocclusions and large pose variations are particularly difficult. To overcomethese difficulties, we propose a novel framework termed Mask-FPAN. It uses ade-occlusion module that learns to parse occluded faces in a semi-supervisedway. In particular, face landmark localization, face occlusionstimations, anddetected head poses are taken into account. A 3D morphable face model combinedwith the UV GAN improves the robustness of 2D face parsing. In addition, weintroduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for faceparing work. The proposed Mask-FPAN framework addresses the face parsingproblem in the wild and shows significant performance improvements with MIOUfrom 0.7353 to 0.9013 compared to the state-of-the-art on challenging facedatasets.",,,,,http://arxiv.org/pdf/2212.09098v4
