title,summary,tag,affiliation,summary_zh,tag_zh,url
SImProv: Scalable Image Provenance Framework for Robust Content  Attribution,"  We present SImProv - a scalable image provenance framework to match a queryimage back to a trusted database of originals and identify possiblemanipulations on the query. SImProv consists of three stages: a scalable searchstage for retrieving top-k most similar images; a re-ranking andnear-duplicated detection stage for identifying the original among thecandidates; and finally a manipulation detection and visualization stage forlocalizing regions within the query that may have been manipulated to differfrom the original. SImProv is robust to benign image transformations thatcommonly occur during online redistribution, such as artifacts due to noise andrecompression degradation, as well as out-of-place transformations due to imagepadding, warping, and changes in size and shape. Robustness towardsout-of-place transformations is achieved via the end-to-end training of adifferentiable warping module within the comparator architecture. Wedemonstrate effective retrieval and manipulation detection over a dataset of100 million images.",,,,,http://arxiv.org/pdf/2206.14245v2
Unified Object Detector for Different Modalities based on Vision  Transformers,"  Traditional systems typically require different models for processingdifferent modalities, such as one model for RGB images and another for depthimages. Recent research has demonstrated that a single model for one modalitycan be adapted for another using cross-modality transfer learning. In thispaper, we extend this approach by combining cross/inter-modality transferlearning with a vision transformer to develop a unified detector that achievessuperior performance across diverse modalities. Our research envisions anapplication scenario for robotics, where the unified system seamlessly switchesbetween RGB cameras and depth sensors in varying lighting conditions.Importantly, the system requires no model architecture or weight updates toenable this smooth transition. Specifically, the system uses the depth sensorduring low-lighting conditions (night time) and both the RGB camera and depthsensor or RGB caemra only in well-lit environments. We evaluate our unifiedmodel on the SUN RGB-D dataset, and demonstrate that it achieves similar orbetter performance in terms of mAP50 compared to state-of-the-art methods inthe SUNRGBD16 category, and comparable performance in point cloud only mode. Wealso introduce a novel inter-modality mixing method that enables our model toachieve significantly better results than previous methods. We provide ourcode, including training/inference logs and model checkpoints, to facilitatereproducibility and further research.\\url{https://github.com/liketheflower/UODDM}",,,,,http://arxiv.org/pdf/2207.01071v2
InterTrack: Interaction Transformer for 3D Multi-Object Tracking,"  3D multi-object tracking (MOT) is a key problem for autonomous vehicles,required to perform well-informed motion planning in dynamic environments.Particularly for densely occupied scenes, associating existing tracks to newdetections remains challenging as existing systems tend to omit criticalcontextual information. Our proposed solution, InterTrack, introduces theInteraction Transformer for 3D MOT to generate discriminative objectrepresentations for data association. We extract state and shape features foreach track and detection, and efficiently aggregate global information viaattention. We then perform a learned regression on each track/detection featurepair to estimate affinities, and use a robust two-stage data association andtrack management approach to produce the final tracks. We validate our approachon the nuScenes 3D MOT benchmark, where we observe significant improvements,particularly on classes with small physical sizes and clustered objects. As ofsubmission, InterTrack ranks 1st in overall AMOTA among methods usingCenterPoint detections.",,,,,http://arxiv.org/pdf/2208.08041v2
FS-BAN: Born-Again Networks for Domain Generalization Few-Shot  Classification,"  Conventional Few-shot classification (FSC) aims to recognize samples fromnovel classes given limited labeled data. Recently, domain generalization FSC(DG-FSC) has been proposed with the goal to recognize novel class samples fromunseen domains. DG-FSC poses considerable challenges to many models due to thedomain shift between base classes (used in training) and novel classes(encountered in evaluation). In this work, we make two novel contributions totackle DG-FSC. Our first contribution is to propose Born-Again Network (BAN)episodic training and comprehensively investigate its effectiveness for DG-FSC.As a specific form of knowledge distillation, BAN has been shown to achieveimproved generalization in conventional supervised classification with aclosed-set setup. This improved generalization motivates us to study BAN forDG-FSC, and we show that BAN is promising to address the domain shiftencountered in DG-FSC. Building on the encouraging findings, our second (major)contribution is to propose Few-Shot BAN (FS-BAN), a novel BAN approach forDG-FSC. Our proposed FS-BAN includes novel multi-task learning objectives:Mutual Regularization, Mismatched Teacher, and Meta-Control Temperature, eachof these is specifically designed to overcome central and unique challenges inDG-FSC, namely overfitting and domain discrepancy. We analyze different designchoices of these techniques. We conduct comprehensive quantitative andqualitative analysis and evaluation over six datasets and three baselinemodels. The results suggest that our proposed FS-BAN consistently improves thegeneralization performance of baseline models and achieves state-of-the-artaccuracy for DG-FSC. Project Page: https://yunqing-me.github.io/Born-Again-FS/.",,,,,http://arxiv.org/pdf/2208.10930v4
SwinFIR: Revisiting the SwinIR with Fast Fourier Convolution and  Improved Training for Image Super-Resolution,"  Transformer-based methods have achieved impressive image restorationperformance due to their capacities to model long-range dependency compared toCNN-based methods. However, advances like SwinIR adopts the window-based andlocal attention strategy to balance the performance and computational overhead,which restricts employing large receptive fields to capture global informationand establish long dependencies in the early layers. To further improve theefficiency of capturing global information, in this work, we propose SwinFIR toextend SwinIR by replacing Fast Fourier Convolution (FFC) components, whichhave the image-wide receptive field. We also revisit other advanced techniques,i.e, data augmentation, pre-training, and feature ensemble to improve theeffect of image reconstruction. And our feature ensemble method enables theperformance of the model to be considerably enhanced without increasing thetraining and testing time. We applied our algorithm on multiple popularlarge-scale benchmarks and achieved state-of-the-art performance comparing tothe existing methods. For example, our SwinFIR achieves the PSNR of 32.83 dB onManga109 dataset, which is 0.8 dB higher than the state-of-the-art SwinIRmethod.",,,,,http://arxiv.org/pdf/2208.11247v2
Understanding the Tricks of Deep Learning in Medical Image Segmentation:  Challenges and Future Directions,"  Over the past few years, the rapid development of deep learning technologiesfor computer vision has significantly improved the performance of medical imagesegmentation (MedISeg). However, the diverse implementation strategies ofvarious models have led to an extremely complex MedISeg system, resulting in apotential problem of unfair result comparisons. In this paper, we collect aseries of MedISeg tricks for different model implementation phases (i.e.,pre-training model, data pre-processing, data augmentation, modelimplementation, model inference, and result post-processing), andexperimentally explore the effectiveness of these tricks on consistentbaselines. With the extensive experimental results on both the representative2D and 3D medical image datasets, we explicitly clarify the effect of thesetricks. Moreover, based on the surveyed tricks, we also open-sourced a strongMedISeg repository, where each component has the advantage of plug-and-play. Webelieve that this milestone work not only completes a comprehensive andcomplementary survey of the state-of-the-art MedISeg approaches, but alsooffers a practical guide for addressing the future medical image processingchallenges including but not limited to small dataset, class imbalancelearning, multi-modality learning, and domain adaptation. The code and trainingweights have been released at: https://github.com/hust-linyi/seg_trick.",,,,,http://arxiv.org/pdf/2209.10307v2
Make-A-Story: Visual Memory Conditioned Consistent Story Generation,"  There has been a recent explosion of impressive generative models that canproduce high quality images (or videos) conditioned on text descriptions.However, all such approaches rely on conditional sentences that containunambiguous descriptions of scenes and main actors in them. Therefore employingsuch models for more complex task of story visualization, where naturallyreferences and co-references exist, and one requires to reason about when tomaintain consistency of actors and backgrounds across frames/scenes, and whennot to, based on story progression, remains a challenge. In this work, weaddress the aforementioned challenges and propose a novel autoregressivediffusion-based framework with a visual memory module that implicitly capturesthe actor and background context across the generated frames.Sentence-conditioned soft attention over the memories enables effectivereference resolution and learns to maintain scene and actor consistency whenneeded. To validate the effectiveness of our approach, we extend the MUGENdataset and introduce additional characters, backgrounds and referencing inmulti-sentence storylines. Our experiments for story generation on the MUGEN,the PororoSV and the FlintstonesSV dataset show that our method not onlyoutperforms prior state-of-the-art in generating frames with high visualquality, which are consistent with the story, but also models appropriatecorrespondences between the characters and the background.",,,,,http://arxiv.org/pdf/2211.13319v3
Pose-disentangled Contrastive Learning for Self-supervised Facial  Representation,"  Self-supervised facial representation has recently attracted increasingattention due to its ability to perform face understanding without relying onlarge-scale annotated datasets heavily. However, analytically, currentcontrastive-based self-supervised learning (SSL) still performsunsatisfactorily for learning facial representation. More specifically,existing contrastive learning (CL) tends to learn pose-invariant features thatcannot depict the pose details of faces, compromising the learning performance.To conquer the above limitation of CL, we propose a novel Pose-disentangledContrastive Learning (PCL) method for general self-supervised facialrepresentation. Our PCL first devises a pose-disentangled decoder (PDD) with adelicately designed orthogonalizing regulation, which disentangles thepose-related features from the face-aware features; therefore, pose-related andother pose-unrelated facial information could be performed in individualsubnetworks and do not affect each other\'s training. Furthermore, we introducea pose-related contrastive learning scheme that learns pose-related informationbased on data augmentation of the same image, which would deliver moreeffective face-aware representation for various downstream tasks. We conductedlinear evaluation on four challenging downstream facial understanding tasks,ie, facial expression recognition, face recognition, AU detection and head poseestimation. Experimental results demonstrate that our method significantlyoutperforms state-of-the-art SSL methods. Code is available athttps://github.com/DreamMr/PCL}{https://github.com/DreamMr/PCL",,,,,http://arxiv.org/pdf/2211.13490v2
Diff-Font: Diffusion Model for Robust One-Shot Font Generation,"  Font generation is a difficult and time-consuming task, especially in thoselanguages using ideograms that have complicated structures with a large numberof characters, such as Chinese. To solve this problem, few-shot font generationand even one-shot font generation have attracted a lot of attention. However,most existing font generation methods may still suffer from (i) largecross-font gap challenge; (ii) subtle cross-font variation problem; and (iii)incorrect generation of complicated characters. In this paper, we propose anovel one-shot font generation method based on a diffusion model, namedDiff-Font, which can be stably trained on large datasets. The proposed modelaims to generate the entire font library by giving only one sample as thereference. Specifically, a large stroke-wise dataset is constructed, and astroke-wise diffusion model is proposed to preserve the structure and thecompletion of each generated character. To our best knowledge, the proposedDiff-Font is the first work that developed diffusion models to handle the fontgeneration task. The well-trained Diff-Font is not only robust to font gap andfont variation, but also achieved promising performance on difficult charactergeneration. Compared to previous font generation methods, our model reachesstate-of-the-art performance both qualitatively and quantitatively.",,,,,http://arxiv.org/pdf/2212.05895v3
Mask-FPAN: Semi-Supervised Face Parsing in the Wild With De-Occlusion  and UV GAN,"  Fine-grained semantic segmentation of a person\'s face and head, includingfacial parts and head components, has progressed a great deal in recent years.However, it remains a challenging task, whereby considering ambiguousocclusions and large pose variations are particularly difficult. To overcomethese difficulties, we propose a novel framework termed Mask-FPAN. It uses ade-occlusion module that learns to parse occluded faces in a semi-supervisedway. In particular, face landmark localization, face occlusionstimations, anddetected head poses are taken into account. A 3D morphable face model combinedwith the UV GAN improves the robustness of 2D face parsing. In addition, weintroduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for faceparing work. The proposed Mask-FPAN framework addresses the face parsingproblem in the wild and shows significant performance improvements with MIOUfrom 0.7353 to 0.9013 compared to the state-of-the-art on challenging facedatasets.",,,,,http://arxiv.org/pdf/2212.09098v4
A Large-scale Film Style Dataset for Learning Multi-frequency Driven  Film Enhancement,"  Film, a classic image style, is culturally significant to the wholephotographic industry since it marks the birth of photography. However, filmphotography is time-consuming and expensive, necessitating a more efficientmethod for collecting film-style photographs. Numerous datasets that haveemerged in the field of image enhancement so far are not film-specific. Inorder to facilitate film-based image stylization research, we constructFilmSet, a large-scale and high-quality film style dataset. Our datasetincludes three different film types and more than 5000 in-the-wild highresolution images. Inspired by the features of FilmSet images, we propose anovel framework called FilmNet based on Laplacian Pyramid for stylizing imagesacross frequency bands and achieving film style outcomes. Experiments revealthat the performance of our model is superior than state-of-the-art techniques.The link of code and data is \\url{https://github.com/CXH-Research/FilmNet}.",,,,,http://arxiv.org/pdf/2301.08880v2
One-class Damage Detector Using Deeper Fully-Convolutional Data  Descriptions for Civil Application,"  Infrastructure managers must maintain high standards to ensure usersatisfaction during the lifecycle of infrastructures. Surveillance cameras andvisual inspections have enabled progress in automating the detection ofanomalous features and assessing the occurrence of deterioration. However,collecting damage data is typically time consuming and requires repeatedinspections. The one-class damage detection approach has an advantage in thatnormal images can be used to optimize model parameters. Additionally, visualevaluation of heatmaps enables us to understand localized anomalous features.The authors highlight damage vision applications utilized in the robustproperty and localized damage explainability. First, we propose a civil-purposeapplication for automating one-class damage detection reproducing a fullyconvolutional data description (FCDD) as a baseline model. We have obtainedaccurate and explainable results demonstrating experimental studies on concretedamage and steel corrosion in civil engineering. Additionally, to develop amore robust application, we applied our method to another outdoor domain thatcontains complex and noisy backgrounds using natural disaster datasetscollected using various devices. Furthermore, we propose a valuable solution ofdeeper FCDDs focusing on other powerful backbones to improve the performance ofdamage detection and implement ablation studies on disaster datasets. The keyresults indicate that the deeper FCDDs outperformed the baseline FCDD ondatasets representing natural disaster damage caused by hurricanes, typhoons,earthquakes, and four-event disasters.",,,,,http://arxiv.org/pdf/2303.01732v3
SILOP: An Automated Framework for Semantic Segmentation Using Image  Labels Based on Object Perimeters,"  Achieving high-quality semantic segmentation predictions using onlyimage-level labels enables a new level of real-world applicability. Althoughstate-of-the-art networks deliver reliable predictions, the amount ofhandcrafted pixel-wise annotations to enable these results are not feasible inmany real-world applications. Hence, several works have already targeted thisbottleneck, using classifier-based networks like Class ActivationMaps~\\cite{CAM} (CAMs) as a base. Addressing CAM\'s weaknesses of fuzzy bordersand incomplete predictions, state-of-the-art approaches rely only on addingregulations to the classifier loss or using pixel-similarity-based refinementafter the fact. We propose a framework that introduces an additional moduleusing object perimeters for improved saliency. We define object perimeterinformation as the line separating the object and background. Our newPerimeterFit module will be applied to pre-refine the CAM predictions beforeusing the pixel-similarity-based network. In this way, our PerimeterFitincreases the quality of the CAM prediction while simultaneously improving thefalse negative rate. We investigated a wide range of state-of-the-artunsupervised semantic segmentation networks and edge detection techniques tocreate useful perimeter maps, which enable our framework to predict objectlocations with sharper perimeters. We achieved up to 1.5% improvement overframeworks without our PerimeterFit module. We conduct an exhaustive analysisto illustrate that SILOP enhances existing state-of-the-art frameworks forimage-level-based semantic segmentation. The framework is open-source andaccessible online at https://github.com/ErikOstrowski/SILOP.","semantic segmentation, image-level supervision, Class Activation Maps","Technische Universit¨at Wien (TU Wien), Austria和New York University Abu Dhabi (NYUAD), United Arab Emirates (UAE)",这篇文章介绍了一个自动化的框架SILOP，该框架使用基于物体周长的图像标签提高了语义分割的准确性，使得使用图像级别注释的语义分割领域得到了新的应用。该框架通过使用物体周长信息，提高了预测的效果和错误负样本率，并且在实验中证明了其对现有框架的改进。,语义分割，图像级监督，Class Activation Maps,http://arxiv.org/pdf/2303.07892v3
Equiangular Basis Vectors,"  We propose Equiangular Basis Vectors (EBVs) for classification tasks. In deepneural networks, models usually end with a k-way fully connected layer withsoftmax to handle different classification tasks. The learning objective ofthese methods can be summarized as mapping the learned feature representationsto the samples\' label space. While in metric learning approaches, the mainobjective is to learn a transformation function that maps training data pointsfrom the original space to a new space where similar points are closer whiledissimilar points become farther apart. Different from previous methods, ourEBVs generate normalized vector embeddings as ""predefined classifiers"" whichare required to not only be with the equal status between each other, but alsobe as orthogonal as possible. By minimizing the spherical distance of theembedding of an input between its categorical EBV in training, the predictionscan be obtained by identifying the categorical EBV with the smallest distanceduring inference. Various experiments on the ImageNet-1K dataset and otherdownstream tasks demonstrate that our method outperforms the general fullyconnected classifier while it does not introduce huge additional computationcompared with classical metric learning methods. Our EBVs won the first placein the 2022 DIGIX Global AI Challenge, and our code is open-source andavailable at https://github.com/NJUST-VIPGroup/Equiangular-Basis-Vectors.",,,,,http://arxiv.org/pdf/2303.11637v2
Towards Efficient Task-Driven Model Reprogramming with Foundation Models,"  Vision foundation models exhibit impressive power, benefiting from theextremely large model capacity and broad training data. However, in practice,downstream scenarios may only support a small model due to the limitedcomputational resources or efficiency considerations. Moreover, the data usedfor pretraining foundation models are usually invisible and very different fromthe target data of downstream tasks. This brings a critical challenge for thereal-world application of foundation models: one has to transfer the knowledgeof a foundation model to the downstream task that has a quite differentarchitecture with only downstream target data. Existing transfer learning orknowledge distillation methods depend on either the same model structure orfinetuning of the foundation model. Thus, naively introducing these methods canbe either infeasible or very inefficient. To address this, we propose aTask-Driven Model Reprogramming (TDMR) framework. Specifically, we reprogramthe foundation model to project the knowledge into a proxy space, whichalleviates the adverse effect of task mismatch and domain inconsistency. Then,we reprogram the target model via progressive distillation from the proxy spaceto efficiently learn the knowledge from the reprogrammed foundation model. TDMRis compatible with different pre-trained model types (CNN, transformer or theirmix) and limited target data, and promotes the wide applications of visionfoundation models to downstream tasks in a cost-effective manner. Extensiveexperiments on different downstream classification tasks and target modelstructures demonstrate the effectiveness of our methods with both CNNs andtransformer foundation models.",,,,,http://arxiv.org/pdf/2304.02263v2
Self-Supervised Learning from Non-Object Centric Images with a Geometric  Transformation Sensitive Architecture,"  Most invariance-based self-supervised methods rely on single object-centricimages (e.g., ImageNet images) for pretraining, learning invariantrepresentations from geometric transformations. However, when images are notobject-centric, the semantics of the image can be significantly altered due tocropping. Furthermore, as the model becomes insensitive to geometrictransformations, it may struggle to capture location information. For thisreason, we propose a Geometric Transformation Sensitive Architecture designedto learn features that are sensitive to geometric transformations, specificallyfocusing on four-fold rotation, random crop, and multi-crop. Our methodencourages the student to be sensitive by using targets that are sensitive tothose transforms via pooling and rotating of the teacher feature map andpredicting rotation. Additionally, as training insensitively to multi-cropencourages local-to-global correspondence, the model can capture long-termdependencies. We use patch correspondence loss to encourage correspondencebetween patches with similar features, instead of enforcing correspondencebetween views of the image. This approach allows us to capture long-termdependencies in a more appropriate way. Our approach demonstrates improvedperformance when using non-object-centric images as pretraining data comparedto other methods that learn geometric transformation-insensitiverepresentations. We surpass the DINO baseline in tasks including imageclassification, semantic segmentation, detection, and instance segmentationwith improvements of 4.9 $Top-1 Acc$, 3.3 $mIoU$, 3.4 $AP^b$, and 2.7 $AP^m$.Code and pretrained models are publicly available at:https://github.com/bok3948/GTSA",,,,,http://arxiv.org/pdf/2304.08014v3
Learning Situation Hyper-Graphs for Video Question Answering,"  Answering questions about complex situations in videos requires not onlycapturing the presence of actors, objects, and their relations but also theevolution of these relationships over time. A situation hyper-graph is arepresentation that describes situations as scene sub-graphs for video framesand hyper-edges for connected sub-graphs and has been proposed to capture allsuch information in a compact structured form. In this work, we propose anarchitecture for Video Question Answering (VQA) that enables answeringquestions related to video content by predicting situation hyper-graphs, coinedSituation Hyper-Graph based Video Question Answering (SHG-VQA). To this end, wetrain a situation hyper-graph decoder to implicitly identify graphrepresentations with actions and object/human-object relationships from theinput video clip. and to use cross-attention between the predicted situationhyper-graphs and the question embedding to predict the correct answer. Theproposed method is trained in an end-to-end manner and optimized by a VQA losswith the cross-entropy function and a Hungarian matching loss for the situationgraph prediction. The effectiveness of the proposed architecture is extensivelyevaluated on two challenging benchmarks: AGQA and STAR. Our results show thatlearning the underlying situation hyper-graphs helps the system tosignificantly improve its performance for novel challenges of videoquestion-answering tasks.",,,,,http://arxiv.org/pdf/2304.08682v2
Improving Post-Training Quantization on Object Detection with Task  Loss-Guided Lp Metric,"  Efficient inference for object detection networks is a major challenge onedge devices. Post-Training Quantization (PTQ), which transforms afull-precision model into low bit-width directly, is an effective andconvenient approach to reduce model inference complexity. But it suffers severeaccuracy drop when applied to complex tasks such as object detection. PTQoptimizes the quantization parameters by different metrics to minimize theperturbation of quantization. The p-norm distance of feature maps before andafter quantization, Lp, is widely used as the metric to evaluate perturbation.For the specialty of object detection network, we observe that the parameter pin Lp metric will significantly influence its quantization performance. Weindicate that using a fixed hyper-parameter p does not achieve optimalquantization performance. To mitigate this problem, we propose a framework,DetPTQ, to assign different p values for quantizing different layers using anObject Detection Output Loss (ODOL), which represents the task loss of objectdetection. DetPTQ employs the ODOL-based adaptive Lp metric to select theoptimal quantization parameters. Experiments show that our DetPTQ outperformsthe state-of-the-art PTQ methods by a significant margin on both 2D and 3Dobject detectors. For example, we achieve31.1/31.7(quantization/full-precision) mAP on RetinaNet-ResNet18 with 4-bitweight and 4-bit activation.",,,,,http://arxiv.org/pdf/2304.09785v3
Universal Domain Adaptation via Compressive Attention Matching,"  Universal domain adaptation (UniDA) aims to transfer knowledge from thesource domain to the target domain without any prior knowledge about the labelset. The challenge lies in how to determine whether the target samples belongto common categories. The mainstream methods make judgments based on the samplefeatures, which overemphasizes global information while ignoring the mostcrucial local objects in the image, resulting in limited accuracy. To addressthis issue, we propose a Universal Attention Matching (UniAM) framework byexploiting the self-attention mechanism in vision transformer to capture thecrucial object information. The proposed framework introduces a novelCompressive Attention Matching (CAM) approach to explore the core informationby compressively representing attentions. Furthermore, CAM incorporates aresidual-based measurement to determine the sample commonness. By utilizing themeasurement, UniAM achieves domain-wise and category-wise Common FeatureAlignment (CFA) and Target Class Separation (TCS). Notably, UniAM is the firstmethod utilizing the attention in vision transformer directly to performclassification tasks. Extensive experiments show that UniAM outperforms thecurrent state-of-the-art methods on various benchmark datasets.",,,,,http://arxiv.org/pdf/2304.11862v2
Discriminative Co-Saliency and Background Mining Transformer for  Co-Salient Object Detection,"  Most previous co-salient object detection works mainly focus on extractingco-salient cues via mining the consistency relations across images whileignoring explicit exploration of background regions. In this paper, we proposea Discriminative co-saliency and background Mining Transformer framework (DMT)based on several economical multi-grained correlation modules to explicitlymine both co-saliency and background information and effectively model theirdiscrimination. Specifically, we first propose a region-to-region correlationmodule for introducing inter-image relations to pixel-wise segmentationfeatures while maintaining computational efficiency. Then, we use two types ofpre-defined tokens to mine co-saliency and background information via ourproposed contrast-induced pixel-to-token correlation and co-saliencytoken-to-token correlation modules. We also design a token-guided featurerefinement module to enhance the discriminability of the segmentation featuresunder the guidance of the learned tokens. We perform iterative mutual promotionfor the segmentation feature extraction and token construction. Experimentalresults on three benchmark datasets demonstrate the effectiveness of ourproposed method. The source code is available at:https://github.com/dragonlee258079/DMT.",,,,,http://arxiv.org/pdf/2305.00514v2
Caption Anything: Interactive Image Description with Diverse Multimodal  Controls,"  Controllable image captioning is an emerging multimodal topic that aims todescribe the image with natural language following human purpose,$\\textit{e.g.}$, looking at the specified regions or telling in a particulartext style. State-of-the-art methods are trained on annotated pairs of inputcontrols and output captions. However, the scarcity of such well-annotatedmultimodal data largely limits their usability and scalability for interactiveAI systems. Leveraging unimodal instruction-following foundation models is apromising alternative that benefits from broader sources of data. In thispaper, we present Caption AnyThing (CAT), a foundation model augmented imagecaptioning framework supporting a wide range of multimodel controls: 1) visualcontrols, including points, boxes, and trajectories; 2) language controls, suchas sentiment, length, language, and factuality. Powered by Segment AnythingModel (SAM) and ChatGPT, we unify the visual and language prompts into amodularized framework, enabling the flexible combination between differentcontrols. Extensive case studies demonstrate the user intention alignmentcapabilities of our framework, shedding light on effective user interactionmodeling in vision-language applications. Our code is publicly available athttps://github.com/ttengwang/Caption-Anything.",,,,,http://arxiv.org/pdf/2305.02677v2
Incremental 3D Semantic Scene Graph Prediction from RGB Sequences,"  3D semantic scene graphs are a powerful holistic representation as theydescribe the individual objects and depict the relation between them. They arecompact high-level graphs that enable many tasks requiring scene reasoning. Inreal-world settings, existing 3D estimation methods produce robust predictionsthat mostly rely on dense inputs. In this work, we propose a real-timeframework that incrementally builds a consistent 3D semantic scene graph of ascene given an RGB image sequence. Our method consists of a novel incrementalentity estimation pipeline and a scene graph prediction network. The proposedpipeline simultaneously reconstructs a sparse point map and fuses entityestimation from the input images. The proposed network estimates 3D semanticscene graphs with iterative message passing using multi-view and geometricfeatures extracted from the scene entities. Extensive experiments on the 3RScandataset show the effectiveness of the proposed method in this challenging task,outperforming state-of-the-art approaches.",,,,,http://arxiv.org/pdf/2305.02743v2
Towards End-to-End Semi-Supervised Table Detection with Deformable  Transformer,"  Table detection is the task of classifying and localizing table objectswithin document images. With the recent development in deep learning methods,we observe remarkable success in table detection. However, a significant amountof labeled data is required to train these models effectively. Manysemi-supervised approaches are introduced to mitigate the need for asubstantial amount of label data. These approaches use CNN-based detectors thatrely on anchor proposals and post-processing stages such as NMS. To tacklethese limitations, this paper presents a novel end-to-end semi-supervised tabledetection method that employs the deformable transformer for detecting tableobjects. We evaluate our semi-supervised method on PubLayNet, DocBank, ICADR-19and TableBank datasets, and it achieves superior performance compared toprevious methods. It outperforms the fully supervised method (Deformabletransformer) by +3.4 points on 10\\% labels of TableBank-both dataset and theprevious CNN-based semi-supervised approach (Soft Teacher) by +1.8 points on10\\% labels of PubLayNet dataset. We hope this work opens new possibilitiestowards semi-supervised and unsupervised table detection methods.",,Tahira Shehzadi,"1Department of Computer Science, Technical University of Kaiserslautern, 67663 Kaiserslautern, Germany",,http://arxiv.org/pdf/2305.02769v2
APR: Online Distant Point Cloud Registration Through Aggregated Point  Cloud Reconstruction,"  For many driving safety applications, it is of great importance to accuratelyregister LiDAR point clouds generated on distant moving vehicles. However, suchpoint clouds have extremely different point density and sensor perspective onthe same object, making registration on such point clouds very hard. In thispaper, we propose a novel feature extraction framework, called APR, for onlinedistant point cloud registration. Specifically, APR leverages an autoencoderdesign, where the autoencoder reconstructs a denser aggregated point cloud withseveral frames instead of the original single input point cloud. Our designforces the encoder to extract features with rich local geometry informationbased on one single input point cloud. Such features are then used for onlinedistant point cloud registration. We conduct extensive experiments againststate-of-the-art (SOTA) feature extractors on KITTI and nuScenes datasets.Results show that APR outperforms all other extractors by a large margin,increasing average registration recall of SOTA extractors by 7.1% on LoKITTIand 4.6% on LoNuScenes. Code is available at https://github.com/liuQuan98/APR.",,,,,http://arxiv.org/pdf/2305.02893v2
DocDiff: Document Enhancement via Residual Diffusion Models,"  Removing degradation from document images not only improves their visualquality and readability, but also enhances the performance of numerousautomated document analysis and recognition tasks. However, existingregression-based methods optimized for pixel-level distortion reduction tend tosuffer from significant loss of high-frequency information, leading todistorted and blurred text edges. To compensate for this major deficiency, wepropose DocDiff, the first diffusion-based framework specifically designed fordiverse challenging document enhancement problems, including documentdeblurring, denoising, and removal of watermarks and seals. DocDiff consists oftwo modules: the Coarse Predictor (CP), which is responsible for recovering theprimary low-frequency content, and the High-Frequency Residual Refinement (HRR)module, which adopts the diffusion models to predict the residual(high-frequency information, including text edges), between the ground-truthand the CP-predicted image. DocDiff is a compact and computationally efficientmodel that benefits from a well-designed network architecture, an optimizedtraining loss objective, and a deterministic sampling process with short timesteps. Extensive experiments demonstrate that DocDiff achieves state-of-the-art(SOTA) performance on multiple benchmark datasets, and can significantlyenhance the readability and recognizability of degraded document images.Furthermore, our proposed HRR module in pre-trained DocDiff is plug-and-playand ready-to-use, with only 4.17M parameters. It greatly sharpens the textedges generated by SOTA deblurring methods without additional joint training.Available codes: https://github.com/Royalvice/DocDiff",Document Enhancement; Conditional Diffusion Models; Frequency Separation; Document Analysis,Beijing University of Posts and Telecommunications,本文提出了一种文本图像增强的方法——DocDiff，基于扩散模型设计，利用粗糙预测和高频残差细化两个模块实现文档去模糊、去噪声、去水印等多个任务的处理。该方法结构简单、运行高效，实验结果显示其性能优异，能有效提高文本图像的清晰度和可读性。同时，其预训练模型中的高频残差细化模块也可以作为插件，不需要额外的联合训练即可大幅改善去模糊效果。,文本图像增强，条件扩散模型，文档分析,http://arxiv.org/pdf/2305.03892v1
Prompt What You Need: Enhancing Segmentation in Rainy Scenes with  Anchor-based Prompting,"  Semantic segmentation in rainy scenes is a challenging task due to thecomplex environment, class distribution imbalance, and limited annotated data.To address these challenges, we propose a novel framework that utilizessemi-supervised learning and pre-trained segmentation foundation model toachieve superior performance. Specifically, our framework leverages thesemi-supervised model as the basis for generating raw semantic segmentationresults, while also serving as a guiding force to prompt pre-trained foundationmodel to compensate for knowledge gaps with entropy-based anchors. In addition,to minimize the impact of irrelevant segmentation masks generated by thepre-trained foundation model, we also propose a mask filtering and fusionmechanism that optimizes raw semantic segmentation results based on theprinciple of minimum risk. The proposed framework achieves superiorsegmentation performance on the Rainy WCity dataset and is awarded the firstprize in the sub-track of STRAIN in ICME 2023 Grand Challenges.",,,,,http://arxiv.org/pdf/2305.03902v1
Annotation-efficient learning for OCT segmentation,"  Deep learning has been successfully applied to OCT segmentation. However, fordata from different manufacturers and imaging protocols, and for differentregions of interest (ROIs), it requires laborious and time-consuming dataannotation and training, which is undesirable in many scenarios, such assurgical navigation and multi-center clinical trials. Here we propose anannotation-efficient learning method for OCT segmentation that couldsignificantly reduce annotation costs. Leveraging self-supervised generativelearning, we train a Transformer-based model to learn the OCT imagery. Then weconnect the trained Transformer-based encoder to a CNN-based decoder, to learnthe dense pixel-wise prediction in OCT segmentation. These training phases useopen-access data and thus incur no annotation costs, and the pre-trained modelcan be adapted to different data and ROIs without re-training. Based on thegreedy approximation for the k-center problem, we also introduce an algorithmfor the selective annotation of the target data. We verified our method onpublicly-available and private OCT datasets. Compared to the widely-used U-Netmodel with 100% training data, our method only requires ~10% of the data forachieving the same segmentation accuracy, and it speeds the training up to ~3.5times. Furthermore, our proposed method outperforms other potential strategiesthat could improve annotation efficiency. We think this emphasis on learningefficiency may help improve the intelligence and application penetration ofOCT-based technologies. Our code and pre-trained model are publicly availableathttps://github.com/SJTU-Intelligent-Optics-Lab/Annotation-efficient-learning-for-OCT-segmentation.",,,,,http://arxiv.org/pdf/2305.03936v1
Structural and Statistical Texture Knowledge Distillation for Semantic  Segmentation,"  Existing knowledge distillation works for semantic segmentation mainly focuson transferring high-level contextual knowledge from teacher to student.However, low-level texture knowledge is also of vital importance forcharacterizing the local structural pattern and global statistical property,such as boundary, smoothness, regularity and color contrast, which may not bewell addressed by high-level deep features. In this paper, we are intended totake full advantage of both structural and statistical texture knowledge andpropose a novel Structural and Statistical Texture Knowledge Distillation(SSTKD) framework for semantic segmentation. Specifically, for structuraltexture knowledge, we introduce a Contourlet Decomposition Module (CDM) thatdecomposes low-level features with iterative Laplacian pyramid and directionalfilter bank to mine the structural texture knowledge. For statisticalknowledge, we propose a Denoised Texture Intensity Equalization Module (DTIEM)to adaptively extract and enhance statistical texture knowledge throughheuristics iterative quantization and denoised operation. Finally, eachknowledge learning is supervised by an individual loss function, forcing thestudent network to mimic the teacher better from a broader perspective.Experiments show that the proposed method achieves state-of-the-art performanceon Cityscapes, Pascal VOC 2012 and ADE20K datasets.",,,,,http://arxiv.org/pdf/2305.03944v1
Feature Chirality in Deep Learning Models,"  As deep learning applications extensively increase by leaps and bounds, theirinterpretability has become increasingly prominent. As a universal property,chirality exists widely in nature, and applying it to the explanatory researchof deep learning may be helpful to some extent. Inspired by a recent study thatused CNN (convolutional neural network), which applied visual chirality, todistinguish whether an image is flipped or not. In this paper, we study featurechirality innovatively, which shows how the statistics of deep learning models\'feature data are changed by training. We rethink the feature-level chiralityproperty, propose the feature chirality, and give the measure. Our analysis offeature chirality on AlexNet, VGG, and ResNet reveals similar but surprisingresults, including the prevalence of feature chirality in these models, theinitialization methods of the models do not affect feature chirality. Our workshows that feature chirality implies model evaluation, interpretability of themodel, and model parameters optimization.",,,,,http://arxiv.org/pdf/2305.03966v1
Multi-object Video Generation from Single Frame Layouts,"  In this paper, we study video synthesis with emphasis on simplifying thegeneration conditions. Most existing video synthesis models or datasets aredesigned to address complex motions of a single object, lacking the ability ofcomprehensively understanding the spatio-temporal relationships among multipleobjects. Besides, current methods are usually conditioned on intricateannotations (e.g. video segmentations) to generate new videos, beingfundamentally less practical. These motivate us to generate multi-object videosconditioning exclusively on object layouts from a single frame. To solve abovechallenges and inspired by recent research on image generation from layouts, wehave proposed a novel video generative framework capable of synthesizing globalscenes with local objects, via implicit neural representations and layoutmotion self-inference. Our framework is a non-trivial adaptation from imagegeneration methods, and is new to this field. In addition, our model has beenevaluated on two widely-used video recognition benchmarks, demonstratingeffectiveness compared to the baseline model.",,,,,http://arxiv.org/pdf/2305.03983v1
Weighted Point Cloud Normal Estimation,"  Existing normal estimation methods for point clouds are often less robust tosevere noise and complex geometric structures. Also, they usually ignore thecontributions of different neighbouring points during normal estimation, whichleads to less accurate results. In this paper, we introduce a weighted normalestimation method for 3D point cloud data. We innovate in two key points: 1) wedevelop a novel weighted normal regression technique that predicts point-wiseweights from local point patches and use them for robust, feature-preservingnormal regression; 2) we propose to conduct contrastive learning between pointpatches and the corresponding ground-truth normals of the patches\' centralpoints as a pre-training process to facilitate normal regression. Comprehensiveexperiments demonstrate that our method can robustly handle noisy and complexpoint clouds, achieving state-of-the-art performance on both synthetic andreal-world datasets.",,,,,http://arxiv.org/pdf/2305.04007v1
Exploring One-shot Semi-supervised Federated Learning with A Pre-trained  Diffusion Model,"  Federated learning is a privacy-preserving collaborative learning approach.Recently, some studies have proposed the semi-supervised federated learningsetting to handle the commonly seen real-world scenarios with labeled data onthe server and unlabeled data on the clients. However, existing methods stillface challenges such as high communication costs, training pressure on theclient devices, and distribution differences among the server and the clients.In this paper, we introduce the powerful pre-trained diffusion models intofederated learning and propose FedDISC, a Federated Diffusion InspiredSemi-supervised Co-training method, to address these challenges. Specifically,we first extract prototypes from the labeled data on the server and send themto the clients. The clients then use these prototypes to predict pseudo-labelsof the local data, and compute the cluster centroids and domain-specificfeatures to represent their personalized distributions. After adding noise, theclients send these features and their corresponding pseudo-labels back to theserver, which uses a pre-trained diffusion model to conditionally generatepseudo-samples complying with the client distributions and train an aggregatedmodel on them. Our method does not require local training and only involvesforward inference on the clients. Our extensive experiments on DomainNet,Openimage, and NICO++ demonstrate that the proposed FedDISC method effectivelyaddresses the one-shot semi-supervised problem on Non-IID clients andoutperforms the compared SOTA methods. We also demonstrate throughvisualization that it is of neglectable possibility for FedDISC to leakprivacy-sensitive information of the clients.",,,,,http://arxiv.org/pdf/2305.04063v1
PointCMP: Contrastive Mask Prediction for Self-supervised Learning on  Point Cloud Videos,"  Self-supervised learning can extract representations of good quality fromsolely unlabeled data, which is appealing for point cloud videos due to theirhigh labelling cost. In this paper, we propose a contrastive mask prediction(PointCMP) framework for self-supervised learning on point cloud videos.Specifically, our PointCMP employs a two-branch structure to achievesimultaneous learning of both local and global spatio-temporal information. Ontop of this two-branch structure, a mutual similarity based augmentation moduleis developed to synthesize hard samples at the feature level. By maskingdominant tokens and erasing principal channels, we generate hard samples tofacilitate learning representations with better discrimination andgeneralization performance. Extensive experiments show that our PointCMPachieves the state-of-the-art performance on benchmark datasets and outperformsexisting full-supervised counterparts. Transfer learning results demonstratethe superiority of the learned representations across different datasets andtasks.",,,,,http://arxiv.org/pdf/2305.04075v1
Transform-Equivariant Consistency Learning for Temporal Sentence  Grounding,"  This paper addresses the temporal sentence grounding (TSG). Although existingmethods have made decent achievements in this task, they not only severely relyon abundant video-query paired data for training, but also easily fail into thedataset distribution bias. To alleviate these limitations, we introduce a novelEquivariant Consistency Regulation Learning (ECRL) framework to learn morediscriminative query-related frame-wise representations for each video, in aself-supervised manner. Our motivation comes from that the temporal boundary ofthe query-guided activity should be consistently predicted under variousvideo-level transformations. Concretely, we first design a series ofspatio-temporal augmentations on both foreground and background video segmentsto generate a set of synthetic video samples. In particular, we devise aself-refine module to enhance the completeness and smoothness of the augmentedvideo. Then, we present a novel self-supervised consistency loss (SSCL) appliedon the original and augmented videos to capture their invariant query-relatedsemantic by minimizing the KL-divergence between the sequence similarity of twovideos and a prior Gaussian distribution of timestamp distance. At last, ashared grounding head is introduced to predict the transform-equivariantquery-guided segment boundaries for both the original and augmented videos.Extensive experiments on three challenging datasets (ActivityNet, TACoS, andCharades-STA) demonstrate both effectiveness and efficiency of our proposedECRL framework.",,,,,http://arxiv.org/pdf/2305.04123v1
Context-Aware Chart Element Detection,"  As a prerequisite of chart data extraction, the accurate detection of chartbasic elements is essential and mandatory. In contrast to object detection inthe general image domain, chart element detection relies heavily on contextinformation as charts are highly structured data visualization formats. Toaddress this, we propose a novel method CACHED, which stands for Context-AwareChart Element Detection, by integrating a local-global context fusion moduleconsisting of visual context enhancement and positional context encoding withthe Cascade R-CNN framework. To improve the generalization of our method forbroader applicability, we refine the existing chart element categorization andstandardized 18 classes for chart basic elements, excluding plot elements. OurCACHED method, with the updated category of chart elements, achievesstate-of-the-art performance in our experiments, underscoring the importance ofcontext in chart element detection. Extending our method to the bar plotdetection task, we obtain the best result on the PMC test dataset.",,,,,http://arxiv.org/pdf/2305.04151v1
PhysBench: A Benchmark Framework for Remote Physiological Sensing with  New Dataset and Baseline,"  In recent years, due to the widespread use of internet videos, physiologicalremote sensing has gained more and more attention in the fields of affectivecomputing and telemedicine. Recovering physiological signals from facial videosis a challenging task that involves a series of preprocessing, imagealgorithms, and post-processing to finally restore waveforms. We propose acomplete and efficient end-to-end training and testing framework that providesfair comparisons for different algorithms through unified preprocessing andpost-processing. In addition, we introduce a highly synchronized losslessformat dataset along with a lightweight algorithm. The dataset contains over 32hours (3.53M frames) of video from 58 subjects; by training on our collecteddataset both our proposed algorithm as well as existing ones can achieveimprovements.",,,,,http://arxiv.org/pdf/2305.04161v1
YOLOCS: Object Detection based on Dense Channel Compression for Feature  Spatial Solidification,"  In this study, we examine the associations between channel features andconvolutional kernels during the processes of feature purification and gradientbackpropagation, with a focus on the forward and backward propagation withinthe network. Consequently, we propose a method called Dense Channel Compressionfor Feature Spatial Solidification. Drawing upon the central concept of thismethod, we introduce two innovative modules for backbone and head networks: theDense Channel Compression for Feature Spatial Solidification Structure (DCFS)and the Asymmetric Multi-Level Compression Decoupled Head (ADH). Whenintegrated into the YOLOv5 model, these two modules demonstrate exceptionalperformance, resulting in a modified model referred to as YOLOCS. Evaluated onthe MSCOCO dataset, the large, medium, and small YOLOCS models yield AP of50.1%, 47.6%, and 42.5%, respectively. Maintaining inference speeds remarkablysimilar to those of the YOLOv5 model, the large, medium, and small YOLOCSmodels surpass the YOLOv5 model\'s AP by 1.1%, 2.3%, and 5.2%, respectively.",,,,,http://arxiv.org/pdf/2305.04170v1
Robust Image Ordinal Regression with Controllable Image Generation,"  Image ordinal regression has been mainly studied along the line of exploitingthe order of categories. However, the issues of class imbalance and categoryoverlap that are very common in ordinal regression were largely overlooked. Asa result, the performance on minority categories is often unsatisfactory. Inthis paper, we propose a novel framework called CIG based on controllable imagegeneration to directly tackle these two issues. Our main idea is to generateextra training samples with specific labels near category boundaries, and thesample generation is biased toward the less-represented categories. To achievecontrollable image generation, we seek to separate structural and categoricalinformation of images based on structural similarity, categorical similarity,and reconstruction constraints. We evaluate the effectiveness of our new CIGapproach in three different image ordinal regression scenarios. The resultsdemonstrate that CIG can be flexibly integrated with off-the-shelf imageencoders or ordinal regression models to achieve improvement, and further, theimprovement is more significant for minority categories.",,,,,http://arxiv.org/pdf/2305.04213v1
Visual Causal Scene Refinement for Video Question Answering,"  Existing methods for video question answering (VideoQA) often suffer fromspurious correlations between different modalities, leading to a failure inidentifying the dominant visual evidence and the intended question. Moreover,these methods function as black boxes, making it difficult to interpret thevisual scene during the QA process. In this paper, to discover critical videosegments and frames that serve as the visual causal scene for generatingreliable answers, we present a causal analysis of VideoQA and propose aframework for cross-modal causal relational reasoning, named Visual CausalScene Refinement (VCSR). Particularly, a set of causal front-door interventionoperations is introduced to explicitly find the visual causal scenes at bothsegment and frame levels. Our VCSR involves two essential modules: i) theQuestion-Guided Refiner (QGR) module, which refines consecutive video framesguided by the question semantics to obtain more representative segment featuresfor causal front-door intervention; ii) the Causal Scene Separator (CSS)module, which discovers a collection of visual causal and non-causal scenesbased on the visual-linguistic causal relevance and estimates the causal effectof the scene-separating intervention in a contrastive learning manner.Extensive experiments on the NExT-QA, Causal-VidQA, and MSRVTT-QA datasetsdemonstrate the superiority of our VCSR in discovering visual causal scene andachieving robust video question answering.","Video Question Answering, Causality, Multimodal",Sun-Yat-Sen University,本文提出了一个多模态视频问答的框架，名为Visual Causal Scene Reﬁnement (VCSR)，通过因果关系分析，提出了一种跨模态因果关系推理框架来发现关键视频段落和帧，以生成可靠的答案。本文提出的方法可以发现关键视频片段和帧，提高视频问答的鲁棒性。,视频问答、因果关系、多模态,http://arxiv.org/pdf/2305.04224v1
RFR-WWANet: Weighted Window Attention-Based Recovery Feature Resolution  Network for Unsupervised Image Registration,"  The Swin transformer has recently attracted attention in medical imageanalysis due to its computational efficiency and long-range modelingcapability, which enables the establishment of more distant relationshipsbetween corresponding voxels. However, transformer-based models split imagesinto tokens, which results in transformers that can only model and outputcoarse-grained spatial information representations. To address this issue, wepropose Recovery Feature Resolution Network (RFRNet), which enables thetransformer to contribute with fine-grained spatial information and richsemantic correspondences. Furthermore, shifted window partitioning operationsare inflexible, indicating that they cannot perceive the semantic informationover uncertain distances and automatically bridge the global connectionsbetween windows. Therefore, we present a Weighted Window Attention (WWA) toautomatically build global interactions between windows after the regular andcyclic shifted window partitioning operations for Swin transformer blocks. Theproposed unsupervised deformable image registration model, named RFR-WWANet,senses the long-range correlations, thereby facilitating meaningful semanticrelevance of anatomical structures. Qualitative and quantitative results showthat RFR-WWANet achieves significant performance improvements over baselinemethods. Ablation experiments demonstrate the effectiveness of the RFRNet andWWA designs.",,,,,http://arxiv.org/pdf/2305.04236v1
Estimation of control area in badminton doubles with pose information  from top and back view drone videos,"  The application of visual tracking to the performance analysis of sportsplayers in dynamic competitions is vital for effective coaching. In racketsports, most previous studies have focused on analyzing and assessing singlesplayers without occlusion in broadcast videos and discrete representations(e.g., stroke) that ignore meaningful spatial distributions. In this work, wepresent the first annotated drone dataset from top and back views in badmintondoubles and propose a framework to estimate the control area probability map,which can be used to evaluate teamwork performance. We present an efficientframework of deep neural networks that enables the calculation of fullprobability surfaces, which utilizes the embedding of a Gaussian mixture map ofplayers\' positions and graph convolution of their poses. In the experiment, weverify our approach by comparing various baselines and discovering thecorrelations between the score and control area. Furthermore, we propose thepractical application of assessing optimal positioning to provide instructionsduring a game. Our approach can visually and quantitatively evaluate players\'movements, providing valuable insights into doubles teamwork.",,,,,http://arxiv.org/pdf/2305.04247v1
Multi-Space Neural Radiance Fields,"  Existing Neural Radiance Fields (NeRF) methods suffer from the existence ofreflective objects, often resulting in blurry or distorted rendering. Insteadof calculating a single radiance field, we propose a multi-space neuralradiance field (MS-NeRF) that represents the scene using a group of featurefields in parallel sub-spaces, which leads to a better understanding of theneural network toward the existence of reflective and refractive objects. Ourmulti-space scheme works as an enhancement to existing NeRF methods, with onlysmall computational overheads needed for training and inferring the extra-spaceoutputs. We demonstrate the superiority and compatibility of our approach usingthree representative NeRF-based models, i.e., NeRF, Mip-NeRF, and Mip-NeRF 360.Comparisons are performed on a novelly constructed dataset consisting of 25synthetic scenes and 7 real captured scenes with complex reflection andrefraction, all having 360-degree viewpoints. Extensive experiments show thatour approach significantly outperforms the existing single-space NeRF methodsfor rendering high-quality scenes concerned with complex light paths throughmirror-like objects. Our code and dataset will be publicly available athttps://zx-yin.github.io/msnerf.",,,,,http://arxiv.org/pdf/2305.04268v1
Neural Voting Field for Camera-Space 3D Hand Pose Estimation,"  We present a unified framework for camera-space 3D hand pose estimation froma single RGB image based on 3D implicit representation. As opposed to recentworks, most of which first adopt holistic or pixel-level dense regression toobtain relative 3D hand pose and then follow with complex second-stageoperations for 3D global root or scale recovery, we propose a novel unified 3Ddense regression scheme to estimate camera-space 3D hand pose via dense 3Dpoint-wise voting in camera frustum. Through direct dense modeling in 3D domaininspired by Pixel-aligned Implicit Functions for 3D detailed reconstruction,our proposed Neural Voting Field (NVF) fully models 3D dense local evidence andhand global geometry, helping to alleviate common 2D-to-3D ambiguities.Specifically, for a 3D query point in camera frustum and its pixel-alignedimage feature, NVF, represented by a Multi-Layer Perceptron, regresses: (i) itssigned distance to the hand surface; (ii) a set of 4D offset vectors (1D votingweight and 3D directional vector to each hand joint). Following a vote-castingscheme, 4D offset vectors from near-surface points are selected to calculatethe 3D hand joint coordinates by a weighted average. Experiments demonstratethat NVF outperforms existing state-of-the-art algorithms on FreiHAND datasetfor camera-space 3D hand pose estimation. We also adapt NVF to the classic taskof root-relative 3D hand pose estimation, for which NVF also obtainsstate-of-the-art results on HO3D dataset.",,,,,http://arxiv.org/pdf/2305.04328v1
Segmentation of the veterinary cytological images for fast neoplastic  tumors diagnosis,"  This paper shows the machine learning system which performs instancesegmentation of cytological images in veterinary medicine. Eleven cell typeswere used directly and indirectly in the experiments, including damaged andunrecognized categories. The deep learning models employed in the systemachieve a high score of average precision and recall metrics, i.e. 0.94 and 0.8respectively, for the selected three types of tumors. This variety of labeltypes allowed us to draw a meaningful conclusion that there are relatively fewmistakes for tumor cell types. Additionally, the model learned tumor cellfeatures well enough to avoid misclassification mistakes of one tumor type intoanother. The experiments also revealed that the quality of the results improveswith the dataset size (excluding the damaged cells). It is worth noting thatall the experiments were done using a custom dedicated dataset provided by thecooperating vet doctors.",,,,,http://arxiv.org/pdf/2305.04332v1
Living in a Material World: Learning Material Properties from  Full-Waveform Flash Lidar Data for Semantic Segmentation,"  Advances in lidar technology have made the collection of 3D point clouds fastand easy. While most lidar sensors return per-point intensity (or reflectance)values along with range measurements, flash lidar sensors are able to provideinformation about the shape of the return pulse. The shape of the returnwaveform is affected by many factors, including the distance that the lightpulse travels and the angle of incidence with a surface. Importantly, the shapeof the return waveform also depends on the material properties of thereflecting surface. In this paper, we investigate whether the material type orclass can be determined from the full-waveform response. First, as a proof ofconcept, we demonstrate that the extra information about material class, ifknown accurately, can improve performance on scene understanding tasks such assemantic segmentation. Next, we learn two different full-waveform materialclassifiers: a random forest classifier and a temporal convolutional neuralnetwork (TCN) classifier. We find that, in some cases, material types can bedistinguished, and that the TCN generally performs better across a wider rangeof materials. However, factors such as angle of incidence, material colour, andmaterial similarity may hinder overall performance.",,,,,http://arxiv.org/pdf/2305.04334v1
TaLU: A Hybrid Activation Function Combining Tanh and Rectified Linear  Unit to Enhance Neural Networks,"  The application of the deep learning model in classification plays animportant role in the accurate detection of the target objects. However, theaccuracy is affected by the activation function in the hidden and output layer.In this paper, an activation function called TaLU, which is a combination ofTanh and Rectified Linear Units (ReLU), is used to improve the prediction. ReLUactivation function is used by many deep learning researchers for itscomputational efficiency, ease of implementation, intuitive nature, etc.However, it suffers from a dying gradient problem. For instance, when the inputis negative, its output is always zero because its gradient is zero. A numberof researchers used different approaches to solve this issue. Some of the mostnotable are LeakyReLU, Softplus, Softsign, Elu, ThresholdedReLU, etc. Thisresearch developed TaLU, a modified activation function combining Tanh andReLU, which mitigates the dying gradient problem of ReLU. The deep learningmodel with the proposed activation function was tested on MNIST and CIFAR-10,and it outperforms ReLU and some other studied activation functions in terms ofaccuracy(from 0\\% upto 6\\% in most cases, when used with Batch Normalizationand a reasonable learning rate).",,,,,http://arxiv.org/pdf/2305.04402v1
Improving 2D face recognition via fine-level facial depth generation and  RGB-D complementary feature learning,"  Face recognition in complex scenes suffers severe challenges coming fromperturbations such as pose deformation, ill illumination, partial occlusion.Some methods utilize depth estimation to obtain depth corresponding to RGB toimprove the accuracy of face recognition. However, the depth generated by themsuffer from image blur, which introduces noise in subsequent RGB-D facerecognition tasks. In addition, existing RGB-D face recognition methods areunable to fully extract complementary features. In this paper, we propose afine-grained facial depth generation network and an improved multimodalcomplementary feature learning network. Extensive experiments on the Lock3DFacedataset and the IIIT-D dataset show that the proposed FFDGNet and I MCFLNet canimprove the accuracy of RGB-D face recognition while achieving thestate-of-the-art performance.",,,,,http://arxiv.org/pdf/2305.04426v1
Adversarial Examples Detection with Enhanced Image Difference Features  based on Local Histogram Equalization,"  Deep Neural Networks (DNNs) have recently made significant progress in manyfields. However, studies have shown that DNNs are vulnerable to adversarialexamples, where imperceptible perturbations can greatly mislead DNNs even ifthe full underlying model parameters are not accessible. Various defensemethods have been proposed, such as feature compression and gradient masking.However, numerous studies have proven that previous methods create detection ordefense against certain attacks, which renders the method ineffective in theface of the latest unknown attack methods. The invisibility of adversarialperturbations is one of the evaluation indicators for adversarial exampleattacks, which also means that the difference in the local correlation ofhigh-frequency information in adversarial examples and normal examples can beused as an effective feature to distinguish the two. Therefore, we propose anadversarial example detection framework based on a high-frequency informationenhancement strategy, which can effectively extract and amplify the featuredifferences between adversarial examples and normal examples. Experimentalresults show that the feature augmentation module can be combined with existingdetection models in a modular way under this framework. Improve the detector\'sperformance and reduce the deployment cost without modifying the existingdetection model.",,,,,http://arxiv.org/pdf/2305.04436v1
Vision Transformer Off-the-Shelf: A Surprising Baseline for Few-Shot  Class-Agnostic Counting,"  Class-agnostic counting (CAC) aims to count objects of interest from a queryimage given few exemplars. This task is typically addressed by extracting thefeatures of query image and exemplars respectively with (un)shared featureextractors and by matching their feature similarity, leading to anextract-\\textit{then}-match paradigm. In this work, we show that CAC can besimplified in an extract-\\textit{and}-match manner, particularly using apretrained and plain vision transformer (ViT) where feature extraction andsimilarity matching are executed simultaneously within the self-attention. Wereveal the rationale of such simplification from a decoupled view of theself-attention and point out that the simplification is only made possible ifthe query and exemplar tokens are concatenated as input. The resulting model,termed CACViT, simplifies the CAC pipeline and unifies the feature spacesbetween the query image and exemplars. In addition, we find CACViT naturallyencodes background information within self-attention, which helps reducebackground disturbance. Further, to compensate the loss of the scale and theorder-of-magnitude information due to resizing and normalization in ViT, wepresent two effective strategies for scale and magnitude embedding. Extensiveexperiments on the FSC147 and the CARPK datasets show that CACViT significantlyoutperforms state-of-the-art CAC approaches in both effectiveness (23.60% errorreduction) and generalization, which suggests CACViT provides a concise andstrong baseline for CAC. Code will be available.",,,,,http://arxiv.org/pdf/2305.04440v1
Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion  Models,"  Recently large-scale language-image models (e.g., text-guided diffusionmodels) have considerably improved the image generation capabilities togenerate photorealistic images in various domains. Based on this success,current image editing methods use texts to achieve intuitive and versatilemodification of images. To edit a real image using diffusion models, one mustfirst invert the image to a noisy latent from which an edited image is sampledwith a target text prompt. However, most methods lack one of the following:user-friendliness (e.g., additional masks or precise descriptions of the inputimage are required), generalization to larger domains, or high fidelity to theinput image. In this paper, we design an accurate and quick inversiontechnique, Prompt Tuning Inversion, for text-driven image editing.Specifically, our proposed editing method consists of a reconstruction stageand an editing stage. In the first stage, we encode the information of theinput image into a learnable conditional embedding via Prompt Tuning Inversion.In the second stage, we apply classifier-free guidance to sample the editedimage, where the conditional embedding is calculated by linearly interpolatingbetween the target embedding and the optimized one obtained in the first stage.This technique ensures a superior trade-off between editability and highfidelity to the input image of our method. For example, we can change the colorof a specific object while preserving its original shape and background underthe guidance of only a target text prompt. Extensive experiments on ImageNetdemonstrate the superior editing performance of our method compared to thestate-of-the-art baselines.",,,,,http://arxiv.org/pdf/2305.04441v1
FashionTex: Controllable Virtual Try-on with Text and Texture,"  Virtual try-on attracts increasing research attention as a promising way forenhancing the user experience for online cloth shopping. Though existingmethods can generate impressive results, users need to provide a well-designedreference image containing the target fashion clothes that often do not exist.To support user-friendly fashion customization in full-body portraits, wepropose a multi-modal interactive setting by combining the advantages of bothtext and texture for multi-level fashion manipulation. With the carefullydesigned fashion editing module and loss functions, FashionTex framework cansemantically control cloth types and local texture patterns without annotatedpairwise training data. We further introduce an ID recovery module to maintainthe identity of input portrait. Extensive experiments have demonstrated theeffectiveness of our proposed pipeline.",,,,,http://arxiv.org/pdf/2305.04451v1
Generalized Universal Domain Adaptation with Generative Flow Networks,"  We introduce a new problem in unsupervised domain adaptation, termed asGeneralized Universal Domain Adaptation (GUDA), which aims to achieve preciseprediction of all target labels including unknown categories. GUDA bridges thegap between label distribution shift-based and label space mismatch-basedvariants, essentially categorizing them as a unified problem, guiding to acomprehensive framework for thoroughly solving all the variants. The keychallenge of GUDA is developing and identifying novel target categories whileestimating the target label distribution. To address this problem, we takeadvantage of the powerful exploration capability of generative flow networksand propose an active domain adaptation algorithm named GFlowDA, which selectsdiverse samples with probabilities proportional to a reward function. Toenhance the exploration capability and effectively perceive the target labeldistribution, we tailor the states and rewards, and introduce an efficientsolution for parent exploration and state transition. We also propose atraining paradigm for GUDA called Generalized Universal Adversarial Network(GUAN), which involves collaborative optimization between GUAN and GFlowNet.Theoretical analysis highlights the importance of exploration, and extensiveexperiments on benchmark datasets demonstrate the superiority of GFlowDA.",,,,,http://arxiv.org/pdf/2305.04466v1
Video Object Segmentation in Panoptic Wild Scenes,"  In this paper, we introduce semi-supervised video object segmentation (VOS)to panoptic wild scenes and present a large-scale benchmark as well as abaseline method for it. Previous benchmarks for VOS with sparse annotations arenot sufficient to train or evaluate a model that needs to process all possibleobjects in real-world scenarios. Our new benchmark (VIPOSeg) containsexhaustive object annotations and covers various real-world object categorieswhich are carefully divided into subsets of thing/stuff and seen/unseen classesfor comprehensive evaluation. Considering the challenges in panoptic VOS, wepropose a strong baseline method named panoptic object association withtransformers (PAOT), which uses panoptic identification to associate objectswith a pyramid architecture on multiple scales. Experimental results show thatVIPOSeg can not only boost the performance of VOS models by panoptic trainingbut also evaluate them comprehensively in panoptic scenes. Previous methods forclassic VOS still need to improve in performance and efficiency when dealingwith panoptic scenes, while our PAOT achieves SOTA performance with goodefficiency on VIPOSeg and previous VOS benchmarks. PAOT also ranks 1st in theVOT2022 challenge. Our dataset is available athttps://github.com/yoxu515/VIPOSeg-Benchmark.",,,,,http://arxiv.org/pdf/2305.04470v1
Scene Text Recognition with Image-Text Matching-guided Dictionary,"  Employing a dictionary can efficiently rectify the deviation between thevisual prediction and the ground truth in scene text recognition methods.However, the independence of the dictionary on the visual features may lead toincorrect rectification of accurate visual predictions. In this paper, wepropose a new dictionary language model leveraging the Scene Image-TextMatching(SITM) network, which avoids the drawbacks of the explicit dictionarylanguage model: 1) the independence of the visual features; 2) noisy choice incandidates etc. The SITM network accomplishes this by using Image-TextContrastive (ITC) Learning to match an image with its corresponding text amongcandidates in the inference stage. ITC is widely used in vision-languagelearning to pull the positive image-text pair closer in feature space. Inspiredby ITC, the SITM network combines the visual features and the text features ofall candidates to identify the candidate with the minimum distance in thefeature space. Our lexicon method achieves better results(93.8\\% accuracy) thanthe ordinary method results(92.1\\% accuracy) on six mainstream benchmarks.Additionally, we integrate our method with ABINet and establish newstate-of-the-art results on several benchmarks.",,,,,http://arxiv.org/pdf/2305.04524v1
SNT: Sharpness-Minimizing Network Transformation for Fast  Compression-friendly Pretraining,"  Model compression has become the de-facto approach for optimizing theefficiency of vision models. Recently, the focus of most compression effortshas shifted to post-training scenarios due to the very high cost of large-scalepretraining. This has created the need to build compressible models fromscratch, which can effectively be compressed after training. In this work, wepresent a sharpness-minimizing network transformation (SNT) method appliedduring pretraining that can create models with desirable compressibility andgeneralizability features. We compare our approach to a well-knownsharpness-minimizing optimizer to validate its efficacy in creating a flat losslandscape. To the best of our knowledge, SNT is the first pretraining methodthat uses an architectural transformation to generate compression-friendlynetworks. We find that SNT generalizes across different compression tasks andnetwork backbones, delivering consistent improvements over the ADAM baselinewith up to 2% accuracy improvement on weight pruning and 5.4% accuracyimprovement on quantization. Code to reproduce our results will be madepublicly available.",,,,,http://arxiv.org/pdf/2305.04526v1
LMPT: Prompt Tuning with Class-Specific Embedding Loss for Long-tailed  Multi-Label Visual Recognition,"  Long-tailed multi-label visual recognition (LTML) task is a highlychallenging task due to the label co-occurrence and imbalanced datadistribution. In this work, we propose a unified framework for LTML, namelyprompt tuning with class-specific embedding loss (LMPT), capturing the semanticfeature interactions between categories by combining text and image modalitydata and improving the performance synchronously on both head and tail classes.Specifically, LMPT introduces the embedding loss function with class-aware softmargin and re-weighting to learn class-specific contexts with the benefit oftextual descriptions (captions), which could help establish semanticrelationships between classes, especially between the head and tail classes.Furthermore, taking into account the class imbalance, the distribution-balancedloss is adopted as the classification loss function to further improve theperformance on the tail classes without compromising head classes. Extensiveexperiments are conducted on VOC-LT and COCO-LT datasets, which demonstratesthat the proposed method significantly surpasses the previous state-of-the-artmethods and zero-shot CLIP in LTML. Our codes are fully available at\\url{https://github.com/richard-peng-xia/LMPT}.",,,,,http://arxiv.org/pdf/2305.04536v1
High Quality Large-Scale 3-D Urban Mapping with Multi-Master TomoSAR,"  Multi-baseline interferometric synthetic aperture radar (InSAR) techniquesare effective approaches for retrieving the 3-D information of urban areas. Inorder to obtain a plausible reconstruction, it is necessary to use large-stackinterferograms. Hence, these methods are commonly not appropriate forlarge-scale 3-D urban mapping using TanDEM-X data where only a few acquisitionsare available in average for each city. This work proposes a new SARtomographic processing framework to work with those extremely small stacks,which integrates the non-local filtering into SAR tomography inversion. Theapplicability of the algorithm is demonstrated using a TanDEM-X multi-baselinestack with 5 bistatic interferograms over the whole city of Munich, Germany.Systematic comparison of our result with airborne LiDAR data shows that therelative height accuracy of two third buildings is within two meters, whichoutperforms the TanDEM-X raw DEM. The promising performance of the proposedalgorithm paved the first step towards high quality large-scale 3-D urbanmapping.",,,,,http://arxiv.org/pdf/2305.04541v1
Privacy-Preserving Representations are not Enough -- Recovering Scene  Content from Camera Poses,"  Visual localization is the task of estimating the camera pose from which agiven image was taken and is central to several 3D computer visionapplications. With the rapid growth in the popularity of AR/VR/MR devices andcloud-based applications, privacy issues are becoming a very important aspectof the localization process. Existing work on privacy-preserving localizationaims to defend against an attacker who has access to a cloud-based service. Inthis paper, we show that an attacker can learn about details of a scene withoutany access by simply querying a localization service. The attack is based onthe observation that modern visual localization algorithms are robust tovariations in appearance and geometry. While this is in general a desiredproperty, it also leads to algorithms localizing objects that are similarenough to those present in a scene. An attacker can thus query a server with alarge enough set of images of objects, \\eg, obtained from the Internet, andsome of them will be localized. The attacker can thus learn about objectplacements from the camera poses returned by the service (which is the minimalinformation returned by such a service). In this paper, we develop aproof-of-concept version of this attack and demonstrate its practicalfeasibility. The attack does not place any requirements on the localizationalgorithm used, and thus also applies to privacy-preserving representations.Current work on privacy-preserving representations alone is thus insufficient.",,,,,http://arxiv.org/pdf/2305.04603v1
Target-driven One-Shot Unsupervised Domain Adaptation,"  In this paper, we introduce a novel framework for the challenging problem ofOne-Shot Unsupervised Domain Adaptation (OSUDA), which aims to adapt to atarget domain with only a single unlabeled target sample. Unlike existingapproaches that rely on large labeled source and unlabeled target data, ourTarget-driven One-Shot UDA (TOS-UDA) approach employs a learnable augmentationstrategy guided by the target sample\'s style to align the source distributionwith the target distribution. Our method consists of three modules: anaugmentation module, a style alignment module, and a classifier. Unlikeexisting methods, our augmentation module allows for strong transformations ofthe source samples, and the style of the single target sample available isexploited to guide the augmentation by ensuring perceptual similarity.Furthermore, our approach integrates augmentation with style alignment,eliminating the need for separate pre-training on additional datasets. Ourmethod outperforms or performs comparably to existing OS-UDA methods on theDigits and DomainNet benchmarks.",,,,,http://arxiv.org/pdf/2305.04628v1
ReGeneration Learning of Diffusion Models with Rich Prompts for  Zero-Shot Image Translation,"  Large-scale text-to-image models have demonstrated amazing ability tosynthesize diverse and high-fidelity images. However, these models are oftenviolated by several limitations. Firstly, they require the user to provideprecise and contextually relevant descriptions for the desired imagemodifications. Secondly, current models can impose significant changes to theoriginal image content during the editing process. In this paper, we exploreReGeneration learning in an image-to-image Diffusion model (ReDiffuser), thatpreserves the content of the original image without human prompting and therequisite editing direction is automatically discovered within the textembedding space. To ensure consistent preservation of the shape during imageediting, we propose cross-attention guidance based on regeneration learning.This novel approach allows for enhanced expression of the target domainfeatures while preserving the original shape of the image. In addition, weintroduce a cooperative update strategy, which allows for efficientpreservation of the original shape of an image, thereby improving the qualityand consistency of shape preservation throughout the editing process. Ourproposed method leverages an existing pre-trained text-image diffusion modelwithout any additional training. Extensive experiments show that the proposedmethod outperforms existing work in both real and synthetic image editing.",,,,,http://arxiv.org/pdf/2305.04651v1
Self-supervised Learning for Pre-Training 3D Point Clouds: A Survey,"  Point cloud data has been extensively studied due to its compact form andflexibility in representing complex 3D structures. The ability of point clouddata to accurately capture and represent intricate 3D geometry makes it anideal choice for a wide range of applications, including computer vision,robotics, and autonomous driving, all of which require an understanding of theunderlying spatial structures. Given the challenges associated with annotatinglarge-scale point clouds, self-supervised point cloud representation learninghas attracted increasing attention in recent years. This approach aims to learngeneric and useful point cloud representations from unlabeled data,circumventing the need for extensive manual annotations. In this paper, wepresent a comprehensive survey of self-supervised point cloud representationlearning using DNNs. We begin by presenting the motivation and general trendsin recent research. We then briefly introduce the commonly used datasets andevaluation metrics. Following that, we delve into an extensive exploration ofself-supervised point cloud representation learning methods based on thesetechniques. Finally, we share our thoughts on some of the challenges andpotential issues that future research in self-supervised learning forpre-training 3D point clouds may encounter.",,,,,http://arxiv.org/pdf/2305.04691v1
Learning to Generate Poetic Chinese Landscape Painting with Calligraphy,"  In this paper, we present a novel system (denoted as Polaca) to generatepoetic Chinese landscape painting with calligraphy. Unlike previous singleimage-to-image painting generation, Polaca takes the classic poetry as inputand outputs the artistic landscape painting image with the correspondingcalligraphy. It is equipped with three different modules to complete the wholepiece of landscape painting artwork: the first one is a text-to-image module togenerate landscape painting image, the second one is an image-to-image moduleto generate stylistic calligraphy image, and the third one is an image fusionmodule to fuse the two images into a whole piece of aesthetic artwork.",,,,,http://arxiv.org/pdf/2305.04719v1
Large-scale and Efficient Texture Mapping Algorithm via Loopy Belief  Propagation,"  Texture mapping as a fundamental task in 3D modeling has been wellestablished for well-acquired aerial assets under consistent illumination, yetit remains a challenge when it is scaled to large datasets with images undervarying views and illuminations. A well-performed texture mapping algorithmmust be able to efficiently select views, fuse and map textures from theseviews to mesh models, at the same time, achieve consistent radiometry over theentire model. Existing approaches achieve efficiency either by limiting thenumber of images to one view per face, or simplifying global inferences to onlyachieve local color consistency. In this paper, we break this tie by proposinga novel and efficient texture mapping framework that allows the use of multipleviews of texture per face, at the same time to achieve global colorconsistency. The proposed method leverages a loopy belief propagation algorithmto perform an efficient and global-level probabilistic inferences to rankcandidate views per face, which enables face-level multi-view texture fusionand blending. The texture fusion algorithm, being non-parametric, bringsanother advantage over typical parametric post color correction methods, due toits improved robustness to non-linear illumination differences. The experimentson three different types of datasets (i.e. satellite dataset, unmanned-aerialvehicle dataset and close-range dataset) show that the proposed method hasproduced visually pleasant and texturally consistent results in all scenarios,with an added advantage of consuming less running time as compared to the stateof the art methods, especially for large-scale dataset such assatellite-derived models.","Geospatial data analysis, texture mapping, belief propagation, multiple labels, image blending. ","Xiao Ling, Rongjun Qin, Senior Member, IEEE",,,http://arxiv.org/pdf/2305.04763v1
OSTA: One-shot Task-adaptive Channel Selection for Semantic Segmentation  of Multichannel Images,"  Semantic segmentation of multichannel images is a fundamental task for manyapplications. Selecting an appropriate channel combination from the originalmultichannel image can improve the accuracy of semantic segmentation and reducethe cost of data storage, processing and future acquisition. Existing channelselection methods typically use a reasonable selection procedure to determine adesirable channel combination, and then train a semantic segmentation networkusing that combination. In this study, the concept of pruning from a supernetis used for the first time to integrate the selection of channel combinationand the training of a semantic segmentation network. Based on this concept, aOne-Shot Task-Adaptive (OSTA) channel selection method is proposed for thesemantic segmentation of multichannel images. OSTA has three stages, namely thesupernet training stage, the pruning stage and the fine-tuning stage. Theoutcomes of six groups of experiments (L7Irish3C, L7Irish2C, L8Biome3C,L8Biome2C, RIT-18 and Semantic3D) demonstrated the effectiveness and efficiencyof OSTA. OSTA achieved the highest segmentation accuracies in all tests (62.49%(mIoU), 75.40% (mIoU), 68.38% (mIoU), 87.63% (mIoU), 66.53% (mA) and 70.86%(mIoU), respectively). It even exceeded the highest accuracies of exhaustivetests (61.54% (mIoU), 74.91% (mIoU), 67.94% (mIoU), 87.32% (mIoU), 65.32% (mA)and 70.27% (mIoU), respectively), where all possible channel combinations weretested. All of this can be accomplished within a predictable and relativelyefficient timeframe, ranging from 101.71% to 298.1% times the time required totrain the segmentation network alone. In addition, there were interestingfindings that were deemed valuable for several fields.",,,,,http://arxiv.org/pdf/2305.04766v1
PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR  Point Clouds,"  In order to deal with the sparse and unstructured raw point clouds, LiDARbased 3D object detection research mostly focuses on designing dedicated localpoint aggregators for fine-grained geometrical modeling. In this paper, werevisit the local point aggregators from the perspective of allocatingcomputational resources. We find that the simplest pillar based models performsurprisingly well considering both accuracy and latency. Additionally, we showthat minimal adaptions from the success of 2D object detection, such asenlarging receptive field, significantly boost the performance. Extensiveexperiments reveal that our pillar based networks with modernized designs interms of architecture and training render the state-of-the-art performance onthe two popular benchmarks: Waymo Open Dataset and nuScenes. Our resultschallenge the common intuition that the detailed geometry modeling is essentialto achieve high performance for 3D object detection.",,,,,http://arxiv.org/pdf/2305.04925v1
RelPose++: Recovering 6D Poses from Sparse-view Observations,"  We address the task of estimating 6D camera poses from sparse-view image sets(2-8 images). This task is a vital pre-processing stage for nearly allcontemporary (neural) reconstruction algorithms but remains challenging givensparse views, especially for objects with visual symmetries and texture-lesssurfaces. We build on the recent RelPose framework which learns a network thatinfers distributions over relative rotations over image pairs. We extend thisapproach in two key ways; first, we use attentional transformer layers toprocess multiple images jointly, since additional views of an object mayresolve ambiguous symmetries in any given image pair (such as the handle of amug that becomes visible in a third view). Second, we augment this network toalso report camera translations by defining an appropriate coordinate systemthat decouples the ambiguity in rotation estimation from translationprediction. Our final system results in large improvements in 6D poseprediction over prior art on both seen and unseen object categories and alsoenables pose estimation and 3D reconstruction for in-the-wild objects.",,,,,http://arxiv.org/pdf/2305.04926v1
Robust Pose Transfer with Dynamic Details using Neural Video Rendering,"  Pose transfer of human videos aims to generate a high fidelity video of atarget person imitating actions of a source person. A few studies have madegreat progress either through image translation with deep latent features orneural rendering with explicit 3D features. However, both of them rely on largeamounts of training data to generate realistic results, and the performancedegrades on more accessible internet videos due to insufficient trainingframes. In this paper, we demonstrate that the dynamic details can be preservedeven trained from short monocular videos. Overall, we propose a neural videorendering framework coupled with an image-translation-based dynamic detailsgeneration network (D2G-Net), which fully utilizes both the stability ofexplicit 3D features and the capacity of learning components. To be specific, anovel texture representation is presented to encode both the static andpose-varying appearance characteristics, which is then mapped to the imagespace and rendered as a detail-rich frame in the neural rendering stage.Moreover, we introduce a concise temporal loss in the training stage tosuppress the detail flickering that is made more visible due to high-qualitydynamic details generated by our method. Through extensive comparisons, wedemonstrate that our neural human video renderer is capable of achieving bothclearer dynamic details and more robust performance even on accessible shortvideos with only 2k - 4k frames.",,,,,http://arxiv.org/pdf/2106.14132v3
TPC: Transformation-Specific Smoothing for Point Cloud Models,"  Point cloud models with neural network architectures have achieved greatsuccess and have been widely used in safety-critical applications, such asLidar-based recognition systems in autonomous vehicles. However, such modelsare shown vulnerable to adversarial attacks which aim to apply stealthysemantic transformations such as rotation and tapering to mislead modelpredictions. In this paper, we propose a transformation-specific smoothingframework TPC, which provides tight and scalable robustness guarantees forpoint cloud models against semantic transformation attacks. We first categorizecommon 3D transformations into three categories: additive (e.g., shearing),composable (e.g., rotation), and indirectly composable (e.g., tapering), and wepresent generic robustness certification strategies for all categoriesrespectively. We then specify unique certification protocols for a range ofspecific semantic transformations and their compositions. Extensive experimentson several common 3D transformations show that TPC significantly outperformsthe state of the art. For example, our framework boosts the certified accuracyagainst twisting transformation along z-axis (within 20$^\\circ$) from 20.3$\\%$to 83.8$\\%$. Codes and models are available athttps://github.com/chuwd19/Point-Cloud-Smoothing.",,,,,http://arxiv.org/pdf/2201.12733v5
Blind Image Deconvolution Using Variational Deep Image Prior,"  Conventional deconvolution methods utilize hand-crafted image priors toconstrain the optimization. While deep-learning-based methods have simplifiedthe optimization by end-to-end training, they fail to generalize well to blursunseen in the training dataset. Thus, training image-specific models isimportant for higher generalization. Deep image prior (DIP) provides anapproach to optimize the weights of a randomly initialized network with asingle degraded image by maximum a posteriori (MAP), which shows that thearchitecture of a network can serve as the hand-crafted image prior. Differentfrom the conventional hand-crafted image priors that are statisticallyobtained, it is hard to find a proper network architecture because therelationship between images and their corresponding network architectures isunclear. As a result, the network architecture cannot provide enough constraintfor the latent sharp image. This paper proposes a new variational deep imageprior (VDIP) for blind image deconvolution, which exploits additivehand-crafted image priors on latent sharp images and approximates adistribution for each pixel to avoid suboptimal solutions. Our mathematicalanalysis shows that the proposed method can better constrain the optimization.The experimental results further demonstrate that the generated images havebetter quality than that of the original DIP on benchmark datasets. The sourcecode of our VDIP is available athttps://github.com/Dong-Huo/VDIP-Deconvolution.",,,,,http://arxiv.org/pdf/2202.00179v2
Blind2Unblind: Self-Supervised Image Denoising with Visible Blind Spots,"  Real noisy-clean pairs on a large scale are costly and difficult to obtain.Meanwhile, supervised denoisers trained on synthetic data perform poorly inpractice. Self-supervised denoisers, which learn only from single noisy images,solve the data collection problem. However, self-supervised denoising methods,especially blindspot-driven ones, suffer sizable information loss during inputor network design. The absence of valuable information dramatically reduces theupper bound of denoising performance. In this paper, we propose a simple yetefficient approach called Blind2Unblind to overcome the information loss inblindspot-driven denoising methods. First, we introduce a global-aware maskmapper that enables global perception and accelerates training. The mask mappersamples all pixels at blind spots on denoised volumes and maps them to the samechannel, allowing the loss function to optimize all blind spots at once.Second, we propose a re-visible loss to train the denoising network and makeblind spots visible. The denoiser can learn directly from raw noise imageswithout losing information or being trapped in identity mapping. We alsotheoretically analyze the convergence of the re-visible loss. Extensiveexperiments on synthetic and real-world datasets demonstrate the superiorperformance of our approach compared to previous work. Code is available athttps://github.com/demonsjin/Blind2Unblind.",,,,,http://arxiv.org/pdf/2203.06967v3
Provable Defense Against Geometric Transformations,"  Geometric image transformations that arise in the real world, such as scalingand rotation, have been shown to easily deceive deep neural networks (DNNs).Hence, training DNNs to be certifiably robust to these perturbations iscritical. However, no prior work has been able to incorporate the objective ofdeterministic certified robustness against geometric transformations into thetraining procedure, as existing verifiers are exceedingly slow. To addressthese challenges, we propose the first provable defense for deterministiccertified geometric robustness. Our framework leverages a novel GPU-optimizedverifier that can certify images between 60$\\times$ to 42,600$\\times$ fasterthan existing geometric robustness verifiers, and thus unlike existing works,is fast enough for use in training. Across multiple datasets, our results showthat networks trained via our framework consistently achieve state-of-the-artdeterministic certified geometric robustness and clean accuracy. Furthermore,for the first time, we verify the geometric robustness of a neural network forthe challenging, real-world setting of autonomous driving.",,,,,http://arxiv.org/pdf/2207.11177v3
A knee cannot have lung disease: out-of-distribution detection with  in-distribution voting using the medical example of chest X-ray  classification,"  To investigate the impact of OOD radiographs on existing chest X-rayclassification models and to increase their robustness against OOD data. Thestudy employed the commonly used chest X-ray classification model, CheXnet,trained on the chest X-ray 14 data set, and tested its robustness against OODdata using three public radiography data sets: IRMA, Bone Age, and MURA, andthe ImageNet data set. To detect OOD data for multi-label classification, weproposed in-distribution voting (IDV). The OOD detection performance ismeasured across data sets using the area under the receiver operatingcharacteristic curve (AUC) analysis and compared with Mahalanobis-based OODdetection, MaxLogit, MaxEnergy and self-supervised OOD detection (SS OOD).Without additional OOD detection, the chest X-ray classifier failed to discardany OOD images, with an AUC of 0.5. The proposed IDV approach trained on ID(chest X-ray 14) and OOD data (IRMA and ImageNet) achieved, on average, 0.999OOD AUC across the three data sets, surpassing all other OOD detection methods.Mahalanobis-based OOD detection achieved an average OOD detection AUC of 0.982.IDV trained solely with a few thousand ImageNet images had an AUC 0.913, whichwas higher than MaxLogit (0.726), MaxEnergy (0.724), and SS OOD (0.476). Theperformance of all tested OOD detection methods did not translate well toradiography data sets, except Mahalanobis-based OOD detection and the proposedIDV method. Training solely on ID data led to incorrect classification of OODimages as ID, resulting in increased false positive rates. IDV substantiallyimproved the model\'s ID classification performance, even when trained with datathat will not occur in the intended use case or test set, without additionalinference overhead.",,,,,http://arxiv.org/pdf/2208.01077v2
Persuasion Strategies in Advertisements,"  Modeling what makes an advertisement persuasive, i.e., eliciting the desiredresponse from consumer, is critical to the study of propaganda, socialpsychology, and marketing. Despite its importance, computational modeling ofpersuasion in computer vision is still in its infancy, primarily due to thelack of benchmark datasets that can provide persuasion-strategy labelsassociated with ads. Motivated by persuasion literature in social psychologyand marketing, we introduce an extensive vocabulary of persuasion strategiesand build the first ad image corpus annotated with persuasion strategies. Wethen formulate the task of persuasion strategy prediction with multi-modallearning, where we design a multi-task attention fusion model that can leverageother ad-understanding tasks to predict persuasion strategies. Further, weconduct a real-world case study on 1600 advertising campaigns of 30 Fortune-500companies where we use our model\'s predictions to analyze which strategies workwith different demographics (age and gender). The dataset also provides imagesegmentation masks, which labels persuasion strategies in the corresponding adimages on the test split. We publicly release our code and datasethttps://midas-research.github.io/persuasion-advertisements/.",,,,,http://arxiv.org/pdf/2208.09626v2
DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic  Models,"  Diffusion probabilistic models (DPMs) have achieved impressive success inhigh-resolution image synthesis, especially in recent large-scale text-to-imagegeneration applications. An essential technique for improving the samplequality of DPMs is guided sampling, which usually needs a large guidance scaleto obtain the best sample quality. The commonly-used fast sampler for guidedsampling is DDIM, a first-order diffusion ODE solver that generally needs 100to 250 steps for high-quality samples. Although recent works propose dedicatedhigh-order solvers and achieve a further speedup for sampling without guidance,their effectiveness for guided sampling has not been well-tested before. Inthis work, we demonstrate that previous high-order fast samplers suffer frominstability issues, and they even become slower than DDIM when the guidancescale grows large. To further speed up guided sampling, we proposeDPM-Solver++, a high-order solver for the guided sampling of DPMs. DPM-Solver++solves the diffusion ODE with the data prediction model and adopts thresholdingmethods to keep the solution matches training data distribution. We furtherpropose a multistep variant of DPM-Solver++ to address the instability issue byreducing the effective step size. Experiments show that DPM-Solver++ cangenerate high-quality samples within only 15 to 20 steps for guided sampling bypixel-space and latent-space DPMs.",,,,,http://arxiv.org/pdf/2211.01095v2
YoloCurvSeg: You Only Label One Noisy Skeleton for Vessel-style  Curvilinear Structure Segmentation,"  Weakly-supervised learning (WSL) has been proposed to alleviate the conflictbetween data annotation cost and model performance through employingsparsely-grained (i.e., point-, box-, scribble-wise) supervision and has shownpromising performance, particularly in the image segmentation field. However,it is still a very challenging task due to the limited supervision, especiallywhen only a small number of labeled samples are available. Additionally, almostall existing WSL segmentation methods are designed for star-convex structureswhich are very different from curvilinear structures such as vessels andnerves. In this paper, we propose a novel sparsely annotated segmentationframework for curvilinear structures, named YoloCurvSeg. A very essentialcomponent of YoloCurvSeg is image synthesis. Specifically, a backgroundgenerator delivers image backgrounds that closely match the real distributionsthrough inpainting dilated skeletons. The extracted backgrounds are thencombined with randomly emulated curves generated by a Space ColonizationAlgorithm-based foreground generator and through a multilayer patch-wisecontrastive learning synthesizer. In this way, a synthetic dataset with bothimages and curve segmentation labels is obtained, at the cost of only one or afew noisy skeleton annotations. Finally, a segmenter is trained with thegenerated dataset and possibly an unlabeled dataset. The proposed YoloCurvSegis evaluated on four publicly available datasets (OCTA500, CORN, DRIVE andCHASEDB1) and the results show that YoloCurvSeg outperforms state-of-the-artWSL segmentation methods by large margins. With only one noisy skeletonannotation (respectively 0.14%, 0.03%, 1.40%, and 0.65% of the fullannotation), YoloCurvSeg achieves more than 97% of the fully-supervisedperformance on each dataset. Code and datasets will be released athttps://github.com/llmir/YoloCurvSeg.",,,,,http://arxiv.org/pdf/2212.05566v4
BKinD-3D: Self-Supervised 3D Keypoint Discovery from Multi-View Videos,"  Quantifying motion in 3D is important for studying the behavior of humans andother animals, but manual pose annotations are expensive and time-consuming toobtain. Self-supervised keypoint discovery is a promising strategy forestimating 3D poses without annotations. However, current keypoint discoveryapproaches commonly process single 2D views and do not operate in the 3D space.We propose a new method to perform self-supervised keypoint discovery in 3Dfrom multi-view videos of behaving agents, without any keypoint or bounding boxsupervision in 2D or 3D. Our method, BKinD-3D, uses an encoder-decoderarchitecture with a 3D volumetric heatmap, trained to reconstructspatiotemporal differences across multiple views, in addition to joint lengthconstraints on a learned 3D skeleton of the subject. In this way, we discoverkeypoints without requiring manual supervision in videos of humans and rats,demonstrating the potential of 3D keypoint discovery for studying behavior.",,,,,http://arxiv.org/pdf/2212.07401v2
From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language  Models,"  Large language models (LLMs) have demonstrated excellent zero-shotgeneralization to new language tasks. However, effective utilization of LLMsfor zero-shot visual question-answering (VQA) remains challenging, primarilydue to the modality disconnection and task disconnection between LLM and VQAtask. End-to-end training on vision and language data may bridge thedisconnections, but is inflexible and computationally expensive. To addressthis issue, we propose \\emph{Img2Prompt}, a plug-and-play module that providesthe prompts that can bridge the aforementioned modality and taskdisconnections, so that LLMs can perform zero-shot VQA tasks without end-to-endtraining. In order to provide such prompts, we further employ LLM-agnosticmodels to provide prompts that can describe image content and self-constructedquestion-answer pairs, which can effectively guide LLM to perform zero-shot VQAtasks. Img2Prompt offers the following benefits: 1) It can flexibly work withvarious LLMs to perform VQA. 2)~Without the needing of end-to-end training, itsignificantly reduces the cost of deploying LLM for zero-shot VQA tasks. 3) Itachieves comparable or better performance than methods relying on end-to-endtraining. For example, we outperform Flamingo \\cite{Deepmind:Flamingo2022} by5.6\\% on VQAv2. On the challenging A-OKVQA dataset, our method even outperformsfew-shot methods by as much as 20\\%.",,,,,http://arxiv.org/pdf/2212.10846v3
Dual PatchNorm,"  We propose Dual PatchNorm: two Layer Normalization layers (LayerNorms),before and after the patch embedding layer in Vision Transformers. Wedemonstrate that Dual PatchNorm outperforms the result of exhaustive search foralternative LayerNorm placement strategies in the Transformer block itself. Inour experiments, incorporating this trivial modification, often leads toimproved accuracy over well-tuned Vision Transformers and never hurts.",,,,,http://arxiv.org/pdf/2302.01327v3
2D-Empowered Point Cloud Analytics on the Edge,"  3D object detection plays a pivotal role in many applications, most notablyautonomous driving and robotics. These applications are commonly deployed onedge devices to promptly interact with the environment, and often require nearreal-time response. With limited computation power, it is challenging toexecute 3D detection on the edge using highly complex neural networks. Commonapproaches such as offloading to the cloud induce significant latency overheadsdue to the large amount of point cloud data during transmission. To resolve thetension between wimpy edge devices and compute-intensive inference workloads,we explore the possibility of empowering fast 2D detection to extrapolate 3Dbounding boxes. To this end, we present Moby, a novel system that demonstratesthe feasibility and potential of our approach. We design a transformationpipeline for Moby that generates 3D bounding boxes efficiently and accuratelybased on 2D detection results without running 3D detectors. Further, we devisea frame offloading scheduler that decides when to launch the 3D detectorjudiciously in the cloud to avoid the errors from accumulating. Extensiveevaluations on NVIDIA Jetson TX2 with real-world autonomous driving datasetsdemonstrate that Moby offers up to 91.9% latency improvement with modestaccuracy loss over state of the art.",,,,,http://arxiv.org/pdf/2302.09221v2
Mover: Mask and Recovery based Facial Part Consistency Aware Method for  Deepfake Video Detection,"  Deepfake techniques have been widely used for malicious purposes, promptingextensive research interest in developing Deepfake detection methods. Deepfakemanipulations typically involve tampering with facial parts, which can resultin inconsistencies across different parts of the face. For instance, Deepfaketechniques may change smiling lips to an upset lip, while the eyes remainsmiling. Existing detection methods depend on specific indicators of forgery,which tend to disappear as the forgery patterns are improved. To address thelimitation, we propose Mover, a new Deepfake detection model that exploitsunspecific facial part inconsistencies, which are inevitable weaknesses ofDeepfake videos. Mover randomly masks regions of interest (ROIs) and recoversfaces to learn unspecific features, which makes it difficult for fake faces tobe recovered, while real faces can be easily recovered. Specifically, given areal face image, we first pretrain a masked autoencoder to learn facial partconsistency by dividing faces into three parts and randomly masking ROIs, whichare then recovered based on the unmasked facial parts. Furthermore, to maximizethe discrepancy between real and fake videos, we propose a novel model withdual networks that utilize the pretrained encoder and masked autoencoder,respectively. 1) The pretrained encoder is finetuned for capturing the encodingof inconsistent information in the given video. 2) The pretrained maskedautoencoder is utilized for mapping faces and distinguishing real and fakevideos. Our extensive experiments on standard benchmarks demonstrate that Moveris highly effective.",,,,,http://arxiv.org/pdf/2303.01740v2
SVD-DIP: Overcoming the Overfitting Problem in DIP-based CT  Reconstruction,"  The deep image prior (DIP) is a well-established unsupervised deep learningmethod for image reconstruction; yet it is far from being flawless. The DIPoverfits to noise if not early stopped, or optimized via a regularizedobjective. We build on the regularized fine-tuning of a pretrained DIP, byadopting a novel strategy that restricts the learning to the adaptation ofsingular values. The proposed SVD-DIP uses ad hoc convolutional layers whosepretrained parameters are decomposed via the singular value decomposition.Optimizing the DIP then solely consists in the fine-tuning of the singularvalues, while keeping the left and right singular vectors fixed. We thoroughlyvalidate the proposed method on real-measured $\\mu$CT data of a lotus root aswell as two medical datasets (LoDoPaB and Mayo). We report significantlyimproved stability of the DIP optimization, by overcoming the overfitting tonoise.",,,,,http://arxiv.org/pdf/2303.15748v2
Exploiting the Distortion-Semantic Interaction in Fisheye Data,"  In this work, we present a methodology to shape a fisheye-specificrepresentation space that reflects the interaction between distortion andsemantic context present in this data modality. Fisheye data has the widerfield of view advantage over other types of cameras, but this comes at theexpense of high radial distortion. As a result, objects further from the centerexhibit deformations that make it difficult for a model to identify theirsemantic context. While previous work has attempted architectural and trainingaugmentation changes to alleviate this effect, no work has attempted to guidethe model towards learning a representation space that reflects thisinteraction between distortion and semantic context inherent to fisheye data.We introduce an approach to exploit this relationship by first extractingdistortion class labels based on an object\'s distance from the center of theimage. We then shape a backbone\'s representation space with a weightedcontrastive loss that constrains objects of the same semantic class anddistortion class to be close to each other within a lower dimensional embeddingspace. This backbone trained with both semantic and distortion information isthen fine-tuned within an object detection setting to empirically evaluate thequality of the learnt representation. We show this method leads to performanceimprovements by as much as 1.1% mean average precision over standard objectdetection strategies and .6% improvement over other state of the artrepresentation learning approaches.",,,,,http://arxiv.org/pdf/2305.00079v2
Attack-SAM: Towards Attacking Segment Anything Model With Adversarial  Examples,"  Segment Anything Model (SAM) has attracted significant attention recently,due to its impressive performance on various downstream tasks in a zero-shortmanner. Computer vision (CV) area might follow the natural language processing(NLP) area to embark on a path from task-specific vision models towardfoundation models. However, deep vision models are widely recognized asvulnerable to adversarial examples, which fool the model to make wrongpredictions with imperceptible perturbation. Such vulnerability to adversarialattacks causes serious concerns when applying deep models to security-sensitiveapplications. Therefore, it is critical to know whether the vision foundationmodel SAM can also be fooled by adversarial attacks. To the best of ourknowledge, our work is the first of its kind to conduct a comprehensiveinvestigation on how to attack SAM with adversarial examples. With the basicattack goal set to mask removal, we investigate the adversarial robustness ofSAM in the full white-box setting and transfer-based black-box settings. Beyondthe basic goal of mask removal, we further investigate and find that it ispossible to generate any desired mask by the adversarial attack.",,,,,http://arxiv.org/pdf/2305.00866v2
Using Spatio-Temporal Dual-Stream Network with Self-Supervised Learning  for Lung Tumor Classification on Radial Probe Endobronchial Ultrasound Video,"  The purpose of this study is to develop a computer-aided diagnosis system forclassifying benign and malignant lung lesions, and to assist physicians inreal-time analysis of radial probe endobronchial ultrasound (EBUS) videos.During the biopsy process of lung cancer, physicians use real-time ultrasoundimages to find suitable lesion locations for sampling. However, most of theseimages are difficult to classify and contain a lot of noise. Previous studieshave employed 2D convolutional neural networks to effectively differentiatebetween benign and malignant lung lesions, but doctors still need to manuallyselect good-quality images, which can result in additional labor costs. Inaddition, the 2D neural network has no ability to capture the temporalinformation of the ultrasound video, so it is difficult to obtain therelationship between the features of the continuous images. This study designsan automatic diagnosis system based on a 3D neural network, uses the SlowFastarchitecture as the backbone to fuse temporal and spatial features, and usesthe SwAV method of contrastive learning to enhance the noise robustness of themodel. The method we propose includes the following advantages, such as (1)using clinical ultrasound films as model input, thereby reducing the need forhigh-quality image selection by physicians, (2) high-accuracy classification ofbenign and malignant lung lesions can assist doctors in clinical diagnosis andreduce the time and risk of surgery, and (3) the capability to classify welleven in the presence of significant image noise. The AUC, accuracy, precision,recall and specificity of our proposed method on the validation set reached0.87, 83.87%, 86.96%, 90.91% and 66.67%, respectively. The results haveverified the importance of incorporating temporal information and theeffectiveness of using the method of contrastive learning on featureextraction.",,,,,http://arxiv.org/pdf/2305.02719v2
OctFormer: Octree-based Transformers for 3D Point Clouds,"  We propose octree-based transformers, named OctFormer, for 3D point cloudlearning. OctFormer can not only serve as a general and effective backbone for3D point cloud segmentation and object detection but also have linearcomplexity and is scalable for large-scale point clouds. The key challenge inapplying transformers to point clouds is reducing the quadratic, thusoverwhelming, computation complexity of attentions. To combat this issue,several works divide point clouds into non-overlapping windows and constrainattentions in each local window. However, the point number in each windowvaries greatly, impeding the efficient execution on GPU. Observing thatattentions are robust to the shapes of local windows, we propose a novel octreeattention, which leverages sorted shuffled keys of octrees to partition pointclouds into local windows containing a fixed number of points while permittingshapes of windows to change freely. And we also introduce dilated octreeattention to expand the receptive field further. Our octree attention can beimplemented in 10 lines of code with open-sourced libraries and runs 17 timesfaster than other point cloud attentions when the point number exceeds 200k.Built upon the octree attention, OctFormer can be easily scaled up and achievesstate-of-the-art performances on a series of 3D segmentation and detectionbenchmarks, surpassing previous sparse-voxel-based CNNs and point cloudtransformers in terms of both efficiency and effectiveness. Notably, on thechallenging ScanNet200 dataset, OctFormer outperforms sparse-voxel-based CNNsby 7.3 in mIoU. Our code and trained models are available athttps://wang-ps.github.io/octformer.",,,,,http://arxiv.org/pdf/2305.03045v2
Clothes Grasping and Unfolding Based on RGB-D Semantic Segmentation,"  Clothes grasping and unfolding is a core step in robotic-assisted dressing.Most existing works leverage depth images of clothes to train a deeplearning-based model to recognize suitable grasping points. These methods oftenutilize physics engines to synthesize depth images to reduce the cost of reallabeled data collection. However, the natural domain gap between synthetic andreal images often leads to poor performance of these methods on real data.Furthermore, these approaches often struggle in scenarios where grasping pointsare occluded by the clothing item itself. To address the above challenges, wepropose a novel Bi-directional Fractal Cross Fusion Network (BiFCNet) forsemantic segmentation, enabling recognition of graspable regions in order toprovide more possibilities for grasping. Instead of using depth images only, wealso utilize RGB images with rich color features as input to our network inwhich the Fractal Cross Fusion (FCF) module fuses RGB and depth data byconsidering global complex features based on fractal geometry. To reduce thecost of real data collection, we further propose a data augmentation methodbased on an adversarial strategy, in which the color and geometrictransformations simultaneously process RGB and depth data while maintaining thelabel correspondence. Finally, we present a pipeline for clothes grasping andunfolding from the perspective of semantic segmentation, through the additionof a strategy for grasp point selection from segmentation regions based onclothing flatness measures, while taking into account the grasping direction.We evaluate our BiFCNet on the public dataset NYUDv2 and obtained comparableperformance to current state-of-the-art models. We also deploy our model on aBaxter robot, running extensive grasping and unfolding experiments as part ofour ablation studies, achieving an 84% success rate.",,,,,http://arxiv.org/pdf/2305.03259v2
Physics-based network fine-tuning for robust quantitative susceptibility  mapping from high-pass filtered phase,"  Purpose: To improve the generalization ability of convolutional neuralnetwork (CNN) based prediction of quantitative susceptibility mapping (QSM)from high-pass filtered phase (HPFP) image. Methods: The proposed networkaddresses two common generalization issues that arise when using a pre-trainednetwork to predict QSM from HPFP: a) data with unseen voxel sizes, and b) datawith unknown high-pass filter parameters. A network fine-tuning step based on ahigh-pass filtering dipole convolution forward model is proposed to reduce thegeneralization error of the pre-trained network. A progressive Unetarchitecture is proposed to improve prediction accuracy without increasingfine-tuning computational cost. Results: In retrospective studies using RMSE,PSNR, SSIM and HFEN as quality metrics, the performance of both Unet andprogressive Unet was improved after physics-based fine-tuning at all voxelsizes and most high-pass filtering cutoff frequencies tested in the experiment.Progressive Unet slightly outperformed Unet both before and after fine-tuning.In a prospective study, image sharpness was improved after physics-basedfine-tuning for both Unet and progressive Unet. Compared to Unet, progressiveUnet had better agreement of regional susceptibility values with reference QSM.Conclusion: The proposed method shows improved robustness compared to thepre-trained network without fine-tuning when the test dataset deviates fromtraining. Our code is available at https://github.com/Jinwei1209/SWI_to_QSM/",,,,,http://arxiv.org/pdf/2305.03844v1
White Matter Hyperintensities Segmentation Using Probabilistic TransUNet,"  White Matter Hyperintensities (WMH) are areas of the brain that have higherintensity than other normal brain regions on Magnetic Resonance Imaging (MRI)scans. WMH is often associated with small vessel disease in the brain, makingearly detection of WMH important. However, there are two common issues in thedetection of WMH: high ambiguity and difficulty in detecting small WMH. In thisstudy, we propose a method called Probabilistic TransUNet to address theprecision of small object segmentation and the high ambiguity of medicalimages. To measure model performance, we conducted a k-fold cross validationand cross dataset robustness experiment. Based on the experiments, the additionof a probabilistic model and the use of a transformer-based approach were ableto achieve better performance.",,,,,http://arxiv.org/pdf/2305.03912v1
Adaptive loose optimization for robust question answering,"  Question answering methods are well-known for leveraging data bias, such asthe language prior in visual question answering and the position bias inmachine reading comprehension (extractive question answering). Currentdebiasing methods often come at the cost of significant in-distributionperformance to achieve favorable out-of-distribution generalizability, whilenon-debiasing methods sacrifice a considerable amount of out-of-distributionperformance in order to obtain high in-distribution performance. Therefore, itis challenging for them to deal with the complicated changing real-worldsituations. In this paper, we propose a simple yet effective novel lossfunction with adaptive loose optimization, which seeks to make the best of bothworlds for question answering. Our main technical contribution is to reduce theloss adaptively according to the ratio between the previous and currentoptimization state on mini-batch training data. This loose optimization can beused to prevent non-debiasing methods from overlearning data bias whileenabling debiasing methods to maintain slight bias learning. Experiments on thevisual question answering datasets, including VQA v2, VQA-CP v1, VQA-CP v2,GQA-OOD, and the extractive question answering dataset SQuAD demonstrate thatour approach enables QA methods to obtain state-of-the-art in- andout-of-distribution performance in most cases. The source code has beenreleased publicly in \\url{https://github.com/reml-group/ALO}.",,,,,http://arxiv.org/pdf/2305.03971v1
Towards Prompt-robust Face Privacy Protection via Adversarial Decoupling  Augmentation Framework,"  Denoising diffusion models have shown remarkable potential in variousgeneration tasks. The open-source large-scale text-to-image model, StableDiffusion, becomes prevalent as it can generate realistic artistic or facialimages with personalization through fine-tuning on a limited number of newsamples. However, this has raised privacy concerns as adversaries can acquirefacial images online and fine-tune text-to-image models for malicious editing,leading to baseless scandals, defamation, and disruption to victims\' lives.Prior research efforts have focused on deriving adversarial loss fromconventional training processes for facial privacy protection throughadversarial perturbations. However, existing algorithms face two issues: 1)they neglect the image-text fusion module, which is the vital module oftext-to-image diffusion models, and 2) their defensive performance is unstableagainst different attacker prompts. In this paper, we propose the AdversarialDecoupling Augmentation Framework (ADAF), addressing these issues by targetingthe image-text fusion module to enhance the defensive performance of facialprivacy protection algorithms. ADAF introduces multi-level text-relatedaugmentations for defense stability against various attacker prompts.Concretely, considering the vision, text, and common unit space, we proposeVision-Adversarial Loss, Prompt-Robust Augmentation, and Attention-DecouplingLoss. Extensive experiments on CelebA-HQ and VGGFace2 demonstrate ADAF\'spromising performance, surpassing existing algorithms.",,,,,http://arxiv.org/pdf/2305.03980v1
Unlocking Low-Light-Rainy Image Restoration by Pairwise Degradation  Feature Vector Guidance,"  Rain in the dark is a common natural phenomenon. Photos captured in such acondition significantly impact the performance of various nighttime activities,such as autonomous driving, surveillance systems, and night photography. Whileexisting methods designed for low-light enhancement or deraining show promisingperformance, they have limitations in simultaneously addressing the task ofbrightening low light and removing rain. Furthermore, using a cascade approach,such as ``deraining followed by low-light enhancement\'\' or vice versa, may leadto difficult-to-handle rain patterns or excessively blurred and overexposedimages. To overcome these limitations, we propose an end-to-end network called$L^{2}RIRNet$ which can jointly handle low-light enhancement and deraining. Ournetwork mainly includes a Pairwise Degradation Feature Vector ExtractionNetwork (P-Net) and a Restoration Network (R-Net). P-Net can learn degradationfeature vectors on the dark and light areas separately, using contrastivelearning to guide the image restoration process. The R-Net is responsible forrestoring the image. We also introduce an effective Fast Fourier - ResNetDetail Guidance Module (FFR-DG) that initially guides image restoration usingdetail image that do not contain degradation information but focus on texturedetail information. Additionally, we contribute a dataset containing syntheticand real-world low-light-rainy images. Extensive experiments demonstrate thatour $L^{2}RIRNet$ outperforms existing methods in both synthetic and complexreal-world scenarios.",,,,,http://arxiv.org/pdf/2305.03997v1
Degradation-Noise-Aware Deep Unfolding Transformer for Hyperspectral  Image Denoising,"  Hyperspectral imaging (HI) has emerged as a powerful tool in diverse fieldssuch as medical diagnosis, industrial inspection, and agriculture, owing to itsability to detect subtle differences in physical properties through highspectral resolution. However, hyperspectral images (HSIs) are often quite noisybecause of narrow band spectral filtering. To reduce the noise in HSI datacubes, both model-driven and learning-based denoising algorithms have beenproposed. However, model-based approaches rely on hand-crafted priors andhyperparameters, while learning-based methods are incapable of estimating theinherent degradation patterns and noise distributions in the imaging procedure,which could inform supervised learning. Secondly, learning-based algorithmspredominantly rely on CNN and fail to capture long-range dependencies,resulting in limited interpretability. This paper proposes aDegradation-Noise-Aware Unfolding Network (DNA-Net) that addresses theseissues. Firstly, DNA-Net models sparse noise, Gaussian noise, and explicitlyrepresent image prior using transformer. Then the model is unfolded into anend-to-end network, the hyperparameters within the model are estimated from thenoisy HSI and degradation model and utilizes them to control each iteration.Additionally, we introduce a novel U-Shaped Local-Non-local-SpectralTransformer (U-LNSA) that captures spectral correlation, local contents, andnon-local dependencies simultaneously. By integrating U-LNSA into DNA-Net, wepresent the first Transformer-based deep unfolding HSI denoising method.Experimental results show that DNA-Net outperforms state-of-the-art methods,and the modeling of noise distributions helps in cases with heavy noise.",,,,,http://arxiv.org/pdf/2305.04047v1
SST-ReversibleNet: Reversible-prior-based Spectral-Spatial Transformer  for Efficient Hyperspectral Image Reconstruction,"  Spectral image reconstruction is an important task in snapshot compressedimaging. This paper aims to propose a new end-to-end framework with iterativecapabilities similar to a deep unfolding network to improve reconstructionaccuracy, independent of optimization conditions, and to reduce the number ofparameters. A novel framework called the reversible-prior-based method isproposed. Inspired by the reversibility of the optical path, thereversible-prior-based framework projects the reconstructions back into themeasurement space, and then the residuals between the projected data and thereal measurements are fed into the network for iteration. The reconstructionsubnet in the network then learns the mapping of the residuals to the truevalues to improve reconstruction accuracy. In addition, a novelspectral-spatial transformer is proposed to account for the global correlationof spectral data in both spatial and spectral dimensions while balancingnetwork depth and computational complexity, in response to the shortcomings ofexisting transformer-based denoising modules that ignore spatial texturefeatures or learn local spatial features at the expense of global spatialfeatures. Extensive experiments show that our SST-ReversibleNet significantlyoutperforms state-of-the-art methods on simulated and real HSI datasets, whilerequiring lower computational and storage costs.https://github.com/caizeyu1992/SST",,,,,http://arxiv.org/pdf/2305.04054v1
UIT-OpenViIC: A Novel Benchmark for Evaluating Image Captioning in  Vietnamese,"  Image Captioning is one of the vision-language tasks that still interest theresearch community worldwide in the 2020s. MS-COCO Caption benchmark iscommonly used to evaluate the performance of advanced captioning models,although it was published in 2015. Recent captioning models trained on theMS-COCO Caption dataset only have good performance in language patterns ofEnglish; they do not have such good performance in contexts captured in Vietnamor fluently caption images using Vietnamese. To contribute to the low-resourcesresearch community as in Vietnam, we introduce a novel image captioning datasetin Vietnamese, the Open-domain Vietnamese Image Captioning dataset(UIT-OpenViIC). The introduced dataset includes complex scenes captured inVietnam and manually annotated by Vietnamese under strict rules andsupervision. In this paper, we present in more detail the dataset creationprocess. From preliminary analysis, we show that our dataset is challenging torecent state-of-the-art (SOTA) Transformer-based baselines, which performedwell on the MS COCO dataset. Then, the modest results prove that UIT-OpenViIChas room to grow, which can be one of the standard benchmarks in Vietnamese forthe research community to evaluate their captioning models. Furthermore, wepresent a CAMO approach that effectively enhances the image representationability by a multi-level encoder output fusion mechanism, which helps improvethe quality of generated captions compared to previous captioning models.",,,,,http://arxiv.org/pdf/2305.04166v1
Unlocking the Power of Open Set : A New Perspective for Open-set Noisy  Label Learning,"  Learning from noisy data has attracted much attention, where most methodsfocus on closed-set label noise. However, a more common scenario in the realworld is the presence of both open-set and closed-set noise. Existing methodstypically identify and handle these two types of label noise separately bydesigning a specific strategy for each type. However, in many real-worldscenarios, it would be challenging to identify open-set examples, especiallywhen the dataset has been severely corrupted. Unlike the previous works, weexplore how models behave when faced open-set examples, and find that a part ofopen-set examples gradually get integrated into certain known classes, which isbeneficial for the seperation among known classes. Motivated by the phenomenon,in this paper, we propose a novel two-step contrastive learning method calledCECL, which aims to deal with both types of label noise by exploiting theuseful information of open-set examples. Specifically, we incorporate someopen-set examples into closed-set classes to enhance performance while treatingothers as delimiters to improve representative ability. Extensive experimentson synthetic and real-world datasets with diverse label noise demonstrate thatCECL can outperform state-of-the-art methods.",,,,,http://arxiv.org/pdf/2305.04203v1
RATs-NAS: Redirection of Adjacent Trails on GCN for Neural Architecture  Search,"  Various hand-designed CNN architectures have been developed, such as VGG,ResNet, DenseNet, etc., and achieve State-of-the-Art (SoTA) levels on differenttasks. Neural Architecture Search (NAS) now focuses on automatically findingthe best CNN architecture to handle the above tasks. However, the verificationof a searched architecture is very time-consuming and makes predictor-basedmethods become an essential and important branch of NAS. Two commonly usedtechniques to build predictors are graph-convolution networks (GCN) andmultilayer perceptron (MLP). In this paper, we consider the difference betweenGCN and MLP on adjacent operation trails and then propose the RedirectedAdjacent Trails NAS (RATs-NAS) to quickly search for the desired neural networkarchitecture. The RATs-NAS consists of two components: the Redirected AdjacentTrails GCN (RATs-GCN) and the Predictor-based Search Space Sampling (P3S)module. RATs-GCN can change trails and their strengths to search for a betterneural network architecture. DSS can rapidly focus on tighter intervals ofFLOPs in the search space. Based on our observations on cell-based NAS, webelieve that architectures with similar FLOPs will perform similarly. Finally,the RATs-NAS consisting of RATs-GCN and DSS beats WeakNAS, Arch-Graph, andothers by a significant margin on three sub-datasets of NASBench-201.",,,,,http://arxiv.org/pdf/2305.04206v1
Segmentation and Vascular Vectorization for Coronary Artery by  Geometry-based Cascaded Neural Network,"  Segmentation of the coronary artery is an important task for the quantitativeanalysis of coronary computed tomography angiography (CCTA) images and is beingstimulated by the field of deep learning. However, the complex structures withtiny and narrow branches of the coronary artery bring it a great challenge.Coupled with the medical image limitations of low resolution and poor contrast,fragmentations of segmented vessels frequently occur in the prediction.Therefore, a geometry-based cascaded segmentation method is proposed for thecoronary artery, which has the following innovations: 1) Integrating geometricdeformation networks, we design a cascaded network for segmenting the coronaryartery and vectorizing results. The generated meshes of the coronary artery arecontinuous and accurate for twisted and sophisticated coronary arterystructures, without fragmentations. 2) Different from mesh annotationsgenerated by the traditional marching cube method from voxel-based labels, afiner vectorized mesh of the coronary artery is reconstructed with theregularized morphology. The novel mesh annotation benefits the geometry-basedsegmentation network, avoiding bifurcation adhesion and point cloud dispersionin intricate branches. 3) A dataset named CCA-200 is collected, consisting of200 CCTA images with coronary artery disease. The ground truths of 200 casesare coronary internal diameter annotations by professional radiologists.Extensive experiments verify our method on our collected dataset CCA-200 andpublic ASOCA dataset, with a Dice of 0.778 on CCA-200 and 0.895 on ASOCA,showing superior results. Especially, our geometry-based model generates anaccurate, intact and smooth coronary artery, devoid of any fragmentations ofsegmented vessels.",,,,,http://arxiv.org/pdf/2305.04208v1
"Design, Implementation and Evaluation of an External Pose-Tracking  System for Underwater Cameras","  In order to advance underwater computer vision and robotics from labenvironments and clear water scenarios to the deep dark ocean or murky coastalwaters, representative benchmarks and realistic datasets with ground truthinformation are required. In particular, determining the camera pose isessential for many underwater robotic or photogrammetric applications and knownground truth is mandatory to evaluate the performance of e.g., simultaneouslocalization and mapping approaches in such extreme environments. This paperpresents the conception, calibration and implementation of an externalreference system for determining the underwater camera pose in real-time. Theapproach, based on an HTC Vive tracking system in air, calculates theunderwater camera pose by fusing the poses of two controllers tracked above thewater surface of a tank. It is shown that the mean deviation of this approachto an optical marker based reference in air is less than 3 mm and 0.3{\\deg}.Finally, the usability of the system for underwater applications isdemonstrated.",,,,,http://arxiv.org/pdf/2305.04226v1
Instance-Variant Loss with Gaussian RBF Kernel for 3D Cross-modal  Retriveal,"  3D cross-modal retrieval is gaining attention in the multimedia community.Central to this topic is learning a joint embedding space to represent datafrom different modalities, such as images, 3D point clouds, and polygon meshes,to extract modality-invariant and discriminative features. Hence, theperformance of cross-modal retrieval methods heavily depends on therepresentational capacity of this embedding space. Existing methods treat allinstances equally, applying the same penalty strength to instances with varyingdegrees of difficulty, ignoring the differences between instances. This canresult in ambiguous convergence or local optima, severely compromising theseparability of the feature space. To address this limitation, we propose anInstance-Variant loss to assign different penalty strengths to differentinstances, improving the space separability. Specifically, we assign differentpenalty weights to instances positively related to their intra-class distance.Simultaneously, we reduce the cross-modal discrepancy between features bylearning a shared weight vector for the same class data from differentmodalities. By leveraging the Gaussian RBF kernel to evaluate samplesimilarity, we further propose an Intra-Class loss function that minimizes theintra-class distance among same-class instances. Extensive experiments on three3D cross-modal datasets show that our proposed method surpasses recentstate-of-the-art approaches.",Unknown,Unknown,未知,未知,http://arxiv.org/pdf/2305.04239v1
Dual Residual Attention Network for Image Denoising,"  In image denoising, deep convolutional neural networks (CNNs) can obtainfavorable performance on removing spatially invariant noise. However, many ofthese networks cannot perform well on removing the real noise (i.e. spatiallyvariant noise) generated during image acquisition or transmission, whichseverely sets back their application in practical image denoising tasks.Instead of continuously increasing the network depth, many researchers haverevealed that expanding the width of networks can also be a useful way toimprove model performance. It also has been verified that feature filtering canpromote the learning ability of the models. Therefore, in this paper, wepropose a novel Dual-branch Residual Attention Network (DRANet) for imagedenoising, which has both the merits of a wide model architecture andattention-guided feature learning. The proposed DRANet includes two differentparallel branches, which can capture complementary features to enhance thelearning ability of the model. We designed a new residual attention block (RAB)and a novel hybrid dilated residual attention block (HDRAB) for the upper andthe lower branches, respectively. The RAB and HDRAB can capture rich localfeatures through multiple skip connections between different convolutionallayers, and the unimportant features are dropped by the residual attentionmodules. Meanwhile, the long skip connections in each branch, and the globalfeature fusion between the two parallel branches can capture the globalfeatures as well. Moreover, the proposed DRANet uses downsampling operationsand dilated convolutions to increase the size of the receptive field, which canenable DRANet to capture more image context information. Extensive experimentsdemonstrate that compared with other state-of-the-art denoising methods, ourDRANet can produce competitive denoising performance both on synthetic andreal-world noise removal.",,,,,http://arxiv.org/pdf/2305.04269v1
RSC-VAE: Recoding Semantic Consistency Based VAE for One-Class Novelty  Detection,"  In recent years, there is an increasing interests in reconstruction basedgenerative models for image One-Class Novelty Detection, most of which onlyfocus on image-level information. While in this paper, we further exploit thelatent space of Variational Auto-encoder (VAE), a typical reconstruction basedmodel, and we innovatively divide it into three regions:Normal/Anomalous/Unknown-semantic-region. Based on this hypothesis, we proposea new VAE architecture, Recoding Semantic Consistency Based VAE (RSC-VAE),combining VAE with recoding mechanism and constraining the semantic consistencyof two encodings. We come up with three training modes of RSC-VAE: 1. One-ClassTraining Mode, alleviating False Positive problem of normal samples; 2.Distributionally-Shifted Training Mode, alleviating False Negative problem ofanomalous samples; 3. Extremely-Imbalanced Training Mode, introducing a smallnumber of anomalous samples for training to enhance the second mode. Theexperimental results on multiple datasets demonstrate that our mechanismachieves state-of-the-art performance in various baselines including VAE.",,,,,http://arxiv.org/pdf/2305.04275v1
Learning from synthetic data generated with GRADE,"  Recently, synthetic data generation and realistic rendering has advancedtasks like target tracking and human pose estimation. Simulations for mostrobotics applications are obtained in (semi)static environments, with specificsensors and low visual fidelity. To solve this, we present a fully customizableframework for generating realistic animated dynamic environments (GRADE) forrobotics research, first introduced in [1]. GRADE supports full simulationcontrol, ROS integration, realistic physics, while being in an engine thatproduces high visual fidelity images and ground truth data. We use GRADE togenerate a dataset focused on indoor dynamic scenes with people and flyingobjects. Using this, we evaluate the performance of YOLO and Mask R-CNN on thetasks of segmenting and detecting people. Our results provide evidence thatusing data generated with GRADE can improve the model performance when used fora pre-training step. We also show that, even training using only syntheticdata, can generalize well to real-world images in the same application domainsuch as the ones from the TUM-RGBD dataset. The code, results, trained models,and the generated data are provided as open-source athttps://eliabntt.github.io/grade-rr.",,,,,http://arxiv.org/pdf/2305.04282v1
HashCC: Lightweight Method to Improve the Quality of the Camera-less  NeRF Scene Generation,"  Neural Radiance Fields has become a prominent method of scene generation viaview synthesis. A critical requirement for the original algorithm to learnmeaningful scene representation is camera pose information for each image in adata set. Current approaches try to circumnavigate this assumption withmoderate success, by learning approximate camera positions alongside learningneural representations of a scene. This requires complicated camera models,causing a long and complicated training process, or results in a lack oftexture and sharp details in rendered scenes. In this work we introduce HashColor Correction (HashCC) -- a lightweight method for improving Neural RadianceFields rendered image quality, applicable also in situations where camerapositions for a given set of images are unknown.",,,,,http://arxiv.org/pdf/2305.04296v1
Poses as Queries: Image-to-LiDAR Map Localization with Transformers,"  High-precision vehicle localization with commercial setups is a crucialtechnique for high-level autonomous driving tasks. Localization with amonocular camera in LiDAR map is a newly emerged approach that achievespromising balance between cost and accuracy, but estimating pose by findingcorrespondences between such cross-modal sensor data is challenging, therebydamaging the localization accuracy. In this paper, we address the problem byproposing a novel Transformer-based neural network to register 2D images into3D LiDAR map in an end-to-end manner. Poses are implicitly represented ashigh-dimensional feature vectors called pose queries and can be iterativelyupdated by interacting with the retrieved relevant information from cross-modelfeatures using attention mechanism in a proposed POse Estimator Transformer(POET) module. Moreover, we apply a multiple hypotheses aggregation method thatestimates the final poses by performing parallel optimization on multiplerandomly initialized pose queries to reduce the network uncertainty.Comprehensive analysis and experimental results on public benchmark concludethat the proposed image-to-LiDAR map localization network could achievestate-of-the-art performances in challenging cross-modal localization tasks.",,,,,http://arxiv.org/pdf/2305.04298v1
Few Shot Learning for Medical Imaging: A Comparative Analysis of  Methodologies and Formal Mathematical Framework,"  Deep learning becomes an elevated context regarding disposing of many machinelearning tasks and has shown a breakthrough upliftment to extract features fromunstructured data. Though this flourishing context is developing in the medicalimage processing sector, scarcity of problem-dependent training data has becomea larger issue in the way of easy application of deep learning in the medicalsector. To unravel the confined data source, researchers have developed a modelthat can solve machine learning problems with fewer data called ``Few shotlearning"". Few hot learning algorithms determine to solve the data limitationproblems by extracting the characteristics from a small dataset throughclassification and segmentation methods. In the medical sector, there isfrequently a shortage of available datasets in respect of some confidentialdiseases. Therefore, Few shot learning gets the limelight in this data scarcitysector. In this chapter, the background and basic overview of a few shots oflearning is represented. Henceforth, the classification of few-shot learning isdescribed also. Even the paper shows a comparison of methodological approachesthat are applied in medical image analysis over time. The current advancementin the implementation of few-shot learning concerning medical imaging isillustrated. The future scope of this domain in the medical imaging sector isfurther described.",,,,,http://arxiv.org/pdf/2305.04401v1
Towards Accurate Human Motion Prediction via Iterative Refinement,"  Human motion prediction aims to forecast an upcoming pose sequence given apast human motion trajectory. To address the problem, in this work we proposeFreqMRN, a human motion prediction framework that takes into account both thekinematic structure of the human body and the temporal smoothness nature ofmotion. Specifically, FreqMRN first generates a fixed-size motion historysummary using a motion attention module, which helps avoid inaccurate motionpredictions due to excessively long motion inputs. Then, supervised by theproposed spatial-temporal-aware, velocity-aware and global-smoothness-awarelosses, FreqMRN iteratively refines the predicted motion though the proposedmotion refinement module, which converts motion representations back and forthbetween pose space and frequency space. We evaluate FreqMRN on several standardbenchmark datasets, including Human3.6M, AMASS and 3DPW. Experimental resultsdemonstrate that FreqMRN outperforms previous methods by large margins for bothshort-term and long-term predictions, while demonstrating superior robustness.",,,,,http://arxiv.org/pdf/2305.04443v1
Locally Attentional SDF Diffusion for Controllable 3D Shape Generation,"  Although the recent rapid evolution of 3D generative neural networks greatlyimproves 3D shape generation, it is still not convenient for ordinary users tocreate 3D shapes and control the local geometry of generated shapes. To addressthese challenges, we propose a diffusion-based 3D generation framework --locally attentional SDF diffusion, to model plausible 3D shapes, via 2D sketchimage input. Our method is built on a two-stage diffusion model. The firststage, named occupancy-diffusion, aims to generate a low-resolution occupancyfield to approximate the shape shell. The second stage, named SDF-diffusion,synthesizes a high-resolution signed distance field within the occupied voxelsdetermined by the first stage to extract fine geometry. Our model is empoweredby a novel view-aware local attention mechanism for image-conditioned shapegeneration, which takes advantage of 2D image patch features to guide 3D voxelfeature learning, greatly improving local controllability and modelgeneralizability. Through extensive experiments in sketch-conditioned andcategory-conditioned 3D shape generation tasks, we validate and demonstrate theability of our method to provide plausible and diverse 3D shapes, as well asits superior controllability and generalizability over existing work. Our codeand trained models are available athttps://zhengxinyang.github.io/projects/LAS-Diffusion.html",,,,,http://arxiv.org/pdf/2305.04461v1
Vision Lanauge Pre-training by Contrastive Learning with Cross-Modal  Similarity Regulation,"  Cross-modal contrastive learning in vision language pretraining (VLP) facesthe challenge of (partial) false negatives. In this paper, we study thisproblem from the perspective of Mutual Information (MI) optimization. It iscommon sense that InfoNCE loss used in contrastive learning will maximize thelower bound of MI between anchors and their positives, while we theoreticallyprove that MI involving negatives also matters when noises commonly exist.Guided by a more general lower bound form for optimization, we propose acontrastive learning strategy regulated by progressively refined cross-modalsimilarity, to more accurately optimize MI between an image/text anchor and itsnegative texts/images instead of improperly minimizing it. Our method performscompetitively on four downstream cross-modal tasks and systematically balancesthe beneficial and harmful effects of (partial) false negative samples undertheoretical guidance.",,,,,http://arxiv.org/pdf/2305.04474v1
IIITD-20K: Dense captioning for Text-Image ReID,"  Text-to-Image (T2I) ReID has attracted a lot of attention in the recent past.CUHK-PEDES, RSTPReid and ICFG-PEDES are the three available benchmarks toevaluate T2I ReID methods. RSTPReid and ICFG-PEDES comprise of identities fromMSMT17 but due to limited number of unique persons, the diversity is limited.On the other hand, CUHK-PEDES comprises of 13,003 identities but has relativelyshorter text description on average. Further, these datasets are captured in arestricted environment with limited number of cameras. In order to furtherdiversify the identities and provide dense captions, we propose a novel datasetcalled IIITD-20K. IIITD-20K comprises of 20,000 unique identities captured inthe wild and provides a rich dataset for text-to-image ReID. With a minimum of26 words for a description, each image is densely captioned. We furthersynthetically generate images and fine-grained captions using Stable-diffusionand BLIP models trained on our dataset. We perform elaborate experiments usingstate-of-art text-to-image ReID models and vision-language pre-trained modelsand present a comprehensive analysis of the dataset. Our experiments alsoreveal that synthetically generated data leads to a substantial performanceimprovement in both same dataset as well as cross dataset settings. Our datasetis available at https://bit.ly/3pkA3Rj.",,,,,http://arxiv.org/pdf/2305.04497v1
Pedestrian Behavior Maps for Safety Advisories: CHAMP Framework and  Real-World Data Analysis,"  It is critical for vehicles to prevent any collisions with pedestrians.Current methods for pedestrian collision prevention focus on integrating visualpedestrian detectors with Automatic Emergency Braking (AEB) systems which cantrigger warnings and apply brakes as a pedestrian enters a vehicle\'s path.Unfortunately, pedestrian-detection-based systems can be hindered in certainsituations such as night-time or when pedestrians are occluded. Our systemaddresses such issues using an online, map-based pedestrian detectionaggregation system where common pedestrian locations are learned after repeatedpasses of locations. Using a carefully collected and annotated dataset in LaJolla, CA, we demonstrate the system\'s ability to learn pedestrian zones andgenerate advisory notices when a vehicle is approaching a pedestrian despitechallenges like dark lighting or pedestrian occlusion. Using the number ofcorrect advisories, false advisories, and missed advisories to define precisionand recall performance metrics, we evaluate our system and discuss futurepositive effects with further data collection. We have made our code availableat https://github.com/s7desai/ped-mapping, and a video demonstration of theCHAMP system at https://youtu.be/dxeCrS_Gpkw.",,,,,http://arxiv.org/pdf/2305.04506v1
Multi-Temporal Lip-Audio Memory for Visual Speech Recognition,"  Visual Speech Recognition (VSR) is a task to predict a sentence or word fromlip movements. Some works have been recently presented which use audio signalsto supplement visual information. However, existing methods utilize onlylimited information such as phoneme-level features and soft labels of AutomaticSpeech Recognition (ASR) networks. In this paper, we present a Multi-TemporalLip-Audio Memory (MTLAM) that makes the best use of audio signals to complementinsufficient information of lip movements. The proposed method is mainlycomposed of two parts: 1) MTLAM saves multi-temporal audio features producedfrom short- and long-term audio signals, and the MTLAM memorizes avisual-to-audio mapping to load stored multi-temporal audio features fromvisual features at the inference phase. 2) We design an audio temporal model toproduce multi-temporal audio features capturing the context of neighboringwords. In addition, to construct effective visual-to-audio mapping, the audiotemporal models can generate audio features time-aligned with visual features.Through extensive experiments, we validate the effectiveness of the MTLAMachieving state-of-the-art performances on two public VSR datasets.",,,,,http://arxiv.org/pdf/2305.04542v1
SwinDocSegmenter: An End-to-End Unified Domain Adaptive Transformer for  Document Instance Segmentation,"  Instance-level segmentation of documents consists in assigning a class-awareand instance-aware label to each pixel of the image. It is a key step indocument parsing for their understanding. In this paper, we present a unifiedtransformer encoder-decoder architecture for en-to-end instance segmentation ofcomplex layouts in document images. The method adapts a contrastive trainingwith a mixed query selection for anchor initialization in the decoder. Lateron, it performs a dot product between the obtained query embeddings and thepixel embedding map (coming from the encoder) for semantic reasoning. Extensiveexperimentation on competitive benchmarks like PubLayNet, PRIMA, HistoricalJapanese (HJ), and TableBank demonstrate that our model with SwinL backboneachieves better segmentation performance than the existing state-of-the-artapproaches with the average precision of \\textbf{93.72}, \\textbf{54.39},\\textbf{84.65} and \\textbf{98.04} respectively under one billion parameters.The code is made publicly available at:\\href{https://github.com/ayanban011/SwinDocSegmenter}{github.com/ayanban011/SwinDocSegmenter}",,,,,http://arxiv.org/pdf/2305.04609v1
Riesz networks: scale invariant neural networks in a single forward pass,"  Scale invariance of an algorithm refers to its ability to treat objectsequally independently of their size. For neural networks, scale invariance istypically achieved by data augmentation. However, when presented with a scalefar outside the range covered by the training set, neural networks may fail togeneralize.  Here, we introduce the Riesz network, a novel scale invariant neural network.Instead of standard 2d or 3d convolutions for combining spatial information,the Riesz network is based on the Riesz transform which is a scale equivariantoperation. As a consequence, this network naturally generalizes to unseen oreven arbitrary scales in a single forward pass. As an application example, weconsider detecting and segmenting cracks in tomographic images of concrete. Inthis context, \'scale\' refers to the crack thickness which may vary stronglyeven within the same sample. To prove its scale invariance, the Riesz networkis trained on one fixed crack width. We then validate its performance insegmenting simulated and real tomographic images featuring a wide range ofcrack widths. An additional experiment is carried out on the MNIST Large Scaledata set.",,,,,http://arxiv.org/pdf/2305.04665v1
ElasticHash: Semantic Image Similarity Search by Deep Hashing with  Elasticsearch,"  We present ElasticHash, a novel approach for high-quality, efficient, andlarge-scale semantic image similarity search. It is based on a deep hashingmodel to learn hash codes for fine-grained image similarity search in naturalimages and a two-stage method for efficiently searching binary hash codes usingElasticsearch (ES). In the first stage, a coarse search based on short hashcodes is performed using multi-index hashing and ES terms lookup of neighboringhash codes. In the second stage, the list of results is re-ranked by computingthe Hamming distance on long hash codes. We evaluate the retrieval performanceof \\textit{ElasticHash} for more than 120,000 query images on about 6.9 milliondatabase images of the OpenImages data set. The results show that our approachachieves high-quality retrieval results and low search latencies.",,,,,http://arxiv.org/pdf/2305.04710v1
Strategy for Rapid Diabetic Retinopathy Exposure Based on Enhanced  Feature Extraction Processing,"  In the modern world, one of the most severe eye infections brought on bydiabetes is known as diabetic retinopathy, which will result in retinal damage,and, thus, lead to blindness. Diabetic retinopathy can be well treated withearly diagnosis. Retinal fundus images of humans are used to screen for lesionsin the retina. However, detecting DR in the early stages is challenging due tothe minimal symptoms. Furthermore, the occurrence of diseases linked tovascular anomalies brought on by DR aids in diagnosing the condition.Nevertheless, the resources required for manually identifying the lesions arehigh. Similarly, training for Convolutional Neural Networks is moretime-consuming. This proposed research aims to improve diabetic retinopathydiagnosis by developing an enhanced deep learning model for timely DRidentification that is potentially more accurate than existing CNN-basedmodels. The proposed model will detect various lesions from retinal images inthe early stages. First, characteristics are retrieved from the retinal funduspicture and put into the EDLM for classification. For dimensionality reduction,EDLM is used. Additionally, the classification and feature extraction processesare optimized using the stochastic gradient descent optimizer. The EDLMeffectiveness is assessed on the KAG GLE dataset with 3459 retinal images, andresults are compared over VGG16, VGG19, RESNET18, RESNET34, and RESNET50.",,,,,http://arxiv.org/pdf/2305.04724v1
Controllable Light Diffusion for Portraits,"  We introduce light diffusion, a novel method to improve lighting inportraits, softening harsh shadows and specular highlights while preservingoverall scene illumination. Inspired by professional photographers\' diffusersand scrims, our method softens lighting given only a single portrait photo.Previous portrait relighting approaches focus on changing the entire lightingenvironment, removing shadows (ignoring strong specular highlights), orremoving shading entirely. In contrast, we propose a learning based method thatallows us to control the amount of light diffusion and apply it on in-the-wildportraits. Additionally, we design a method to synthetically generate plausibleexternal shadows with sub-surface scattering effects while conforming to theshape of the subject\'s face. Finally, we show how our approach can increase therobustness of higher level vision applications, such as albedo estimation,geometry estimation and semantic segmentation.",本文提出了一种肖像照明的新方法——光扩散，可以使照片中的阴影和高光变得柔和，同时保持整体场景的照明。通过训练学习，可以控制光的扩散程度并应用于野外肖像。此外，作者还提出了一种方法，可以合成可能的外部阴影，并可以估计未删除的自发光。最后，作者展示了该方法如何提高更高级别的视觉应用程序的鲁棒性，例如反照率估计、几何估计和语义分割。,可控肖像照明（Computer Vision领域）,,,http://arxiv.org/pdf/2305.04745v1
Toeplitz Neural Network for Sequence Modeling,"  Sequence modeling has important applications in natural language processingand computer vision. Recently, the transformer-based models have shown strongperformance on various sequence modeling tasks, which rely on attention tocapture pairwise token relations, and position embedding to inject positionalinformation. While showing good performance, the transformer models areinefficient to scale to long input sequences, mainly due to the quadraticspace-time complexity of attention. To overcome this inefficiency, we proposeto model sequences with a relative position encoded Toeplitz matrix and use aToeplitz matrix-vector production trick to reduce the space-time complexity ofthe sequence modeling to log linear. A lightweight sub-network called relativeposition encoder is proposed to generate relative position coefficients with afixed budget of parameters, enabling the proposed Toeplitz neural network todeal with varying sequence lengths. In addition, despite being trained on512-token sequences, our model can extrapolate input sequence length up to 14Ktokens in inference with consistent performance. Extensive experiments onautoregressive and bidirectional language modeling, image modeling, and thechallenging Long-Range Arena benchmark show that our method achieves betterperformance than its competitors in most downstream tasks while beingsignificantly faster. The code is available athttps://github.com/OpenNLPLab/Tnn.",,,,,http://arxiv.org/pdf/2305.04749v1
MultiModal-GPT: A Vision and Language Model for Dialogue with Humans,"  We present a vision and language model named MultiModal-GPT to conductmulti-round dialogue with humans. MultiModal-GPT can follow variousinstructions from humans, such as generating a detailed caption, counting thenumber of interested objects, and answering general questions from users.MultiModal-GPT is parameter-efficiently fine-tuned from OpenFlamingo, withLow-rank Adapter (LoRA) added both in the cross-attention part and theself-attention part of the language model. We first construct instructiontemplates with vision and language data for multi-modality instruction tuningto make the model understand and follow human instructions. We find the qualityof training data is vital for the dialogue performance, where few datacontaining short answers can lead the model to respond shortly to anyinstructions. To further enhance the ability to chat with humans of theMultiModal-GPT, we utilize language-only instruction-following data to trainthe MultiModal-GPT jointly. The joint training of language-only andvisual-language instructions with the \\emph{same} instruction templateeffectively improves dialogue performance. Various demos show the ability ofcontinuous dialogue of MultiModal-GPT with humans. Code and demo are athttps://github.com/open-mmlab/Multimodal-GPT",,,,,http://arxiv.org/pdf/2305.04790v1
Learning to Evaluate the Artness of AI-generated Images,"  Assessing the artness of AI-generated images continues to be a challengewithin the realm of image generation. Most existing metrics cannot be used toperform instance-level and reference-free artness evaluation. This paperpresents ArtScore, a metric designed to evaluate the degree to which an imageresembles authentic artworks by artists (or conversely photographs), therebyoffering a novel approach to artness assessment. We first blend pre-trainedmodels for photo and artwork generation, resulting in a series of mixed models.Subsequently, we utilize these mixed models to generate images exhibitingvarying degrees of artness with pseudo-annotations. Each photorealistic imagehas a corresponding artistic counterpart and a series of interpolated imagesthat range from realistic to artistic. This dataset is then employed to train aneural network that learns to estimate quantized artness levels of arbitraryimages. Extensive experiments reveal that the artness levels predicted byArtScore align more closely with human artistic evaluation than existingevaluation metrics, such as Gram loss and ArtFID.",,,,,http://arxiv.org/pdf/2305.04923v1
Multi-Label Classification of Thoracic Diseases using Dense  Convolutional Network on Chest Radiographs,"  Traditional methods of identifying pathologies in X-ray images rely heavilyon skilled human interpretation and are often time-consuming. The advent ofdeep learning techniques has enabled the development of automated diseasediagnosis systems, but the performance of such systems is dependent on thequality of the model and the level of interpretability it provides. In thispaper, we propose a multi-label disease diagnosis model for chest X-rays usinga dense convolutional neural network (DenseNet) and model interpretabilityusing GRADCAM. We trained our model using frontal X-rays and evaluated itsperformance using various quantitative metrics, including the area under thereceiver operating characteristic curve (AUC). Our proposed model achieved thehighest AUC score of 0.896 for the condition Cardiomegaly with an accuracy of0.826, while the lowest AUC score was obtained for Nodule, at 0.655 with anaccuracy of 0.66. To promote model interpretability and build trust in decisionmaking, we generated heatmaps on X-rays to visualize the regions where themodel paid attention to make certain predictions. Additionally, we estimatedthe uncertainty in model predictions by presenting the confidence interval ofour measurements. Our proposed automated disease diagnosis model obtained highperformance metrics in multi-label disease diagnosis tasks and providedvisualization of model predictions for model interpretability.",,,,,http://arxiv.org/pdf/2202.03583v2
Delving into the Openness of CLIP,"  Contrastive Language-Image Pre-training (CLIP) formulates imageclassification as an image-to-text matching task, i.e., matching images to thecorresponding natural language descriptions instead of discrete category IDs.This allows for open-vocabulary visual recognition, where the model canrecognize images from an open class set (also known as an open vocabulary) in azero-shot manner. However, evaluating the openness of CLIP-like models ischallenging, as the models are open to arbitrary vocabulary in theory, buttheir accuracy varies in practice. To address this, we resort to an incrementalperspective to assess the openness through vocabulary expansions, and defineextensibility to measure a model\'s ability to handle novel classes. Ourevaluation shows that CLIP-like models are not truly open, and theirperformance deteriorates as the vocabulary expands. We further dissect thefeature space of CLIP from the perspectives of representation alignment anduniformity. Our investigation reveals that the overestimation of openness isdue to confusion among competing text features, rather than a failure tocapture the similarity between image features and text features of novelclasses. We hope that our investigation and analysis will facilitate futureresearch on the CLIP openness issue.",,,,,http://arxiv.org/pdf/2206.01986v3
An Image Processing approach to identify solar plages observed at 393.37  nm by the Kodaikanal Solar Observatory,"  Solar plages, which are bright regions on the Sun\'s surface, are an importantindicator of solar activity. In this study, we propose an automated algorithmfor identifying solar plages in Ca K wavelength solar data obtained from theKodaikanal Solar Observatory. The algorithm successfully annotates all visuallyidentifiable plages in an image and outputs the corresponding calculated plageindex. We perform a time series analysis of the plage index (rolling mean)across multiple solar cycles to test the algorithm\'s reliability androbustness. The results show a strong correlation between the calculated plageindex and those reported in a previous study. The correlation coefficientsobtained for all the solar cycles are higher than 0.90, indicating thereliability of the model. We also suggest that adjusting the hyperparametersappropriately for a specific image using our web-based app can increase themodel\'s efficiency. The algorithm has been deployed on the Streamlit CommunityCloud platform, where users can upload images and customize the hyperparametersfor desired results. The input data used in this study is freely available fromthe KSO data archive, and the code and the generated data are publiclyavailable on our GitHub repository. Our proposed algorithm provides anefficient and reliable method for identifying solar plages, which can aid thestudy of solar activity and its impact on the Earth\'s climate, technology, andspace weather.",,,,,http://arxiv.org/pdf/2209.10631v3
"CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features  for a Disentangled, Interpretable, and Controllable Text-Guided Face  Manipulation","  Recently introduced Contrastive Language-Image Pre-Training (CLIP) bridgesimages and text by embedding them into a joint latent space. This opens thedoor to ample literature that aims to manipulate an input image by providing atextual explanation. However, due to the discrepancy between image and textembeddings in the joint space, using text embeddings as the optimization targetoften introduces undesired artifacts in the resulting images. Disentanglement,interpretability, and controllability are also hard to guarantee formanipulation. To alleviate these problems, we propose to define corpussubspaces spanned by relevant prompts to capture specific imagecharacteristics. We introduce CLIP Projection-Augmentation Embedding (PAE) asan optimization target to improve the performance of text-guided imagemanipulation. Our method is a simple and general paradigm that can be easilycomputed and adapted, and smoothly incorporated into any CLIP-based imagemanipulation algorithm. To demonstrate the effectiveness of our method, weconduct several theoretical and empirical studies. As a case study, we utilizethe method for text-guided semantic face editing. We quantitatively andqualitatively demonstrate that PAE facilitates a more disentangled,interpretable, and controllable image manipulation with state-of-the-artquality and accuracy.",,,,,http://arxiv.org/pdf/2210.03919v4
MMRNet: Improving Reliability for Multimodal Object Detection and  Segmentation for Bin Picking via Multimodal Redundancy,"  Recently, there has been tremendous interest in industry 4.0 infrastructureto address labor shortages in global supply chains. Deploying artificialintelligence-enabled robotic bin picking systems in real world has becomeparticularly important for reducing stress and physical demands of workerswhile increasing speed and efficiency of warehouses. To this end, artificialintelligence-enabled robotic bin picking systems may be used to automate orderpicking, but with the risk of causing expensive damage during an abnormal eventsuch as sensor failure. As such, reliability becomes a critical factor fortranslating artificial intelligence research to real world applications andproducts. In this paper, we propose a reliable object detection andsegmentation system with MultiModal Redundancy (MMRNet) for tackling objectdetection and segmentation for robotic bin picking using data from differentmodalities. This is the first system that introduces the concept of multimodalredundancy to address sensor failure issues during deployment. In particular,we realize the multimodal redundancy framework with a gate fusion module anddynamic ensemble learning. Finally, we present a new label-free multi-modalconsistency (MC) score that utilizes the output from all modalities to measurethe overall system output reliability and uncertainty. Through experiments, wedemonstrate that in an event of missing modality, our system provides a muchmore reliable performance compared to baseline models. We also demonstrate thatour MC score is a more reliability indicator for outputs during inference timecompared to the model generated confidence scores that are oftenover-confident.",,,,,http://arxiv.org/pdf/2210.10842v3
Data Models for Dataset Drift Controls in Machine Learning With Optical  Images,"  Camera images are ubiquitous in machine learning research. They also play acentral role in the delivery of important services spanning medicine andenvironmental surveying. However, the application of machine learning models inthese domains has been limited because of robustness concerns. A primaryfailure mode are performance drops due to differences between the training anddeployment data. While there are methods to prospectively validate therobustness of machine learning models to such dataset drifts, existingapproaches do not account for explicit models of the primary object ofinterest: the data. This limits our ability to study and understand therelationship between data generation and downstream machine learning modelperformance in a physically accurate manner. In this study, we demonstrate howto overcome this limitation by pairing traditional machine learning withphysical optics to obtain explicit and differentiable data models. Wedemonstrate how such data models can be constructed for image data and used tocontrol downstream machine learning model performance related to dataset drift.The findings are distilled into three applications. First, drift synthesisenables the controlled generation of physically faithful drift test cases topower model selection and targeted generalization. Second, the gradientconnection between machine learning task model and data model allows advanced,precise tolerancing of task model sensitivity to changes in the datageneration. These drift forensics can be used to precisely specify theacceptable data environments in which a task model may be run. Third, driftoptimization opens up the possibility to create drifts that can help the taskmodel learn better faster, effectively optimizing the data generating processitself. A guide to access the open code and datasets is available athttps://github.com/aiaudit-org/raw2logit.",,,,,http://arxiv.org/pdf/2211.02578v3
Twin-S: A Digital Twin for Skull-base Surgery,"  Purpose: Digital twins are virtual interactive models of the real world,exhibiting identical behavior and properties. In surgical applications,computational analysis from digital twins can be used, for example, to enhancesituational awareness. Methods: We present a digital twin framework forskull-base surgeries, named Twin-S, which can be integrated within variousimage-guided interventions seamlessly. Twin-S combines high-precision opticaltracking and real-time simulation. We rely on rigorous calibration routines toensure that the digital twin representation precisely mimics all real-worldprocesses. Twin-S models and tracks the critical components of skull-basesurgery, including the surgical tool, patient anatomy, and surgical camera.Significantly, Twin-S updates and reflects real-world drilling of theanatomical model in frame rate. Results: We extensively evaluate the accuracyof Twin-S, which achieves an average 1.39 mm error during the drilling process.We further illustrate how segmentation masks derived from the continuouslyupdated digital twin can augment the surgical microscope view in a mixedreality setting, where bone requiring ablation is highlighted to providesurgeons additional situational awareness. Conclusion: We present Twin-S, adigital twin environment for skull-base surgery. Twin-S tracks and updates thevirtual model in real-time given measurements from modern trackingtechnologies. Future research on complementing optical tracking withhigher-precision vision-based approaches may further increase the accuracy ofTwin-S.",,,,,http://arxiv.org/pdf/2211.11863v2
Diffusion-SDF: Text-to-Shape via Voxelized Diffusion,"  With the rising industrial attention to 3D virtual modeling technology,generating novel 3D content based on specified conditions (e.g. text) hasbecome a hot issue. In this paper, we propose a new generative 3D modelingframework called Diffusion-SDF for the challenging task of text-to-shapesynthesis. Previous approaches lack flexibility in both 3D data representationand shape generation, thereby failing to generate highly diversified 3D shapesconforming to the given text descriptions. To address this, we propose a SDFautoencoder together with the Voxelized Diffusion model to learn and generaterepresentations for voxelized signed distance fields (SDFs) of 3D shapes.Specifically, we design a novel UinU-Net architecture that implants alocal-focused inner network inside the standard U-Net architecture, whichenables better reconstruction of patch-independent SDF representations. Weextend our approach to further text-to-shape tasks including text-conditionedshape completion and manipulation. Experimental results show that Diffusion-SDFgenerates both higher quality and more diversified 3D shapes that conform wellto given text descriptions when compared to previous approaches. Code isavailable at: https://github.com/ttlmh/Diffusion-SDF",,,,,http://arxiv.org/pdf/2212.03293v2
Self-Attention Amortized Distributional Projection Optimization for  Sliced Wasserstein Point-Cloud Reconstruction,"  Max sliced Wasserstein (Max-SW) distance has been widely known as a solutionfor less discriminative projections of sliced Wasserstein (SW) distance. Inapplications that have various independent pairs of probability measures,amortized projection optimization is utilized to predict the ``max"" projectingdirections given two input measures instead of using projected gradient ascentmultiple times. Despite being efficient, Max-SW and its amortized versioncannot guarantee metricity property due to the sub-optimality of the projectedgradient ascent and the amortization gap. Therefore, we propose to replaceMax-SW with distributional sliced Wasserstein distance with von Mises-Fisher(vMF) projecting distribution (v-DSW). Since v-DSW is a metric with anynon-degenerate vMF distribution, its amortized version can guarantee themetricity when performing amortization. Furthermore, current amortized modelsare not permutation invariant and symmetric. To address the issue, we designamortized models based on self-attention architecture. In particular, we adoptefficient self-attention architectures to make the computation linear in thenumber of supports. With the two improvements, we derive self-attentionamortized distributional projection optimization and show its appealingperformance in point-cloud reconstruction and its downstream applications.",,,,,http://arxiv.org/pdf/2301.04791v2
GlyphDiffusion: Text Generation as Image Generation,"  Diffusion models have become a new generative paradigm for text generation.Considering the discrete categorical nature of text, in this paper, we proposeGlyphDiffusion, a novel diffusion approach for text generation via text-guidedimage generation. Our key idea is to render the target text as a glyph imagecontaining visual language content. In this way, conditional text generationcan be cast as a glyph image generation task, and it is then natural to applycontinuous diffusion models to discrete texts. Specially, we utilize a cascadedarchitecture (ie a base and a super-resolution diffusion model) to generatehigh-fidelity glyph images, conditioned on the input text. Furthermore, wedesign a text grounding module to transform and refine the visual languagecontent from generated glyph images into the final texts. In experiments overfour conditional text generation tasks and two classes of metrics (ie qualityand diversity), GlyphDiffusion can achieve comparable or even better resultsthan several baselines, including pretrained language models. Our model alsomakes significant improvements compared to the recent diffusion model.",,,,,http://arxiv.org/pdf/2304.12519v2
From Association to Generation: Text-only Captioning by Unsupervised  Cross-modal Mapping,"  With the development of Vision-Language Pre-training Models (VLPMs)represented by CLIP and ALIGN, significant breakthroughs have been achieved forassociation-based visual tasks such as image classification and image-textretrieval by the zero-shot capability of CLIP without fine-tuning. However,CLIP is hard to apply to generation-based tasks. This is due to the lack ofdecoder architecture and pre-training tasks for generation. Although previousworks have created generation capacity for CLIP through additional languagemodels, a modality gap between the CLIP representations of different modalitiesand the inability of CLIP to model the offset of this gap, which fails theconcept to transfer across modalities. To solve the problem, we try to mapimages/videos to the language modality and generate captions from the languagemodality. In this paper, we propose the K-nearest-neighbor Cross-modalityMapping (Knight), a zero-shot method from association to generation. Withtext-only unsupervised training, Knight achieves State-of-the-Art performancein zero-shot methods for image captioning and video captioning. Our code isavailable at https://github.com/junyangwang0410/Knight.",,,,,http://arxiv.org/pdf/2304.13273v3
Extraction of volumetric indices from echocardiography: which deep  learning solution for clinical use?,"  Deep learning-based methods have spearheaded the automatic analysis ofechocardiographic images, taking advantage of the publication of multiple openaccess datasets annotated by experts (CAMUS being one of the largest publicdatabases). However, these models are still considered unreliable by cliniciansdue to unresolved issues concerning i) the temporal consistency of theirpredictions, and ii) their ability to generalize across datasets. In thiscontext, we propose a comprehensive comparison between the current bestperforming methods in medical/echocardiographic image segmentation, with aparticular focus on temporal consistency and cross-dataset aspects. Weintroduce a new private dataset, named CARDINAL, of apical two-chamber andapical four-chamber sequences, with reference segmentation over the fullcardiac cycle. We show that the proposed 3D nnU-Net outperforms alternative 2Dand recurrent segmentation methods. We also report that the best models trainedon CARDINAL, when tested on CAMUS without any fine-tuning, still manage toperform competitively with respect to prior methods. Overall, the experimentalresults suggest that with sufficient training data, 3D nnU-Net could become thefirst automated tool to finally meet the standards of an everyday clinicaldevice.",,,,,http://arxiv.org/pdf/2305.01997v2
Fairness in Image Search: A Study of Occupational Stereotyping in Image  Retrieval and its Debiasing,"  Multi-modal search engines have experienced significant growth and widespreaduse in recent years, making them the second most common internet use. Whilesearch engine systems offer a range of services, the image search field hasrecently become a focal point in the information retrieval community, as theadage goes, ""a picture is worth a thousand words"". Although popular searchengines like Google excel at image search accuracy and agility, there is anongoing debate over whether their search results can be biased in terms ofgender, language, demographics, socio-cultural aspects, and stereotypes. Thispotential for bias can have a significant impact on individuals\' perceptionsand influence their perspectives.  In this paper, we present our study on bias and fairness in web search, witha focus on keyword-based image search. We first discuss several kinds of biasesthat exist in search systems and why it is important to mitigate them. Wenarrow down our study to assessing and mitigating occupational stereotypes inimage search, which is a prevalent fairness issue in image retrieval. For theassessment of stereotypes, we take gender as an indicator. We explore variousopen-source and proprietary APIs for gender identification from images. Withthese, we examine the extent of gender bias in top-tanked image search resultsobtained for several occupational keywords. To mitigate the bias, we thenpropose a fairness-aware re-ranking algorithm that optimizes (a) relevance ofthe search result with the keyword and (b) fairness w.r.t genders identified.We experiment on 100 top-ranked images obtained for 10 occupational keywordsand consider random re-ranking and re-ranking based on relevance as baselines.Our experimental results show that the fairness-aware re-ranking algorithmproduces rankings with better fairness scores and competitive relevance scoresthan the baselines.",,,,,http://arxiv.org/pdf/2305.03881v1
NL-CS Net: Deep Learning with Non-Local Prior for Image Compressive  Sensing,"  Deep learning has been applied to compressive sensing (CS) of imagessuccessfully in recent years. However, existing network-based methods are oftentrained as the black box, in which the lack of prior knowledge is often thebottleneck for further performance improvement. To overcome this drawback, thispaper proposes a novel CS method using non-local prior which combines theinterpretability of the traditional optimization methods with the speed ofnetwork-based methods, called NL-CS Net. We unroll each phase from iteration ofthe augmented Lagrangian method solving non-local and sparse regularizedoptimization problem by a network. NL-CS Net is composed of the up-samplingmodule and the recovery module. In the up-sampling module, we use learnableup-sampling matrix instead of a predefined one. In the recovery module,patch-wise non-local network is employed to capture long-range featurecorrespondences. Important parameters involved (e.g. sampling matrix, nonlineartransforms, shrinkage thresholds, step size, $etc.$) are learned end-to-end,rather than hand-crafted. Furthermore, to facilitate practical implementation,orthogonal and binary constraints on the sampling matrix are simultaneouslyadopted. Extensive experiments on natural images and magnetic resonance imaging(MRI) demonstrate that the proposed method outperforms the state-of-the-artmethods while maintaining great interpretability and speed.",,,,,http://arxiv.org/pdf/2305.03899v1
HateMM: A Multi-Modal Dataset for Hate Video Classification,"  Hate speech has become one of the most significant issues in modern society,having implications in both the online and the offline world. Due to this, hatespeech research has recently gained a lot of traction. However, most of thework has primarily focused on text media with relatively little work on imagesand even lesser on videos. Thus, early stage automated video moderationtechniques are needed to handle the videos that are being uploaded to keep theplatform safe and healthy. With a view to detect and remove hateful contentfrom the video sharing platforms, our work focuses on hate video detectionusing multi-modalities. To this end, we curate ~43 hours of videos fromBitChute and manually annotate them as hate or non-hate, along with the framespans which could explain the labelling decision. To collect the relevantvideos we harnessed search keywords from hate lexicons. We observe various cuesin images and audio of hateful videos. Further, we build deep learningmulti-modal models to classify the hate videos and observe that using all themodalities of the videos improves the overall hate speech detection performance(accuracy=0.798, macro F1-score=0.790) by ~5.7% compared to the best uni-modalmodel in terms of macro F1 score. In summary, our work takes the first steptoward understanding and modeling hateful videos on video hosting platformssuch as BitChute.",,,,,http://arxiv.org/pdf/2305.03915v1
Beyond the Model: Data Pre-processing Attack to Deep Learning Models in  Android Apps,"  The increasing popularity of deep learning (DL) models and the advantages ofcomputing, including low latency and bandwidth savings on smartphones, have ledto the emergence of intelligent mobile applications, also known as DL apps, inrecent years. However, this technological development has also given rise toseveral security concerns, including adversarial examples, model stealing, anddata poisoning issues. Existing works on attacks and countermeasures foron-device DL models have primarily focused on the models themselves. However,scant attention has been paid to the impact of data processing disturbance onthe model inference. This knowledge disparity highlights the need foradditional research to fully comprehend and address security issues related todata processing for on-device models. In this paper, we introduce a dataprocessing-based attacks against real-world DL apps. In particular, our attackcould influence the performance and latency of the model without affecting theoperation of a DL app. To demonstrate the effectiveness of our attack, we carryout an empirical study on 517 real-world DL apps collected from Google Play.Among 320 apps utilizing MLkit, we find that 81.56\\% of them can besuccessfully attacked.  The results emphasize the importance of DL app developers being aware of andtaking actions to secure on-device models from the perspective of dataprocessing.",,,,,http://arxiv.org/pdf/2305.03963v1
Gradient Leakage Defense with Key-Lock Module for Federated Learning,"  Federated Learning (FL) is a widely adopted privacy-preserving machinelearning approach where private data remains local, enabling securecomputations and the exchange of local model gradients between local clientsand third-party parameter servers. However, recent findings reveal that privacymay be compromised and sensitive information potentially recovered from sharedgradients. In this study, we offer detailed analysis and a novel perspective onunderstanding the gradient leakage problem. These theoretical works lead to anew gradient leakage defense technique that secures arbitrary modelarchitectures using a private key-lock module. Only the locked gradient istransmitted to the parameter server for global model aggregation. Our proposedlearning method is resistant to gradient leakage attacks, and the key-lockmodule is designed and trained to ensure that, without the private informationof the key-lock module: a) reconstructing private training data from the sharedgradient is infeasible; and b) the global model\'s inference performance issignificantly compromised. We discuss the theoretical underpinnings of whygradients can leak private information and provide theoretical proof of ourmethod\'s effectiveness. We conducted extensive empirical evaluations with atotal of forty-four models on several popular benchmarks, demonstrating therobustness of our proposed approach in both maintaining model performance anddefending against gradient leakage attacks.",,,,,http://arxiv.org/pdf/2305.04095v1
Transformer-Based Hierarchical Clustering for Brain Network Analysis,"  Brain networks, graphical models such as those constructed from MRI, havebeen widely used in pathological prediction and analysis of brain functions.Within the complex brain system, differences in neuronal connection strengthsparcellate the brain into various functional modules (network communities),which are critical for brain analysis. However, identifying such communitieswithin the brain has been a nontrivial issue due to the complexity of neuronalinteractions. In this work, we propose a novel interpretable transformer-basedmodel for joint hierarchical cluster identification and brain networkclassification. Extensive experimental results on real-world brain networkdatasets show that with the help of hierarchical clustering, the model achievesincreased accuracy and reduced runtime complexity while providing plausibleinsight into the functional organization of brain regions. The implementationis available at https://github.com/DDVD233/THC.",,,,,http://arxiv.org/pdf/2305.04142v1
X-LLM: Bootstrapping Advanced Large Language Models by Treating  Multi-Modalities as Foreign Languages,"  Large language models (LLMs) have demonstrated remarkable language abilities.GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilitiesbeyond previous visual language models. We attribute this to the use of moreadvanced LLMs compared with previous multimodal models. Unfortunately, themodel architecture and training strategies of GPT-4 are unknown. To endow LLMswith multimodal capabilities, we propose X-LLM, which converts Multi-modalities(images, speech, videos) into foreign languages using X2L interfaces and inputsthem into a large Language model (ChatGLM). Specifically, X-LLM aligns multiplefrozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X\'\'denotes multi-modalities such as image, speech, and videos, and ``L\'\' denoteslanguages. X-LLM\'s training consists of three stages: (1) Converting MultimodalInformation: The first stage trains each X2L interface to align with itsrespective single-modal encoder separately to convert multimodal informationinto languages. (2) Aligning X2L representations with the LLM: single-modalencoders are aligned with the LLM through X2L interfaces independently. (3)Integrating multiple modalities: all single-modal encoders are aligned with theLLM through X2L interfaces to integrate multimodal capabilities into the LLM.Our experiments show that X-LLM demonstrates impressive multimodel chatabilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseenimages/instructions, and yields a 84.5\\% relative score compared with GPT-4 ona synthetic multimodal instruction-following dataset. And we also conductquantitative tests on using LLM for ASR and multimodal ASR, hoping to promotethe era of LLM-based speech recognition.",,,,,http://arxiv.org/pdf/2305.04160v1
Text-to-Image Diffusion Models can be Easily Backdoored through  Multimodal Data Poisoning,"  With the help of conditioning mechanisms, the state-of-the-art diffusionmodels have achieved tremendous success in guided image generation,particularly in text-to-image synthesis. To gain a better understanding of thetraining process and potential risks of text-to-image synthesis, we perform asystematic investigation of backdoor attack on text-to-image diffusion modelsand propose BadT2I, a general multimodal backdoor attack framework that tamperswith image synthesis in diverse semantic levels. Specifically, we performbackdoor attacks on three levels of the vision semantics: Pixel-Backdoor,Object-Backdoor and Style-Backdoor. By utilizing a regularization loss, ourmethods efficiently inject backdoors into a large-scale text-to-image diffusionmodel while preserving its utility with benign inputs. We conduct empiricalexperiments on Stable Diffusion, the widely-used text-to-image diffusion model,demonstrating that the large-scale diffusion model can be easily backdooredwithin a few fine-tuning steps. We conduct additional experiments to explorethe impact of different types of textual triggers. Besides, we discuss thebackdoor persistence during further training, the findings of which provideinsights for the development of backdoor defense methods.",,,,,http://arxiv.org/pdf/2305.04175v1
Bi-Mapper: Holistic BEV Semantic Mapping for Autonomous Driving,"  A semantic map of the road scene, covering fundamental road elements, is anessential ingredient in autonomous driving systems. It provides importantperception foundations for positioning and planning when rendered in theBird\'s-Eye-View (BEV). Currently, the prior knowledge of hypothetical depth canguide the learning of translating front perspective views into BEV directlywith the help of calibration parameters. However, it suffers from geometricdistortions in the representation of distant objects. In addition, anotherstream of methods without prior knowledge can learn the transformation betweenfront perspective views and BEV implicitly with a global view. Considering thatthe fusion of different learning methods may bring surprising beneficialeffects, we propose a Bi-Mapper framework for top-down road-scene semanticunderstanding, which incorporates a global view and local prior knowledge. Toenhance reliable interaction between them, an asynchronous mutual learningstrategy is proposed. At the same time, an Across-Space Loss (ASL) is designedto mitigate the negative impact of geometric distortions. Extensive results onnuScenes and Cam2BEV datasets verify the consistent effectiveness of eachmodule in the proposed Bi-Mapper framework. Compared with exiting road mappingnetworks, the proposed Bi-Mapper achieves 5.0 higher IoU on the nuScenesdataset. Moreover, we verify the generalization performance of Bi-Mapper in areal-world driving scenario. Code will be available athttps://github.com/lynn-yu/Bi-Mapper.",,,,,http://arxiv.org/pdf/2305.04205v1
Performance Gaps of Artificial Intelligence Models Screening Mammography  -- Towards Fair and Interpretable Models,"  Purpose: To analyze the demographic and imaging characteristics associatedwith increased risk of failure for abnormality classification in screeningmammograms. Materials and Methods: This retrospective study used data from theEmory BrEast Imaging Dataset (EMBED) which includes mammograms from 115,931patients imaged at Emory University Healthcare between 2013 to 2020. Clinicaland imaging data includes Breast Imaging Reporting and Data System (BI-RADS)assessment, region of interest coordinates for abnormalities, imaging features,pathologic outcomes, and patient demographics. Multiple deep learning modelswere developed to distinguish between patches of abnormal tissue and randomlyselected patches of normal tissue from the screening mammograms. We assessedmodel performance overall and within subgroups defined by age, race, pathologicoutcome, and imaging characteristics to evaluate reasons formisclassifications. Results: On a test set size of 5,810 studies (13,390patches), a ResNet152V2 model trained to classify normal versus abnormal tissuepatches achieved an accuracy of 92.6% (95% CI = 92.0-93.2%), and area under thereceiver operative characteristics curve 0.975 (95% CI = 0.972-0.978). Imagingcharacteristics associated with higher misclassifications of images includehigher tissue densities (risk ratio [RR]=1.649; p=.010, BI-RADS density C andRR=2.026; p=.003, BI-RADS density D), and presence of architectural distortion(RR=1.026; p&lt;.001). Conclusion: Even though deep learning models forabnormality classification can perform well in screening mammography, wedemonstrate certain imaging features that result in worse model performance.This is the first such work to systematically evaluate breast abnormalityclassification by various subgroups and better-informed developers andend-users of population subgroups which are likely to experience biased modelperformance.",,,,,http://arxiv.org/pdf/2305.04422v1
Breaking Through the Haze: An Advanced Non-Homogeneous Dehazing Method  based on Fast Fourier Convolution and ConvNeXt,"  Haze usually leads to deteriorated images with low contrast, color shift andstructural distortion. We observe that many deep learning based models exhibitexceptional performance on removing homogeneous haze, but they usually fail toaddress the challenge of non-homogeneous dehazing. Two main factors account forthis situation. Firstly, due to the intricate and non uniform distribution ofdense haze, the recovery of structural and chromatic features with highfidelity is challenging, particularly in regions with heavy haze. Secondly, theexisting small scale datasets for non-homogeneous dehazing are inadequate tosupport reliable learning of feature mappings between hazy images and theircorresponding haze-free counterparts by convolutional neural network(CNN)-based models. To tackle these two challenges, we propose a novel twobranch network that leverages 2D discrete wavelete transform (DWT), fastFourier convolution (FFC) residual block and a pretrained ConvNeXt model.Specifically, in the DWT-FFC frequency branch, our model exploits DWT tocapture more high-frequency features. Moreover, by taking advantage of thelarge receptive field provided by FFC residual blocks, our model is able toeffectively explore global contextual information and produce images withbetter perceptual quality. In the prior knowledge branch, an ImageNetpretrained ConvNeXt as opposed to Res2Net is adopted. This enables our model tolearn more supplementary information and acquire a stronger generalizationability. The feasibility and effectiveness of the proposed method isdemonstrated via extensive experiments and ablation studies. The code isavailable at https://github.com/zhouh115/DWT-FFC.",,,,,http://arxiv.org/pdf/2305.04430v1
Robust Traffic Light Detection Using Salience-Sensitive Loss:  Computational Framework and Evaluations,"  One of the most important tasks for ensuring safe autonomous driving systemsis accurately detecting road traffic lights and accurately determining how theyimpact the driver\'s actions. In various real-world driving situations, a scenemay have numerous traffic lights with varying levels of relevance to thedriver, and thus, distinguishing and detecting the lights that are relevant tothe driver and influence the driver\'s actions is a critical safety task. Thispaper proposes a traffic light detection model which focuses on this task byfirst defining salient lights as the lights that affect the driver\'s futuredecisions. We then use this salience property to construct the LAVA SalientLights Dataset, the first US traffic light dataset with an annotated salienceproperty. Subsequently, we train a Deformable DETR object detection transformermodel using Salience-Sensitive Focal Loss to emphasize stronger performance onsalient traffic lights, showing that a model trained with this loss functionhas stronger recall than one trained without.",,,,,http://arxiv.org/pdf/2305.04516v1
Development of a Vision System to Enhance the Reliability of the  Pick-and-Place Robot for Autonomous Testing of Camera Module used in  Smartphones,"  Pick-and-place robots are commonly used in modern industrial manufacturing.For complex devices/parts like camera modules used in smartphones, whichcontain optical parts, electrical components and interfacing connectors, theplacement operation may not absolutely accurate, which may cause damage in thedevice under test during the mechanical movement to make good contact forelectrical functions inspection. In this paper, we proposed an effective visionsystem including hardware and algorithm to enhance the reliability of thepick-and-place robot for autonomous testing memory of camera modules. Withlimited hardware based on camera and raspberry PI and using simplify imageprocessing algorithm based on histogram information, the vision system canconfirm the presence of the camera modules in feeding tray and the placementaccuracy of the camera module in test socket. Through that, the system can workwith more flexibility and avoid damaging the device under test. The system wasexperimentally quantified through testing approximately 2000 camera modules ina stable light condition. Experimental results demonstrate that the systemachieves accuracy of more than 99.92%. With its simplicity and effectiveness,the proposed vision system can be considered as a useful solution for using inpick-and-place systems in industry.",,,,,http://arxiv.org/pdf/2305.04605v1
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning,"  The ability of deep neural networks to continually learn and adapt to asequence of tasks has remained challenging due to catastrophic forgetting ofpreviously learned tasks. Humans, on the other hand, have a remarkable abilityto acquire, assimilate, and transfer knowledge across tasks throughout theirlifetime without catastrophic forgetting. The versatility of the brain can beattributed to the rehearsal of abstract experiences through a complementarylearning system. However, representation rehearsal in vision transformers lacksdiversity, resulting in overfitting and consequently, performance dropssignificantly compared to raw image rehearsal. Therefore, we propose BiRT, anovel representation rehearsal-based continual learning approach using visiontransformers. Specifically, we introduce constructive noises at various stagesof the vision transformer and enforce consistency in predictions with respectto an exponential moving average of the working model. Our method providesconsistent performance gain over raw image and vanilla representation rehearsalon several challenging CL benchmarks, while being memory efficient and robustto natural and adversarial corruptions.",,,,,http://arxiv.org/pdf/2305.04769v1
Reasoning with Language Model Prompting: A Survey,"  Reasoning, as an essential ability for complex problem-solving, can provideback-end support for various real-world applications, such as medicaldiagnosis, negotiation, etc. This paper provides a comprehensive survey ofcutting-edge research on reasoning with language model prompting. We introduceresearch works with comparisons and summaries and provide systematic resourcesto help beginners. We also discuss the potential reasons for emerging suchreasoning abilities and highlight future research directions. Resources areavailable at https://github.com/zjunlp/Prompt4ReasoningPapers (updatedperiodically).",,,,,http://arxiv.org/pdf/2212.09597v3
Energy-based Models are Zero-Shot Planners for Compositional Scene  Rearrangement,"  Language is compositional; an instruction can express multiple relationconstraints to hold among objects in a scene that a robot is tasked torearrange. Our focus in this work is an instructable scene-rearrangingframework that generalizes to longer instructions and to spatial conceptcompositions never seen at training time. We propose to representlanguage-instructed spatial concepts with energy functions over relative objectarrangements. A language parser maps instructions to corresponding energyfunctions and an open-vocabulary visual-language model grounds their argumentsto relevant objects in the scene. We generate goal scene configurations bygradient descent on the sum of energy functions, one per language predicate inthe instruction. Local vision-based policies then re-locate objects to theinferred goal locations. We test our model on established instruction-guidedmanipulation benchmarks, as well as benchmarks of compositional instructions weintroduce. We show our model can execute highly compositional instructionszero-shot in simulation and in the real world. It outperformslanguage-to-action reactive policies and Large Language Model planners by alarge margin, especially for long instructions that involve compositions ofmultiple spatial concepts. Simulation and real-world robot execution videos, aswell as our code and datasets are publicly available on our website:https://ebmplanner.github.io.",,,,,http://arxiv.org/pdf/2304.14391v2
A Variational Perspective on Solving Inverse Problems with Diffusion  Models,"  Diffusion models have emerged as a key pillar of foundation models in visualdomains. One of their critical applications is to universally solve differentdownstream inverse tasks via a single diffusion prior without re-training foreach task. Most inverse tasks can be formulated as inferring a posteriordistribution over data (e.g., a full image) given a measurement (e.g., a maskedimage). This is however challenging in diffusion models since the nonlinear anditerative nature of the diffusion process renders the posterior intractable. Tocope with this challenge, we propose a variational approach that by designseeks to approximate the true posterior distribution. We show that our approachnaturally leads to regularization by denoising diffusion process (RED-Diff)where denoisers at different timesteps concurrently impose different structuralconstraints over the image. To gauge the contribution of denoisers fromdifferent timesteps, we propose a weighting mechanism based onsignal-to-noise-ratio (SNR). Our approach provides a new variationalperspective for solving inverse problems with diffusion models, allowing us toformulate sampling as stochastic optimization, where one can simply applyoff-the-shelf solvers with lightweight iterates. Our experiments for imagerestoration tasks such as inpainting and superresolution demonstrate thestrengths of our method compared with state-of-the-art sampling-based diffusionmodels.",,,,,http://arxiv.org/pdf/2305.04391v1
