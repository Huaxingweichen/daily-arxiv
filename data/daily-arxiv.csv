title,summary,tag,affiliation,summary_zh,tag_zh,url
SImProv: Scalable Image Provenance Framework for Robust Content  Attribution,"  We present SImProv - a scalable image provenance framework to match a queryimage back to a trusted database of originals and identify possiblemanipulations on the query. SImProv consists of three stages: a scalable searchstage for retrieving top-k most similar images; a re-ranking andnear-duplicated detection stage for identifying the original among thecandidates; and finally a manipulation detection and visualization stage forlocalizing regions within the query that may have been manipulated to differfrom the original. SImProv is robust to benign image transformations thatcommonly occur during online redistribution, such as artifacts due to noise andrecompression degradation, as well as out-of-place transformations due to imagepadding, warping, and changes in size and shape. Robustness towardsout-of-place transformations is achieved via the end-to-end training of adifferentiable warping module within the comparator architecture. Wedemonstrate effective retrieval and manipulation detection over a dataset of100 million images.","image provenance, image matching, visual search","CVSSP, University of Surrey, UK; Adobe Research",提出了一个可扩展的图像溯源框架SImProv，能够将查询图像与信任数据库的原图进行匹配和检测图像篡改。利用可扩展的搜索阶段和可视化阶段来检测图像匹配和篡改。,图像溯源，图像匹配，视觉搜索,http://arxiv.org/pdf/2206.14245v2
Unified Object Detector for Different Modalities based on Vision  Transformers,"  Traditional systems typically require different models for processingdifferent modalities, such as one model for RGB images and another for depthimages. Recent research has demonstrated that a single model for one modalitycan be adapted for another using cross-modality transfer learning. In thispaper, we extend this approach by combining cross/inter-modality transferlearning with a vision transformer to develop a unified detector that achievessuperior performance across diverse modalities. Our research envisions anapplication scenario for robotics, where the unified system seamlessly switchesbetween RGB cameras and depth sensors in varying lighting conditions.Importantly, the system requires no model architecture or weight updates toenable this smooth transition. Specifically, the system uses the depth sensorduring low-lighting conditions (night time) and both the RGB camera and depthsensor or RGB caemra only in well-lit environments. We evaluate our unifiedmodel on the SUN RGB-D dataset, and demonstrate that it achieves similar orbetter performance in terms of mAP50 compared to state-of-the-art methods inthe SUNRGBD16 category, and comparable performance in point cloud only mode. Wealso introduce a novel inter-modality mixing method that enables our model toachieve significantly better results than previous methods. We provide ourcode, including training/inference logs and model checkpoints, to facilitatereproducibility and further research.\\url{https://github.com/liketheflower/UODDM}",multi-modal object detection,"1 Mobi Systems, Inc, 2 Hunter College, CUNY和The Graduate Center, CUNY",本文提出了一种基于Vision Transformer的统一对象检测方法，可以在不同的模态下进行检测，并且在不同的光照条件下，无需更新模型架构或权重就可以实现无缝的转换。该方法可用于机器人领域。作者在SUN RGB-D数据集上评估了该方法。同时，作者还介绍了一种新颖的交叉模态混合方法，以实现比以前方法更好的结果。,多模态对象检测,http://arxiv.org/pdf/2207.01071v2
FS-BAN: Born-Again Networks for Domain Generalization Few-Shot  Classification,"  Conventional Few-shot classification (FSC) aims to recognize samples fromnovel classes given limited labeled data. Recently, domain generalization FSC(DG-FSC) has been proposed with the goal to recognize novel class samples fromunseen domains. DG-FSC poses considerable challenges to many models due to thedomain shift between base classes (used in training) and novel classes(encountered in evaluation). In this work, we make two novel contributions totackle DG-FSC. Our first contribution is to propose Born-Again Network (BAN)episodic training and comprehensively investigate its effectiveness for DG-FSC.As a specific form of knowledge distillation, BAN has been shown to achieveimproved generalization in conventional supervised classification with aclosed-set setup. This improved generalization motivates us to study BAN forDG-FSC, and we show that BAN is promising to address the domain shiftencountered in DG-FSC. Building on the encouraging findings, our second (major)contribution is to propose Few-Shot BAN (FS-BAN), a novel BAN approach forDG-FSC. Our proposed FS-BAN includes novel multi-task learning objectives:Mutual Regularization, Mismatched Teacher, and Meta-Control Temperature, eachof these is specifically designed to overcome central and unique challenges inDG-FSC, namely overfitting and domain discrepancy. We analyze different designchoices of these techniques. We conduct comprehensive quantitative andqualitative analysis and evaluation over six datasets and three baselinemodels. The results suggest that our proposed FS-BAN consistently improves thegeneralization performance of baseline models and achieves state-of-the-artaccuracy for DG-FSC. Project Page: https://yunqing-me.github.io/Born-Again-FS/.","Few-shot classification, domain generalization, born-again network, episodic training, meta-learning","Information Systems Technology and Design Pillar, Singapore University of Technology and Design",本文提出Born-Again Networks(BAN)用于解决领域泛化Few-shot classification的问题，针对该问题提出了Few-Shot BAN(FS-BAN)方法，并通过全面的定量和定性分析评估结果，表明FS-BAN能够显著提高基线模型的泛化性能，达到领域泛化Few-shot classification的最先进准确度。,"Few-shot classification, domain generalization, born-again network, episodic training, meta-learning",http://arxiv.org/pdf/2208.10930v4
SwinFIR: Revisiting the SwinIR with Fast Fourier Convolution and  Improved Training for Image Super-Resolution,"  Transformer-based methods have achieved impressive image restorationperformance due to their capacities to model long-range dependency compared toCNN-based methods. However, advances like SwinIR adopts the window-based andlocal attention strategy to balance the performance and computational overhead,which restricts employing large receptive fields to capture global informationand establish long dependencies in the early layers. To further improve theefficiency of capturing global information, in this work, we propose SwinFIR toextend SwinIR by replacing Fast Fourier Convolution (FFC) components, whichhave the image-wide receptive field. We also revisit other advanced techniques,i.e, data augmentation, pre-training, and feature ensemble to improve theeffect of image reconstruction. And our feature ensemble method enables theperformance of the model to be considerably enhanced without increasing thetraining and testing time. We applied our algorithm on multiple popularlarge-scale benchmarks and achieved state-of-the-art performance comparing tothe existing methods. For example, our SwinFIR achieves the PSNR of 32.83 dB onManga109 dataset, which is 0.8 dB higher than the state-of-the-art SwinIRmethod.","Image super-resolution, Transformer, Image restoration, Neural networks, Convolutional neural networks",Samsung Research China - Beijing (SRC-B),本文提出了一种基于Fast Fourier Convolution (FFC)和改进训练的技术SwinFIR，用于图像超分辨率的任务。相较于现有的SwinIR方法，SwinFIR在多个数据集上均取得了更好的性能。,图像超分辨率、Transformer、图像恢复、神经网络、卷积神经网络,http://arxiv.org/pdf/2208.11247v2
Understanding the Tricks of Deep Learning in Medical Image Segmentation:  Challenges and Future Directions,"  Over the past few years, the rapid development of deep learning technologiesfor computer vision has significantly improved the performance of medical imagesegmentation (MedISeg). However, the diverse implementation strategies ofvarious models have led to an extremely complex MedISeg system, resulting in apotential problem of unfair result comparisons. In this paper, we collect aseries of MedISeg tricks for different model implementation phases (i.e.,pre-training model, data pre-processing, data augmentation, modelimplementation, model inference, and result post-processing), andexperimentally explore the effectiveness of these tricks on consistentbaselines. With the extensive experimental results on both the representative2D and 3D medical image datasets, we explicitly clarify the effect of thesetricks. Moreover, based on the surveyed tricks, we also open-sourced a strongMedISeg repository, where each component has the advantage of plug-and-play. Webelieve that this milestone work not only completes a comprehensive andcomplementary survey of the state-of-the-art MedISeg approaches, but alsooffers a practical guide for addressing the future medical image processingchallenges including but not limited to small dataset, class imbalancelearning, multi-modality learning, and domain adaptation. The code and trainingweights have been released at: https://github.com/hust-linyi/seg_trick.",,,,,http://arxiv.org/pdf/2209.10307v2
Make-A-Story: Visual Memory Conditioned Consistent Story Generation,"  There has been a recent explosion of impressive generative models that canproduce high quality images (or videos) conditioned on text descriptions.However, all such approaches rely on conditional sentences that containunambiguous descriptions of scenes and main actors in them. Therefore employingsuch models for more complex task of story visualization, where naturallyreferences and co-references exist, and one requires to reason about when tomaintain consistency of actors and backgrounds across frames/scenes, and whennot to, based on story progression, remains a challenge. In this work, weaddress the aforementioned challenges and propose a novel autoregressivediffusion-based framework with a visual memory module that implicitly capturesthe actor and background context across the generated frames.Sentence-conditioned soft attention over the memories enables effectivereference resolution and learns to maintain scene and actor consistency whenneeded. To validate the effectiveness of our approach, we extend the MUGENdataset and introduce additional characters, backgrounds and referencing inmulti-sentence storylines. Our experiments for story generation on the MUGEN,the PororoSV and the FlintstonesSV dataset show that our method not onlyoutperforms prior state-of-the-art in generating frames with high visualquality, which are consistent with the story, but also models appropriatecorrespondences between the characters and the background.",,,,,http://arxiv.org/pdf/2211.13319v3
Pose-disentangled Contrastive Learning for Self-supervised Facial  Representation,"  Self-supervised facial representation has recently attracted increasingattention due to its ability to perform face understanding without relying onlarge-scale annotated datasets heavily. However, analytically, currentcontrastive-based self-supervised learning (SSL) still performsunsatisfactorily for learning facial representation. More specifically,existing contrastive learning (CL) tends to learn pose-invariant features thatcannot depict the pose details of faces, compromising the learning performance.To conquer the above limitation of CL, we propose a novel Pose-disentangledContrastive Learning (PCL) method for general self-supervised facialrepresentation. Our PCL first devises a pose-disentangled decoder (PDD) with adelicately designed orthogonalizing regulation, which disentangles thepose-related features from the face-aware features; therefore, pose-related andother pose-unrelated facial information could be performed in individualsubnetworks and do not affect each other\'s training. Furthermore, we introducea pose-related contrastive learning scheme that learns pose-related informationbased on data augmentation of the same image, which would deliver moreeffective face-aware representation for various downstream tasks. We conductedlinear evaluation on four challenging downstream facial understanding tasks,ie, facial expression recognition, face recognition, AU detection and head poseestimation. Experimental results demonstrate that our method significantlyoutperforms state-of-the-art SSL methods. Code is available athttps://github.com/DreamMr/PCL}{https://github.com/DreamMr/PCL",,,,,http://arxiv.org/pdf/2211.13490v2
Diff-Font: Diffusion Model for Robust One-Shot Font Generation,"  Font generation is a difficult and time-consuming task, especially in thoselanguages using ideograms that have complicated structures with a large numberof characters, such as Chinese. To solve this problem, few-shot font generationand even one-shot font generation have attracted a lot of attention. However,most existing font generation methods may still suffer from (i) largecross-font gap challenge; (ii) subtle cross-font variation problem; and (iii)incorrect generation of complicated characters. In this paper, we propose anovel one-shot font generation method based on a diffusion model, namedDiff-Font, which can be stably trained on large datasets. The proposed modelaims to generate the entire font library by giving only one sample as thereference. Specifically, a large stroke-wise dataset is constructed, and astroke-wise diffusion model is proposed to preserve the structure and thecompletion of each generated character. To our best knowledge, the proposedDiff-Font is the first work that developed diffusion models to handle the fontgeneration task. The well-trained Diff-Font is not only robust to font gap andfont variation, but also achieved promising performance on difficult charactergeneration. Compared to previous font generation methods, our model reachesstate-of-the-art performance both qualitatively and quantitatively.",,,,,http://arxiv.org/pdf/2212.05895v3
Mask-FPAN: Semi-Supervised Face Parsing in the Wild With De-Occlusion  and UV GAN,"  Fine-grained semantic segmentation of a person\'s face and head, includingfacial parts and head components, has progressed a great deal in recent years.However, it remains a challenging task, whereby considering ambiguousocclusions and large pose variations are particularly difficult. To overcomethese difficulties, we propose a novel framework termed Mask-FPAN. It uses ade-occlusion module that learns to parse occluded faces in a semi-supervisedway. In particular, face landmark localization, face occlusionstimations, anddetected head poses are taken into account. A 3D morphable face model combinedwith the UV GAN improves the robustness of 2D face parsing. In addition, weintroduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for faceparing work. The proposed Mask-FPAN framework addresses the face parsingproblem in the wild and shows significant performance improvements with MIOUfrom 0.7353 to 0.9013 compared to the state-of-the-art on challenging facedatasets.",,,,,http://arxiv.org/pdf/2212.09098v4
