title$summary$tag$affiliation$summary_zh$url
Self-supervised Pre-training with Masked Shape Prediction for 3D Scene  Understanding$  Masked signal modeling has greatly advanced self-supervised pre-training forlanguage and 2D images. However, it is still not fully explored in 3D sceneunderstanding. Thus, this paper introduces Masked Shape Prediction (MSP), a newframework to conduct masked signal modeling in 3D scenes. MSP uses theessential 3D semantic cue, i.e., geometric shape, as the prediction target formasked points. The context-enhanced shape target consisting of explicit shapecontext and implicit deep shape feature is proposed to facilitate exploitingcontextual cues in shape prediction. Meanwhile, the pre-training architecturein MSP is carefully designed to alleviate the masked shape leakage from pointcoordinates. Experiments on multiple 3D understanding tasks on both indoor andoutdoor datasets demonstrate the effectiveness of MSP in learning good featurerepresentations to consistently boost downstream performance.$Keywords: self-supervised pre-training, 3D scene understanding, masked signal modeling, geometric shape, context-enhanced shape target, deep shape feature, network design.$ä½œè€…ï¼šLi Jiangï¼ˆMax Planck Institute for Informaticsï¼‰ï¼ŒZetong Yangï¼ˆCUHKï¼‰ï¼ŒShaoshuai Shiã€Vladislav Golyanikã€Dengxin Daiã€Bernt Schieleï¼ˆMax Planck Institute for Informaticsï¼‰$æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è‡ªç›‘ç£é¢„è®­ç»ƒæ¡†æ¶â€”â€”Masked Shape Prediction (MSP)ï¼Œä»¥æ©ç ä¿¡å·å»ºæ¨¡ä¸ºæ ¸å¿ƒï¼Œç€é‡äºä¸‰ç»´åœºæ™¯ä¸‹çš„é¢„è®­ç»ƒã€‚è¯¥æ¡†æ¶ä½¿ç”¨å‡ ä½•å½¢çŠ¶ä½œä¸ºé¢„æµ‹ç›®æ ‡æ¥è¿›è¡Œæ©ç ç‚¹çš„é¢„æµ‹ï¼Œå¹¶æå‡ºäº†ä¸Šä¸‹æ–‡å¢å¼ºå½¢çŠ¶ç›®æ ‡æ¥ä¿ƒè¿›åˆ©ç”¨å½¢çŠ¶é¢„æµ‹çš„ä¸Šä¸‹æ–‡çº¿ç´¢ã€‚ä¸ºäº†é¿å…ç‚¹åæ ‡æ³„éœ²æ©ç å½¢çŠ¶ï¼Œæœ¬æ–‡è®¾è®¡äº†å¤šç§MSPç½‘ç»œæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®¤å†…å’Œå®¤å¤–æ•°æ®é›†ä¸Šçš„å¤šä¸ªä¸‰ç»´åœºæ™¯ç†è§£ä»»åŠ¡ä¸­å…·æœ‰è‰¯å¥½çš„è¡¨ç°ã€‚$http://arxiv.org/pdf/2305.05026v1
Crack Detection of Asphalt Concrete Using Combined Fracture Mechanics  and Digital Image Correlation$"  Cracking is a common failure mode in asphalt concrete (AC) pavements. Manytests have been developed to characterize the fracture behavior of AC. Accuratecrack detection during testing is crucial to describe AC fracture behavior.This paper proposed a framework to detect surface cracks in AC specimens usingtwo-dimensional digital image correlation (DIC). Two significant drawbacks inprevious research in this field were addressed. First, a multi-seed incrementalreliability-guided DIC was proposed to solve the decorrelation issue due tolarge deformation and discontinuities. The method was validated using syntheticdeformed images. A correctly implemented analysis could accurately measurestrains up to 450\\%, even with significant discontinuities (cracks) present inthe deformed image. Second, a robust method was developed to detect cracksbased on displacement fields. The proposed method uses critical crack tipopening displacement ($\\delta_c$) to define the onset of cleavage fracture. Theproposed method relies on well-developed fracture mechanics theory. Theproposed threshold $\\delta_c$ has a physical meaning and can be easilydetermined from DIC measurement. The method was validated using an extendedfinite element model. The framework was implemented to measure the crackpropagation rate while conducting the Illinois-flexibility index test on two ACmixes. The calculated rates could distinguish mixes based on their crackingpotential. The proposed framework could be applied to characterize AC crackingphenomenon, evaluate its fracture properties, assess asphalt mixture testingprotocols, and develop theoretical models."$Asphalt Concrete, Crack Detection, Fracture Mechanics, Digital Image Correlation, Pavement Cracking, Fracture Properties, Testing Protocols.$ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šZehui Zhu, Ph.D.1*å’ŒImad L. Al-Qadi, Ph.D.2ï¼ˆ1ä¼Šåˆ©è¯ºä¼Šå¤§å­¦å„å·´çº³-é¦™æ§Ÿåˆ†æ ¡åœŸæœ¨ä¸ç¯å¢ƒå·¥ç¨‹ç³»ï¼›2ä¼Šåˆ©è¯ºä¼Šå¤§å­¦å„å·´çº³-é¦™æ§Ÿåˆ†æ ¡åœŸæœ¨ä¸ç¯å¢ƒå·¥ç¨‹ç³»ï¼‰$æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä½¿ç”¨äºŒç»´æ•°å­—å›¾åƒç›¸å…³æ³•ï¼ˆDICï¼‰æ£€æµ‹æ²¥é’æ··å‡åœŸæ ·å“è¡¨é¢çš„è£‚ç¼ã€‚é€šè¿‡æå‡ºå¯é æ€§å¯¼å‘DICæ–¹æ³•è§£å†³ç”±äºå¤§å˜å½¢å’Œä¸è¿ç»­æ€§å¯¼è‡´çš„ç›¸å…³æ€§é—®é¢˜ï¼Œå¹¶åŸºäºä½ç§»åœºå¼€å‘äº†ä¸€ç§é²æ£’çš„æ£€æµ‹è£‚ç¼çš„æ–¹æ³•ï¼Œåˆ©ç”¨ä¸´ç•Œè£‚ç¼å°–ç«¯å¼ å¼€ä½ç§»ï¼ˆğ›¿ğ‘ï¼‰æ¥å®šä¹‰è£‚çº¹æ‰©å±•ã€‚æ­¤æ–¹æ³•åŸºäºç ´è£‚åŠ›å­¦ç†è®ºå¹¶ä½¿ç”¨æ‰©å±•æœ‰é™å…ƒæ¨¡å‹è¿›è¡ŒéªŒè¯ï¼Œç»éªŒè¯å¯ç”¨äºè¯„ä¼°ACææ–™çš„è£‚çº¹æ€§èƒ½å’Œææ–™æµ‹è¯•åè®®ã€‚åœ¨è¿›è¡Œä¼Šåˆ©è¯ºä¼Š-æŸ”åº¦æŒ‡æ•°æµ‹è¯•æ—¶è®¡ç®—è£‚çº¹æ‰©å±•é€Ÿç‡ï¼Œå¯åŒºåˆ†ä¸åŒæ··å‡åœŸææ–™çš„å¼€è£‚æ½œåŠ›ã€‚è¯¥æ¡†æ¶å¯ç”¨äºè¡¨å¾ACå¼€è£‚ç°è±¡ï¼Œå¹¶ä¸ºå…¶æé«˜ç†è®ºæ¨¡å‹æä¾›åŸºç¡€ã€‚$http://arxiv.org/pdf/2305.05057v1
Wood-sleeper Decayed Detection for Rural Railway Prognostics Using  Unsupervised Deeper FCDDs$  It is critical for railway managers to maintain a high standard to ensureuser safety during daily operations. Top-view or side-view cameras and GPSpositioning system have enabled progress toward automating the periodicinspection of defective features and assessing the deteriorated status of therailway components. Frequently, collecting deteriorated status data constraintstime consuming and repeated data acquisition, because the temporal occurrenceis extremely imbalanced. Supervised learning approach requires thousands ofpaired dataset of defective raw images and annotated labels. However, one-classclassification approach has a merit that fewer images enables us to optimizethe parameters for training normal and anomalous feature. Simultaneously, thevisual heat map explanation enables us to discriminate the localized damagefeature. In this paper, we propose a prognostic discriminator pipeline toautomate one-class damage classification towards defective railway components.We also sensitivity analyze toward the backbone and the receptive field basedon convolutional neural networks (CNNs) using pretrained networks: baselineCNN27, VGG16, ResNet101, and Inception Networks. We also visualize theexplanation of the defective railway feature using a transposed Gaussianupsampling. We demonstrate our application for railway inspection in anopen-accessed dataset of defective railway components, and wood-sleeperdeterioration in rural railway. The heatmap is so important that thehazard-marks could cause an operational delay, an urgent inspection, andunexpected accident to passenger impact in railway inspection. Furthermore, wemention its usability for prognostic monitoring and future works for railwaycomponents inspection in the predictive maintenance of railway systems.$Keywords: Wood-sleeper, Railway Prognostics, Unsupervised Learning, FCDDs, Deteriorated Status, One-class Classification, Convolutional Neural Networks, Heat Map Explanation, Rural Railway, Derailment Risk, Predictive Maintenance.$"ä½œè€…ï¼šTakato Yasunoã€Masahiro Okanoå’ŒJunichiro Fujiiï¼ˆæœºæ„ï¼šYachiyo Engineeringæ ªå¼ä¼šç¤¾ï¼Œæ—¥æœ¬ä¸œäº¬å°ä¸œåŒº111-8648ï¼‰
æ–‡ç« é¢˜ç›®ï¼šWood-sleeper Decayed Detection for Rural Railway Prognostics Using Unsupervised Deeper FCDDsï¼ˆåˆ©ç”¨æ— ç›‘ç£æ·±å±‚FCDDæ£€æµ‹å†œæ‘é“è·¯æœ¨æ•è…æœ½ï¼‰
æ‘˜è¦ï¼šæœ¬ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§é¢„åé‰´åˆ«å™¨ç®¡é“ï¼Œä»¥è‡ªåŠ¨åŒ–ä¸€ç±»æŸä¼¤åˆ†ç±»ï¼Œç”¨äºæ£€æµ‹é“è·¯ç»„ä»¶çš„æŸä¼¤ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜å¯¹åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„éª¨å¹²å’Œæ¥å—åŸŸçš„æ•æ„Ÿæ€§è¿›è¡Œäº†åˆ†æï¼Œå¹¶å¯¹å››ç§CNNè¿›è¡Œäº†é¢„è®­ç»ƒç½‘ç»œçš„å®éªŒæ¯”è¾ƒã€‚ä½œè€…ä»¬è¿˜ä½¿ç”¨è½¬ç½®é«˜æ–¯ä¸Šé‡‡æ ·å¯è§†åŒ–äº†æŸåé“è·¯ç»„ä»¶çš„è§£é‡Šã€‚æœ€åï¼Œä½œè€…ä»¬åœ¨å¼€æ”¾å¼ä¸æŸåé“è·¯ç»„ä»¶å’Œå†œæ‘é“è·¯æœ¨æ•é€€åŒ–æ•°æ®é›†ä¸Šå¯¹ç®—æ³•è¿›è¡Œäº†éªŒè¯ï¼Œæœ¬ç¯‡æ–‡ç« ä¸ºä»Šåçš„é¢„æµ‹ç»´æŠ¤é“è·¯ç³»ç»Ÿæä¾›äº†æœ‰ç›Šçš„å¯ç¤ºã€‚"$æœ¬æ–‡ç ”ç©¶äº†åŸºäºæ— ç›‘ç£æ·±åº¦FCDDsçš„æœ¨æ•è…çƒ‚æ£€æµ‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ç”¨äºé¢„æµ‹å†œæ‘é“è·¯ç»„ä»¶çš„æŸåçŠ¶æ€ã€‚ä½œè€…æå‡ºäº†ä¸€ç§é¢„æµ‹é‰´åˆ«å™¨æµæ°´çº¿ï¼Œç”¨äºè‡ªåŠ¨åˆ†ç±»æŸåé“è·¯ç»„ä»¶çš„ä¸€ç±»æŸåã€‚æ­¤å¤–ï¼Œé’ˆå¯¹å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„ä¸»å¹²å’Œæ„Ÿå—é‡ï¼Œä½œè€…è¿›è¡Œäº†æ•æ„Ÿæ€§åˆ†æï¼Œå¹¶ä½¿ç”¨é¢„è®­ç»ƒç½‘ç»œï¼ˆåŸºçº¿CNN27ã€VGG16ã€ResNet101å’ŒInception Networksï¼‰å¯è§†åŒ–äº†æŸåé“è·¯ç»„ä»¶çš„è¯´æ˜ã€‚ä½œè€…åœ¨å…¬å¼€æ•°æ®é›†ä¸­å¯¹é“è·¯æ£€æŸ¥è¿›è¡Œäº†åº”ç”¨ï¼Œå¹¶é’ˆå¯¹å†œæ‘é“è·¯çš„æœ¨æ•æŸåè¿›è¡Œäº†æ¼”ç¤ºã€‚æœ¬æ–‡è®¤ä¸ºå¯è§†åŒ–çƒ­å›¾å¯¹äºæ£€æŸ¥é“è·¯å®‰å…¨è‡³å…³é‡è¦ï¼Œå› ä¸ºå±é™©æ ‡å¿—å¯èƒ½ä¼šå¯¼è‡´è¿è¥å»¶è¯¯ï¼Œç´§æ€¥æ£€æŸ¥å’Œæ„å¤–å½±å“ä¹˜å®¢åœ¨é“è·¯æ£€æŸ¥ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜æ¢è®¨äº†è¯¥æ–¹æ³•åœ¨é“è·¯ç³»ç»Ÿé¢„æµ‹æ€§ç»´æŠ¤ä¸­ç”¨äºé¢„æµ‹ç›‘æµ‹å’Œæœªæ¥å·¥ä½œçš„å¯ç”¨æ€§ã€‚$http://arxiv.org/pdf/2305.05103v1
Dual flow fusion model for concrete surface crack segmentation$  Cracks and other diseases are important factors that threaten the safeoperation of transportation infrastructure. Traditional manual detection andultrasonic instrument detection consume a lot of time and resource costs. Withthe development of deep learning technology, many deep learning models arewidely used in actual visual segmentation tasks. The detection method based onthe deep learning model has the advantages of high detection accuracy, fastdetection speed and simple operation. However, the crack segmentation based ondeep learning has problems such as sensitivity to background noise, roughedges, and lack of robustness. Therefore, this paper proposes a fissuresegmentation model based on two-stream fusion, which simultaneously inputsimages into two designed processing streams to independently extractlong-distance dependent and local detail features, and realizes adaptiveprediction through a dual-head mechanism. At the same time, a new interactivefusion mechanism is proposed to guide the complementarity of different levelsof features to realize the location and identification of cracks in complexbackgrounds. Finally, we propose an edge optimization method to improvesegmentation accuracy. Experiments have proved that the F1 value of thesegmentation results on the DeepCrack[1] public dataset reached 93.7%, and theIOU value reached 86.6%; the F1 value of the segmentation results on theCRACK500[2] dataset reached 78.1%, and the IOU value reached 66.0%.$can be seen that concrete surface crack segmentation is a key area in transportation infrastructure safety. The proposed dual flow fusion model utilizes deep learning technology, with a focus on sensitivity to background noise and robustness, to improve the accuracy of crack segmentation. Key terms include deep learning, segmentation, feature interaction, and transformer.$"is necessary to develop efficient and accurate methods for concrete surface crack segmentation.

æœ¬è®ºæ–‡çš„ä½œè€…ä¸ºæ®µé›¨å·ã€æ—è¿…ã€å”æ–‡å¿ ã€æ›²å°ç£Šï¼Œæ‰€å±æœºæ„ä¸ºåŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦ã€‚è®ºæ–‡æ ‡é¢˜ä¸ºã€ŠDual flow fusion model for concrete surface crack segmentationã€‹ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºåŒæµèåˆçš„è£‚ç¼åˆ†å‰²æ¨¡å‹ï¼Œé€šè¿‡å°†å›¾åƒåŒæ—¶è¾“å…¥ä¸¤ä¸ªè®¾è®¡çš„å¤„ç†æµä»¥ç‹¬ç«‹æå–è¿œè·ç¦»ä¾èµ–å’Œå±€éƒ¨ç»†èŠ‚ç‰¹å¾ï¼Œå¹¶é€šè¿‡åŒå¤´æœºåˆ¶å®ç°è‡ªé€‚åº”é¢„æµ‹ã€‚åŒæ—¶ï¼Œæå‡ºä¸€ç§æ–°çš„äº¤äº’å¼èåˆæœºåˆ¶ï¼Œä»¥æŒ‡å¯¼ä¸åŒçº§åˆ«ç‰¹å¾çš„äº’è¡¥æ€§ï¼Œå®ç°åœ¨å¤æ‚èƒŒæ™¯ä¸­å¯¹è£‚ç¼çš„å®šä½å’Œè¯†åˆ«ã€‚æœ€åï¼Œæå‡ºäº†ä¸€ç§è¾¹ç¼˜ä¼˜åŒ–æ–¹æ³•ä»¥æé«˜åˆ†å‰²ç²¾åº¦ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨DeepCrackå…¬å…±æ•°æ®é›†ä¸Šï¼Œåˆ†å‰²ç»“æœçš„F1å€¼è¾¾åˆ°93.7ï¼…ï¼ŒIOUå€¼è¾¾åˆ°86.6ï¼…ï¼›åœ¨CRACK500æ•°æ®é›†ä¸Šï¼Œåˆ†å‰²ç»“æœçš„F1å€¼è¾¾åˆ°78.1ï¼…ï¼ŒIOUå€¼è¾¾åˆ°66.0ï¼…ã€‚ æœ¬æ–‡çš„å…³é”®è¯ä¸ºå˜å‹å™¨ã€è£‚ç¼åˆ†å‰²ã€ç‰¹å¾äº¤äº’ã€‚"$is essential to accurately detect and segment concrete cracks in transportation infrastructure. Traditional detection methods are time-consuming and costly, while deep learning-based segmentation methods have high accuracy but are sensitive to noise and lack robustness. In this paper, a dual flow fusion model is proposed for concrete surface crack segmentation, which inputs images into two processing streams to extract long-distance dependent and local detail features and uses an interactive fusion mechanism to achieve adaptive prediction and complementarity of features. An edge optimization method is also proposed to improve segmentation accuracy. Experimental results on public datasets show high F1 and IOU values for segmentation accuracy. This model can be used for efficient and accurate concrete crack detection in transportation infrastructure maintenance.$http://arxiv.org/pdf/2305.05132v1
Linguistic More: Taking a Further Step toward Efficient and Accurate  Scene Text Recognition$"  Vision model have gained increasing attention due to their simplicity andefficiency in Scene Text Recognition (STR) task. However, due to lacking theperception of linguistic knowledge and information, recent vision models sufferfrom two problems: (1) the pure vision-based query results in attention drift,which usually causes poor recognition and is summarized as linguisticinsensitive drift (LID) problem in this paper. (2) the visual feature issuboptimal for the recognition in some vision-missing cases (e.g. occlusion,etc.). To address these issues, we propose a $\\textbf{L}$inguistic$\\textbf{P}$erception $\\textbf{V}$ision model (LPV), which explores thelinguistic capability of vision model for accurate text recognition. Toalleviate the LID problem, we introduce a Cascade Position Attention (CPA)mechanism that obtains high-quality and accurate attention maps throughstep-wise optimization and linguistic information mining. Furthermore, a GlobalLinguistic Reconstruction Module (GLRM) is proposed to improve therepresentation of visual features by perceiving the linguistic information inthe visual space, which gradually converts visual features into semanticallyrich ones during the cascade process. Different from previous methods, ourmethod obtains SOTA results while keeping low complexity (92.4% accuracy withonly 8.11M parameters). Code is available at$\\href{https://github.com/CyrilSterling/LPV}{https://github.com/CyrilSterling/LPV}$."$Keywords: Scene Text Recognition, linguistic perception, attention drift, Cascade Position Attention, Global Linguistic Reconstruction Module.$"ä½œè€…ï¼šBoqiang Zhang, Hongtao Xie, Yuxin Wang, Jianjun Xu, Yongdong Zhang
æœºæ„ï¼šä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦ï¼ˆUniversity of Science and Technology of China, Hefei, Chinaï¼‰"$è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§è§£å†³åœºæ™¯æ–‡æœ¬è¯†åˆ«ä¸­æ³¨æ„åŠ›æ¼‚ç§»å’Œè§†è§‰ç‰¹å¾å­ä¼˜åŒ–é—®é¢˜çš„æ–°æ–¹æ³•â€”â€”è¯­è¨€æ„ŸçŸ¥è§†è§‰æ¨¡å‹ï¼ˆLPVï¼‰ã€‚é€šè¿‡çº§è”ä½ç½®æ³¨æ„åŠ›æœºåˆ¶å’Œå…¨å±€è¯­è¨€é‡æ„æ¨¡å—ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿè·å–é«˜è´¨é‡å’Œå‡†ç¡®çš„æ³¨æ„åŠ›å›¾ä»¥åŠè¯­ä¹‰ä¸°å¯Œçš„è§†è§‰ç‰¹å¾ï¼Œå¹¶åœ¨ä¿æŒä½å¤æ‚æ€§çš„åŒæ—¶å®ç°äº†SOTAç»“æœï¼ˆ92.4%å‡†ç¡®ç‡ï¼Œä»…æœ‰8.11Mä¸ªå‚æ•°ï¼‰ã€‚è¯¥è®ºæ–‡åˆ†æäº†åœºæ™¯æ–‡æœ¬è¯†åˆ«ä¸­æ³¨æ„åŠ›æ¼‚ç§»çš„é—®é¢˜åŠå…¶åŸå› ï¼Œå¹¶æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜é€šè¿‡æ„ŸçŸ¥è§†è§‰ç©ºé—´ä¸­çš„è¯­è¨€ä¿¡æ¯ï¼Œå°†è§†è§‰ç‰¹å¾é€æ­¥è½¬åŒ–ä¸ºè¯­ä¹‰ä¸°å¯Œçš„ç‰¹å¾ã€‚è®ºæ–‡å¯¹è¯¥æ¨¡å‹çš„æ€§èƒ½è¿›è¡Œäº†å……åˆ†çš„å®éªŒéªŒè¯ã€‚$http://arxiv.org/pdf/2305.05140v1
A Mountain-Shaped Single-Stage Network for Accurate Image Restoration$  Image restoration is the task of aiming to obtain a high-quality image from acorrupt input image, such as deblurring and deraining. In image restoration, itis typically necessary to maintain a complex balance between spatial detailsand contextual information. Although a multi-stage network can optimallybalance these competing goals and achieve significant performance, this alsoincreases the system\'s complexity. In this paper, we propose a mountain-shapedsingle-stage design base on a simple U-Net architecture, which removes orreplaces unnecessary nonlinear activation functions to achieve the abovebalance with low system complexity. Specifically, we propose a feature fusionmiddleware (FFM) mechanism as an information exchange component between theencoder-decoder architectural levels. It seamlessly integrates upper-layerinformation into the adjacent lower layer, sequentially down to the lowestlayer. Finally, all information is fused into the original image resolutionmanipulation level. This preserves spatial details and integrates contextualinformation, ensuring high-quality image restoration. In addition, we propose amulti-head attention middle block (MHAMB) as a bridge between the encoder anddecoder to capture more global information and surpass the limitations of thereceptive field of CNNs. Extensive experiments demonstrate that our approach,named as M3SNet, outperforms previous state-of-the-art models while using lessthan half the computational costs, for several image restoration tasks, such asimage deraining and deblurring.$Keywords: image restoration, deblurring, deraining, U-Net architecture, feature fusion middleware, multi-head attention middle block, M3SNet, computational costs.$ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šHu Gao, Jing Yang, Ying Zhang, Ning Wang, Jingfan Yang, Depeng Dangï¼ˆåŒ—äº¬å¸ˆèŒƒå¤§å­¦äººå·¥æ™ºèƒ½å­¦é™¢ï¼‰$æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå•é˜¶æ®µç½‘ç»œçš„å±±å½¢è®¾è®¡ï¼Œå¯ä»¥åœ¨ä½ç³»ç»Ÿå¤æ‚åº¦ä¸‹å®ç°é«˜è´¨é‡å›¾åƒæ¢å¤ã€‚åœ¨å›¾åƒæ¢å¤ä¸­ï¼Œéœ€è¦ä¿æŒç©ºé—´ç»†èŠ‚å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ä¹‹é—´çš„å¤æ‚å¹³è¡¡ã€‚å°½ç®¡å¤šé˜¶æ®µç½‘ç»œå¯ä»¥ä¼˜åŒ–å¹³è¡¡è¿™äº›ç«äº‰ç›®æ ‡å¹¶å®ç°æ˜¾è‘—æ€§èƒ½ï¼Œä½†ä¹Ÿå¢åŠ äº†ç³»ç»Ÿçš„å¤æ‚æ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡ä½¿ç”¨ç‰¹å¾èåˆä¸­é—´ä»¶å’Œå¤šå¤´æ³¨æ„åŠ›ä¸­é—´å—æœºåˆ¶ï¼Œæ— ç¼åœ°å°†ä¸Šå±‚ä¿¡æ¯é›†æˆåˆ°ç›¸é‚»çš„ä¸‹å±‚ï¼Œä»¥ä¿ç•™ç©ºé—´ç»†èŠ‚å’Œé›†æˆä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œå®ç°é«˜è´¨é‡å›¾åƒæ¢å¤ï¼Œå¹¶åœ¨å‡ ä¸ªå›¾åƒæ¢å¤ä»»åŠ¡ä¸­ä¼˜äºä»¥å‰çš„æœ€æ–°æ¨¡å‹ï¼ŒåŒæ—¶ä½¿ç”¨ä¸åˆ°ä¸€åŠçš„è®¡ç®—æˆæœ¬ã€‚$http://arxiv.org/pdf/2305.05146v1
Multi-Granularity Denoising and Bidirectional Alignment for Weakly  Supervised Semantic Segmentation$  Weakly supervised semantic segmentation (WSSS) models relying on classactivation maps (CAMs) have achieved desirable performance comparing to thenon-CAMs-based counterparts. However, to guarantee WSSS task feasible, we needto generate pseudo labels by expanding the seeds from CAMs which is complex andtime-consuming, thus hindering the design of efficient end-to-end(single-stage) WSSS approaches. To tackle the above dilemma, we resort to theoff-the-shelf and readily accessible saliency maps for directly obtainingpseudo labels given the image-level class labels. Nevertheless, the salientregions may contain noisy labels and cannot seamlessly fit the target objects,and saliency maps can only be approximated as pseudo labels for simple imagescontaining single-class objects. As such, the achieved segmentation model withthese simple images cannot generalize well to the complex images containingmulti-class objects. To this end, we propose an end-to-end multi-granularitydenoising and bidirectional alignment (MDBA) model, to alleviate the noisylabel and multi-class generalization issues. Specifically, we propose theonline noise filtering and progressive noise detection modules to tackleimage-level and pixel-level noise, respectively. Moreover, a bidirectionalalignment mechanism is proposed to reduce the data distribution gap at bothinput and output space with simple-to-complex image synthesis andcomplex-to-simple adversarial learning. MDBA can reach the mIoU of 69.5\\% and70.2\\% on validation and test sets for the PASCAL VOC 2012 dataset. The sourcecodes and models have been made available at\\url{https://github.com/NUST-Machine-Intelligence-Laboratory/MDBA}.$Field keywords: Weakly supervised semantic segmentation, class activation maps, saliency maps, multi-granularity denoising, bidirectional alignment, image-level label, noisy label.$ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šTao Chen, Yazhou Yao å’Œ Jinhui Tangï¼ˆå—äº¬ç†å·¥å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢ï¼Œä¸­å›½ï¼‰$æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼±ç›‘ç£çš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œä½¿ç”¨äº†å¤šç²’åº¦å»å™ªå’ŒåŒå‘å¯¹é½çš„æŠ€æœ¯æ¥è§£å†³ä¼—æ‰€å‘¨çŸ¥çš„é—®é¢˜ï¼Œå³åŸºäºç±»æ¿€æ´»å›¾ï¼ˆCAMï¼‰çš„æ¨¡å‹éœ€è¦æ‰©å±•ç§å­ä»¥ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œè¿™ä¸€è¿‡ç¨‹è¾ƒä¸ºå¤æ‚å’Œè€—æ—¶ï¼Œå¹¶ä¸”é™åˆ¶äº†è®¾è®¡é«˜æ•ˆå•é˜¶æ®µçš„ç«¯åˆ°ç«¯æ¨¡å‹ã€‚ä½œè€…æå‡ºä½¿ç”¨æ˜“å¾—åˆ°çš„æ˜¾è‘—æ€§å›¾æ¥æ›¿ä»£CAMç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œå¹¶ä½¿ç”¨åœ¨çº¿å»å™ªå’Œæ¸è¿›å™ªå£°æ£€æµ‹æ¨¡å—æ¥å¤„ç†å™ªå£°æ ‡ç­¾ï¼Œä»¥åŠåŒå‘å¯¹é½æœºåˆ¶æ¥å‡å°‘è¾“å…¥å’Œè¾“å‡ºç©ºé—´ä¸Šçš„æ•°æ®åˆ†å¸ƒå·®å¼‚ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹åœ¨PASCAL VOC 2012æ•°æ®é›†ä¸Šè¾¾åˆ°äº†69.5%å’Œ70.2%çš„mIoUã€‚$http://arxiv.org/pdf/2305.05154v1
Child Palm-ID: Contactless Palmprint Recognition for Children$  Effective distribution of nutritional and healthcare aid for children,particularly infants and toddlers, in some of the least developed and mostimpoverished countries of the world, is a major problem due to the lack ofreliable identification documents. Biometric authentication technology has beeninvestigated to address child recognition in the absence of reliable IDdocuments. We present a mobile-based contactless palmprint recognition system,called Child Palm-ID, which meets the requirements of usability, hygiene, cost,and accuracy for child recognition. Using a contactless child palmprintdatabase, Child-PalmDB1, consisting of 19,158 images from 1,020 unique palms(in the age range of 6 mos. to 48 mos.), we report a TAR=94.11% @ FAR=0.1%. Theproposed Child Palm-ID system is also able to recognize adults, achieving aTAR=99.4% on the CASIA contactless palmprint database and a TAR=100% on theCOEP contactless adult palmprint database, both @ FAR=0.1%. These accuraciesare competitive with the SOTA provided by COTS systems. Despite these highaccuracies, we show that the TAR for time-separated child-palmprints is only78.1% @ FAR=0.1%.$Keywords: child recognition, contactless palmprint recognition, biometric authentication, healthcare aid, malnourishment, identification documents.$"ä½œè€…ï¼šAkash Godbole, Steven A. Grosz å’Œ Anil K. Jainï¼ˆå¯†æ­‡æ ¹å·ç«‹å¤§å­¦ï¼‰
æœºæ„ï¼šå¯†æ­‡æ ¹å·ç«‹å¤§å­¦ï¼ˆMichigan State Universityï¼‰"$æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹å©´å¹¼å„¿ç­‰æ— å¯é èº«ä»½è¯æ˜çš„å„¿ç«¥çš„éæ¥è§¦å¼æ‰‹æŒè¯†åˆ«ç³»ç»Ÿâ€”â€”Child Palm-IDã€‚éšç€ç§»åŠ¨è®¾å¤‡çš„åº”ç”¨ï¼Œè¯¥æŠ€æœ¯å¯åœ¨å¿…è¦æ—¶ç”¨äºåˆ†å‘è¥å…»å’ŒåŒ»ç–—æ´åŠ©ã€‚æ–‡ç« è¯¦ç»†ä»‹ç»äº†è¯¥ç³»ç»Ÿåœ¨å¯ç”¨æ€§ã€å«ç”Ÿã€æˆæœ¬å’Œå‡†ç¡®æ€§æ–¹é¢çš„è¦æ±‚ï¼Œä½¿ç”¨1,020ä¸ªå„¿ç«¥æ‰‹æŒï¼ˆå¹´é¾„åœ¨6ä¸ªæœˆè‡³48ä¸ªæœˆä¹‹é—´ï¼‰çš„19,158å¼ å›¾åƒæ„å»ºäº†å„¿ç«¥æ‰‹æŒæ•°æ®åº“ï¼Œå¹¶æŠ¥å‘Šäº†94.11ï¼…@ FAR = 0.1ï¼…çš„çœŸæ­£è¯†åˆ«ç‡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥ç³»ç»Ÿä¹Ÿèƒ½å¤Ÿè¯†åˆ«æˆäººï¼ŒCASIAæ— æ¥è§¦å¼æ‰‹æŒæ•°æ®åº“çš„çœŸæ­£è¯†åˆ«ç‡ä¸º99.4ï¼…ï¼ŒCOEPæ— æ¥è§¦å¼æˆäººæ‰‹æŒæ•°æ®åº“çš„çœŸæ­£è¯†åˆ«ç‡ä¸º100ï¼…ã€‚ä½†æ˜¯ï¼Œå¯¹äºæ—¶é—´åˆ†éš”çš„å„¿ç«¥æ‰‹æŒå›¾åƒï¼ŒçœŸæ­£è¯†åˆ«ç‡ä»…ä¸º78.1ï¼…@ FAR = 0.1ï¼…ã€‚æ–‡ç« å¼ºè°ƒï¼Œå„¿ç«¥éšç§ä¿æŠ¤æ˜¯ä½¿ç”¨è¯¥æŠ€æœ¯çš„å…³é”®é—®é¢˜ä¹‹ä¸€ã€‚$http://arxiv.org/pdf/2305.05161v1
SRIL: Selective Regularization for Class-Incremental Learning$  Human intelligence gradually accepts new information and accumulatesknowledge throughout the lifespan. However, deep learning models suffer from acatastrophic forgetting phenomenon, where they forget previous knowledge whenacquiring new information. Class-Incremental Learning aims to create anintegrated model that balances plasticity and stability to overcome thischallenge. In this paper, we propose a selective regularization method thataccepts new knowledge while maintaining previous knowledge. We first introducean asymmetric feature distillation method for old and new classes inspired bycognitive science, using the gradient of classification and knowledgedistillation losses to determine whether to perform pattern completion orpattern separation. We also propose a method to selectively interpolate theweight of the previous model for a balance between stability and plasticity,and we adjust whether to transfer through model confidence to ensure theperformance of the previous class and enable exploratory learning. We validatethe effectiveness of the proposed method, which surpasses the performance ofexisting methods through extensive experimental protocols using CIFAR-100,ImageNet-Subset, and ImageNet-Full.$Specific domain keywords: Class-Incremental Learning, Selective Regularization, Catastrophic forgetting, Continual learning, Knowledge distillation, Response-based methods, Relation-based methods, Feature-based methods, Cognitive science, Plasticity, Stability, Pattern completion, Pattern separation, Model confidence, Exploratory learning, CIFAR-100, ImageNet-Subset, ImageNet-Full.$"ä½œè€…ï¼šJisu Han, Jaemin Na, Wonjun Hwang
æœºæ„ï¼šAjou University"$æœ¬æ–‡æå‡ºäº†ä¸€ç§é€‰æ‹©æ€§æ­£åˆ™åŒ–çš„æ–¹æ³•ï¼Œä»¥å®ç°ç±»å¢é‡å­¦ä¹ ã€‚éšç€æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å­¦ä¹ ï¼Œå®ƒä»¬å¾€å¾€ä¼šå¿˜è®°ä¹‹å‰å­¦åˆ°çš„çŸ¥è¯†ï¼Œè¿™å°±æ˜¯ç¾éš¾æ€§é—å¿˜ç°è±¡ã€‚è€Œç±»å¢é‡å­¦ä¹ çš„ç›®æ ‡æ˜¯åˆ›é€ ä¸€ä¸ªå¹³è¡¡å¯å¡‘æ€§å’Œç¨³å®šæ€§çš„ç»¼åˆæ¨¡å‹æ¥å…‹æœè¿™ä¸ªæŒ‘æˆ˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§éå¯¹ç§°ç‰¹å¾è’¸é¦æ–¹æ³•ï¼Œä½¿ç”¨åˆ†ç±»å’ŒçŸ¥è¯†è’¸é¦æŸå¤±çš„æ¢¯åº¦æ¥ç¡®å®šæ˜¯æ‰§è¡Œæ¨¡å¼è¡¥é½è¿˜æ˜¯æ¨¡å¼åˆ†ç¦»ï¼Œä»¥ç»´æŠ¤æ—§çš„å’Œæ–°çš„ç±»çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–¹æ³•æ¥æœ‰é€‰æ‹©æ€§åœ°æ’å€¼å‰ä¸€æ¨¡å‹çš„æƒé‡ï¼Œä»¥å®ç°ç¨³å®šæ€§å’Œå¯å¡‘æ€§ä¹‹é—´çš„å¹³è¡¡ï¼Œå¹¶æ ¹æ®æ¨¡å‹ç½®ä¿¡åº¦è°ƒæ•´æ˜¯å¦è¿›è¡Œè½¬ç§»ä»¥ç¡®ä¿ä»¥å‰ç±»çš„æ€§èƒ½å¹¶å¯ç”¨æ¢ç´¢æ€§å­¦ä¹ ã€‚é€šè¿‡CIFAR-100ï¼ŒImageNet-Subsetå’ŒImageNet-Fullçš„å¹¿æ³›å®éªŒè¯´æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¶…è¿‡äº†ç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚$http://arxiv.org/pdf/2305.05175v1
Hybrid Transformer and CNN Attention Network for Stereo Image  Super-resolution$  Multi-stage strategies are frequently employed in image restoration tasks.While transformer-based methods have exhibited high efficiency in single-imagesuper-resolution tasks, they have not yet shown significant advantages overCNN-based methods in stereo super-resolution tasks. This can be attributed totwo key factors: first, current single-image super-resolution transformers areunable to leverage the complementary stereo information during the process;second, the performance of transformers is typically reliant on sufficientdata, which is absent in common stereo-image super-resolution algorithms. Toaddress these issues, we propose a Hybrid Transformer and CNN Attention Network(HTCAN), which utilizes a transformer-based network for single-imageenhancement and a CNN-based network for stereo information fusion. Furthermore,we employ a multi-patch training strategy and larger window sizes to activatemore input pixels for super-resolution. We also revisit other advancedtechniques, such as data augmentation, data ensemble, and model ensemble toreduce overfitting and data bias. Finally, our approach achieved a score of23.90dB and emerged as the winner in Track 1 of the NTIRE 2023 Stereo ImageSuper-Resolution Challenge.$Hybrid Transformer, CNN Attention Network, Stereo Image Super-resolution, Transformer, CNN, multi-patch training, data augmentation, data ensemble, model ensemble.$ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šMing Cheng, Haoyu Ma, Qiufang Ma, Xiaopeng Sun, Weiqi Li, Zhenyu Zhang, Xuhan Sheng, Shijie Zhao, Junlin Li, Li Zhang (ByteDance Inc, Peking University Shenzhen Graduate School)$æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ··åˆTransformerå’ŒCNNæ³¨æ„åŠ›ç½‘ç»œ (HTCAN) æ¥è§£å†³ç«‹ä½“å›¾åƒè¶…åˆ†è¾¨ç‡ä¸­çš„é—®é¢˜ã€‚ä¼ ç»Ÿçš„Transformeræ–¹æ³•é€šå¸¸åªè€ƒè™‘å•ç‹¬å›¾åƒï¼Œè€ŒHTCANå°†Transformerå’ŒCNNç»“åˆèµ·æ¥ï¼Œåˆ©ç”¨ç«‹ä½“å›¾åƒä¸­çš„äº’è¡¥ä¿¡æ¯ï¼Œæé«˜ç‰¹å¾æå–å’Œè¶…åˆ†è¾¨ç‡æ•ˆæœã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜é‡‡ç”¨äº†å¤šä¸ªè¡¥ä¸è®­ç»ƒç­–ç•¥å’Œæ›´å¤§çš„çª—å£å°ºå¯¸æ¥å¢åŠ è¾“å…¥åƒç´ çš„æ¿€æ´»æ•°é‡ï¼Œå‡å°‘æ•°æ®åå·®ã€‚æœ€ç»ˆï¼Œè¯¥æ–¹æ³•åœ¨ NTIRE 2023 ç«‹ä½“å›¾åƒè¶…åˆ†è¾¨ç‡æŒ‘æˆ˜èµ›ä¸­è·å¾—äº†æœ€é«˜åˆ†æ•°å¹¶è·èƒœã€‚$http://arxiv.org/pdf/2305.05177v1
Boosting Visual-Language Models by Exploiting Hard Samples$  Large vision and language models, such as Contrastive Language-ImagePre-training (CLIP), are rapidly becoming the industry norm for matching imagesand texts. In order to improve its zero-shot recognition performance, currentresearch either adds additional web-crawled image-text pairs or designs newtraining losses. However, the additional costs associated with training fromscratch and data collection substantially hinder their deployment. In thispaper, we present HELIP, a low-cost strategy for boosting the performance ofwell-trained CLIP models by finetuning them with hard samples over originaltraining data. Mixing hard examples into each batch, the well-trained CLIPmodel is then fine-tuned using the conventional contrastive alignment objectiveand a margin loss to distinguish between normal and hard negative data. HELIPis deployed in a plug-and-play fashion to existing models. On a comprehensivezero-shot and retrieval benchmark, without training the model from scratch orutilizing additional data, HELIP consistently boosts existing models to achieveleading performance. In particular, HELIP boosts ImageNet zero-shot accuracy ofSLIP by 3.05 and 4.47 when pretrained on CC3M and CC12M respectively. Inaddition, a systematic evaluation of zero-shot and linear probing experimentsacross fine-grained classification datasets demonstrates a consistentperformance improvement and validates the efficacy of HELIP . When pretrainingon CC3M, HELIP boosts zero-shot performance of CLIP and SLIP by 8.4\\% and18.6\\% on average respectively, and linear probe performance by 9.5\\% and 3.0\\%on average respectively.$Computer vision, natural language processing, contrastive language-image pretraining, zero-shot recognition, hard samples, margin loss.$æ–‡ç« çš„ä½œè€…æ˜¯ Haonan Wangã€Minbin Huangã€Runhui Huangã€Lanqing Hongã€Hang Xuã€Tianyang Huã€Xiaodan Liang å’Œ Zhenguo Liï¼Œæœºæ„åˆ†åˆ«ä¸º National University of Singaporeã€Sun Yat-sen University å’Œ Huawei Noahâ€™s Ark Labã€‚$è¯¥è®ºæ–‡ç ”ç©¶äº†åˆ©ç”¨éš¾æ ·æœ¬æ¥æé«˜å·²ç»è®­ç»ƒå¥½çš„CLIPæ¨¡å‹æ€§èƒ½çš„æ–¹æ³•ã€‚è¿™ç§åä¸ºHELIPçš„ä½æˆæœ¬ç­–ç•¥æ˜¯å°†éš¾ä¾‹å­æ··åˆåˆ°æ¯ä¸ªæ‰¹æ¬¡ä¸­ï¼Œå¹¶é€šè¿‡ä¸€ç§è¾¹ç¼˜æŸå¤±æ¥åŒºåˆ†æ™®é€šå’Œéš¾ä»¥è´Ÿæ ·æœ¬ï¼Œä»è€Œå¾®è°ƒå¥½çš„CLIPæ¨¡å‹ï¼Œä»¥æé«˜å…¶é›¶æ ·æœ¬è¯†åˆ«æ€§èƒ½ã€‚æœ¬æ–‡ç³»ç»Ÿè¯„ä¼°äº†é›¶æ ·æœ¬å’Œçº¿æ€§æ¢é’ˆå®éªŒåœ¨ç»†ç²’åº¦åˆ†ç±»æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼ŒéªŒè¯äº†HELIPç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸ä½¿ç”¨é¢å¤–æ•°æ®å’Œä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹çš„æƒ…å†µä¸‹ï¼ŒHELIPç­–ç•¥å§‹ç»ˆæé«˜ç°æœ‰æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä½¿SLIPæ¨¡å‹åœ¨é¢„è®­ç»ƒCC3Må’ŒCC12Mçš„æƒ…å†µä¸‹ImageNetçš„é›¶æ ·æœ¬åˆ†ç±»ç²¾åº¦æé«˜äº†3.05å’Œ4.47ã€‚åŒæ—¶ï¼Œåœ¨CC3Mé¢„è®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒHELIPç­–ç•¥å°†CLIPå’ŒSLIPçš„é›¶æ ·æœ¬è¡¨ç°åˆ†åˆ«æé«˜8.4%å’Œ18.6%çš„å¹³å‡å€¼ï¼Œå¹¶å°†çº¿æ€§æ¢æµ‹è¡¨ç°åˆ†åˆ«æé«˜9.5%å’Œ3.0%çš„å¹³å‡å€¼ã€‚æœ¬æ–‡åœ¨CLIPæ¨¡å‹çš„é›¶æ ·æœ¬å’Œæ£€ç´¢åŸºå‡†æµ‹è¯•ä¸Šå±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¡¨æ˜ä¼˜åŒ–ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°æé«˜å…¶æ€§èƒ½ï¼Œè€Œä¸éœ€è¦é¢å¤–çš„å¼€é”€æ¥æ”¶é›†æ•°æ®å’Œé‡æ–°è®­ç»ƒæ¨¡å‹ã€‚$http://arxiv.org/pdf/2305.05208v1
Novel Synthetic Data Tool for Data-Driven Cardboard Box Localization$  Application of neural networks in industrial settings, such as automatedfactories with bin-picking solutions requires costly production of largelabeled data-sets. This paper presents an automatic data generation tool with aprocedural model of a cardboard box. We briefly demonstrate the capabilities ofthe system, its various parameters and empirically prove the usefulness of thegenerated synthetic data by training a simple neural network. We make samplesynthetic data generated by the tool publicly available.$Keywords: synthetic data, neural applications, intelligent robotics, cardboard box localization, data-driven.$ä½œè€…ï¼šPeter KravÃ¡rï¼ˆæ·å…‹é©¬è¨é‡Œå…‹å¤§å­¦ä¿¡æ¯å­¦é™¢ï¼‰å’ŒLukÃ¡Å¡ GajdoÅ¡echï¼ˆæ–¯æ´›ä¼å…‹åº·ç±³ä¹Œæ–¯å¤§å­¦æ•°å­¦ã€ç‰©ç†å’Œä¿¡æ¯å­¦é™¢ï¼ŒSkeletex Researchï¼‰ã€‚$æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºæ•°æ®é©±åŠ¨çš„çº¸ç®±å®šä½çš„æ–°å‹åˆæˆæ•°æ®å·¥å…·ã€‚åœ¨è‡ªåŠ¨åŒ–å·¥å‚ä¸­åº”ç”¨ç¥ç»ç½‘ç»œï¼Œä¾‹å¦‚å…·æœ‰æ‹¾å–è£…ç½®çš„è‡ªåŠ¨åŒ–å·¥å‚ï¼Œéœ€è¦æ˜‚è´µçš„å¤§è§„æ¨¡æ ‡è®°æ•°æ®é›†çš„ç”Ÿäº§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨æ•°æ®ç”Ÿæˆå·¥å…·ï¼Œå…¶ä¸­åŒ…å«çº¸æ¿ç®±çš„è¿‡ç¨‹æ¨¡å‹ã€‚æˆ‘ä»¬ç®€è¦å±•ç¤ºäº†è¯¥ç³»ç»Ÿçš„åŠŸèƒ½ã€å„ç§å‚æ•°ï¼Œå¹¶é€šè¿‡è®­ç»ƒä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œç»éªŒæ€§åœ°è¯æ˜äº†ç”Ÿæˆçš„åˆæˆæ•°æ®çš„æœ‰ç”¨æ€§ã€‚æˆ‘ä»¬å…¬å¼€æä¾›äº†ç”±è¯¥å·¥å…·ç”Ÿæˆçš„æ ·æœ¬åˆæˆæ•°æ®ã€‚å…³é”®è¯ï¼šåˆæˆæ•°æ®ï¼›ç¥ç»åº”ç”¨ï¼›æ™ºèƒ½æœºå™¨äººã€‚$http://arxiv.org/pdf/2305.05215v1
DynamicKD: An Effective Knowledge Distillation via Dynamic Entropy  Correction-Based Distillation for Gap Optimizing$  The knowledge distillation uses a high-performance teacher network to guidethe student network. However, the performance gap between the teacher andstudent networks can affect the student\'s training. This paper proposes a novelknowledge distillation algorithm based on dynamic entropy correction to reducethe gap by adjusting the student instead of the teacher. Firstly, the effect ofchanging the output entropy (short for output information entropy) in thestudent on the distillation loss is analyzed in theory. This paper shows thatcorrecting the output entropy can reduce the gap. Then, a knowledgedistillation algorithm based on dynamic entropy correction is created, whichcan correct the output entropy in real-time with an entropy controller updateddynamically by the distillation loss. The proposed algorithm is validated onthe CIFAR100 and ImageNet. The comparison with various state-of-the-artdistillation algorithms shows impressive results, especially in the experimenton the CIFAR100 regarding teacher-student pair resnet32x4-resnet8x4. Theproposed algorithm raises 2.64 points over the traditional distillationalgorithm and 0.87 points over the state-of-the-art algorithm CRD inclassification accuracy, demonstrating its effectiveness and efficiency.$Key Domain Keywords: Knowledge Distillation, Deep Neural Networks, Convolutional Neural Networks, CNN Compression, CNN Acceleration.$ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šæœ±æ¾å²­ï¼ˆè¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦æ™ºèƒ½æ„ŸçŸ¥ä¸å›¾åƒç†è§£æ•™è‚²éƒ¨é‡ç‚¹å®éªŒå®¤ï¼‰ï¼Œå°šè£åï¼ˆé€šè®¯ä½œè€…ï¼Œè¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦æ™ºèƒ½æ„ŸçŸ¥ä¸å›¾åƒç†è§£æ•™è‚²éƒ¨é‡ç‚¹å®éªŒå®¤ï¼‰ï¼Œè¢æ³¢ï¼ˆå—æ–¹ç§‘æŠ€å¤§å­¦è„‘ç§‘å­¦ä¸æ™ºèƒ½æŠ€æœ¯å¹¿ä¸œçœå®éªŒå®¤ï¼‰ï¼Œå¼ ä¼Ÿé€šï¼ˆè¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦æ™ºèƒ½æ„ŸçŸ¥ä¸å›¾åƒç†è§£æ•™è‚²éƒ¨é‡ç‚¹å®éªŒå®¤ï¼‰ï¼Œææ´‹æ´‹ï¼ˆè¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦æ™ºèƒ½æ„ŸçŸ¥ä¸å›¾åƒç†è§£æ•™è‚²éƒ¨é‡ç‚¹å®éªŒå®¤ï¼‰ï¼Œç„¦åˆ©æˆï¼ˆè¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦æ™ºèƒ½æ„ŸçŸ¥ä¸å›¾åƒç†è§£æ•™è‚²éƒ¨é‡ç‚¹å®éªŒå®¤ï¼‰ã€‚$æœ¬æ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€ç†µæ ¡æ­£çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œé€šè¿‡è°ƒæ•´å­¦ç”Ÿç½‘ç»œè€Œä¸æ˜¯æ•™å¸ˆç½‘ç»œï¼Œæ¥å‡å°‘æ•™å¸ˆç½‘ç»œå’Œå­¦ç”Ÿç½‘ç»œçš„æ€§èƒ½å·®è·ï¼Œä»è€Œä¼˜åŒ–å­¦ç”Ÿç½‘ç»œçš„è®­ç»ƒã€‚æ–‡ç« é¦–å…ˆåœ¨ç†è®ºä¸Šåˆ†æäº†æ”¹å˜å­¦ç”Ÿè¾“å‡ºç†µå¯¹è’¸é¦æŸå¤±çš„å½±å“ï¼Œè¯æ˜äº†ç†µæ ¡æ­£å¯ä»¥å‡å°‘æ€§èƒ½å·®è·ã€‚ç„¶åï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºåŠ¨æ€ç†µæ ¡æ­£çš„çŸ¥è¯†è’¸é¦ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥é€šè¿‡åœ¨è’¸é¦æŸå¤±ä¸­åŠ¨æ€æ›´æ–°çš„ç†µæ§åˆ¶å™¨å®æ—¶æ ¡æ­£å­¦ç”Ÿç½‘ç»œçš„è¾“å‡ºç†µã€‚è¯¥ç®—æ³•åœ¨CIFAR100å’ŒImageNetæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¹¶ä¸å„ç§æœ€å…ˆè¿›çš„è’¸é¦ç®—æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼Œè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç‰¹åˆ«æ˜¯åœ¨æ•™å¸ˆ-å­¦ç”Ÿå¯¹resnet32x4-resnet8x4çš„å®éªŒä¸­ï¼Œè¯¥ç®—æ³•çš„åˆ†ç±»ç²¾åº¦æ¯”ä¼ ç»Ÿè’¸é¦ç®—æ³•æé«˜äº†2.64ç‚¹ï¼Œæ¯”æœ€å…ˆè¿›çš„ç®—æ³•CRDæé«˜äº†0.87ç‚¹ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚æ–‡ç« å…³é”®è¯åŒ…æ‹¬ï¼šå·ç§¯ç¥ç»ç½‘ç»œã€çŸ¥è¯†è’¸é¦ã€CNNå‹ç¼©ã€CNNåŠ é€Ÿã€‚$http://arxiv.org/pdf/2305.05233v1
Patch-DrosoNet: Classifying Image Partitions With Fly-Inspired Models  For Lightweight Visual Place Recognition$  Visual place recognition (VPR) enables autonomous systems to localizethemselves within an environment using image information. While ConvolutionNeural Networks (CNNs) currently dominate state-of-the-art VPR performance,their high computational requirements make them unsuitable for platforms withbudget or size constraints. This has spurred the development of lightweightalgorithms, such as DrosoNet, which employs a voting system based on multiplebio-inspired units. In this paper, we present a novel training approach forDrosoNet, wherein separate models are trained on distinct regions of areference image, allowing them to specialize in the visual features of thatspecific section. Additionally, we introduce a convolutional-like predictionmethod, in which each DrosoNet unit generates a set of place predictions foreach portion of the query image. These predictions are then combined using thepreviously introduced voting system. Our approach significantly improves uponthe VPR performance of previous work while maintaining an extremely compact andlightweight algorithm, making it suitable for resource-constrained platforms.$Visual place recognition, lightweight algorithms, DrosoNet, bio-inspired models, image partitions, convolutional-like prediction, resource-constrained platforms.$ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šBruno Arcanjo, Bruno Ferrarini, Michael Milford, Klaus D. McDonald-Maier and Shoaib Ehsan (University of Essex and Queensland University of Technology)$æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è½»é‡çº§çš„è§†è§‰åœ°ç‚¹è¯†åˆ«ï¼ˆVPRï¼‰ç®—æ³•Patch-DrosoNet, è¯¥ç®—æ³•ä½¿ç”¨åŸºäºå¤šä¸ªç”Ÿç‰©å¯å‘å•å…ƒçš„æŠ•ç¥¨ç³»ç»Ÿæ¥è¿›è¡Œè¯†åˆ«ã€‚ä¼ ç»Ÿçš„VPRç®—æ³•ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å…·æœ‰é«˜è®¡ç®—éœ€æ±‚ï¼Œä¸é€‚åˆå¤§å¤šæ•°åµŒå…¥å¼è®¾å¤‡ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡åœ¨å‚è€ƒå›¾åƒçš„ä¸åŒåŒºåŸŸä¸Šè®­ç»ƒå•ç‹¬çš„æ¨¡å‹ï¼Œä½¿å®ƒä»¬ä¸“é—¨é’ˆå¯¹è¯¥ç‰¹å®šéƒ¨åˆ†çš„è§†è§‰ç‰¹å¾è¿›è¡Œè®­ç»ƒï¼Œå¹¶æå‡ºäº†ä¸€ç§ç±»å·ç§¯çš„é¢„æµ‹æ–¹æ³•ï¼Œä½¿æ¯ä¸ªDrosoNetå•å…ƒä¸ºæŸ¥è¯¢å›¾åƒçš„æ¯ä¸ªéƒ¨åˆ†ç”Ÿæˆä¸€ç»„ä½ç½®é¢„æµ‹ã€‚è¿™äº›é¢„æµ‹ç„¶åä½¿ç”¨æŠ•ç¥¨ç³»ç»Ÿè¿›è¡Œç»„åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨VPRæ€§èƒ½æ–¹é¢æ˜æ˜¾ä¼˜äºä»¥å¾€çš„å·¥ä½œï¼Œå¹¶ä¿æŒæå…¶ç´§å‡‘å’Œè½»é‡çº§çš„ç®—æ³•ï¼Œé€‚ç”¨äºèµ„æºå—é™çš„å¹³å°ã€‚$http://arxiv.org/pdf/2305.05256v1
Guided Focal Stack Refinement Network for Light Field Salient Object  Detection$  Light field salient object detection (SOD) is an emerging research directionattributed to the richness of light field data. However, most existing methodslack effective handling of focal stacks, therefore making the latter involvedin a lot of interfering information and degrade the performance of SOD. Toaddress this limitation, we propose to utilize multi-modal features to refinefocal stacks in a guided manner, resulting in a novel guided focal stackrefinement network called GFRNet. To this end, we propose a guided refinementand fusion module (GRFM) to refine focal stacks and aggregate multi-modalfeatures. In GRFM, all-in-focus (AiF) and depth modalities are utilized torefine focal stacks separately, leading to two novel sub-modules for differentmodalities, namely AiF-based refinement module (ARM) and depth-based refinementmodule (DRM). Such refinement modules enhance structural and positionalinformation of salient objects in focal stacks, and are able to improve SODaccuracy. Experimental results on four benchmark datasets demonstrate thesuperiority of our GFRNet model against 12 state-of-the-art models.$"Keywords: Light field, salient object detection, focal stack, refinement, multi-modal fusion. 

This paper proposes a novel framework called guided focal stack refinement network (GFRNet) for light field salient object detection (SOD). Existing methods lack effective handling of focal stacks, leading to reduced SOD performance with interfering information. GFRNet integrates a guided refinement and fusion module (GRFM) that reï¬nes focal stacks and aggregates multi-modal features. Two sub-modules, AiF-based refinement module (ARM) and depth-based refinement module (DRM), are proposed for guided refinement based on modality-specific properties. Experimental results demonstrate the superiority of GFRNet compared to 12 state-of-the-art models on four benchmark datasets."$"ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šè¢æ³¢ï¼Œæ±Ÿå°§ï¼Œå‚…å¯ä»ï¼Œèµµç¦éªï¼Œå››å·å¤§å­¦è®¡ç®—æœºç§‘å­¦å­¦é™¢ï¼Œä¸­å›½æˆéƒ½610065ã€‚

æ ‡é¢˜ï¼šGUIDED FOCAL STACK REFINEMENT NETWORK FOR LIGHT FIELD SALIENT OBJECT DETECTION"$æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼•å¯¼å¼ç„¦ç‚¹æ ˆç»†åŒ–ç½‘ç»œï¼Œç”¨äºå¤„ç†å…‰åœºé²œæ˜ç›®æ ‡æ£€æµ‹ã€‚ç”±äºç„¦ç‚¹æ ˆä¸­çš„å¹²æ‰°ä¿¡æ¯ç¼ºä¹æœ‰æ•ˆå¤„ç†ï¼Œå¯¼è‡´ç°æœ‰æ–¹æ³•çš„æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤šæ¨¡æ€ç‰¹å¾ä»¥å¼•å¯¼æ–¹å¼ç»†åŒ–ç„¦ç‚¹æ ˆçš„æ–°æ–¹æ³•ï¼Œå‘½åä¸ºGFRNetã€‚é€šè¿‡å¼•å¯¼ç»†åŒ–å’Œèåˆæ¨¡å—ï¼ˆGRFMï¼‰ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ä¸ªå­æ¨¡å—æ¥è¿›è¡Œå¼•å¯¼å¼ç»†åŒ–ï¼Œåˆ†åˆ«æ˜¯åŸºäºAiFï¼ˆå…¨èšç„¦ï¼‰å’Œæ·±åº¦çš„ç»†åŒ–æ¨¡å—ï¼ˆARMå’ŒDRMï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸12ç§æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸æ¯”ï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜è¶Šæ€§ã€‚$http://arxiv.org/pdf/2305.05260v1
Fooling State-of-the-Art Deepfake Detection with High-Quality Deepfakes$  Due to the rising threat of deepfakes to security and privacy, it is mostimportant to develop robust and reliable detectors. In this paper, we examinethe need for high-quality samples in the training datasets of such detectors.Accordingly, we show that deepfake detectors proven to generalize well onmultiple research datasets still struggle in real-world scenarios withwell-crafted fakes. First, we propose a novel autoencoder for face swappingalongside an advanced face blending technique, which we utilize to generate 90high-quality deepfakes. Second, we feed those fakes to a state-of-the-artdetector, causing its performance to decrease drastically. Moreover, wefine-tune the detector on our fakes and demonstrate that they contain usefulclues for the detection of manipulations. Overall, our results provide insightsinto the generalization of deepfake detectors and suggest that their trainingdatasets should be complemented by high-quality fakes since training on mereresearch data is insufficient.$Deepfake detection, face swapping, forgery, dataset, high-quality samples, deep-learning frameworks, security, privacy, image and video manipulation, convolutional neural networks (CNNs), target identity, driving identity, appearance information, encoder, decoder, visual quality, realism, deepfake databases, variability in generation methods, visual artifacts, poorly trained models, realism in deepfake datasets.$"ä½œè€…ï¼šArian Beckmann, Anna Hilsmann, Peter Eisert

æœºæ„ï¼šFraunhofer Heinrich-Hertz-Institute, Humboldt University of Berlin, Berlin, Germany"$æœ¬æ–‡æ¢è®¨äº†ç”±äºDeepfakeå¯¹å®‰å…¨å’Œéšç§çš„å¨èƒæ—¥ç›Šå¢åŠ ï¼Œå¼€å‘é²æ£’å¯é çš„Deepfakeæ£€æµ‹å™¨çš„é‡è¦æ€§ã€‚æ–‡ç« æŒ‡å‡ºï¼Œè®­ç»ƒé›†ä¸­éœ€è¦é«˜è´¨é‡çš„æ ·æœ¬ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªç¼–ç å™¨ç”¨äºé¢éƒ¨äº¤æ¢ï¼Œä»¥åŠé«˜çº§çš„é¢éƒ¨èåˆæŠ€æœ¯ï¼Œç”Ÿæˆäº†90ä¸ªé«˜è´¨é‡çš„Deepfakeæ ·æœ¬ã€‚ä½œè€…å°†è¿™äº›æ ·æœ¬è¾“å…¥å…ˆè¿›çš„æ£€æµ‹å™¨ä¸­ï¼Œæ£€æµ‹å™¨çš„è¡¨ç°æ˜¾è‘—ä¸‹é™ã€‚ä½œè€…è¯´æ˜ï¼Œè¿™äº›é«˜è´¨é‡Deepfakeæ ·æœ¬å¯ä»¥ä½œä¸ºæ£€æµ‹ä¼ªé€ çš„æœ‰ç”¨çº¿ç´¢ã€‚æœ¬æ–‡ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè®­ç»ƒé›†åº”è¯¥è¡¥å……é«˜è´¨é‡çš„Deepfakeæ ·æœ¬ï¼Œå› ä¸ºä»…ä»…ä½¿ç”¨ç°æœ‰æ•°æ®é›†çš„è®­ç»ƒæ˜¯ä¸å¤Ÿçš„ã€‚$http://arxiv.org/pdf/2305.05282v1
CAMIL: Context-Aware Multiple Instance Learning for Whole Slide Image  Classification$  Cancer diagnoses typically involve human pathologists examining whole slideimages (WSIs) of tissue section biopsies to identify tumor cells and theirsubtypes. However, artificial intelligence (AI)-based models, particularlyweakly supervised approaches, have recently emerged as viable alternatives.Weakly supervised approaches often use image subsections or tiles as input,with the overall classification of the WSI based on attention scores assignedto each tile. However, this method overlooks the potential for falsepositives/negatives because tumors can be heterogeneous, with cancer and normalcells growing in patterns larger than a single tile. Such errors at the tilelevel could lead to misclassification at the tumor level. To address thislimitation, we developed a novel deep learning pooling operator called CHARM(Contrastive Histopathology Attention Resolved Models). CHARM leverages thedependencies among single tiles within a WSI and imposes contextual constraintsas prior knowledge to multiple instance learning models. We tested CHARM on thesubtyping of non-small cell lung cancer (NSLC) and lymph node (LN) metastasis,and the results demonstrated its superiority over other state-of-the-art weaklysupervised classification algorithms. Furthermore, CHARM facilitatesinterpretability by visualizing regions of attention.$Context-aware multiple instance learning for whole slide image classification, cancer, histopathology, multiple instance learning, weakly-supervised learning, attention model.$"ä½œè€…ï¼šOlga Fourkioti, Avi Arampatzis, Chen Jin, Mat De Vries, Daniel Alexander, Chris Bakal
æœºæ„ï¼šThe Institute of Cancer Researchã€Democritus University of Thraceã€University College London
è®ºæ–‡é¢˜ç›®ï¼šCAMIL: Context-Aware Multiple Instance Learning for Whole Slide Image Classification"$æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºæ•´å¼ åˆ‡ç‰‡å›¾åƒåˆ†ç±»çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¤šå®ä¾‹å­¦ä¹ æ–¹æ³•ã€‚ä¼ ç»Ÿçš„å¼±ç›‘ç£æ–¹æ³•ä½¿ç”¨å›¾åƒå­åŒºåŸŸæˆ–ç“¦ç‰‡ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡å¯¹æ¯ä¸ªç“¦ç‰‡åˆ†é…æ³¨æ„åŠ›åˆ†æ•°æ¥è¿›è¡Œæ•´å¼ å›¾åƒçš„åˆ†ç±»ã€‚ç„¶è€Œï¼Œè¯¥æ–¹æ³•å¿½ç•¥äº†è‚¿ç˜¤å¯ä»¥æ˜¯å¼‚è´¨æ€§çš„ï¼Œå¹¶ä¸”ç™Œç»†èƒå’Œæ­£å¸¸ç»†èƒåœ¨å•ä¸ªç“¦ç‰‡ä¹‹å¤–çš„åŒºåŸŸç”Ÿé•¿çš„å¯èƒ½æ€§ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºCHARMçš„æ–°å‹æ·±åº¦å­¦ä¹ æ± åŒ–è¿ç®—ç¬¦ï¼Œå®ƒåˆ©ç”¨WSIå†…å•ä¸ªç“¦ç‰‡ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå¹¶å°†ä¸Šä¸‹æ–‡çº¦æŸä½œä¸ºå…ˆéªŒçŸ¥è¯†åŠ å…¥å¤šå®ä¾‹å­¦ä¹ æ¨¡å‹ä¸­ã€‚åœ¨éå°ç»†èƒè‚ºç™Œï¼ˆNSLCï¼‰å’Œæ·‹å·´ç»“ï¼ˆLNï¼‰è½¬ç§»äºšå‹åˆ†ç±»æ–¹é¢ï¼ŒCHARMåœ¨å¼±ç›‘ç£åˆ†ç±»ç®—æ³•ä¸­å±•ç¤ºäº†å…¶ä¼˜è¶Šæ€§ã€‚æ­¤å¤–ï¼ŒCHARMé€šè¿‡å¯è§†åŒ–å…³æ³¨åŒºåŸŸæ¥ä¾¿äºè§£é‡Šã€‚å…³é”®è¯ï¼šç™Œç—‡ã€ç»„ç»‡ç—…ç†å­¦ã€å¤šå®ä¾‹å­¦ä¹ ã€å¼±ç›‘ç£å­¦ä¹ ã€å…³æ³¨æ¨¡å‹ã€‚$http://arxiv.org/pdf/2305.05314v1
TPS++: Attention-Enhanced Thin-Plate Spline for Scene Text Recognition$  Text irregularities pose significant challenges to scene text recognizers.Thin-Plate Spline (TPS)-based rectification is widely regarded as an effectivemeans to deal with them. Currently, the calculation of TPS transformationparameters purely depends on the quality of regressed text borders. It ignoresthe text content and often leads to unsatisfactory rectified results forseverely distorted text. In this work, we introduce TPS++, anattention-enhanced TPS transformation that incorporates the attention mechanismto text rectification for the first time. TPS++ formulates the parametercalculation as a joint process of foreground control point regression andcontent-based attention score estimation, which is computed by a dedicateddesigned gated-attention block. TPS++ builds a more flexible content-awarerectifier, generating a natural text correction that is easier to read by thesubsequent recognizer. Moreover, TPS++ shares the feature backbone with therecognizer in part and implements the rectification at feature-level ratherthan image-level, incurring only a small overhead in terms of parameters andinference time. Experiments on public benchmarks show that TPS++ consistentlyimproves the recognition and achieves state-of-the-art accuracy. Meanwhile, itgeneralizes well on different backbones and recognizers. Code is athttps://github.com/simplify23/TPS_PP.$Keywords of this article: scene text recognition, Thin-Plate Spline, attention mechanism, text rectification.$ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šTianlun Zhengã€Zhineng Chenã€Jinfeng Baiã€Hongtao Xieã€Yu-Gang Jiangï¼ˆä¸Šæµ·æ™ºèƒ½è§†è§‰è®¡ç®—ååŒåˆ›æ–°ä¸­å¿ƒï¼Œå¤æ—¦å¤§å­¦è®¡ç®—æœºç§‘å­¦å­¦é™¢ï¼Œä¸­å›½ï¼›æ˜æ—¥å…ˆç”Ÿç”Ÿæ´»ï¼Œä¸­å›½ï¼›ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦ï¼Œä¸­å›½ï¼‰$æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åœºæ™¯æ–‡æœ¬è¯†åˆ«æ–¹æ³•ï¼Œç§°ä½œTPS++ï¼Œå®ƒæ˜¯Thin-Plate Splineï¼ˆTPSï¼‰å˜æ¢çš„å¢å¼ºç‰ˆæœ¬ï¼Œé¦–æ¬¡å°†æ³¨æ„åŠ›æœºåˆ¶å¼•å…¥åˆ°æ–‡æœ¬æ ¡æ­£ä¸­ã€‚TPS++å°†å‚æ•°è®¡ç®—å½¢å¼åŒ–ä¸ºå‰æ™¯æ§åˆ¶ç‚¹å›å½’å’ŒåŸºäºå†…å®¹çš„æ³¨æ„åŠ›å¾—åˆ†ä¼°è®¡çš„è”åˆè¿‡ç¨‹ï¼Œç”±ä¸“é—¨è®¾è®¡çš„é—¨æ§æ³¨æ„åŠ›å—è¿›è¡Œè®¡ç®—ã€‚TPS++å»ºç«‹äº†ä¸€ä¸ªæ›´çµæ´»çš„å†…å®¹æ„ŸçŸ¥ä¿®æ­£å™¨ï¼Œç”Ÿæˆè‡ªç„¶çš„æ–‡æœ¬æ ¡æ­£ï¼Œæ›´å®¹æ˜“è¢«åç»­çš„è¯†åˆ«å™¨è¯†åˆ«ã€‚å¦å¤–ï¼ŒTPS++ä¸è¯†åˆ«å™¨å…±äº«ç‰¹å¾éª¨å¹²éƒ¨åˆ†ï¼Œå¹¶åœ¨ç‰¹å¾çº§åˆ«è€Œä¸æ˜¯å›¾åƒçº§åˆ«ä¸Šå®ç°æ ¡æ­£ï¼Œå› æ­¤ä¸ä¼šå¸¦æ¥å¤ªå¤šçš„å‚æ•°å’Œæ¨ç†æ—¶é—´å¼€é”€ã€‚å®éªŒè¡¨æ˜ï¼ŒTPS ++åœ¨å…¬å…±åŸºå‡†ä¸Šå§‹ç»ˆæé«˜äº†è¯†åˆ«ç‡ï¼Œå¹¶å®ç°äº†æœ€å…ˆè¿›çš„ç²¾åº¦ã€‚ä¸æ­¤åŒæ—¶ï¼Œå®ƒåœ¨ä¸åŒçš„éª¨å¹²ç½‘ç»œå’Œè¯†åˆ«å™¨ä¸Šå…·æœ‰å¾ˆå¥½çš„æ³›åŒ–æ€§èƒ½ã€‚$http://arxiv.org/pdf/2305.05322v1
GPT-NAS: Neural Architecture Search with the Generative Pre-Trained  Model$  Neural Architecture Search (NAS) has emerged as one of the effective methodsto design the optimal neural network architecture automatically. Althoughneural architectures have achieved human-level performances in several tasks,few of them are obtained from the NAS method. The main reason is the hugesearch space of neural architectures, making NAS algorithms inefficient. Thiswork presents a novel architecture search algorithm, called GPT-NAS, thatoptimizes neural architectures by Generative Pre-Trained (GPT) model. InGPT-NAS, we assume that a generative model pre-trained on a large-scale corpuscould learn the fundamental law of building neural architectures. Therefore,GPT-NAS leverages the generative pre-trained (GPT) model to propose reasonablearchitecture components given the basic one. Such an approach can largelyreduce the search space by introducing prior knowledge in the search process.Extensive experimental results show that our GPT-NAS method significantlyoutperforms seven manually designed neural architectures and thirteenarchitectures provided by competing NAS methods. In addition, our ablationstudy indicates that the proposed algorithm improves the performance of finelytuned neural architectures by up to about 12% compared to those without GPT,further demonstrating its effectiveness in searching neural architectures.$Keywords: Neural architecture search, Generative Pre-Trained model, search space, image classification, deep learning. The article proposes a novel NAS algorithm called GPT-NAS, which optimizes neural architectures using a Generative Pre-Trained (GPT) model to reduce the search space by introducing prior knowledge in the search process. The algorithm's effectiveness is demonstrated through extensive experimental results, outperforming manually designed and competing NAS methods. The proposed algorithm improves the performance of finely-tuned neural architectures by up to about 12% compared to those without GPT. The article addresses the main challenge of NAS: the vast search space of possible architectures.$"ä½œè€…ï¼šCaiyang Yu, Xianggen Liu, Chenwei Tang, Wentao Feng å’Œ Jiancheng Lv

æœºæ„ï¼šå››å·å¤§å­¦è®¡ç®—æœºç§‘å­¦å­¦é™¢"$æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç¥ç»æ¶æ„æœç´¢ç®—æ³•ï¼Œç§°ä¸ºGPT-NASï¼Œå®ƒåˆ©ç”¨é¢„å…ˆè®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹æ¥ä¼˜åŒ–ç¥ç»ç½‘ç»œæ¶æ„ã€‚GPT-NASå‡è®¾é¢„è®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹å¯ä»¥å­¦ä¹ æ„å»ºç¥ç»ç½‘ç»œæ¶æ„çš„åŸºæœ¬æ³•åˆ™ï¼Œé€šè¿‡æå‡ºåˆç†çš„æ¶æ„ç»„ä»¶æ¥å¤§å¤§å‡å°‘æœç´¢ç©ºé—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-NASæ–¹æ³•æ˜¾è‘—ä¼˜äºä¸ƒä¸ªæ‰‹åŠ¨è®¾è®¡çš„ç¥ç»æ¶æ„å’Œç«äº‰NASæ–¹æ³•æä¾›çš„åä¸‰ä¸ªæ¶æ„ã€‚æ­¤å¤–ï¼Œæ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œä¸ä¸ä½¿ç”¨GPTç›¸æ¯”ï¼Œæ‰€æå‡ºçš„ç®—æ³•å¯ä»¥å°†ç²¾è°ƒç¥ç»æ¶æ„çš„æ€§èƒ½æé«˜çº¦12ï¼…ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†å…¶åœ¨æœç´¢ç¥ç»æ¶æ„æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚$http://arxiv.org/pdf/2305.05351v1
Unsupervised Writer Retrieval using NetRVLAD and Graph Similarity  Reranking$  This paper presents an unsupervised approach for writer retrieval based onclustering SIFT descriptors detected at keypoint locations resulting inpseudo-cluster labels. With those cluster labels, a residual network followedby our proposed NetRVLAD, an encoding layer with reduced complexity compared toNetVLAD, is trained on 32x32 patches at keypoint locations. Additionally, wesuggest a graph-based reranking algorithm called SGR to exploit similarities ofthe page embeddings to boost the retrieval performance. Our approach isevaluated on two historical datasets (Historical-WI and HisIR19). We include anevaluation of different backbones and NetRVLAD. It competes with related workon historical datasets without using explicit encodings. We set a newState-of-the-art on both datasets by applying our reranking scheme and showthat our approach achieves comparable performance on a modern dataset as well.$Computer Vision, Writer Retrieval, NetRVLAD, Graph Similarity Reranking, Document Analysis.$"ä½œè€…ï¼šMarco Peerã€Florian Kleberã€Robert Sablatnig
æœºæ„ï¼šç»´ä¹Ÿçº³å·¥ä¸šå¤§å­¦è®¡ç®—æœºè§†è§‰å®éªŒå®¤ï¼ˆComputer Vision Lab, TU Wienï¼‰"$è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„å†™ä½œè€…æ£€ç´¢æ–¹æ³•ï¼ŒåŸºäºåœ¨å…³é”®ç‚¹ä½ç½®æ£€æµ‹åˆ°çš„SIFTæè¿°ç¬¦è¿›è¡Œèšç±»ï¼Œå¹¶ä½¿ç”¨ä¼ªèšç±»æ ‡ç­¾è®­ç»ƒä¸€ä¸ªæ®‹å·®ç½‘ç»œï¼Œåæ¥ä¸€ä¸ªç¼–ç å±‚ï¼ˆNetRVLADï¼‰å¯¹å…³é”®ç‚¹ä½ç½®çš„32Ã—32è¡¥ä¸è¿›è¡Œç¼–ç ã€‚æ­¤å¤–ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºå›¾å½¢ç›¸ä¼¼æ€§é‡æ–°æ’åçš„SGRç®—æ³•ï¼Œä»¥æ­¤æé«˜æ£€ç´¢æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªå†å²æ•°æ®é›†ï¼ˆHistorical-WIå’ŒHisIR19ï¼‰ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä½¿ç”¨ä¸åŒçš„éª¨å¹²ç½‘ç»œå’ŒNetRVLADè¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å†å²æ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä¸”æ— éœ€ä½¿ç”¨æ˜¾å¼ç¼–ç ã€‚é€šè¿‡åº”ç”¨é‡æ’ç®—æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šå‡å–å¾—äº†æœ€æ–°æ’åï¼Œå¹¶åœ¨ç°ä»£æ•°æ®é›†ä¸Šè¾¾åˆ°äº†å¯æ¯”è¾ƒçš„æ€§èƒ½ã€‚$http://arxiv.org/pdf/2305.05358v1
MSVQ: Self-Supervised Learning with Multiple Sample Views and Queues$"  Self-supervised methods based on contrastive learning have achieved greatsuccess in unsupervised visual representation learning. However, most methodsunder this framework suffer from the problem of false negative samples.Inspired by mean shift for self-supervised learning, we propose a new simpleframework, namely Multiple Sample Views and Queues (MSVQ). We jointly constructa soft label on-the-fly by introducing two complementary and symmetric ways:multiple augmented positive views and two momentum encoders forming varioussemantic features of negative samples. Two teacher networks perform similarityrelationship calculations with negative samples and then transfer thisknowledge to the student. Let the student mimic the similar relationshipbetween the samples, thus giving the student a more flexible ability toidentify false negative samples in the dataset. The classification results onfour benchmark image datasets demonstrate the high effectiveness and efficiencyof our approach compared to some classical methods. Source code and pretrainedmodels are available at $\\href{https://github.com/pc-cp/MSVQ}{this~http~URL}$."$"Keywords: Self-supervised learning, Contrastive learning, Knowledge distillation, Data augmentation, Momentum encoder. 

Summary: This paper proposes a new self-supervised learning framework, Multiple Sample Views and Queues (MSVQ), which addresses the problem of false negative samples in contrastive learning. MSVQ uses two complementary ways to construct a soft label on-the-fly, including multiple augmented positive views and two momentum encoders forming various semantic features of negative samples. The similarity relationship calculations with negative samples are performed by two teacher networks and then transferred to the student network. This improves the student's ability to identify false negative samples in the dataset. MSVQ achieves high effectiveness and efficiency compared to classical methods on four benchmark image datasets."$"æ–‡ç« ä½œè€…ï¼šChen Peng, Xianzhong Long, Yun Li
ä½œè€…æœºæ„ï¼šå—äº¬é‚®ç”µå¤§å­¦è®¡ç®—æœºç§‘å­¦å­¦é™¢ï¼Œä¸­å›½å—äº¬210023
æ–‡ç« æ ‡é¢˜ï¼šMSVQï¼šä½¿ç”¨å¤šä¸ªæ ·æœ¬è§†å›¾å’Œé˜Ÿåˆ—çš„è‡ªç›‘ç£å­¦ä¹ 
æ‘˜è¦ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œåä¸ºâ€œå¤šä¸ªæ ·æœ¬è§†å›¾å’Œé˜Ÿåˆ—ï¼ˆMSVQï¼‰â€ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å¤šä¸ªå¢å¼ºè¿‡çš„æ­£æ ·æœ¬è§†å›¾å’Œä¸¤ä¸ªåŠ¨é‡ç¼–ç å™¨æ¥å½¢æˆå„ç§è´Ÿæ ·æœ¬çš„è¯­ä¹‰ç‰¹å¾ã€‚é€šè¿‡ä¸¤ä¸ªæ•™å¸ˆç½‘ç»œå¯¹è´Ÿæ ·æœ¬è¿›è¡Œç›¸ä¼¼åº¦å…³ç³»è®¡ç®—ï¼Œç„¶åå°†è¿™äº›çŸ¥è¯†ä¼ é€’ç»™å­¦ç”Ÿç½‘ç»œï¼Œä½¿å­¦ç”Ÿç½‘ç»œèƒ½å¤Ÿæ›´çµæ´»åœ°è¯†åˆ«æ•°æ®é›†ä¸­çš„é”™è¯¯è´Ÿæ ·æœ¬ã€‚åœ¨å››ä¸ªåŸºå‡†å›¾åƒæ•°æ®é›†ä¸Šå¯¹åˆ†ç±»ç»“æœè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ç›¸å¯¹äºå…¶ä»–ç»å…¸æ–¹æ³•çš„é«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚"$è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè‡ªç›‘ç£å­¦ä¹ çš„æ–°æ¡†æ¶â€”â€”å¤šæ ·æœ¬è§‚ç‚¹ä¸é˜Ÿåˆ—ï¼ˆMSVQï¼‰ï¼Œç”¨äºè§£å†³å¯¹æ¯”å­¦ä¹ ä¸­å‡é˜´æ€§æ ·æœ¬çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å¤šä¸ªå¢å¼ºçš„æ­£æ ·æœ¬è§†å›¾å’Œä¸¤ä¸ªåŠ¨é‡ç¼–ç å™¨å½¢æˆè´Ÿæ ·æœ¬çš„å„ç§è¯­ä¹‰ç‰¹å¾æ¥å®ç°å®æ—¶è½¯æ ‡ç­¾çš„è”åˆæ„å»ºã€‚ä¸¤ä¸ªæ•™å¸ˆç½‘ç»œæ‰§è¡Œä¸è´Ÿæ ·æœ¬çš„ç›¸ä¼¼åº¦å…³ç³»è®¡ç®—ï¼Œç„¶åå°†è¿™ä¸ªçŸ¥è¯†ä¼ é€’ç»™å­¦ç”Ÿã€‚åœ¨å››ä¸ªåŸºå‡†å›¾åƒæ•°æ®é›†ä¸Šçš„åˆ†ç±»ç»“æœè¡¨æ˜ï¼Œä¸ä¸€äº›ç»å…¸æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰é«˜æ•ˆæ€§å’Œé«˜æ•ˆæ€§çš„ä¼˜åŠ¿ã€‚$http://arxiv.org/pdf/2305.05370v1
"Restormer-Plus for Real World Image Deraining: One State-of-the-Art  Solution to the GT-RAIN Challenge (CVPR 2023 UG$^2$+ Track 3)"$"  This technical report presents our Restormer-Plus approach, which wassubmitted to the GT-RAIN Challenge (CVPR 2023 UG$^2$+ Track 3). Detailsregarding the challenge are available athttp://cvpr2023.ug2challenge.org/track3.html. Our Restormer-Plus outperformedall other submitted solutions in terms of peak signal-to-noise ratio (PSNR). Itconsists mainly of four modules: the single image de-raining module, the medianfiltering module, the weighted averaging module, and the post-processingmodule. We named the single-image de-raining module Restormer-X, which is builton Restormer and performed on each rainy image. The median filtering module isemployed as a median operator for the 300 rainy images associated with eachscene. The weighted averaging module combines the median filtering results withthat of Restormer-X to alleviate overfitting if we only use Restormer-X.Finally, the post-processing module is used to improve the brightnessrestoration. Together, these modules render Restormer-Plus to be onestate-of-the-art solution to the GT-RAIN Challenge. Our code is available athttps://github.com/ZJLAB-AMMI/Restormer-Plus."$Keywords: image deraining, Restormer-Plus, de-raining module, median filtering, weighted averaging, post-processing.$"ä½œè€…åï¼ˆæœºæ„ï¼‰: éƒ‘è¶…è¶…ï¼ˆæµ™æ±Ÿå®éªŒå®¤ï¼‰ã€ç‹é²å¹³ï¼ˆæµ™æ±Ÿå®éªŒå®¤ï¼‰ã€åˆ˜æ–Œï¼ˆæµ™æ±Ÿå®éªŒå®¤ï¼‰
æœ¬ç¯‡è®ºæ–‡é¢˜ä¸ºRestormer-Plus for Real World Image Deraining: One State-of-the-Art Solution to the GT-RAIN Challenge (CVPR 2023 UG2+ Track 3)ï¼Œæäº¤ç»™äº†CVPR 2023 UG2+ Track 3çš„GT-RAIN Challengeã€‚æœºæ„ä¸ºæµ™æ±Ÿå®éªŒå®¤åº”ç”¨æ•°å­¦ä¸æœºå™¨æ™ºèƒ½ç ”ç©¶ä¸­å¿ƒã€‚æœ¬æ–‡æå‡ºäº†åä¸ºRestormer-Plusçš„è§£å†³æ–¹æ¡ˆï¼Œå®ƒåŒ…å«å››ä¸ªæ¨¡å—ï¼šå•å›¾åƒå»é›¨æ¨¡å—ã€ä¸­å€¼æ»¤æ³¢æ¨¡å—ã€åŠ æƒå¹³å‡æ¨¡å—å’Œåå¤„ç†æ¨¡å—ã€‚æ­¤å¤–ï¼Œæ–‡ä¸­è¿˜ä»‹ç»äº†Restormer-Xçš„ä¸¤ç§ç‰ˆæœ¬ï¼Œå³åŸºæœ¬ç‰ˆæœ¬Restormerå’Œæ”¹è¿›ç‰ˆæœ¬Restormeryã€‚è¯¥æ–‡è®¤ä¸ºRestormer-Plusæ˜¯GT-RAIN Challengeçš„ä¸€ç§æœ€å…ˆè¿›çš„è§£å†³æ–¹æ¡ˆã€‚"$è¿™ç¯‡è®ºæ–‡ä¸­ä»‹ç»äº†ä¸€ç§åä¸ºRestormer-Plusçš„å›¾åƒå»é›¨æŠ€æœ¯ï¼Œæ˜¯é’ˆå¯¹CVPR 2023 UG2+ Track 3 GT-RAIN Challengeæäº¤çš„ç”³è¯·çš„ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚å…¶æ ¸å¿ƒæ¨¡å—åŒ…æ‹¬å•ä¸ªå›¾åƒå»é›¨æ¨¡å—ï¼Œä¸­å€¼æ»¤æ³¢æ¨¡å—ï¼ŒåŠ æƒå¹³å‡æ¨¡å—å’Œåå¤„ç†æ¨¡å—ã€‚é€šè¿‡ä½¿ç”¨è¿™äº›æ¨¡å—ï¼ŒRestormer-Plusåœ¨å³°å€¼ä¿¡å™ªæ¯”æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¢«è®¤ä¸ºæ˜¯è¯¥æŒ‘æˆ˜çš„æœ€ä½³è§£å†³æ–¹æ¡ˆä¹‹ä¸€ã€‚è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†Restormer-Plusçš„æ¯ä¸ªæ¨¡å—ï¼Œå¹¶è¯¦ç»†ä»‹ç»äº†è¯¥æ–¹æ³•çš„å®éªŒè®¾ç½®ã€‚è¯¥æ–¹æ³•åœ¨å»é™¤é›¨å¤©å›¾åƒæ–¹é¢å–å¾—äº†ä¼˜å¼‚æˆæœï¼Œå…¶ä»£ç ä¹Ÿå¯ä»¥é€šè¿‡å…¬å¼€ç½‘ç«™è¿›è¡Œè®¿é—®ã€‚$http://arxiv.org/pdf/2305.05454v1
Real-time instance segmentation with polygons using an  Intersection-over-Union loss$"  Predicting a binary mask for an object is more accurate but also morecomputationally expensive than a bounding box. Polygonal masks as developed inCenterPoly can be a good compromise. In this paper, we improve over CenterPolyby enhancing the classical regression L1 loss with a novel region-based lossand a novel order loss, as well as with a new training process for the verticesprediction head. Moreover, the previous methods that predict polygonal masksuse different coordinate systems, but it is not clear if one is better thananother, if we abstract the architecture requirement. We therefore investigatetheir impact on the prediction. We also use a new evaluation protocol withoracle predictions for the detection head, to further isolate the segmentationprocess and better compare the polygonal masks with binary masks. Our instancesegmentation method is trained and tested with challenging datasets containingurban scenes, with a high density of road users. Experiments show, inparticular, that using a combination of a regression loss and a region-basedloss allows significant improvements on the Cityscapes and IDD test setcompared to CenterPoly. Moreover the inference stage remains fast enough toreach real-time performance with an average of 0.045 s per frame for2048$\\times$1024 images on a single RTX 2070 GPU. The code is available$\\href{https://github.com/KatiaJDL/CenterPoly-v2}{\\text{here}}$."$Computer vision, real-time instance segmentation, polygons, Intersection-over-Union loss, urban scenes, mask approximation.$"ä½œè€…ï¼šKatia Jodogne-del Litto, Guillaume-Alexandre Bilodeauï¼ˆPolytechnique MontrÃ©alï¼‰
æœºæ„ï¼šLITIV Lab., Polytechnique MontrÃ©al
æ‘˜è¦ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨å¤šè¾¹å½¢å’ŒIoUæŸå¤±å®ç°å®æ—¶ç›®æ ‡å®ä¾‹åˆ†å‰²çš„æ–¹æ³•ã€‚ä¸Bounding Boxç›¸æ¯”ï¼Œå¤šè¾¹å½¢æ©ç å¯ä»¥æä¾›æ›´å‡†ç¡®çš„é¢„æµ‹ï¼Œä½†è®¡ç®—ä»£ä»·ä¹Ÿæ›´é«˜ã€‚ä¸ºäº†å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡å°†å›å½’L1æŸå¤±åŠ å¼ºï¼Œä»¥åŒ…æ‹¬ä¸€ç§æ–°çš„åŒºåŸŸæŸå¤±ã€ä¸€ç§æ–°çš„æ’åºæŸå¤±å’Œä¸€ç§é€‚ç”¨äºé¡¶ç‚¹é¢„æµ‹å¤´çš„æ–°çš„è®­ç»ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†ä¸åŒåæ ‡ç³»å¯¹å¤šè¾¹å½¢æ©ç é¢„æµ‹çš„å½±å“ã€‚æˆ‘ä»¬è¿ç”¨æ–°çš„è¯„ä¼°åè®®å¹¶åœ¨åŸå¸‚åœºæ™¯ä¸‹è¿›è¡Œå®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸CenterPolyç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„ç»„åˆå›å½’æŸå¤±å’ŒåŒºåŸŸæŸå¤±çš„æ–¹æ³•å¯¹Cityscapeså’ŒIDDæµ‹è¯•é›†æœ‰æ˜¾ç€çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒåŸºäºå®ä¾‹åˆ†å‰²çš„æ¨ç†é˜¶æ®µå¯åœ¨å•ä¸ªRTX 2070 GPUä¸Šä»¥å¹³å‡æ¯å¸§0.045ç§’çš„é€Ÿåº¦å®ç°å®æ—¶æ€§èƒ½ã€‚ä»£ç å¯åœ¨https://github.com/KatiaJDL/CenterPoly-v2ä¸­è·å¾—ã€‚"$æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§å®æ—¶å®ä¾‹åˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å¤šè¾¹å½¢æ¥ä»£æ›¿ä¼ ç»Ÿçš„äºŒè¿›åˆ¶æ©æ¨¡ï¼Œä»è€Œåœ¨å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼ºåŒ–ç»å…¸çš„å›å½’L1æŸå¤±ï¼Œå¹¶å¼•å…¥ä¸€ç§æ–°çš„é¢ç§¯æŸå¤±å’Œä¸€ç§æ–°çš„é¡ºåºæŸå¤±ï¼ŒåŒæ—¶ä½¿ç”¨æ–°çš„è®­ç»ƒæµç¨‹æ¥æ”¹å–„CenterPolyæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ç ”ç©¶äº†åœ¨ä¸åŒåæ ‡ç³»ç»Ÿä¸‹è¿›è¡Œé¢„æµ‹çš„å½±å“ï¼Œå¹¶ä½¿ç”¨äº†ä¸€ç§æ–°çš„è¯„ä¼°åè®®æ¥æ¯”è¾ƒå¤šè¾¹å½¢æ©æ¨¡å’ŒäºŒè¿›åˆ¶æ©æ¨¡çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨Cityscapeså’ŒIDDæ•°æ®é›†ä¸Šä½¿ç”¨è¯¥æ–¹æ³•ç›¸å¯¹äºCenterPolyå…·æœ‰æ˜¾è‘—çš„æ”¹è¿›ï¼Œè€Œä¸”åœ¨å•ä¸ªRTX 2070 GPUä¸Šå®æ—¶å¤„ç†æ•ˆç‡è¾¾åˆ°äº†0.045ç§’æ¯å¸§ã€‚$http://arxiv.org/pdf/2305.05490v1
Self-supervised dense representation learning for live-cell microscopy  with time arrow prediction$  State-of-the-art object detection and segmentation methods for microscopyimages rely on supervised machine learning, which requires laborious manualannotation of training data. Here we present a self-supervised method based ontime arrow prediction pre-training that learns dense image representations fromraw, unlabeled live-cell microscopy videos. Our method builds upon the task ofpredicting the correct order of time-flipped image regions via a single-imagefeature extractor and a subsequent time arrow prediction head. We show that theresulting dense representations capture inherently time-asymmetric biologicalprocesses such as cell divisions on a pixel-level. We furthermore demonstratethe utility of these representations on several live-cell microscopy datasetsfor detection and segmentation of dividing cells, as well as for cell stateclassification. Our method outperforms supervised methods, particularly whenonly limited ground truth annotations are available as is commonly the case inpractice. We provide code at https://github.com/weigertlab/tarrow.$Keywords: Self-supervised learning, Live-cell microscopy, Dense representation learning, Time arrow prediction, Object detection, Cell segmentation, Cell state classification.$"ä½œè€…ï¼šBenjamin Gallusser, Max Stieber, Martin Weigertï¼ˆÃ‰cole polytechnique fÃ©dÃ©rale de Lausanne (EPFL)ï¼‰
æœºæ„ï¼šÃ‰cole polytechnique fÃ©dÃ©rale de Lausanne (EPFL)"$è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè‡ªç›‘ç£å­¦ä¹ çš„æ–¹æ³•ï¼Œé€šè¿‡æ—¶é—´ç®­å¤´é¢„æµ‹æ¥å­¦ä¹ ä»åŸå§‹ã€æœªæ ‡è®°çš„æ´»ç»†èƒæ˜¾å¾®é•œè§†é¢‘ä¸­çš„å¯†é›†å›¾åƒè¡¨ç¤ºã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„æµ‹æ—¶é—´ç¿»è½¬å›¾åƒåŒºåŸŸçš„æ­£ç¡®é¡ºåºçš„ä»»åŠ¡ï¼Œé€šè¿‡å•å›¾åƒç‰¹å¾æå–å™¨å’Œéšåçš„æ—¶é—´ç®­å¤´é¢„æµ‹å¤´æ¥æ„å»ºã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€å¾—åˆ°çš„å¯†é›†è¡¨ç¤ºæ•æ‰äº†å…·æœ‰å†…åœ¨æ—¶é—´ä¸å¯¹ç§°æ€§çš„ç”Ÿç‰©è¿‡ç¨‹ï¼Œä¾‹å¦‚åƒç´ çº§çš„ç»†èƒåˆ†è£‚ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ´»ç»†èƒæ˜¾å¾®é•œæ•°æ®é›†ä¸Šç”¨äºåˆ†è£‚ç»†èƒçš„æ£€æµ‹å’Œåˆ†å‰²ï¼Œä»¥åŠç”¨äºç»†èƒçŠ¶æ€åˆ†ç±»ï¼Œå…¶æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„åŸºäºç›‘ç£å­¦ä¹ çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å®è·µä¸­åªæœ‰æœ‰é™çš„åœ°é¢çœŸç›¸æ ‡æ³¨å¯ç”¨çš„æƒ…å†µä¸‹ã€‚$http://arxiv.org/pdf/2305.05511v1
RMES: Real-Time Micro-Expression Spotting Using Phase From Riesz Pyramid$  Micro-expressions (MEs) are involuntary and subtle facial expressions thatare thought to reveal feelings people are trying to hide. ME spotting detectsthe temporal intervals containing MEs in videos. Detecting such quick andsubtle motions from long videos is difficult. Recent works leverage detailedfacial motion representations, such as the optical flow, and deep learningmodels, leading to high computational complexity. To reduce computationalcomplexity and achieve real-time operation, we propose RMES, a real-time MEspotting framework. We represent motion using phase computed by Riesz Pyramid,and feed this motion representation into a three-stream shallow CNN, whichpredicts the likelihood of each frame belonging to an ME. In comparison tooptical flow, phase provides more localized motion estimates, which areessential for ME spotting, resulting in higher performance. Using phase alsoreduces the required computation of the ME spotting pipeline by 77.8%. Despiteits relative simplicity and low computational complexity, our frameworkachieves state-of-the-art performance on two public datasets: CAS(ME)2 and SAMMLong Videos.$"Keywords: micro-expressions, real-time spotting, Riesz Pyramid, phase, CNN. 

Summary: This paper proposes a real-time micro-expression spotting framework called RMES that reduces computational complexity by using phase features extracted by a Riesz Pyramid in the front end with a lightweight three-stream shallow CNN in the back end. The proposed approach achieves state-of-the-art performance on two public datasets while reducing inference time by 77.8% compared to optical flow."$"ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šYini Fang, Didan Deng, Liang Wu, Frederic Jumellexy, and Bertram Shiï¼ˆé¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ŒYdentity Organizationï¼ŒBright Nation Limitedï¼‰ 

è®ºæ–‡æ ‡é¢˜ï¼šRMES: Real-Time Micro-Expression Spotting Using Phase From Riesz Pyramid"$è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºå®æ—¶å¾®è¡¨æƒ…æ£€æµ‹çš„RMESæ¡†æ¶ã€‚å¾®è¡¨æƒ…æ˜¯æ— æ„è¯†ã€ç»†å¾®çš„é¢éƒ¨è¡¨æƒ…ï¼Œæ­ç¤ºäººä»¬è¯•å›¾éšè—çš„æƒ…æ„Ÿã€‚è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å®æ—¶å¾®è¡¨æƒ…è¯†åˆ«æ¡†æ¶ï¼Œä½¿ç”¨Rieszé‡‘å­—å¡”è®¡ç®—çš„ç›¸ä½æ¥è¡¨ç¤ºè¿åŠ¨ï¼Œå¹¶å°†è¿™ç§è¿åŠ¨è¡¨ç¤ºä¸ºä¸€ä¸ªä¸‰æµçš„æµ…å±‚CNNï¼Œä»¥é¢„æµ‹æ¯ä¸€å¸§å±äºå¾®è¡¨æƒ…çš„å¯èƒ½æ€§ã€‚ä¸å…‰æµç›¸æ¯”ï¼Œç›¸ä½æä¾›æ›´å®šä½çš„è¿åŠ¨ä¼°è®¡ï¼Œå¯¹äºå¾®è¡¨æƒ…è¯†åˆ«è‡³å…³é‡è¦ï¼Œä»è€Œäº§ç”Ÿæ›´é«˜çš„æ€§èƒ½ã€‚ä½¿ç”¨ç›¸ä½è¿˜å¯ä»¥å°†å¾®è¡¨æƒ…æ£€æµ‹ç®¡é“çš„è®¡ç®—éœ€æ±‚é™ä½77.8%ã€‚å°½ç®¡å…¶ç›¸å¯¹ç®€å•å’Œä½è®¡ç®—å¤æ‚åº¦ï¼Œä½†æ­¤æ–¹æ³•åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šéƒ½å–å¾—äº†æœ€æ–°çš„æ€§èƒ½ã€‚$http://arxiv.org/pdf/2305.05523v1
EFE: End-to-end Frame-to-Gaze Estimation$  Despite the recent development of learning-based gaze estimation methods,most methods require one or more eye or face region crops as inputs and producea gaze direction vector as output. Cropping results in a higher resolution inthe eye regions and having fewer confounding factors (such as clothing andhair) is believed to benefit the final model performance. However, thiseye/face patch cropping process is expensive, erroneous, andimplementation-specific for different methods. In this paper, we propose aframe-to-gaze network that directly predicts both 3D gaze origin and 3D gazedirection from the raw frame out of the camera without any face or eyecropping. Our method demonstrates that direct gaze regression from the rawdownscaled frame, from FHD/HD to VGA/HVGA resolution, is possible despite thechallenges of having very few pixels in the eye region. The proposed methodachieves comparable results to state-of-the-art methods in Point-of-Gaze (PoG)estimation on three public gaze datasets: GazeCapture, MPIIFaceGaze, and EVE,and generalizes well to extreme camera view changes.$Keywords: gaze estimation, CNN, remote gaze estimation, frame-to-gaze network, eye tracking, data normalization, facial landmark detection.$"ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šHaldun Balimï¼ˆETH ZÃ¼richï¼‰ã€Seonwook Parkï¼ˆLunit Inc.ï¼‰ã€Xi Wangï¼ˆETH ZÃ¼richï¼‰ã€Xucong Zhangï¼ˆDelft University of Technologyï¼‰ã€Otmar Hilligesï¼ˆETH ZÃ¼richï¼‰ã€‚

æ ‡é¢˜ï¼šEFE: End-to-end Frame-to-Gaze Estimation"$æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç«¯åˆ°ç«¯çš„æ¡†æ¶åˆ°å‡è§†ä¼°è®¡æ–¹æ³•ï¼ˆEFEï¼‰ï¼Œå¯ä»¥ç›´æ¥ä»æ‘„åƒå¤´æ•è·çš„åŸå§‹å¸§é¢„æµ‹ç›®å…‰æ–¹å‘å’Œèµ·ç‚¹ï¼Œæ— éœ€é¢„å¤„ç†æ¨¡å—ã€‚ä¸ä¼ ç»Ÿçš„æ³¨è§†ä¼°è®¡æ–¹æ³•ç›¸æ¯”ï¼ŒEFEå¯ä»¥è·³è¿‡æ•°æ®è§„èŒƒåŒ–å’Œæ³¨è§†ä¼°è®¡å‰çš„äººè„¸å’Œé¢éƒ¨æ ‡å¿—æ£€æµ‹æ­¥éª¤ï¼Œå¹¶å®ç°å¯æ¯”è¾ƒçš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨GazeCaptureã€MPI-IFaceGazeå’ŒEVEä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šå®ç°äº†å¯é çš„Point-of-Gazeï¼ˆPoGï¼‰ä¼°è®¡ï¼Œå¹¶èƒ½å¤Ÿé€‚åº”æç«¯çš„ç›¸æœºè§†è§’å˜åŒ–ã€‚è™½ç„¶æ—©æœŸçš„æ–¹æ³•éœ€è¦ä¸€ä¸ªæˆ–å¤šä¸ªçœ¼ç›æˆ–é¢éƒ¨åŒºåŸŸè£å‰ªä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºæ³¨è§†æ–¹å‘å‘é‡ï¼Œä½†æ˜¯è¿™ç§çœ¼/é¢è¡¥ä¸è£å‰ªè¿‡ç¨‹æ˜¯æ˜‚è´µä¸”å®¹æ˜“å‡ºé”™çš„ã€‚$http://arxiv.org/pdf/2305.05526v1
ColonMapper: topological mapping and localization for colonoscopy$  Mapping and localization in endoluminal cavities from colonoscopies orgastroscopies has to overcome the challenge of significant shape andillumination changes between reobservations of the same endoluminal location.Instead of geometrical maps that strongly rely on a fixed scene geometry,topological maps are more adequate because they focus on visual placerecognition, i.e. the capability to determine if two video shots are imagingthe same location. We propose a topological mapping and localization systemable to operate on real human colonoscopies. The map is a graph where each nodecodes a colon location by a set of real images of that location. The edgesrepresent traversability between two nodes. For close-in-time images, wherescene changes are minor, place recognition can be successfully managed with therecent transformers-based image-matching algorithms. However, under long-termchanges -- such as different colonoscopies of the same patient -- feature-basedmatching fails. To address this, we propose a GeM global descriptor able toachieve high recall with significant changes in the scene. The addition of aBayesian filter processing the map graph boosts the accuracy of the long-termplace recognition, enabling relocalization in a previously built map. In theexperiments, we construct a map during the withdrawal phase of a firstcolonoscopy. Subsequently, we prove the ability to relocalize within this mapduring a second colonoscopy of the same patient two weeks later. Code andmodels will be available upon acceptance.$Key words: topological mapping, localization, colonoscopy, place recognition, feature-based matching, GeM global descriptor, Bayesian filter.$ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šJavier Morlana, Juan D. Tardoså’ŒJ.M.M. Montielï¼ˆè¥¿ç­ç‰™è¨æ‹‰æˆˆè¨å¤§å­¦ï¼‰ï¼šColonMapper: topological mapping and localization for colonoscopy$æœ¬æ–‡æå‡ºäº†ä¸€ç§é€‚ç”¨äºäººä½“ç»“è‚ é•œæ£€æµ‹çš„æ‹“æ‰‘å»ºå›¾å’Œå®šä½ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿå°†ç»“è‚ ä½ç½®ç¼–ç ä¸ºä¸€ç»„è¯¥ä½ç½®çš„å®é™…å›¾åƒï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªè¡¨ç¤ºç»“è‚ çŠ¶æ€çš„å›¾å½¢ï¼Œå¹¶è€ƒè™‘åˆ°äº†å›¾å½¢å˜åŒ–ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§å…¨å±€æè¿°æ–¹æ³•ï¼Œå¹¶é€šè¿‡è´å¶æ–¯æ»¤æ³¢å™¨å¤„ç†åœ°å›¾å›¾å½¢ï¼Œå®ç°äº†é•¿æ—¶é—´å®šä½ã€‚å®éªŒè¡¨æ˜è¯¥ç³»ç»Ÿå¯ä»¥åœ¨åŒä¸€æ‚£è€…çš„ä¸¤æ¬¡ç»“è‚ é•œæ£€æµ‹ä¸­å®ç°é‡å®šä½ã€‚è¯¥ç³»ç»Ÿæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„ç»“è‚ é•œæ£€æµ‹æ–¹æ³•ï¼Œæœ‰åŠ©äºæé«˜ä¸´åºŠæ²»ç–—çš„å‡†ç¡®æ€§ã€‚$http://arxiv.org/pdf/2305.05546v1
Group Activity Recognition via Dynamic Composition and Interaction$  Previous group activity recognition approaches were limited to reasoningusing human relations or finding important subgroups and tended to ignoreindispensable group composition and human-object interactions. This absencemakes a partial interpretation of the scene and increases the interference ofirrelevant actions on the results. Therefore, we propose our DynamicFormer withDynamic composition Module (DcM) and Dynamic interaction Module (DiM) to modelrelations and locations of persons and discriminate the contribution ofparticipants, respectively. Our findings on group composition and human-objectinteraction inspire our core idea. Group composition tells us the location ofpeople and their relations inside the group, while interaction reflects therelation between humans and objects outside the group. We utilize spatial andtemporal encoders in DcM to model our dynamic composition and build DiM toexplore interaction with a novel GCN, which has a transformer inside toconsider the temporal neighbors of human/object. Also, a Multi-level DynamicIntegration is employed to integrate features from different levels. We conductextensive experiments on two public datasets and show that our method achievesstate-of-the-art.$Field: Group Activity Recognition, Dynamic Composition, Dynamic Interaction, Human-Object Interaction, Spatial and Temporal Encoders, Graph Convolutional Network.$"ä½œè€…ï¼šYouliang Zhang, Zhuo Zhou, Wenxuan Liu, Danni Xu, Zheng Wang
æœºæ„ï¼šæ­¦æ±‰å¤§å­¦ï¼Œæ­¦æ±‰ç†å·¥å¤§å­¦ï¼Œæ–°åŠ å¡å›½ç«‹å¤§å­¦"$æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåŠ¨æ€ç»„åˆå’Œäº¤äº’çš„å›¢ä½“æ´»åŠ¨è¯†åˆ«æ–¹æ³•ã€‚ä¹‹å‰çš„å›¢ä½“æ´»åŠ¨è¯†åˆ«æ–¹æ³•ä»…é™äºä½¿ç”¨äººé™…å…³ç³»è¿›è¡Œæ¨ç†æˆ–æ‰¾åˆ°é‡è¦å­ç»„ï¼Œè€Œå¾€å¾€å¿½ç•¥äº†ä¸å¯æˆ–ç¼ºçš„å›¢é˜Ÿç»„æˆå’Œäººç‰©å¯¹è±¡äº¤äº’ï¼Œç¼ºä¹å…¨é¢çš„åœºæ™¯è§£é‡Šï¼Œå¢åŠ äº†æ— å…³åŠ¨ä½œå¯¹ç»“æœçš„å¹²æ‰°ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†åŠ¨æ€ç»„åˆæ¨¡å—(DcM)å’ŒåŠ¨æ€äº¤äº’æ¨¡å—(DiM)æ¥åˆ†åˆ«æ¨¡æ‹Ÿäººç‰©ä¹‹é—´çš„å…³ç³»å’Œä½ç½®ï¼Œä»¥åŠå‚ä¸è€…çš„è´¡çŒ®ã€‚åŠ¨æ€ç»„æˆå’Œäººç‰©å¯¹è±¡äº¤äº’å¯ç¤ºäº†æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³ã€‚å›¢é˜Ÿç»„æˆå‘Šè¯‰æˆ‘ä»¬äººå‘˜çš„ä½ç½®å’Œä»–ä»¬åœ¨å›¢é˜Ÿå†…çš„å…³ç³»ï¼Œè€Œäº¤äº’åæ˜ äº†äººç±»ä¸å›¢é˜Ÿå¤–çš„ç‰©ä½“ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬åˆ©ç”¨DcMä¸­çš„ç©ºé—´å’Œæ—¶é—´ç¼–ç å™¨æ¥æ¨¡æ‹ŸåŠ¨æ€ç»„åˆï¼Œå¹¶å»ºç«‹DiMæ¥æ¢ç´¢ä¸æ–°å‹GCNçš„äº¤äº’ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªè½¬æ¢å™¨æ¥è€ƒè™‘äºº/å¯¹è±¡çš„æ—¶é—´é‚»å±…ã€‚åŒæ—¶ï¼Œæœ¬æ–‡é‡‡ç”¨å¤šçº§åŠ¨æ€é›†æˆæ¥é›†æˆä¸åŒçº§åˆ«çš„ç‰¹å¾ã€‚æœ¬æ–‡åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œå¹¶è¡¨æ˜å…¶æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ã€‚$http://arxiv.org/pdf/2305.05583v1
Region-based Contrastive Pretraining for Medical Image Retrieval with  Anatomic Query$  We introduce a novel Region-based contrastive pretraining for Medical ImageRetrieval (RegionMIR) that demonstrates the feasibility of medical imageretrieval with similar anatomical regions. RegionMIR addresses two majorchallenges for medical image retrieval i) standardization of clinicallyrelevant searching criteria (e.g., anatomical, pathology-based), and ii)localization of anatomical area of interests that are semantically meaningful.In this work, we propose an ROI image retrieval image network that retrievesimages with similar anatomy by extracting anatomical features (via boundingboxes) and evaluate similarity between pairwise anatomy-categorized featuresbetween the query and the database of images using contrastive learning. ROIqueries are encoded using a contrastive-pretrained encoder that was fine-tunedfor anatomy classification, which generates an anatomical-specific latent spacefor region-correlated image retrieval. During retrieval, we compare theanatomically encoded query to find similar features within a feature databasegenerated from training samples, and retrieve images with similar regions fromtraining samples. We evaluate our approach on both anatomy classification andimage retrieval tasks using the Chest ImaGenome Dataset. Our proposed strategyyields an improvement over state-of-the-art pretraining and co-trainingstrategies, from 92.24 to 94.12 (2.03%) classification accuracy in anatomies.We qualitatively evaluate the image retrieval performance demonstratinggeneralizability across multiple anatomies with different morphology.$Medical image retrieval, Region-based contrastive pretraining, Anatomic query, Region-Of-Interest (ROI), contrastive learning, anatomy classification, Chest ImaGenome Dataset.$"ä½œè€…ï¼šHo Hin Leeï¼ˆ1ï¼‰ï¼ŒAlberto Santamaria-Pangï¼ˆ2ï¼‰ï¼ŒJameson Merkowï¼ˆ2ï¼‰ï¼ŒOzan Oktayï¼ˆ3ï¼‰ï¼ŒFernando PÃ©rez-GarcÃ­aï¼ˆ3ï¼‰ï¼ŒJavier Alvarez-Valleï¼ˆ3ï¼‰ï¼ŒIvan Tarapovï¼ˆ2ï¼‰
æœºæ„ï¼š1 Vanderbilt Universityï¼Œ2 Microsoft Health AIï¼Œ3 Microsoft Health Futures"$æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºåŒºåŸŸå¯¹æ¯”é¢„è®­ç»ƒçš„åŒ»å­¦å›¾åƒæ£€ç´¢æ–¹æ³•(RegionMIR)ï¼Œå®ƒå±•ç¤ºäº†åˆ©ç”¨ç±»ä¼¼è§£å‰–åŒºåŸŸè¿›è¡ŒåŒ»å­¦å›¾åƒæ£€ç´¢çš„å¯è¡Œæ€§ã€‚RegionMIRè§£å†³äº†åŒ»å­¦å›¾åƒæ£€ç´¢ä¸­çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼ši)æ ‡å‡†åŒ–ä¸´åºŠç›¸å…³æœç´¢æ ‡å‡†(ä¾‹å¦‚ï¼ŒåŸºäºè§£å‰–å­¦ã€ç—…ç†å­¦çš„æ ‡å‡†)ï¼Œii)å®šä½å…·æœ‰è¯­ä¹‰æ„ä¹‰çš„è§£å‰–åŒºåŸŸã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨åŸºäºå…´è¶£åŒºåŸŸ(ROI)çš„å›¾åƒæœç´¢ï¼Œåœ¨è§„æ¨¡ä¸Šå·¥ä½œï¼Œä½¿ä¸´åºŠåŒ»ç”Ÿèƒ½å¤Ÿæœç´¢å¹¶æ£€ç´¢é€‰å®šçš„ä¸ç›¸åŒè§£å‰–å­¦å’Œ/æˆ–ç›¸ä¼¼ç—…ç†çŠ¶æ€å¯¹åº”çš„ROIsã€‚ä¸ä»¥å‰çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ•è·äº†ç‰¹å®šè§£å‰–åŒºåŸŸçš„ç»†èŠ‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ROIå›¾åƒæ£€ç´¢å›¾åƒç½‘ç»œï¼Œé€šè¿‡æå–è§£å‰–ç‰¹å¾(é€šè¿‡è¾¹ç•Œæ¡†)æ¥æ£€ç´¢å…·æœ‰ç›¸ä¼¼è§£å‰–ç»“æ„çš„å›¾åƒï¼Œå¹¶ä½¿ç”¨å¯¹æ¯”å­¦ä¹ åœ¨æŸ¥è¯¢ä¸å›¾åƒæ•°æ®åº“ä¹‹é—´æ¯”è¾ƒè§£å‰–å­¦ç±»åˆ«ç‰¹å¾å¯¹é—´çš„ç›¸ä¼¼æ€§ã€‚åœ¨é¢„è®­ç»ƒç¼–ç å™¨çš„å¯¹æ¯”å¾®è°ƒä¸‹ï¼ŒROIæŸ¥è¯¢è¢«ç¼–ç ï¼Œç”Ÿæˆä¸åŒºåŸŸç›¸å…³çš„å›¾åƒæ£€ç´¢çš„è§£å‰–ç‰¹å®šæ½œåœ¨ç©ºé—´ã€‚åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒè§£å‰–ç¼–ç çš„æŸ¥è¯¢ï¼Œä»¥æ‰¾åˆ°ä¸è®­ç»ƒæ ·æœ¬ä¸­å…·æœ‰ç›¸ä¼¼ç‰¹å¾çš„å›¾åƒï¼Œç„¶åä»è®­ç»ƒæ ·æœ¬ä¸­æ£€ç´¢å…·æœ‰ç›¸ä¼¼åŒºåŸŸçš„å›¾åƒã€‚æˆ‘ä»¬åœ¨Chest ImaGenomeæ•°æ®é›†ä¸Šå¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†è§£å‰–å­¦åˆ†ç±»å’Œå›¾åƒæ£€ç´¢ä»»åŠ¡çš„è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç­–ç•¥å°†è§£å‰–å­¦åˆ†ç±»çš„å‡†ç¡®æ€§ä»92.24æé«˜åˆ°94.12(2.03%)ï¼Œå¹¶ä¸”æˆ‘ä»¬åœ¨å¤šä¸ªå…·æœ‰ä¸åŒå½¢æ€çš„è§£å‰–å­¦ä¸Š qualitatively è¯„ä¼°äº†å›¾åƒæ£€ç´¢æ€§èƒ½çš„æ³›åŒ–èƒ½åŠ›ã€‚$http://arxiv.org/pdf/2305.05598v1
Privacy-Preserving Collaborative Chinese Text Recognition with Federated  Learning$  In Chinese text recognition, to compensate for the insufficient local dataand improve the performance of local few-shot character recognition, it isoften necessary for one organization to collect a large amount of data fromsimilar organizations. However, due to the natural presence of privateinformation in text data, different organizations are unwilling to shareprivate data, such as addresses and phone numbers. Therefore, it becomesincreasingly important to design a privacy-preserving collaborative trainingframework for the Chinese text recognition task. In this paper, we introducepersonalized federated learning (pFL) into the Chinese text recognition taskand propose the pFedCR algorithm, which significantly improves the modelperformance of each client (organization) without sharing private data.Specifically, based on CRNN, to handle the non-iid problem of client data, weadd several attention layers to the model and design a two-stage trainingapproach for the client. In addition, we fine-tune the output layer of themodel using a virtual dataset on the server, mitigating the problem ofcharacter imbalance in Chinese documents. The proposed approach is validated onpublic benchmarks and two self-built real-world industrial scenario datasets.The experimental results show that the pFedCR algorithm can improve theperformance of local personalized models while also improving theirgeneralization performance on other client data domains. Compared to localtraining within an organization, pFedCR improves model performance by about20%. Compared to other state-of-the-art personalized federated learningmethods, pFedCR improves performance by 6%~8%. Moreover, through federatedlearning, pFedCR can correct erroneous information in the ground truth.$Keywords: Privacy preservation, Collaborative learning, Chinese text recognition, Federated learning, Personalized federated learning, Character imbalance.$"ä½œè€…ï¼šè‹ä¸Šè¶…ï¼Œäºæµ·æ´‹ï¼Œææ–Œï¼Œè–›ç¥¥é˜³ï¼ˆå¤æ—¦å¤§å­¦è®¡ç®—æœºå­¦é™¢ï¼‰

æœºæ„ï¼šå¤æ—¦å¤§å­¦è®¡ç®—æœºç§‘å­¦ç³»"$æœ¬æ–‡æå‡ºäº†ä¸€ç§éšç§ä¿æŠ¤çš„åä½œè®­ç»ƒæ¡†æ¶pFedCRï¼Œç”¨äºæ”¹è¿›ä¸­æ–‡æ–‡æœ¬è¯†åˆ«ä»»åŠ¡ä¸­å±€éƒ¨å®¢æˆ·ç«¯ï¼ˆç»„ç»‡ï¼‰çš„æ¨¡å‹æ€§èƒ½ï¼Œè€Œä¸éœ€è¦å…±äº«ç§æœ‰æ•°æ®ã€‚è¯¥æ¡†æ¶åŸºäºä¸ªæ€§åŒ–è”åˆå­¦ä¹ ï¼ˆpFLï¼‰ï¼Œåœ¨CRNNåŸºç¡€ä¸Šå¼•å…¥äº†å¤šä¸ªæ³¨æ„åŠ›å±‚æ¥å¤„ç†å®¢æˆ·ç«¯æ•°æ®çš„ä¸å¹³è¡¡æ€§ï¼Œå¹¶é‡‡ç”¨è™šæ‹Ÿæ•°æ®é›†æ¥è§£å†³å­—ç¬¦ä¸å¹³è¡¡é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒpFedCRç®—æ³•ä¸ä»…å¯ä»¥æ”¹è¿›æœ¬åœ°ä¸ªæ€§åŒ–æ¨¡å‹çš„æ€§èƒ½ï¼Œè¿˜å¯ä»¥æé«˜å…¶åœ¨å…¶ä»–å®¢æˆ·ç«¯æ•°æ®é¢†åŸŸçš„æ³›åŒ–æ€§èƒ½ã€‚ä¸æœ¬åœ°è®­ç»ƒç›¸æ¯”ï¼ŒpFedCRå¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½çº¦20ï¼…ï¼Œå¹¶ä¸”æ¯”å…¶ä»–æœ€å…ˆè¿›çš„ä¸ªæ€§åŒ–è”åˆå­¦ä¹ æ–¹æ³•æé«˜äº†6ï¼…~8ï¼…çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€šè¿‡è”åˆå­¦ä¹ ï¼ŒpFedCRå¯ä»¥çº æ­£åœ°é¢çœŸå®æ€§ä¸­çš„é”™è¯¯ä¿¡æ¯ã€‚$http://arxiv.org/pdf/2305.05602v1
SwinIA: Self-Supervised Blind-Spot Image Denoising with Zero  Convolutions$  The essence of self-supervised image denoising is to restore the signal fromthe noisy image alone. State-of-the-art solutions for this task rely on theidea of masking pixels and training a fully-convolutional neural network toimpute them. This most often requires multiple forward passes, informationabout the noise model, and intricate regularization functions. In this paper,we propose a Swin Transformer-based Image Autoencoder (SwinIA), the firstconvolution-free architecture for self-supervised denoising. It can be trainedend-to-end with a simple mean squared error loss without masking and does notrequire any prior knowledge about clean data or noise distribution. Despite itssimplicity, SwinIA establishes state-of-the-art on several common benchmarks.$Image denoising, self-supervised learning, Swin Transformer, convolution-free, blind-spot network, neural networks, deep learning, noise model, regularization functions, mean squared error loss, benchmarks.$"ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šMikhail Papkov, Pavel Chizhovï¼ˆå¡”å°”å›¾å¤§å­¦è®¡ç®—æœºç§‘å­¦ç ”ç©¶æ‰€ï¼‰
è®ºæ–‡æ ‡é¢˜ï¼šSwinIA: Self-Supervised Blind-Spot Image Denoising with Zero Convolutions"$è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„è‡ªç›‘ç£å›¾åƒå»å™ªç®—æ³•ï¼ŒSwinIAã€‚ç°æœ‰çš„ç®—æ³•å¤šä¾èµ–äºé®ç›–åƒç´ å’Œå·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹è®­ç»ƒæ¥æ¨æ–­ç¼ºå¤±åƒç´ ï¼Œä½†è¿™ä¼šå¯¼è‡´è€—æ—¶ã€éœ€è¦å…ˆå‰çš„çŸ¥è¯†å’Œå¤æ‚çš„æ­£åˆ™åŒ–å‡½æ•°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒSwinIAæ˜¯ç¬¬ä¸€ä¸ªæ— éœ€å·ç§¯çš„æ¶æ„ï¼Œå¯ä»¥é€šè¿‡ç®€å•çš„å‡æ–¹è¯¯å·®æŸå¤±è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œæ— éœ€é®ç›–åƒç´ å¹¶ä¸”ä¸éœ€è¦å…³äºå™ªå£°åˆ†å¸ƒçš„å…ˆå‰çŸ¥è¯†ã€‚è¯¥ç®—æ³•åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å¥½çš„æ•ˆæœã€‚è®ºæ–‡åŒæ—¶ä»‹ç»äº†å›¾åƒå»å™ªçš„é‡è¦æ€§å’Œç°æœ‰çš„è§£å†³æ–¹æ¡ˆï¼Œè¯´æ˜è‡ªç›‘ç£å»å™ªçš„ä¼˜åŠ¿åœ¨äºæ— éœ€æˆå¯¹çš„å¹²å‡€/å™ªå£°å›¾åƒï¼Œè€Œæ˜¯ä»è®­ç»ƒé›†æœ¬èº«å­¦ä¹ ã€‚$http://arxiv.org/pdf/2305.05651v1
InternChat: Solving Vision-Centric Tasks by Interacting with Chatbots  Beyond Language$  We present an interactive visual framework named InternChat, or iChat forshort. The framework integrates chatbots that have planning and reasoningcapabilities, such as ChatGPT, with non-verbal instructions like pointingmovements that enable users to directly manipulate images or videos on thescreen. Pointing (including gestures, cursors, etc.) movements can provide moreflexibility and precision in performing vision-centric tasks that requirefine-grained control, editing, and generation of visual content. The nameInternChat stands for interaction, nonverbal, and chatbots. Different fromexisting interactive systems that rely on pure language, by incorporatingpointing instructions, the proposed iChat significantly improves the efficiencyof communication between users and chatbots, as well as the accuracy ofchatbots in vision-centric tasks, especially in complicated visual scenarioswhere the number of objects is greater than 2. Additionally, in iChat, anauxiliary control mechanism is used to improve the control capability of LLM,and a large vision-language model termed Husky is fine-tuned for high-qualitymulti-modal dialogue (impressing ChatGPT-3.5-turbo with 93.89% GPT-4 Quality).We hope this work can spark new ideas and directions for future interactivevisual systems. Welcome to watch the code athttps://github.com/OpenGVLab/InternChat.$Keywords: Vision-centric tasks, Chatbots, Non-verbal instructions, Pointing movements, Interactive visual framework, Planning, Reasoning, Language models, Image manipulation, Video editing.$"ä½œè€…ï¼šZhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Qinglong Zhang, Yang Yang, Qingyun Li, Jiashuo Yu, Kunchang Li, Zhe Chen, Xue Yang, Xizhou Zhu, Yali Wang, Limin Wang, Ping Luo, Jifeng Dai, Yu Qiao
æœºæ„ï¼šOpenGVLab, Shanghai AI Laboratory; The University of Hong Kong; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Nanjing University; SenseTime Research; Tsinghua University
GitHubé“¾æ¥ï¼šhttps://github.com/OpenGVLab/InternChat"$æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºInternChatçš„äº¤äº’å¼è§†è§‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å…·æœ‰è®¡åˆ’å’Œæ¨ç†èƒ½åŠ›çš„èŠå¤©æœºå™¨äººï¼ˆä¾‹å¦‚ChatGPTï¼‰å’Œéè¯­è¨€æŒ‡ä»¤ï¼ˆå¦‚æŒ‡å‘æ€§åŠ¨ä½œï¼‰ï¼Œä½¿ç”¨æˆ·å¯ä»¥ç›´æ¥æ“çºµå±å¹•ä¸Šçš„å›¾åƒæˆ–è§†é¢‘ã€‚æŒ‡å‘æ€§åŠ¨ä½œæä¾›äº†æ›´å¤§çš„çµæ´»æ€§å’Œç²¾åº¦ï¼Œå¯¹äºéœ€è¦ç²¾ç»†æ§åˆ¶ã€ç¼–è¾‘å’Œç”Ÿæˆè§†è§‰å†…å®¹çš„è§†è§‰ä¸­å¿ƒä»»åŠ¡å°¤ä¸ºé‡è¦ã€‚ä¸åŒäºç°æœ‰çš„çº¯è¯­è¨€äº¤äº’ç³»ç»Ÿï¼Œé€šè¿‡åŠ å…¥æŒ‡å‘æ€§æŒ‡ä»¤ï¼Œæœ¬æ–‡æå‡ºçš„iChatæ˜¾è‘—æ”¹å–„äº†ç”¨æˆ·å’ŒèŠå¤©æœºå™¨äººä¹‹é—´çš„æ²Ÿé€šæ•ˆç‡ï¼Œä»¥åŠèŠå¤©æœºå™¨äººåœ¨å…·æœ‰å¤§é‡å¯¹è±¡çš„å¤æ‚è§†è§‰åœºæ™¯ä¸­çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒiChatä½¿ç”¨äº†è¾…åŠ©æ§åˆ¶æœºåˆ¶æ¥æé«˜LLMçš„æ§åˆ¶èƒ½åŠ›ï¼Œå¹¶å¯¹ä¸€ä¸ªåä¸ºHuskyçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œä»¥å®ç°é«˜è´¨é‡çš„å¤šæ¨¡æ€å¯¹è¯ã€‚å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½å¼•å‘æœªæ¥äº¤äº’å¼è§†è§‰ç³»ç»Ÿçš„æ–°æƒ³æ³•å’Œæ–¹å‘ã€‚$http://arxiv.org/pdf/2305.05662v1
Indoor Localization and Multi-person Tracking Using Privacy Preserving  Distributed Camera Network with Edge Computing$  Localization of individuals in a built environment is a growing researchtopic. Estimating the positions, face orientation (or gaze direction) andtrajectories of people through space has many uses, such as in crowdmanagement, security, and healthcare. In this work, we present an open-source,low-cost, scalable and privacy-preserving edge computing framework formulti-person localization, i.e. estimating the positions, orientations, andtrajectories of multiple people in an indoor space. Our computing frameworkconsists of 38 Tensor Processing Unit (TPU)-enabled edge computing camerasystems placed in the ceiling of the indoor therapeutic space. The edge computesystems are connected to an on-premise fog server through a secure and privatenetwork. A multi-person detection algorithm and a pose estimation model run onthe edge TPU in real-time to collect features which are used, instead of rawimages, for downstream computations. This ensures the privacy of individuals inthe space, reduces data transmission/storage and improves scalability. Weimplemented a Kalman filter-based multi-person tracking method and astate-of-the-art body orientation estimation method to determine the positionsand facing orientations of multiple people simultaneously in the indoor space.For our study site with size of 18,000 square feet, our system demonstrated anaverage localization error of 1.41 meters, a multiple-object tracking accuracyscore of 62%, and a mean absolute body orientation error of 29{\\deg}, which issufficient for understanding group activity behaviors in indoor environments.Additionally, our study provides practical guidance for deploying the proposedsystem by analyzing various elements of the camera installation with respect totracking accuracy.$Indoor localization, multi-person tracking, privacy-preserving, edge computing, Tensor Processing Unit (TPU), Kalman filter, body orientation estimation, group activity behaviors.$ä½œè€…ï¼šHYEOKHYEN KWONï¼ˆEmory Universityï¼‰ï¼ŒCHAITRA HEDGEï¼ˆGeorgia Institute of Technologyï¼‰ï¼ŒYASHAR KIARASHIï¼ˆEmory Universityï¼‰ï¼ŒVENKATA SIVA KRISHNA MADALAï¼ˆGeorgia Institute of Technologyï¼‰ï¼ŒRATAN SINGHï¼ˆGeorgia Institute of Technologyï¼‰ï¼ŒARJUNSINH NAKUMï¼ˆGeorgia Institute of Technologyï¼‰ï¼ŒROBERT TWEEDYï¼ˆEmory Universityï¼‰ï¼ŒLEANDRO MILETTO TONETTOï¼ˆGeorgia Institute of Technologyï¼‰ï¼ŒCRAIG M. ZIMRINGï¼ˆGeorgia Institute of Technologyï¼‰ï¼ŒGARI D. CLIFFORDï¼ˆEmory Universityï¼‰ã€‚ æœºæ„ï¼šEmory Universityï¼ŒGeorgia Institute of Technologyã€‚$æœ¬ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºå¤šäººå®¤å†…å®šä½å’Œè¿½è¸ªçš„éšç§ä¿æŠ¤åˆ†å¸ƒå¼ç›¸æœºç½‘ç»œä¸è¾¹ç¼˜è®¡ç®—çš„ç³»ç»Ÿæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨ä½æˆæœ¬è¾¹ç¼˜è®¡ç®—ç›¸æœºç³»ç»Ÿï¼Œå®ç°äº†å¯¹å¤šäººçš„å®æ—¶ã€ç²¾ç¡®å®šä½å’Œé¢å‘æ–¹å‘è¿½è¸ªã€‚è¯¥ç›¸æœºç³»ç»Ÿä¸æœ¬åœ°é›¾æœåŠ¡å™¨è¿æ¥ï¼Œå¯ä»¥ä¿è¯æ•°æ®çš„å®‰å…¨æ€§å’Œéšç§æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç³»ç»Ÿå®ç°äº†å¡å°”æ›¼æ»¤æ³¢çš„æƒ…å†µä¸‹ï¼Œè¯¥ç³»ç»Ÿå¯ä»¥è¾¾åˆ°å¹³å‡å®šä½è¯¯å·®1.41ç±³ã€å¤šå¯¹è±¡è¿½è¸ªå‡†ç¡®æ€§å¾—åˆ†62%å’Œå¹³å‡ç»å¯¹äººä½“æ–¹å‘è¯¯å·®29Â°çš„æ•ˆæœã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è®¨è®ºäº†ç³»ç»Ÿçš„å®é™…éƒ¨ç½²ï¼Œå¹¶å¯¹ç›¸æœºçš„å¸ƒå±€å’Œç¯å¢ƒå› ç´ è¿›è¡Œäº†åˆ†æï¼Œä»¥ä¾¿æé«˜å¤šäººè¿½è¸ªçš„å‡†ç¡®ç‡ã€‚$http://arxiv.org/pdf/2305.05062v1
Atmospheric Turbulence Correction via Variational Deep Diffusion$  Atmospheric Turbulence (AT) correction is a challenging restoration task asit consists of two distortions: geometric distortion and spatially variantblur. Diffusion models have shown impressive accomplishments in photo-realisticimage synthesis and beyond. In this paper, we propose a novel deep conditionaldiffusion model under a variational inference framework to solve the ATcorrection problem. We use this framework to improve performance by learninglatent prior information from the input and degradation processes. We use thelearned information to further condition the diffusion model. Experiments areconducted in a comprehensive synthetic AT dataset. We show that the proposedframework achieves good quantitative and qualitative results.$Atmospheric turbulence correction, variational inference, deep diffusion model, image restoration, spatially variant blur, geometrical distortion, deep learning, image synthesis.$"ä½œè€…ï¼šXijun Wang, Santiago LÃ³pez-Tapia, Aggelos K. Katsaggelos

æœºæ„ï¼š1åŒ—è¥¿å¤§å­¦è®¡ç®—æœºç§‘å­¦ç³»ï¼Œä¼Šä¸‡æ–¯é¡¿ï¼Œç¾å›½ï¼›2åŒ—è¥¿å¤§å­¦ç”µæ°”å’Œè®¡ç®—æœºå·¥ç¨‹ç³»ï¼Œä¼Šä¸‡æ–¯é¡¿ï¼Œç¾å›½ã€‚"$æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºå˜åˆ†æ·±åº¦æ‰©æ•£æ¨¡å‹çš„å¤§æ°”æ¹æµæ ¡æ­£æ–¹æ³•ã€‚è¿™æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå®ƒæ¶‰åŠåˆ°ä¸¤ç§æ‰­æ›²ï¼šå‡ ä½•å½¢å˜å’Œç©ºé—´å˜å¼‚çš„æ¨¡ç³Šã€‚æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®åœ¨äºï¼Œé¦–æ¬¡å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºè§£å†³æ³›æ³›çš„å¤§æ°”æ¹æµæ ¡æ­£é—®é¢˜ã€‚åŒæ—¶ï¼Œå¼•å…¥äº†ä¸€ç§å˜åˆ†æ¨ç†å›¾åƒå¤åŸæ¡†æ¶ï¼Œä»è¾“å…¥å’Œé€€åŒ–è¿‡ç¨‹ä¸­å­¦ä¹ ä¸ä»»åŠ¡æœ‰å…³çš„æ½œåœ¨å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶å°†å…¶ä½œä¸ºæ¡ä»¶æ³¨å…¥åˆ°æ‰©æ•£æ¨¡å‹ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ·±åº¦æ‰©æ•£æ¨¡å‹åœ¨åˆæˆå¤§æ°”æ¹æµæ•°æ®é›†ä¸Šå…·æœ‰ä¼˜å¼‚çš„å®šé‡å’Œå®šæ€§è¡¨ç°ã€‚$http://arxiv.org/pdf/2305.05077v1
Less is More: Removing Text-regions Improves CLIP Training Efficiency  and Robustness$  The CLIP (Contrastive Language-Image Pre-training) model and its variants arebecoming the de facto backbone in many applications. However, training a CLIPmodel from hundreds of millions of image-text pairs can be prohibitivelyexpensive. Furthermore, the conventional CLIP model doesn\'t differentiatebetween the visual semantics and meaning of text regions embedded in images.This can lead to non-robustness when the text in the embedded region doesn\'tmatch the image\'s visual appearance. In this paper, we discuss two effectiveapproaches to improve the efficiency and robustness of CLIP training: (1)augmenting the training dataset while maintaining the same number ofoptimization steps, and (2) filtering out samples that contain text regions inthe image. By doing so, we significantly improve the classification andretrieval accuracy on public benchmarks like ImageNet and CoCo. Filtering outimages with text regions also protects the model from typographic attacks. Toverify this, we build a new dataset named ImageNet with Adversarial TextRegions (ImageNet-Attr). Our filter-based CLIP model demonstrates a top-1accuracy of 68.78\\%, outperforming previous models whose accuracy was all below50\\%.$Keywords: CLIP, text-regions, training efficiency, robustness, visual semantics, augmentation, filtering, classifcation, retrieval accuracy, ImageNet, typographic attacks.$"ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šLiangliang Cao, Bowen Zhang, Chen Chen, Yinfei Yang,
Xianzhi Du, Wencong Zhang, Zhiyun Lu, Yantao Zheng ï¼ˆApple AI/MLï¼‰"$æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•æé«˜CLIPæ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œé²æ£’æ€§ï¼Œå› ä¸ºä»æ•°ä»¥äº¿è®¡çš„å›¾åƒ-æ–‡æœ¬å¯¹è®­ç»ƒCLIPæ¨¡å‹æ˜¯æ˜‚è´µçš„ã€‚ä½œè€…æå‡ºäº†ä¸¤ç§æœ‰æ•ˆçš„æ–¹æ³•æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼š(1)åœ¨ä¿æŒç›¸åŒä¼˜åŒ–æ­¥éª¤æ•°é‡çš„æƒ…å†µä¸‹å¢åŠ è®­ç»ƒæ•°æ®é›†ï¼Œ(2)è¿‡æ»¤æ‰åŒ…å«å›¾åƒæ–‡æœ¬åŒºåŸŸçš„æ ·æœ¬ã€‚é€šè¿‡è¿™æ ·åšï¼Œä½œè€…åœ¨åƒImageNetå’ŒCoCoè¿™æ ·çš„å…¬å…±åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†åˆ†ç±»å’Œæ£€ç´¢å‡†ç¡®æ€§ã€‚è¿‡æ»¤æ‰å¸¦æœ‰æ–‡æœ¬åŒºåŸŸçš„å›¾åƒè¿˜å¯ä»¥ä¿æŠ¤æ¨¡å‹å…å—å°åˆ·æ”»å‡»ã€‚ä½œè€…å»ºç«‹äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ImageNet with Adversarial Text Regions (ImageNet-Attr)æ¥éªŒè¯è¿™ä¸€ç‚¹ã€‚ä»–ä»¬çš„åŸºäºè¿‡æ»¤çš„CLIPæ¨¡å‹è¡¨ç°å‡º68.78%çš„top-1å‡†ç¡®ç‡ï¼Œä¼˜äºæ‰€æœ‰ä½äº50%å‡†ç¡®ç‡çš„å…ˆå‰æ¨¡å‹ã€‚ä½œè€…çš„ç ”ç©¶åŠ¨æœºæ˜¯å¯¹å¯¹æ¯”æŸå¤±çš„è§‚å¯Ÿã€‚$http://arxiv.org/pdf/2305.05095v1
Localisation of Mammographic masses by Greedy Backtracking of  Activations in the Stacked Auto-Encoders$  Mammographic image analysis requires accurate localisation of salientmammographic masses. In mammographic computer-aided diagnosis, mass or Regionof Interest (ROI) is often marked by physicians and features are extracted fromthe marked ROI. In this paper, we present a novel mammographic masslocalisation framework, based on the maximal class activations of the stackedauto-encoders. We hypothesize that the image regions activating abnormalclasses in mammographic images will be the breast masses which causes theanomaly. The experiment is conducted using randomly selected 200 mammographicimages (100 normal and 100 abnormal) from IRMA mammographic dataset. Abnormalmass regions marked by an expert radiologist are used as the ground truth. Theproposed method outperforms existing Deep Convolutional Neural Network (DCNN)based techniques in terms of salient region detection accuracy. The proposedgreedy backtracking method is more efficient and does not require a vast numberof labelled training images as in DCNN based method. Such automaticlocalisation method will assist physicians to make accurate decisions on biopsyrecommendations and treatment evaluations.$Keywords: Mammographic image analysis, Salient Region Detection, Auto-encoder, Deep Convolutional Neural Network, Breast mass detection, Biopsy recommendations, Treatment evaluations.$"ä½œè€…ï¼šShamna Pootheri (æ–°åŠ å¡å—æ´‹ç†å·¥å¤§å­¦ç ”ç©¶å‘˜ï¼Œå‰ç ”ç©¶å­¦è€…) å’Œ V K Govindan (å°åº¦å›½å®¶æŠ€æœ¯å¡åˆ©å¡ç‰¹å›½å®¶å·¥ç¨‹å­¦é™¢åèª‰æ•™æˆ)
æœºæ„ï¼šaæ–°åŠ å¡å—æ´‹ç†å·¥å¤§å­¦ï¼Œé‚®ç¼–639798ï¼›bå°åº¦å›½å®¶æŠ€æœ¯å¡åˆ©å¡ç‰¹å›½å®¶å·¥ç¨‹å­¦é™¢ï¼Œé‚®ç¼–673 601"$æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„ä¹³è…ºXçº¿æ‘„å½±è´¨é‡åˆ†ææ–¹æ³•ï¼Œä½¿ç”¨å †å è‡ªåŠ¨ç¼–ç å™¨çš„æœ€å¤§ç±»æ¿€æ´»è¿›è¡Œä¹³è…ºè‚¿å—è¯†åˆ«ã€‚æœ¬ç ”ç©¶å‡è®¾åœ¨ä¹³è…ºXçº¿æ‘„å½±å›¾åƒä¸­ï¼Œå¼‚å¸¸ç±»åˆ«çš„å›¾åƒåŒºåŸŸå°†æ˜¯å¯¼è‡´å¼‚å¸¸çš„ä¹³è…ºè‚¿å—ã€‚å®éªŒä½¿ç”¨äº†æ¥è‡ªIRMAæ•°æ®åº“çš„200å¼ éšæœºé€‰å–çš„ä¹³è…ºXçº¿æ‘„å½±å›¾åƒï¼ˆ100å¼ æ­£å¸¸ï¼Œ100å¼ å¼‚å¸¸ï¼‰ã€‚æ ¹æ®ä¸“å®¶æ”¾å°„ç§‘åŒ»å¸ˆæ ‡è®°çš„å¼‚å¸¸è‚¿å—åŒºåŸŸä½œä¸ºå‚è€ƒæ ‡å‡†ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨çªå‡ºåŒºåŸŸæ£€æµ‹å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºäºDCNNçš„æŠ€æœ¯ã€‚è¯¥è´ªå©ªå›æº¯æ–¹æ³•æ›´åŠ é«˜æ•ˆï¼Œå¹¶ä¸”ä¸éœ€è¦åƒDCNNæ–¹æ³•é‚£æ ·å¤§é‡æ ‡è®°çš„è®­ç»ƒå›¾åƒã€‚æ­¤ç±»è‡ªåŠ¨åŒ–å®šä½æ–¹æ³•å°†æœ‰åŠ©äºåŒ»ç”Ÿå‡†ç¡®å†³å®šæ´»æ£€å»ºè®®å’Œæ²»ç–—è¯„ä¼°ã€‚$http://arxiv.org/pdf/2305.05136v1
Adapt and Align to Improve Zero-Shot Sketch-Based Image Retrieval$  Zero-shot sketch-based image retrieval (ZS-SBIR) is challenging due to thecross-domain nature of sketches and photos, as well as the semantic gap betweenseen and unseen image distributions. Previous methods fine-tune pre-trainedmodels with various side information and learning strategies to learn a compactfeature space that (\\romannumeral1) is shared between the sketch and photodomains and (\\romannumeral2) bridges seen and unseen classes. However, theseefforts are inadequate in adapting domains and transferring knowledge from seento unseen classes. In this paper, we present an effective \\emph{``Adapt andAlign\'\'} approach to address the key challenges. Specifically, we insert simpleand lightweight domain adapters to learn new abstract concepts of the sketchdomain and improve cross-domain representation capabilities. Inspired by recentadvances in image-text foundation models (\\textit{e.g.}, CLIP) on zero-shotscenarios, we explicitly align the learned image embedding with a more semantictext embedding to achieve the desired knowledge transfer from seen to unseenclasses. Extensive experiments on three benchmark datasets and two popularbackbones demonstrate the superiority of our method in terms of retrievalaccuracy and flexibility.$Keywords: zero-shot learning, sketch-based image retrieval, domain adaptation, vision-language alignment, feature representation.$"ä½œè€…ï¼šShiyin Dong, Mingrui Zhu, Nannan Wang, Heng Yang, Xinbo Gaoï¼ˆæ¥è‡ªä¸­å›½è¥¿å®‰çš„è¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦ã€æ·±åœ³AiMallç§‘æŠ€å’Œé‡åº†é‚®ç”µå¤§å­¦ã€‚ï¼‰
æœºæ„ï¼šState Key Laboratory of Integrated Services Networks, Xidian University, Xian, Chinaï¼›Shenzhen AiMall Tech, Shenzhen, Chinaï¼›Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China."$æœ¬æ–‡æå‡ºäº†ä¸€ç§â€œé€‚åº”å’Œå¯¹é½â€çš„æ–¹æ³•æ¥è§£å†³é›¶æ ·æœ¬è‰å›¾å›¾åƒæ£€ç´¢çš„æŒ‘æˆ˜ï¼Œè¯¥ä»»åŠ¡ç”±äºè‰å›¾å’Œç…§ç‰‡çš„è·¨åŸŸæ€§ä»¥åŠå·²çŸ¥å’ŒæœªçŸ¥å›¾åƒåˆ†å¸ƒä¹‹é—´çš„è¯­ä¹‰å·®è·è€Œå˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡æ’å…¥è½»é‡çº§é¢†åŸŸé€‚é…å™¨æ¥å­¦ä¹ è‰å›¾åŸŸçš„æ–°æŠ½è±¡æ¦‚å¿µï¼Œä»è€Œæé«˜è·¨åŸŸè¡¨ç¤ºèƒ½åŠ›ï¼›åŒæ—¶ï¼Œå—æœ€è¿‘å›¾åƒ-æ–‡æœ¬åŸºç¡€æ¨¡å‹çš„å¯å‘ï¼Œæœ¬æ–‡æ˜ç¡®äº†å­¦ä¹ çš„å›¾åƒåµŒå…¥ä¸æ›´è¯­ä¹‰çš„æ–‡æœ¬åµŒå…¥ä¹‹é—´çš„å¯¹é½ï¼Œä»¥å®ç°ä»å·²çŸ¥åˆ°æœªçŸ¥ç±»çš„ç†æƒ³çŸ¥è¯†è½¬ç§»ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†å’Œä¸¤ä¸ªæµè¡Œçš„éª¨å¹²ç½‘ç»œä¸Šå…·æœ‰å‡ºè‰²çš„æ£€ç´¢å‡†ç¡®æ€§å’Œçµæ´»æ€§ã€‚$http://arxiv.org/pdf/2305.05144v1
SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with  Large Language Models$  Diffusion models, which have emerged to become popular text-to-imagegeneration models, can produce high-quality and content-rich images guided bytextual prompts. However, there are limitations to semantic understanding andcommonsense reasoning in existing models when the input prompts are concisenarrative, resulting in low-quality image generation. To improve the capacitiesfor narrative prompts, we propose a simple-yet-effective parameter-efficientfine-tuning approach called the Semantic Understanding and Reasoning adapter(SUR-adapter) for pre-trained diffusion models. To reach this goal, we firstcollect and annotate a new dataset SURD which consists of more than 57,000semantically corrected multi-modal samples. Each sample contains a simplenarrative prompt, a complex keyword-based prompt, and a high-quality image.Then, we align the semantic representation of narrative prompts to the complexprompts and transfer knowledge of large language models (LLMs) to ourSUR-adapter via knowledge distillation so that it can acquire the powerfulsemantic understanding and reasoning capabilities to build a high-qualitytextual semantic representation for text-to-image generation. We conductexperiments by integrating multiple LLMs and popular pre-trained diffusionmodels to show the effectiveness of our approach in enabling diffusion modelsto understand and reason concise natural language without image qualitydegradation. Our approach can make text-to-image diffusion models easier to usewith better user experience, which demonstrates our approach has the potentialfor further advancing the development of user-friendly text-to-image generationmodels by bridging the semantic gap between simple narrative prompts andcomplex keyword-based prompts.$Keywords: text-to-image generation, diffusion models, semantic understanding, large language models, adapter, knowledge distillation, multimodal image generation.$ä½œè€…ï¼šShanshan Zhong, Zhongzhan Huang, Wushao Wen, Jinghui Qin, Liang Linï¼ˆSun Yat-sen University, Guangzhou, Chinaï¼›Guangdong University of Technology, Guangzhou, Chinaï¼‰$æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSemantic Understanding and Reasoning adapter (SUR-adapter)çš„æ–¹æ³•ï¼Œä»¥å¢å¼ºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ç†è§£å’Œå¸¸è¯†æ¨ç†èƒ½åŠ›ï¼Œä»è€Œæ›´å¥½åœ°ç”Ÿæˆæ–‡æœ¬æŒ‡ç¤ºä¸‹çš„å›¾åƒã€‚ä½œè€…è¿˜æ”¶é›†å’Œæ ‡æ³¨äº†ä¸€ä¸ªåä¸ºSURDçš„æ–°æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«è¶…è¿‡57,000ä¸ªè¯­ä¹‰çº æ­£çš„å¤šæ¨¡æ€æ ·æœ¬ã€‚ä»–ä»¬é€šè¿‡çŸ¥è¯†è’¸é¦çš„æ–¹å¼å°†LLMsçš„çŸ¥è¯†è½¬ç§»åˆ°SUR-adapterä¸Šï¼Œä»è€Œèµ‹äºˆå…¶å¼ºå¤§çš„è¯­ä¹‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œä»¥åœ¨ç®€æ´è‡ªç„¶è¯­è¨€çš„æƒ…å†µä¸‹æ›´å¥½åœ°ç”Ÿæˆå›¾åƒã€‚ä½œè€…è¿›è¡Œäº†å¤šä¸ªå®éªŒï¼Œè¯æ˜äº†å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚$http://arxiv.org/pdf/2305.05189v1
LSAS: Lightweight Sub-attention Strategy for Alleviating Attention Bias  Problem$  In computer vision, the performance of deep neural networks (DNNs) is highlyrelated to the feature extraction ability, i.e., the ability to recognize andfocus on key pixel regions in an image. However, in this paper, wequantitatively and statistically illustrate that DNNs have a serious attentionbias problem on many samples from some popular datasets: (1) Position bias:DNNs fully focus on label-independent regions; (2) Range bias: The focusedregions from DNN are not completely contained in the ideal region. Moreover, wefind that the existing self-attention modules can alleviate these biases to acertain extent, but the biases are still non-negligible. To further mitigatethem, we propose a lightweight sub-attention strategy (LSAS), which utilizeshigh-order sub-attention modules to improve the original self-attentionmodules. The effectiveness of LSAS is demonstrated by extensive experiments onwidely-used benchmark datasets and popular attention networks. We release ourcode to help other researchers to reproduce the results ofLSAS~\\footnote{https://github.com/Qrange-group/LSAS}.$Computer vision, deep neural networks, attention bias, sub-attention, lightweight.$"ä½œè€…ï¼šé’ŸçŠçŠï¼ˆä¸­å±±å¤§å­¦ï¼‰ã€æ¸©å´å°‘ï¼ˆä¸­å±±å¤§å­¦ï¼‰ã€ç§¦ç«è¾‰ï¼ˆå¹¿ä¸œå·¥ä¸šå¤§å­¦ï¼‰ã€é™ˆå¼ºæ™®ï¼ˆä¸­å±±å¤§å­¦ï¼‰ã€é»„å¿ å±•ï¼ˆä¸­å±±å¤§å­¦ï¼‰
æœºæ„ï¼šä¸­å±±å¤§å­¦ã€å¹¿ä¸œå·¥ä¸šå¤§å­¦
è®ºæ–‡é¢˜ç›®ï¼šLSAS: Lightweight Sub-attention Strategy for Alleviating Attention Bias Problem*"$æœ¬ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§å­æ³¨æ„åŠ›ç­–ç•¥ï¼ˆLSASï¼‰ï¼Œç”¨äºç¼“è§£æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å›¾åƒè¯†åˆ«ä»»åŠ¡ä¸­å­˜åœ¨çš„æ³¨æ„åå‘é—®é¢˜ã€‚æœ¬æ–‡é€šè¿‡å®éªŒå’Œç»Ÿè®¡æ•°æ®é‡åŒ–æ­ç¤ºäº†DNNåœ¨æŸäº›å¸¸ç”¨æ•°æ®é›†ä¸Šå­˜åœ¨çš„ä¸¥é‡åå‘é—®é¢˜ï¼ŒåŒ…æ‹¬ä½ç½®åå‘å’ŒèŒƒå›´åå‘ï¼Œå¹¶å‘ç°ç°æœ‰çš„è‡ªæ³¨æ„æœºåˆ¶åªèƒ½åœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£è¿™äº›åå‘ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†LSASï¼Œé€šè¿‡åˆ©ç”¨é«˜é˜¶å­æ³¨æ„åŠ›æ¨¡å—æ”¹è¿›åŸå§‹è‡ªæ³¨æ„åŠ›æ¨¡å—æ¥è¿›ä¸€æ­¥ç¼“è§£åå‘é—®é¢˜ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒLSASèƒ½å¤Ÿæ˜¾è‘—æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä¸”æˆ‘ä»¬å¼€æºäº†ä»£ç ä»¥ä¾›å…¶ä»–ç ”ç©¶è€…éªŒè¯å®éªŒç»“æœã€‚$http://arxiv.org/pdf/2305.05200v1
FishRecGAN: An End to End GAN Based Network for Fisheye Rectification  and Calibration$  We propose an end-to-end deep learning approach to rectify fisheye images andsimultaneously calibrate camera intrinsic and distortion parameters. Our methodconsists of two parts: a Quick Image Rectification Module developed with aPix2Pix GAN and Wasserstein GAN (W-Pix2PixGAN), and a Calibration Module with aCNN architecture. Our Quick Rectification Network performs robust rectificationwith good resolution, making it suitable for constant calibration incamera-based surveillance equipment. To achieve high-quality calibration, weuse the straightened output from the Quick Rectification Module as aguidance-like semantic feature map for the Calibration Module to learn thegeometric relationship between the straightened feature and the distortedfeature. We train and validate our method with a large synthesized datasetlabeled with well-simulated parameters applied to a perspective image dataset.Our solution has achieved robust performance in high-resolution with asignificant PSNR value of 22.343.$Keywords: fisheye cameras, rectification, calibration, deep learning, GAN, CNN, Pix2Pix, Wasserstein GAN, synthetic dataset.$"ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šXin Shen, Kyungdon Joo, Jean Ohï¼ˆå¡å†…åŸºæ¢…éš†å¤§å­¦ï¼‰
æ–‡ç« æ ‡é¢˜ï¼šFishRecGAN: An End to End GAN Based Network
for Fisheye Rectiï¬cation and Calibration"$æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºä¿®æ­£é±¼çœ¼å›¾åƒä»¥åŠåŒæ—¶æ ¡å‡†ç›¸æœºå†…åœ¨å’Œç•¸å˜å‚æ•°ã€‚æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªé€šè¿‡Pix2Pix GANå’ŒWasserstein GANå¼€å‘çš„å¿«é€Ÿå›¾åƒçŸ«æ­£æ¨¡å—ï¼ˆW-Pix2PixGANï¼‰ï¼Œä»¥åŠä¸€ä¸ªå…·æœ‰CNNæ¶æ„çš„æ ¡å‡†æ¨¡å—ã€‚å¿«é€ŸçŸ«æ­£ç½‘ç»œå…·æœ‰è‰¯å¥½çš„åˆ†è¾¨ç‡æ€§èƒ½å’Œé²æ£’æ€§ï¼Œé€‚åˆäºç›¸æœºç›‘æ§è®¾å¤‡ä¸­çš„æ’å®šæ ¡å‡†ã€‚ä¸ºäº†å®ç°é«˜è´¨é‡çš„æ ¡å‡†ï¼Œä½œè€…åˆ©ç”¨æ¥è‡ªå¿«é€ŸçŸ«æ­£æ¨¡å—çš„ç›´çº¿è¾“å‡ºä½œä¸ºæ ¡å‡†æ¨¡å—çš„â€œæŒ‡å¯¼â€è¯­ä¹‰ç‰¹å¾å›¾ï¼Œå­¦ä¹ åŠ£åŒ–ç‰¹å¾å’ŒçŸ«æ­£ç‰¹å¾ä¹‹é—´çš„å‡ ä½•å…³ç³»ã€‚ä½œè€…ä½¿ç”¨åŒ…å«ä¸åŒç•¸å˜å‚æ•°æ ‡ç­¾çš„å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼Œæå‡ºçš„æ–¹æ³•å…·æœ‰é²æ£’çš„æ€§èƒ½å’Œæ˜¾è‘—çš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰å€¼ã€‚æœ¬æ–‡é€šè¿‡ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ æ–¹æ³•æä¾›äº†ä¸€ç§è‡ªåŠ¨åŒ–ï¼Œä¸€è‡´æ€§å’Œé«˜æ•ˆç‡çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ç”¨äºå®æ—¶é±¼çœ¼å›¾åƒçŸ«æ­£å’Œç›¸æœºæ ¡å‡†ã€‚$http://arxiv.org/pdf/2305.05222v1
Rotation Synchronization via Deep Matrix Factorization$  In this paper we address the rotation synchronization problem, where theobjective is to recover absolute rotations starting from pairwise ones, wherethe unknowns and the measures are represented as nodes and edges of a graph,respectively. This problem is an essential task for structure from motion andsimultaneous localization and mapping. We focus on the formulation ofsynchronization via neural networks, which has only recently begun to beexplored in the literature. Inspired by deep matrix completion, we expressrotation synchronization in terms of matrix factorization with a deep neuralnetwork. Our formulation exhibits implicit regularization properties and, moreimportantly, is unsupervised, whereas previous deep approaches are supervised.Our experiments show that we achieve comparable accuracy to the closestcompetitors in most scenes, while working under weaker assumptions.$Keywords: rotation synchronization, structure from motion, simultaneous localization and mapping, deep matrix factorization, neural networks, special orthogonal group, gauge ambiguity.$"ä½œè€…ï¼šGK Tejus, Giacomo Zara, Paolo Rota, Andrea Fusielloï¼ŒElisa Ricciå’ŒFederica Arrigoni

æœºæ„ï¼š1å°åº¦ç†å·¥å­¦é™¢ï¼ˆISMï¼‰ä¸¹å·´å¾·åˆ†æ ¡ï¼Œ2ç‰¹ä¼¦æ‰˜å¤§å­¦ï¼Œ3ä¹Œè¿ªå†…å¤§å­¦ï¼Œ4ç±³å…°ç†å·¥å¤§å­¦"$æœ¬æ–‡é’ˆå¯¹æ—‹è½¬åŒæ­¥é—®é¢˜ï¼Œæ—¨åœ¨é€šè¿‡ç¥ç»ç½‘ç»œå®ç°åŒæ­¥ã€‚æ–‡ç« å°†åŒæ­¥é—®é¢˜è¡¨ç¤ºä¸ºä¸€ç§æ·±åº¦çŸ©é˜µåˆ†è§£å½¢å¼ï¼Œå¹¶å…·æœ‰éšå¼æ­£åˆ™åŒ–ç‰¹æ€§ï¼Œä¸”ä¸ºæ— ç›‘ç£å½¢å¼ã€‚æ­¤å‰çš„æ·±åº¦æ–¹æ³•å±äºæœ‰ç›‘ç£å½¢å¼ã€‚æ–‡ç« è¿˜æ¢è®¨äº†å­˜åœ¨å™ªå£°ã€ç¦»ç¾¤æ•°æ®å’Œä¸¢å¤±æ•°æ®æƒ…å†µä¸‹çš„é—®é¢˜ï¼Œå®éªŒç»“æœè¡¨æ˜æ–¹æ³•ç²¾åº¦å¯ä¸å…¶ä»–æ–¹æ³•ç›¸åª²ç¾ï¼Œä½†å‡è®¾æ›´å¼±ã€‚$http://arxiv.org/pdf/2305.05268v1
Mediapipe and CNNs for Real-Time ASL Gesture Recognition$  This research paper describes a realtime system for identifying American SignLanguage (ASL) movements that employs modern computer vision and machinelearning approaches. The suggested method makes use of the Mediapipe libraryfor feature extraction and a Convolutional Neural Network (CNN) for ASL gestureclassification. The testing results show that the suggested system can detectall ASL alphabets with an accuracy of 99.95%, indicating its potential for usein communication devices for people with hearing impairments. The proposedapproach can also be applied to additional sign languages with similar handmotions, potentially increasing the quality of life for people with hearingloss. Overall, the study demonstrates the effectiveness of using Mediapipe andCNN for real-time sign language recognition, making a significant contributionto the field of computer vision and machine learning.$Keywords: Mediapipe, CNN, real-time ASL gesture recognition, computer vision, machine learning, SLR, sign language, finger-spelled gestures, dynamic gestures, continuous recognition.$"ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šRupesh Kumarï¼ˆå°åº¦åŠ å°”å„ç­”ç†å·¥å­¦é™¢CSEç³»ï¼‰ï¼ŒAshutosh Bajpaiï¼ˆå°åº¦åŠ å°”å„ç­”ç†å·¥å­¦é™¢CSEç³»ï¼‰ï¼ŒAyush Sinhaï¼ˆå°åº¦åŠ å°”å„ç­”ç†å·¥å­¦é™¢CSEç³»ï¼‰ï¼ŒS.K Singhï¼ˆå°åº¦åŠ å°”å„ç­”ç†å·¥å­¦é™¢CSEç³»ï¼‰

æœ¬ç ”ç©¶è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåˆ©ç”¨ç°ä»£è®¡ç®—æœºè§†è§‰å’Œæœºå™¨å­¦ä¹ æ–¹æ³•è¯†åˆ«ç¾å›½æ‰‹è¯­ï¼ˆASLï¼‰åŠ¨ä½œçš„å®æ—¶ç³»ç»Ÿã€‚è¯¥æ–¹æ³•åˆ©ç”¨Mediapipeåº“è¿›è¡Œç‰¹å¾æå–å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¿›è¡ŒASLæ‰‹åŠ¿åˆ†ç±»ã€‚æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„ç³»ç»Ÿå¯ä»¥æ£€æµ‹æ‰€æœ‰ASLå­—æ¯ï¼Œç²¾åº¦ä¸º99.95ï¼…ï¼Œè¡¨æ˜å®ƒæœ‰æ½œåŠ›ç”¨äºå¬åŠ›å—æŸäººå£«çš„é€šè®¯è®¾å¤‡ã€‚è¯¥æ–¹æ³•ä¹Ÿå¯ä»¥åº”ç”¨äºå…¶ä»–æ‰‹åŠ¿ç±»ä¼¼çš„æ‰‹è¯­ï¼Œæ½œåœ¨åœ°æé«˜å¬åŠ›æŸå¤±äººå£«çš„ç”Ÿæ´»è´¨é‡ã€‚æ€»çš„æ¥è¯´ï¼Œæœ¬ç ”ç©¶å±•ç¤ºäº†åˆ©ç”¨Mediapipeå’ŒCNNè¿›è¡Œå®æ—¶æ‰‹è¯­è¯†åˆ«çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè®¡ç®—æœºè§†è§‰å’Œæœºå™¨å­¦ä¹ é¢†åŸŸåšå‡ºäº†é‡è¦è´¡çŒ®ã€‚"$æœ¬ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§ä½¿ç”¨ç°ä»£è®¡ç®—æœºè§†è§‰å’Œæœºå™¨å­¦ä¹ æ–¹æ³•è¿›è¡Œå®æ—¶æ£€æµ‹ç¾å›½æ‰‹è¯­è¿åŠ¨çš„ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨Mediapipeåº“è¿›è¡Œç‰¹å¾æå–ï¼Œå¹¶ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¿›è¡ŒASLæ‰‹åŠ¿åˆ†ç±»ã€‚æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿå¯ä»¥ä»¥99.95ï¼…çš„å‡†ç¡®ç‡æ£€æµ‹æ‰€æœ‰ASLå­—æ¯ï¼Œè¡¨æ˜å®ƒåœ¨ä¸ºå¬åŠ›å—æŸäººç¾¤æä¾›é€šä¿¡è®¾å¤‡æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚è¿™ç§æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–å…·æœ‰ç±»ä¼¼æ‰‹åŠ¿çš„æ‰‹è¯­ï¼Œä»è€Œå¯èƒ½æé«˜å¬åŠ›å—æŸè€…çš„ç”Ÿæ´»è´¨é‡ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨Mediapipeå’ŒCNNè¿›è¡Œå®æ—¶æ‰‹è¯­è¯†åˆ«æ˜¯æœ‰æ•ˆçš„ï¼Œä¸ºè®¡ç®—æœºè§†è§‰å’Œæœºå™¨å­¦ä¹ é¢†åŸŸåšå‡ºäº†æ˜¾è‘—è´¡çŒ®ã€‚$http://arxiv.org/pdf/2305.05296v1
Eiffel Tower: A Deep-Sea Underwater Dataset for Long-Term Visual  Localization$  Visual localization plays an important role in the positioning and navigationof robotics systems within previously visited environments. When visits occurover long periods of time, changes in the environment related to seasons orday-night cycles present a major challenge. Under water, the sources ofvariability are due to other factors such as water conditions or growth ofmarine organisms. Yet it remains a major obstacle and a much less studied one,partly due to the lack of data. This paper presents a new deep-sea dataset tobenchmark underwater long-term visual localization. The dataset is composed ofimages from four visits to the same hydrothermal vent edifice over the courseof five years. Camera poses and a common geometry of the scene were estimatedusing navigation data and Structure-from-Motion. This serves as a referencewhen evaluating visual localization techniques. An analysis of the dataprovides insights about the major changes observed throughout the years.Furthermore, several well-established visual localization methods are evaluatedon the dataset, showing there is still room for improvement in underwaterlong-term visual localization. The data is made publicly available athttps://www.seanoe.org/data/00810/92226/.$Keywords: Underwater dataset, long-term visual localization, deep sea, visual localization benchmark, Eiffel Tower vent edifice.$ä½œè€…ï¼šClÃ©mentin Boittiaux, Claire Dune, Maxime Ferrera, AurÃ©lien Arnaubec, Ricard Marxer, Marjolaine Matabos, LoÃ¯c Van Audenhaegeå’ŒVincent Hugelã€‚æœºæ„ï¼šIfremer, UniversitÃ© de Toulon, Aix Marseille Univ, CNRSå’ŒUniv Brestã€‚$æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„æ·±æµ·æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°æ°´ä¸‹é•¿æœŸè§†è§‰å®šä½ã€‚è¯¥æ•°æ®é›†ç”±åŒä¸€æ°´çƒ­å–·å£å †çš„å››æ¬¡æ‹œè®¿æœŸé—´æ‰€æ‹æ‘„çš„å›¾åƒç»„æˆï¼Œå†æ—¶äº”å¹´ã€‚ä½¿ç”¨å¯¼èˆªæ•°æ®å’Œç»“æ„å…‰æ‰«ææŠ€æœ¯ç¡®å®šäº†ç›¸æœºä½ç½®å’Œåœºæ™¯å‡ ä½•ï¼Œå¯ä»¥ç”¨ä½œè¯„ä¼°è§†è§‰å®šä½æŠ€æœ¯çš„å‚è€ƒã€‚æ–‡ç« è¿˜åˆ†æäº†è¿™äº›å¹´ä»½çš„ä¸»è¦å˜åŒ–ï¼Œè¯„ä¼°äº†å‡ ç§è§†è§‰å®šä½æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œæ°´ä¸‹é•¿æœŸè§†è§‰å®šä½ä»æœ‰æ”¹è¿›çš„ç©ºé—´ã€‚åŒæ—¶ï¼Œæ•°æ®é›†å·²ç»å…¬å¼€æä¾›ï¼Œå¯ç”¨äºæ·±æµ·è§†è§‰å®šä½çš„åŸºå‡†æµ‹è¯•ã€‚$http://arxiv.org/pdf/2305.05301v1
Application of Artificial Intelligence in the Classification of  Microscopical Starch Images for Drug Formulation$  Starches are important energy sources found in plants with many uses in thepharmaceutical industry such as binders, disintegrants, bulking agents in drugsand thus require very careful physicochemical analysis for properidentification and verification which includes microscopy. In this work, weapplied artificial intelligence techniques (using transfer learning and deepconvolution neural network CNNs to microscopical images obtained from 9 starchsamples of different botanical sources. Our approach obtained an accuracy of61% when the machine learning model was pretrained on microscopic images fromMicroNet dataset. However the accuracy jumped to 81% for model pretrained onrandom day to day images obtained from Imagenet dataset. The model pretrainedon the imagenet dataset also showed a better precision, recall and f1 scorethan that pretrained on the imagenet dataset.$Keywords: Microscopy, starch, computer vision, convolution neural network, starch classification, transfer learning, artificial intelligence.$"ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šMarvellous Ajala, Blessing Oko, David Oba-Fidelis, Joycelyn Iyasele, Joy I. Odimegwuï¼ˆå°¼æ—¥åˆ©äºšæ‹‰å„æ–¯å¤§å­¦è¯å­¦é™¢è¯å‰‚å­¦ç³»ï¼‰

æ‘˜è¦ï¼š
æ·€ç²‰æ˜¯æ¤ç‰©ä¸­é‡è¦çš„èƒ½æºæ¥æºï¼Œå…·æœ‰åœ¨åˆ¶è¯å·¥ä¸šä¸­ä½œä¸ºç²˜åˆå‰‚ã€åˆ†æ•£å‰‚ã€å¢å®¹å‰‚ç­‰å¤šç§ç”¨é€”ï¼Œå› æ­¤éœ€è¦éå¸¸ä»”ç»†çš„ç‰©ç†åŒ–å­¦åˆ†æè¿›è¡Œæ­£ç¡®çš„é‰´å®šå’ŒéªŒè¯ï¼Œå…¶ä¸­åŒ…æ‹¬æ˜¾å¾®é•œæ£€æŸ¥ã€‚
åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åº”ç”¨äº†äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼ˆä½¿ç”¨è½¬ç§»å­¦ä¹ å’Œæ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œCNNï¼‰æ¥å¯¹æ¥è‡ª9ç§ä¸åŒæ¤ç‰©æ¥æºçš„æ·€ç²‰æ ·å“è¿›è¡Œæ˜¾å¾®é•œå›¾åƒåˆ†æã€‚å½“æœºå™¨å­¦ä¹ æ¨¡å‹é¢„å…ˆè®­ç»ƒäº†æ¥è‡ªMicroNetæ•°æ®é›†çš„æ˜¾å¾®é•œå›¾åƒæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¾—åˆ°äº†61ï¼…çš„å‡†ç¡®ç‡ã€‚ç„¶è€Œï¼Œå½“æ¨¡å‹é¢„å…ˆè®­ç»ƒåœ¨ä»Imagenetæ•°æ®é›†ä¸­è·å–çš„éšæœºæ—¥å¸¸å›¾åƒä¸Šæ—¶ï¼Œå‡†ç¡®ç‡è·ƒå‡è‡³81ï¼…ã€‚é¢„å…ˆè®­ç»ƒåœ¨imagenetæ•°æ®é›†ä¸Šçš„æ¨¡å‹ä¹Ÿæ¯”åœ¨imagenetæ•°æ®é›†ä¸Šé¢„å…ˆè®­ç»ƒçš„æ¨¡å‹è¡¨ç°å‡ºæ›´å¥½çš„ç²¾ç¡®åº¦ã€å¬å›ç‡å’Œf1åˆ†æ•°ã€‚
å…³é”®è¯ï¼šæ˜¾å¾®é•œæœ¯ï¼Œæ·€ç²‰ï¼Œè®¡ç®—æœºè§†è§‰ï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼Œæ·€ç²‰åˆ†ç±»ï¼Œè½¬ç§»å­¦ä¹ ã€‚"$è¿™ç¯‡è®ºæ–‡ä¸»è¦ç ”ç©¶å¦‚ä½•åˆ©ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯å¯¹æ˜¾å¾®é•œä¸‹çš„æ·€ç²‰è´¨å›¾ç‰‡è¿›è¡Œåˆ†ç±»åˆ†æï¼Œè¿›è€Œåœ¨è¯ç‰©åˆ¶å‰‚ä¸­çš„åº”ç”¨ã€‚ç”±äºæ·€ç²‰è´¨åœ¨åˆ¶è¯ä¸­æ‰®æ¼”ç€é‡è¦çš„ä½œç”¨ï¼Œå¦‚æˆä¸ºè¯ç‰©çš„ç²˜åˆå‰‚ã€åˆ†æ•£å‰‚å’Œå¢å¤§å‰‚ï¼Œå› æ­¤å¯¹æ·€ç²‰è´¨çš„ç‰©ç†åŒ–å­¦åˆ†æååˆ†å…³é”®ã€‚åˆ©ç”¨è¿ç§»å­¦ä¹ å’Œæ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼Œç ”ç©¶äººå‘˜å¯¹æ¥è‡ª9ç§æ¤ç‰©æ ·æœ¬çš„æ˜¾å¾®é•œå›¾ç‰‡è¿›è¡Œåˆ†æã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œå¯¹äºä»ä¸åŒæ•°æ®é›†é¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå…¶ç²¾åº¦ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’Œf1å€¼å‡æœ‰æ‰€ä¸åŒï¼Œå…¶ä¸­ä»¥ä»Imagenetæ•°æ®é›†é¢„è®­ç»ƒçš„æ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œå…¶å‡†ç¡®ç‡å¯è¾¾81%ã€‚å…³é”®è¯åŒ…æ‹¬ï¼šæ˜¾å¾®é•œã€æ·€ç²‰è´¨ã€è®¡ç®—æœºè§†è§‰ã€å·ç§¯ç¥ç»ç½‘ç»œã€æ·€ç²‰è´¨åˆ†ç±»å’Œè¿ç§»å­¦ä¹ ã€‚$http://arxiv.org/pdf/2305.05321v1
Trustworthy Multi-phase Liver Tumor Segmentation via Evidence-based  Uncertainty$  Multi-phase liver contrast-enhanced computed tomography (CECT) images conveythe complementary multi-phase information for liver tumor segmentation (LiTS),which are crucial to assist the diagnosis of liver cancer clinically. However,the performances of existing multi-phase liver tumor segmentation(MPLiTS)-based methods suffer from redundancy and weak interpretability, % ofthe fused result, resulting in the implicit unreliability of clinicalapplications. In this paper, we propose a novel trustworthy multi-phase livertumor segmentation (TMPLiTS), which is a unified framework jointly conductingsegmentation and uncertainty estimation. The trustworthy results could assistthe clinicians to make a reliable diagnosis. Specifically, Dempster-ShaferEvidence Theory (DST) is introduced to parameterize the segmentation anduncertainty as evidence following Dirichlet distribution. The reliability ofsegmentation results among multi-phase CECT images is quantified explicitly.Meanwhile, a multi-expert mixture scheme (MEMS) is proposed to fuse themulti-phase evidences, which can guarantee the effect of fusion procedure basedon theoretical analysis. Experimental results demonstrate the superiority ofTMPLiTS compared with the state-of-the-art methods. Meanwhile, the robustnessof TMPLiTS is verified, where the reliable performance can be guaranteedagainst the perturbations.$Keywords: liver tumor segmentation, multi-phase contrast-enhanced CT, deep learning, uncertainty estimation, Dempster-Shafer Evidence Theory.$"ä½œè€…ï¼šChuanfei Hu, Tianyi Xia, Ying Cui, Quchen Zou, Yuancheng Wang, Wenbo Xiao, Shenghong Ju, Xinde Li

æœºæ„ï¼šSoutheast University, Jiangsu Key Laboratory of Molecular and Functional Imaging, Department of Radiology, Zhongda Hospital, School of Medicine, Zhejiang University School of Medicine"$æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„ã€å¯ä¿¡èµ–çš„å¤šç›¸è‚è‚¿ç˜¤åˆ†å‰²æ–¹æ³•ï¼Œå®ƒèƒ½å¤Ÿåœ¨è”åˆåˆ†å‰²å’Œä¸ç¡®å®šæ€§ä¼°è®¡çš„åŸºç¡€ä¸Šï¼Œé‡åŒ–åœ¨ä¸åŒé˜¶æ®µCTå›¾åƒä¸­åˆ†å‰²ç»“æœçš„å¯é æ€§ï¼Œå¹¶åˆ©ç”¨Dempster-Shaferè¯æ®ç†è®º(DST)å’Œå¤šä¸“å®¶æ··åˆæ¨¡å¼(MEMS)æ¥èåˆå¤šç›¸è¯æ®ï¼Œä»è€Œæé«˜è‚è‚¿ç˜¤åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTMPLiTSç›¸è¾ƒäºç°æœ‰çš„åˆ†å‰²æ–¹æ³•å…·æœ‰æ›´ä¼˜è¶Šçš„è¡¨ç°å’Œç¨³å¥æ€§ï¼Œæé«˜äº†åŒ»å­¦åˆ†å‰²çš„å¯é æ€§å’Œå¯ç”¨æ€§ã€‚$http://arxiv.org/pdf/2305.05344v1
Towards the Characterization of Representations Learned via  Capsule-based Network Architectures$  Capsule Networks (CapsNets) have been re-introduced as a more compact andinterpretable alternative to standard deep neural networks. While recentefforts have proved their compression capabilities, to date, theirinterpretability properties have not been fully assessed. Here, we conduct asystematic and principled study towards assessing the interpretability of thesetypes of networks. Moreover, we pay special attention towards analyzing thelevel to which part-whole relationships are indeed encoded within the learnedrepresentation. Our analysis in the MNIST, SVHN, PASCAL-part and CelebAdatasets suggest that the representations encoded in CapsNets might not be asdisentangled nor strictly related to parts-whole relationships as is commonlystated in the literature.$Capsule Networks, Interpretability, Part-Whole Relationships, Deep Neural Networks, Compression, Model Interpretation, Model Explanation, Methodology, Experimental Analysis.$ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šSaja AL-Tawalbehå’ŒJosÃ© Oramasï¼ˆæ¯”åˆ©æ—¶å®‰ç‰¹å«æ™®å¤§å­¦ï¼Œimec-IDLabï¼‰ï¼šTowards the Characterization of Representations Learned via Capsule-based Network Architectures$æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºâ€œCapsule Networks (CapsNets)â€çš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œå¹¶é‡ç‚¹æ¢è®¨äº†è¯¥ç»“æ„æ‰€å»ºç«‹çš„å†…éƒ¨è¡¨å¾èƒ½å¦è§£é‡Šå’Œè¯†åˆ«æ•°æ®é›†ä¸­çš„éƒ¨åˆ†å…³ç³»ã€‚Capsule Networksé‡‡ç”¨äº†ä¸€ç»„ç¥ç»å…ƒä½œä¸ºä¸€ä¸ªèƒ¶å›Šï¼Œç”¨ä¸€ä¸ªæ´»è·ƒå‘é‡è¡¨ç¤ºä¸€ä¸ªç‰¹å®šç±»å‹çš„å®ä½“ï¼Œæ¯”å¦‚å¯¹è±¡æˆ–å¯¹è±¡çš„ä¸€éƒ¨åˆ†ã€‚ä¸åŒäºä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ï¼ŒCapsNetsä½¿ç”¨è¿æ¥åˆ—è¡¨æ˜¾å¼åœ°å°†ä¸€ç»„èƒ¶å›Šä¸å¦ä¸€ç»„èƒ¶å›Šç›¸è”ç³»ï¼Œæ¨¡æ‹Ÿéƒ¨åˆ†-æ•´ä½“å…³ç³»ã€‚æœ¬æ–‡é€šè¿‡å®éªŒç ”ç©¶CapsNetsçš„è§£é‡Šèƒ½åŠ›ï¼Œæå‡ºäº†ä¸¤ç§æ£€éªŒæ–¹æ³•ï¼Œå¹¶åœ¨MNISTã€SVHNã€PASCAL-partå’ŒCelebAç­‰æ•°æ®é›†ä¸Šå¯¹CapsNetsä¸­å­¦ä¹ çš„è¡¨å¾è¿›è¡Œäº†åˆ†æã€‚ç ”ç©¶å‘ç°ï¼ŒCapsNetså¯èƒ½æ²¡æœ‰åƒæ–‡çŒ®ä¸­æ‰€è¿°çš„é‚£æ ·æ˜ç¡®ç¼–ç éƒ¨åˆ†-æ•´ä½“å…³ç³»ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•å¯ä»¥ç”¨äºæå–é‡è¦çš„éƒ¨åˆ†å…³ç³»å•å…ƒï¼Œè¿™äº›å•å…ƒå¯ä»¥æé«˜CapsNetçš„è¡¨ç°æ°´å¹³ã€‚$http://arxiv.org/pdf/2305.05349v1
DC3DCD: unsupervised learning for multiclass 3D point cloud change  detection$  In a constant evolving world, change detection is of prime importance to keepupdated maps. To better sense areas with complex geometry (urban areas inparticular), considering 3D data appears to be an interesting alternative toclassical 2D images. In this context, 3D point clouds (PCs) obtained by LiDARor photogrammetry are very interesting. While recent studies showed theconsiderable benefit of using deep learning-based methods to detect andcharacterize changes into raw 3D PCs, these studies rely on large annotatedtraining data to obtain accurate results. The collection of these annotationsare tricky and time-consuming. The availability of unsupervised or weaklysupervised approaches is then of prime interest. In this paper, we propose anunsupervised method, called DeepCluster 3D Change Detection (DC3DCD), to detectand categorize multiclass changes at point level. We classify our approach inthe unsupervised family given the fact that we extract in a completelyunsupervised way a number of clusters associated with potential changes. Let usprecise that in the end of the process, the user has only to assign a label toeach of these clusters to derive the final change map. Our method builds uponthe DeepCluster approach, originally designed for image classification, tohandle complex raw 3D PCs and perform change segmentation task. An assessmentof the method on both simulated and real public dataset is provided. Theproposed method allows to outperform fully-supervised traditional machinelearning algorithm and to be competitive with fully-supervised deep learningnetworks applied on rasterization of 3D PCs with a mean of IoU over classes ofchange of 57.06% and 66.69% for the simulated and the real datasets,respectively.$Keywords: 3D point clouds, change detection, unsupervised deep learning, deep clustering, LiDAR, photogrammetry, Aerial Laser Scanning.$ä½œè€…ï¼šIris de GÃ©lis, SÃ©bastien LefÃ¨vreå’ŒThomas Corpettiï¼Œæœºæ„ï¼šMagelliumï¼ŒToulouseï¼ŒFranceï¼›IRISAï¼ŒUMR 6074ï¼ŒUniversitÃ© Bretagne Sudï¼ŒVannesï¼ŒFranceï¼›CNRSï¼ŒLETGï¼ŒUMR 6554ï¼ŒRennesï¼ŒFranceã€‚$æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„æ–¹æ³•ï¼Œåä¸ºDC3DCDï¼Œç”¨äºæ£€æµ‹å’Œåˆ†ç±»å¤šç±»åˆ«çš„ç‚¹äº‘å˜åŒ–ã€‚è¯¥æ–¹æ³•å»ºç«‹åœ¨åŸæœ¬ç”¨äºå›¾åƒåˆ†ç±»çš„DeepClusteræ–¹æ³•ä¹‹ä¸Šï¼Œåº”ç”¨äºå¤„ç†å¤æ‚çš„3Dç‚¹äº‘æ•°æ®ä»¥æ‰§è¡Œå˜åŒ–åˆ†å‰²ä»»åŠ¡ã€‚ä¸ä½¿ç”¨å¤§é‡æ ‡æ³¨æ•°æ®çš„ç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–¹æ³•åœ¨æœªæ ‡æ³¨æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå±•ç°äº†å…¶å®ç”¨æ€§å’Œä»·å€¼ã€‚é€šè¿‡æ¨¡æ‹Ÿæ•°æ®å’ŒçœŸå®å…¬å…±æ•°æ®é›†çš„è¯„ä¼°ï¼ŒDC3DCDæ–¹æ³•è¡¨ç°å‡ºäº†å¾ˆå¥½çš„æ•ˆæœï¼Œå¹¶ä¸”åœ¨åŒç±»åˆ«å˜åŒ–çš„IoUå‡å€¼æ–¹é¢ï¼Œä¸åº”ç”¨äº3Dç‚¹äº‘å…‰æ …åŒ–çš„å®Œå…¨ç›‘ç£æ·±åº¦å­¦ä¹ ç½‘ç»œç›¸å½“ã€‚å…¶ä»£ç å…¬å¼€åœ¨githubä¸Šå·²åˆ†äº«ã€‚è¯¥æˆæœæ—¨åœ¨ä¸ºåœ°å›¾æ›´æ–°ã€è‡ªç„¶ç¾å®³ç­‰æ–¹é¢æä¾›åŸºç¡€æ”¯æŒã€‚$http://arxiv.org/pdf/2305.05421v1
Egocentric Hierarchical Visual Semantics$  We are interested in aligning how people think about objects and whatmachines perceive, meaning by this the fact that object recognition, asperformed by a machine, should follow a process which resembles that followedby humans when thinking of an object associated with a certain concept. Theultimate goal is to build systems which can meaningfully interact with theirusers, describing what they perceive in the users\' own terms. As from the fieldof Lexical Semantics, humans organize the meaning of words in hierarchies wherethe meaning of, e.g., a noun, is defined in terms of the meaning of a moregeneral noun, its genus, and of one or more differentiating properties, itsdifferentia. The main tenet of this paper is that object recognition shouldimplement a hierarchical process which follows the hierarchical semanticstructure used to define the meaning of words. We achieve this goal byimplementing an algorithm which, for any object, recursively recognizes itsvisual genus and its visual differentia. In other words, the recognition of anobject is decomposed in a sequence of steps where the locally relevant visualfeatures are recognized. This paper presents the algorithm and a firstevaluation.$Image recognition, visual semantics, hierarchical structure, lexical semantics, genus and differentia, interactive machine learning.$ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šLuca Erculiani, Andrea Bontempelli, Andrea Passerini, and Fausto Giunchiglia (Trentoå¤§å­¦)$æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é€çº§æ£€æµ‹å¯¹è±¡ç‰¹å¾çš„è§†è§‰è¯­ä¹‰å±‚æ¬¡åˆ†æç®—æ³•ï¼Œå¹¶ä¸”å°†å…¶ä¸è¯æ±‡è¯­ä¹‰ä¸­çš„Genuså’ŒDifferentiaçš„å±‚æ¬¡ç»“æ„ç›¸è”ç³»ã€‚è¯¥ç®—æ³•é€šè¿‡é€’å½’åœ°è¯†åˆ«ç›®æ ‡çš„è§†è§‰Genuså’Œè§†è§‰Differentiaï¼Œå°†ç›®æ ‡çš„è¯†åˆ«åˆ†è§£ä¸ºä¸€ç³»åˆ—çš„æ­¥éª¤ã€‚è¿™ç§è§†è§‰è¯­ä¹‰å±‚æ¬¡åˆ†æç®—æ³•çš„å®ç°å¯¹äºæ„å»ºå¯ä»¥æœ‰æ„ä¹‰åœ°ä¸ç”¨æˆ·äº¤äº’çš„ç³»ç»Ÿå…·æœ‰é‡è¦çš„æ„ä¹‰ã€‚æœ¬æ–‡è¿˜å¯¹è¯¥ç®—æ³•è¿›è¡Œäº†åˆæ­¥çš„è¯„ä¼°ã€‚$http://arxiv.org/pdf/2305.05422v1
High-throughput Cotton Phenotyping Big Data Pipeline Lambda Architecture  Computer Vision Deep Neural Networks$  In this study, we propose a big data pipeline for cotton bloom detectionusing a Lambda architecture, which enables real-time and batch processing ofdata. Our proposed approach leverages Azure resources such as Data Factory,Event Grids, Rest APIs, and Databricks. This work is the first to develop anddemonstrate the implementation of such a pipeline for plant phenotyping throughAzure\'s cloud computing service. The proposed pipeline consists of datapreprocessing, object detection using a YOLOv5 neural network model trainedthrough Azure AutoML, and visualization of object detection bounding boxes onoutput images. The trained model achieves a mean Average Precision (mAP) scoreof 0.96, demonstrating its high performance for cotton bloom classification. Weevaluate our Lambda architecture pipeline using 9000 images yielding anoptimized runtime of 34 minutes. The results illustrate the scalability of theproposed pipeline as a solution for deep learning object detection, with thepotential for further expansion through additional Azure processing cores. Thiswork advances the scientific research field by providing a new method forcotton bloom detection on a large dataset and demonstrates the potential ofutilizing cloud computing resources, specifically Azure, for efficient andaccurate big data processing in precision agriculture.$"Keywords: high-throughput cotton phenotyping, big data pipeline, lambda architecture, computer vision, deep neural networks, cloud computing, Azure. 

Summary: This paper proposes a big data pipeline for cotton bloom detection using a Lambda architecture and leveraging Azure resources. The pipeline consists of data preprocessing, object detection using a YOLOv5 neural network model, and visualization of detection bounding boxes. The paper demonstrates the potential of cloud computing resources for efficient and accurate big data processing in precision agriculture. Ultimately, the proposed pipeline advances the scientific research field by providing a new method for cotton bloom detection on a large dataset."$"ä½œè€…ï¼šAmanda Issacï¼ˆç¾å›½ä¹”æ²»äºšå¤§å­¦ï¼‰ï¼ŒAlireza Ebrahimiï¼ˆç¾å›½å—å¡ç½—æ¥çº³å¤§å­¦ï¼‰ï¼ŒJavad Mohammadpour Velnibï¼ˆç¾å›½ä¹”æ²»äºšå¤§å­¦ï¼‰å’ŒGlen Rainscï¼ˆç¾å›½ä¹”æ²»äºšå¤§å­¦ï¼‰
æœºæ„ï¼š
a. ç”µæ°”ä¸è®¡ç®—æœºå·¥ç¨‹å­¦é™¢ï¼Œç¾å›½ä¹”æ²»äºšå¤§å­¦ï¼Œç¾å›½ä¹”æ²»äºšå·é›…å…¸å¸‚
b. æœºæ¢°å·¥ç¨‹ç³»ï¼Œç¾å›½å—å¡ç½—æ¥çº³å¤§å­¦ï¼Œç¾å›½å—å¡ç½—æ¥çº³å·å…‹è±å§†æ£®å¸‚
c. æ˜†è™«å­¦ç³»ï¼Œç¾å›½ä¹”æ²»äºšå¤§å­¦ï¼Œç¾å›½ä¹”æ²»äºšå·è’‚å¤«é¡¿å¸‚"$æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºLambdaæ¶æ„çš„å¤§æ•°æ®ç®¡é“ï¼Œç”¨äºæ£‰èŠ±å¼€èŠ±æ£€æµ‹ï¼Œå®ç°äº†å®æ—¶å’Œæ‰¹å¤„ç†çš„æ•°æ®å¤„ç†ã€‚è¯¥ç®¡é“åˆ©ç”¨Azureèµ„æºï¼Œå¦‚æ•°æ®å·¥å‚ã€äº‹ä»¶ç½‘æ ¼ã€Rest APIå’ŒDatabricksï¼Œå…¶ä¸­åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ï¼Œä½¿ç”¨Azure AutoMLè®­ç»ƒçš„YOLOv5ç¥ç»ç½‘ç»œæ¨¡å‹è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œå¹¶åœ¨è¾“å‡ºå›¾åƒä¸Šå¯¹ç›®æ ‡æ£€æµ‹è¾¹ç•Œæ¡†è¿›è¡Œå¯è§†åŒ–ã€‚è®­ç»ƒæ¨¡å‹çš„mAPå¾—åˆ†ä¸º0.96ï¼Œè¡¨æ˜å…¶ç”¨äºæ£‰èŠ±å¼€èŠ±åˆ†ç±»çš„æ€§èƒ½è¾ƒé«˜ã€‚é€šè¿‡ä½¿ç”¨9,000ä¸ªå›¾åƒè¯„ä¼°æˆ‘ä»¬çš„Lambdaæ¶æ„ç®¡é“ï¼Œå¾—å‡º34åˆ†é’Ÿçš„ä¼˜åŒ–è¿è¡Œæ—¶ï¼Œè¯´æ˜è¯¥ç®¡é“å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œå¹¶å…·æœ‰è¿›ä¸€æ­¥é€šè¿‡Azureå¤„ç†æ ¸å¿ƒè¿›è¡Œæ‰©å±•çš„æ½œåŠ›ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡æä¾›å¤§å‹æ•°æ®é›†çš„æ–°æ–¹æ³•æ¥æ£€æµ‹æ£‰èŠ±å¼€èŠ±ï¼ŒåŒæ—¶å±•ç¤ºäº†åˆ©ç”¨äº‘è®¡ç®—èµ„æºï¼Œç‰¹åˆ«æ˜¯Azureï¼Œè¿›è¡Œé«˜æ•ˆå’Œç²¾ç¡®çš„å¤§æ•°æ®å¤„ç†åœ¨ç²¾å‡†å†œä¸šä¸­çš„æ½œåŠ›ã€‚$http://arxiv.org/pdf/2305.05423v1
Bone Marrow Cytomorphology Cell Detection using InceptionResNetV2$  Critical clinical decision points in haematology are influenced by therequirement of bone marrow cytology for a haematological diagnosis. Bone marrowcytology, however, is restricted to reference facilities with expertise, andlinked to inter-observer variability which requires a long time to process thatcould result in a delayed or inaccurate diagnosis, leaving an unmet need forcutting-edge supporting technologies. This paper presents a novel transferlearning model for Bone Marrow Cell Detection to provide a solution to all thedifficulties faced for the task along with considerable accuracy. The proposedmodel achieved 96.19\\% accuracy which can be used in the future for analysis ofother medical images in this domain.$The article focuses on bone marrow cytology and cell detection, and proposes a new deep learning model called InceptionResNetV2.$"ä½œè€…ï¼šRaisa Fairooz Meem å’Œ Khandaker Tabin Hasan ï¼ˆæ¥è‡ªæœªçŸ¥æœºæ„ï¼‰
Graphical Abstractï¼šæ— å›¾å½¢æ‘˜è¦ï¼Œä»…æä¾›æ–‡ç« æ ‡é¢˜å’Œä½œè€…ä¿¡æ¯"$æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºInceptionRes-NetV2æ¨¡å‹çš„éª¨é«“ç»†èƒæ£€æµ‹æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹æ•°å­—å›¾åƒè¿›è¡Œé¢„å¤„ç†å’Œå¢å¼ºï¼Œè·å–ç»†èƒå›¾åƒï¼Œå¹¶é€šè¿‡InceptionRes-NetV2æ¨¡å‹å¯¹ç»†èƒè¿›è¡Œåˆ†ç±»å’Œå®šä½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰é«˜åº¦çš„ç²¾ç¡®æ€§å’Œå¬å›ç‡ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ç”¨äºéª¨é«“ç»†èƒçš„æ£€æµ‹å’Œå®šä½ã€‚$http://arxiv.org/pdf/2305.05430v1
WikiWeb2M: A Page-Level Multimodal Wikipedia Dataset$  Webpages have been a rich resource for language and vision-language tasks.Yet only pieces of webpages are kept: image-caption pairs, long text articles,or raw HTML, never all in one place. Webpage tasks have resultingly receivedlittle attention and structured image-text data underused. To study multimodalwebpage understanding, we introduce the Wikipedia Webpage 2M (WikiWeb2M) suite;the first to retain the full set of images, text, and structure data availablein a page. WikiWeb2M can be used for tasks like page description generation,section summarization, and contextual image captioning.$"Keywords: Wiki Workshop, Multimodal Data, Webpages, Machine Learning, Text Generation, Vision and Language. 

The article introduces a new dataset, Wikipedia Webpage 2M (WikiWeb2M), which retains the full set of images, text, and structure data available in a page, allowing for greater study of multimodal content understanding with many-to-many text and image relationships. The dataset can be used for tasks such as page description generation, section summarization, and contextual image captioning, and has potential applications in improving interaction with web content and modern content generation. The article discusses the creation process and additional processing done on the dataset, as well as its comparison to other datasets, such as the Wikipedia Image Text (WIT) dataset."$"ä½œè€…ï¼šAndrea Burnsã€Krishna Srinivasanã€Joshua Ainslieã€Geoff Brownã€Kate Saenkoã€Bryan A. Plummerã€Jianmo Niã€Mandy Guoï¼›
æœºæ„ï¼šBoston Universityã€Googleã€FAIR"$æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„å¤šæ¨¡æ€æ•°æ®é›†â€”â€”Wikipedia Webpage 2Mï¼ˆç®€ç§°WikiWeb2Mï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†web pagesçš„æ‰€æœ‰å†…å®¹â€”â€”å›¾ç‰‡ã€æ–‡å­—å’Œç»“æ„æ•°æ®éƒ½ä¿ç•™ä¸‹æ¥çš„æ•°æ®é›†ï¼Œå®ƒå¯ä»¥ç”¨äºé¡µæè¿°ç”Ÿæˆã€æ®µè½æ‘˜è¦å’Œä¸Šä¸‹æ–‡å›¾åƒå­—å¹•ç­‰ä»»åŠ¡ï¼Œå¹¶ä¸ºå¤šæ¨¡æ€å†…å®¹ç†è§£çš„ç ”ç©¶æä¾›äº†å¤§å¹…åº¦çš„å¸®åŠ©ã€‚æ­¤å¤–ï¼Œæ–‡ä¸­è¿˜ä»‹ç»äº†å¦‚ä½•æ„å»ºæ•°æ®é›†å’Œåç»­å¤„ç†è¿‡ç¨‹ï¼ŒåŒ…æ‹¬WITæ•°æ®é›†çš„é‡é‡‡æ ·å’Œåœ¨å¤„ç†å›¾åƒæ—¶çš„ç­›é€‰æ–¹æ³•ç­‰ã€‚æœ€åï¼Œæ–‡ä¸­ç»™å‡ºäº†å®éªŒç»“æœï¼Œè¯æ˜äº†è¯¥æ•°æ®é›†åœ¨å¤šä»»åŠ¡å­¦ä¹ ä¸Šçš„æœ‰æ•ˆæ€§ã€‚$http://arxiv.org/pdf/2305.05432v1
Multiscale Augmented Normalizing Flows for Image Compression$  Most learning-based image compression methods lack efficiency for high imagequality due to their non-invertible design. The decoding function of thefrequently applied compressive autoencoder architecture is only an approximatedinverse of the encoding transform. This issue can be resolved by usinginvertible latent variable models, which allow a perfect reconstruction if noquantization is performed. Furthermore, many traditional image and video codersapply dynamic block partitioning to vary the compression of certain imageregions depending on their content. Inspired by this approach, hierarchicallatent spaces have been applied to learning-based compression networks. In thispaper, we present a novel concept, which adapts the hierarchical latent spacefor augmented normalizing flows, an invertible latent variable model. Our bestperforming model achieved average rate savings of more than 7% over comparablesingle-scale models.$Keywords: learning-based image compression, augmented normalizing flows, hierarchical latent space, rate distortion optimization, end-to-end image compression.$ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šMarc Windsheimer, Fabian Brand, Andr Â´e Kaupï¼ˆFriedrich-Alexander-Universit Â¨at Erlangen-N Â¨urnbergï¼ŒMultimedia Communications and Signal Processingï¼‰$æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„å¤šå°ºåº¦æ•°æ®å¢å¹¿å½’ä¸€åŒ–æµï¼ˆmultiscale augmented normalizing flows, MANFï¼‰æ¨¡å‹ï¼Œç”¨äºå›¾åƒå‹ç¼©ã€‚å­¦ä¹ å¼å›¾åƒå‹ç¼©åœ¨é«˜å“è´¨å›¾åƒæ–¹é¢ç¼ºä¹æ•ˆç‡ï¼Œç”±äºå®ƒä»¬çš„éå¯é€†è®¾è®¡ã€‚ä½¿ç”¨ä¸å¯é€†æ½œå˜é‡æ¨¡å‹ï¼Œå¦‚å¢å¼ºå‹å½’ä¸€åŒ–æµï¼ˆANFï¼‰ï¼Œå¯ä»¥æ”¹å–„å­¦ä¹ å¼å›¾åƒå‹ç¼©çš„æ€§èƒ½ã€‚æœ¬æ–‡è¿˜å°†åˆ†å±‚æ½œå˜é‡ç©ºé—´åº”ç”¨äºMANFç½‘ç»œï¼Œä»¥é€‚åº”å›¾åƒå†…å®¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å•ä¸€å°ºåº¦æ¨¡å‹ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„MANFæ¨¡å‹äº§ç”Ÿäº†å¹³å‡èŠ‚çœ7ï¼…ä»¥ä¸Šçš„æ¯”ç‰¹ç‡ã€‚$http://arxiv.org/pdf/2305.05451v1
Style-A-Video: Agile Diffusion for Arbitrary Text-based Video Style  Transfer$  Large-scale text-to-video diffusion models have demonstrated an exceptionalability to synthesize diverse videos. However, due to the lack of extensivetext-to-video datasets and the necessary computational resources for training,directly applying these models for video stylization remains difficult. Also,given that the noise addition process on the input content is random anddestructive, fulfilling the style transfer task\'s content preservation criteriais challenging. This paper proposes a zero-shot video stylization method namedStyle-A-Video, which utilizes a generative pre-trained transformer with animage latent diffusion model to achieve a concise text-controlled videostylization. We improve the guidance condition in the denoising process,establishing a balance between artistic expression and structure preservation.Furthermore, to decrease inter-frame flicker and avoid the formation ofadditional artifacts, we employ a sampling optimization and a temporalconsistency module. Extensive experiments show that we can attain superiorcontent preservation and stylistic performance while incurring less consumptionthan previous solutions. Code will be available athttps://github.com/haha-lisa/Style-A-Video.$Keywords: Style transfer, text-to-video, video stylization, generative pre-trained transformer, diffusion models, content preservation, temporal consistency.$ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šNisha Huangï¼ˆä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€äººå·¥æ™ºèƒ½å­¦é™¢ï¼‰ã€Yuxin Zhangï¼ˆä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€äººå·¥æ™ºèƒ½å­¦é™¢ï¼‰ã€Weiming Dongï¼ˆä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€äººå·¥æ™ºèƒ½å­¦é™¢ï¼‰ã€‚$æœ¬æ–‡æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬è§†é¢‘é£æ ¼åŒ–æ–¹æ³•ï¼Œåä¸ºStyle-A-Videoã€‚è¯¥æ–¹æ³•åˆ©ç”¨äº†é¢„å…ˆè®­ç»ƒçš„ç”Ÿæˆå¼è½¬æ¢å™¨åŠå›¾åƒæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†ç®€æ´çš„æ–‡æœ¬æ§åˆ¶è§†é¢‘é£æ ¼åŒ–ã€‚ä¸ºäº†è¾¾åˆ°å¯¹è‰ºæœ¯è¡¨ç°å’Œç»“æ„ä¿æŠ¤ä¹‹é—´çš„å¹³è¡¡ï¼Œæ–‡ç« æ”¹å–„äº†å»å™ªè¿‡ç¨‹ä¸­çš„æŒ‡å¯¼æ¡ä»¶ã€‚æ­¤å¤–ï¼Œä¸ºäº†å‡å°‘å¸§é—´é—ªçƒå¹¶é¿å…å½¢æˆé¢å¤–çš„ä¼ªå½±ï¼Œæ–‡ç« è¿˜é‡‡ç”¨äº†æŠ½æ ·ä¼˜åŒ–å’Œæ—¶æ€ä¸€è‡´æ€§æ¨¡å—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºä»¥å¾€çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨è¾¾åˆ°ä¼˜ç§€çš„å†…å®¹ä¿æŠ¤å’Œé£æ ¼è¡¨ç°çš„åŒæ—¶ï¼Œæ¶ˆè€—æ›´å°‘çš„èµ„æºã€‚æ–‡ç« è¿˜æ¢è®¨äº†åœ¨æ–‡æœ¬ä¸è§†é¢‘åŒ¹é…æ•°æ®æ”¶é›†æ–¹é¢çš„é™åˆ¶ï¼Œä»¥åŠå¦‚ä½•åˆ©ç”¨ç°æœ‰çš„æ–‡æœ¬-å›¾åƒæ¨¡å‹ç”Ÿæˆè§†é¢‘çš„å®è·µæ„ä¹‰ã€‚$http://arxiv.org/pdf/2305.05464v1
Integrating Holistic and Local Information to Estimate Emotional  Reaction Intensity$  Video-based Emotional Reaction Intensity (ERI) estimation measures theintensity of subjects\' reactions to stimuli along several emotional dimensionsfrom videos of the subject as they view the stimuli. We propose a multi-modalarchitecture for video-based ERI combining video and audio information. Videoinput is encoded spatially first, frame-by-frame, combining features encodingholistic aspects of the subjects\' facial expressions and features encodingspatially localized aspects of their expressions. Input is then combined acrosstime: from frame-to-frame using gated recurrent units (GRUs), then globally bya transformer. We handle variable video length with a regression token thataccumulates information from all frames into a fixed-dimensional vectorindependent of video length. Audio information is handled similarly: spectralinformation extracted within each frame is integrated across time by a cascadeof GRUs and a transformer with regression token. The video and audio regressiontokens\' outputs are merged by concatenation, then input to a final fullyconnected layer producing intensity estimates. Our architecture achievedexcellent performance on the Hume-Reaction dataset in the ERI EsimationChallenge of the Fifth Competition on Affective Behavior Analysis in-the-Wild(ABAW5). The Pearson Correlation Coefficients between estimated and subjectself-reported scores, averaged across all emotions, were 0.455 on thevalidation dataset and 0.4547 on the test dataset, well above the baselines.The transformer\'s self-attention mechanism enables our architecture to focus onthe most critical video frames regardless of length. Ablation experimentsestablish the advantages of combining holistic/local features and ofmulti-modal integration. Code available at https://github.com/HKUST-NISL/ABAW5.$Emotional Reaction Intensity (ERI) estimation, video-based affective behavior analysis, gated recurrent units (GRUs), transformer, multi-modal integration, Hume-Reaction dataset, Affective Behavior Analysis in-the-Wild (ABAW5), holistic/local features, facial expressions.$"ä½œè€…ï¼šYini Fang, Liang Wu, Frederic Jumelleå’ŒBertram Shi

æœºæ„ï¼šé¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆHong Kong University of Science and Technologyï¼‰å’ŒBright Nation Limited"$æœ¬è®ºæ–‡æå‡ºä¸€ç§å¤šæ¨¡æ€çš„æ¶æ„ï¼Œç”¨äºåŸºäºè§†é¢‘å’ŒéŸ³é¢‘ä¿¡æ¯æ¥ä¼°è®¡æƒ…æ„Ÿååº”å¼ºåº¦ï¼ˆERIï¼‰ã€‚è§†é¢‘è¾“å…¥é¦–å…ˆé€šè¿‡å¸§ä¸å¸§ä¹‹é—´çš„é—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUsï¼‰è¿›è¡Œç©ºé—´ç¼–ç ï¼Œç»“åˆç¼–ç ä¸»ä½“é¢éƒ¨è¡¨æƒ…æ•´ä½“æ–¹é¢å’Œç¼–ç ç©ºé—´å±€éƒ¨æ–¹é¢çš„åŠŸèƒ½ã€‚ç„¶åï¼Œåœ¨æ—¶é—´ä¸Šè¿›è¡Œæ•´åˆï¼šé€šè¿‡å˜æ¢å™¨å¤„ç†ä»å¸§åˆ°å¸§ï¼Œå¹¶ä½¿ç”¨å›å½’æ ‡è®°å¤„ç†å¯å˜è§†é¢‘é•¿åº¦ã€‚éŸ³é¢‘ä¿¡æ¯åŒæ ·å¤„ç†: æ¯å¸§æå–å‡ºé¢‘è°±ä¿¡æ¯ï¼Œå†é€šè¿‡GRUså’Œå˜æ¢å™¨è¿›è¡Œæ—¶é—´æ•´åˆï¼Œæœ€åé€šè¿‡è¿æ¥èåˆè§†é¢‘å’ŒéŸ³é¢‘å›å½’æ ‡è®°çš„è¾“å‡ºï¼Œè¾“å…¥åˆ°æœ€ç»ˆçš„å®Œå…¨è¿æ¥å±‚ä¸­è¿›è¡Œå¼ºåº¦ä¼°è®¡ã€‚å®éªŒè¡¨æ˜ï¼Œç»¼åˆä½¿ç”¨æ•´ä½“/å±€éƒ¨ç‰¹å¾å’Œå¤šæ¨¡æ€æ•´åˆçš„ä¼˜ç‚¹ã€‚è¿™ç§æ¶æ„åœ¨ABAW5çš„ERIä¼°è®¡æŒ‘æˆ˜ä¸­å…·æœ‰å‡ºè‰²çš„æ€§èƒ½ã€‚æ–‡ç« è®¤ä¸ºï¼Œå¤šæ¨¡æ€æ•´åˆçš„å…³é”®åœ¨äºå˜æ¢å™¨çš„è‡ªæˆ‘å…³æ³¨æœºåˆ¶ï¼Œä½¿æ¶æ„èƒ½å¤Ÿèšç„¦äºæœ€å…³é”®çš„è§†é¢‘å¸§è€Œä¸å—é•¿åº¦é™åˆ¶ã€‚$http://arxiv.org/pdf/2305.05534v1
Fashion CUT: Unsupervised domain adaptation for visual pattern  classification in clothes using synthetic data and pseudo-labels$  Accurate product information is critical for e-commerce stores to allowcustomers to browse, filter, and search for products. Product data quality isaffected by missing or incorrect information resulting in poor customerexperience. While machine learning can be used to correct inaccurate or missinginformation, achieving high performance on fashion image classification tasksrequires large amounts of annotated data, but it is expensive to generate dueto labeling costs. One solution can be to generate synthetic data whichrequires no manual labeling. However, training a model with a dataset of solelysynthetic images can lead to poor generalization when performing inference onreal-world data because of the domain shift. We introduce a new unsuperviseddomain adaptation technique that converts images from the synthetic domain intothe real-world domain. Our approach combines a generative neural network and aclassifier that are jointly trained to produce realistic images whilepreserving the synthetic label information. We found that using real-worldpseudo-labels during training helps the classifier to generalize in thereal-world domain, reducing the synthetic bias. We successfully train a visualpattern classification model in the fashion domain without real-worldannotations. Experiments show that our method outperforms other unsuperviseddomain adaptation algorithms.$Fashion CUT: Unsupervised domain adaptation, synthetic data, pattern classification, fashion image classification, generative neural network, pseudo-labels.$"ä½œè€…ï¼šEnric Moreu, Alex Martinelli, Martina Naughton, Philip Kelly, Noel E. O'Connor
æœºæ„ï¼š1Zalando SE, Valeska-Gert-StraÃŸe 5, 10243 Berlin, Germanyï¼›2Insight Centre for Data Analytics, Dublin City University, Dublin, Ireland"$æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ— ç›‘ç£åŸŸè‡ªé€‚åº”æŠ€æœ¯ï¼Œå°†æ¥è‡ªåˆæˆåŸŸçš„å›¾åƒè½¬æ¢ä¸ºçœŸå®ä¸–ç•Œä¸­çš„å›¾åƒï¼Œå¹¶åœ¨æ­¤è¿‡ç¨‹ä¸­ä¿ç•™æ ‡ç­¾ä¿¡æ¯ï¼Œä»¥æé«˜è§†è§‰æ¨¡å¼åˆ†ç±»æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚æ–‡ç« æå‡ºä½¿ç”¨åˆæˆæ•°æ®æ¥è®­ç»ƒæ¨¡å‹ï¼Œé…åˆçœŸå®ä¸–ç•Œä¼ªæ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œä»¥è¾¾åˆ°æé«˜äº§å“æ•°æ®è´¨é‡çš„ç›®çš„ã€‚ä½œè€…åœ¨æ—¶å°šé¢†åŸŸçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–¹æ³•ä¼˜äºå…¶ä»–æ— ç›‘ç£åŸŸè‡ªé€‚åº”ç®—æ³•ã€‚å…³é”®è¯ï¼šåŸŸè‡ªé€‚åº”ã€åˆæˆæ•°æ®ã€æ¨¡å¼åˆ†ç±»ã€‚$http://arxiv.org/pdf/2305.05580v1
Can point cloud networks learn statistical shape models of anatomies?$  Statistical Shape Modeling (SSM) is a valuable tool for investigating andquantifying anatomical variations within populations of anatomies. However,traditional correspondence-based SSM generation methods require atime-consuming re-optimization process each time a new subject is added to thecohort, making the inference process prohibitive for clinical research.Additionally, they require complete geometric proxies (e.g., high-resolutionbinary volumes or surface meshes) as input shapes to construct the SSM.Unordered 3D point cloud representations of shapes are more easily acquiredfrom various medical imaging practices (e.g., thresholded images and surfacescanning). Point cloud deep networks have recently achieved remarkable successin learning permutation-invariant features for different point cloud tasks(e.g., completion, semantic segmentation, classification). However, theirapplication to learning SSM from point clouds is to-date unexplored. In thiswork, we demonstrate that existing point cloud encoder-decoder-based completionnetworks can provide an untapped potential for SSM, capturing population-levelstatistical representations of shapes while reducing the inference burden andrelaxing the input requirement. We discuss the limitations of these techniquesto the SSM application and suggest future improvements. Our work paves the wayfor further exploration of point cloud deep learning for SSM, a promisingavenue for advancing shape analysis literature and broadening SSM to diverseuse cases.$"Keywords: Statistical Shape Modeling, Point Cloud Deep Networks, Morphometrics, Medical Imaging, Deep Learning. 

This paper explores the potential for point cloud deep networks to learn statistical shape models (SSMs) of anatomical variations within populations of anatomies, which can be used for medical research and clinical applications. Traditional SSM generation methods require complete geometric proxies and time-consuming re-optimization when new subjects are added, while point cloud representations are more easily acquired from medical imaging practices. The authors demonstrate that existing point cloud encoder-decoder-based completion networks can provide an untapped potential for SSM, capturing population-level statistical representations of shapes while reducing the inference burden and relaxing the input requirement. The paper also discusses the limitations and suggests future improvements, paving the way for further exploration of point cloud deep learning for SSM and advancing shape analysis literature."$ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼š Jadie Adamsï¼ˆScientific Computing and Imaging Institute, University of Utah, UT, USAï¼‰; Shireen Elhabianï¼ˆScientific Computing and Imaging Institute, University of Utah, UT, USAï¼› School of Computing, University of Utah, UT, USAï¼‰$æœ¬æ–‡æ¢è®¨äº†ç‚¹äº‘æ·±åº¦ç½‘ç»œæ˜¯å¦å¯ä»¥å­¦ä¹ è§£å‰–å­¦æ—ç¾¤çš„ç»Ÿè®¡å½¢çŠ¶æ¨¡å‹ã€‚ä¼ ç»Ÿçš„å¯¹åº”SSMç”Ÿæˆæ–¹æ³•éœ€è¦åœ¨å°†æ–°çš„ä¸»é¢˜æ·»åŠ åˆ°é˜Ÿåˆ—æ—¶è¿›è¡Œè€—æ—¶çš„é‡æ–°ä¼˜åŒ–è¿‡ç¨‹ï¼Œä½¿å¾—æ¨ç†è¿‡ç¨‹å¯¹ä¸´åºŠç ”ç©¶æ¥è¯´æ˜¯æœ‰éšœç¢çš„ã€‚æ­¤å¤–ï¼Œå®ƒä»¬éœ€è¦å®Œæ•´çš„å‡ ä½•ä»£ç†ï¼ˆä¾‹å¦‚é«˜åˆ†è¾¨ç‡äºŒè¿›åˆ¶ä½“ç§¯æˆ–è¡¨é¢ç½‘æ ¼ï¼‰ä½œä¸ºè¾“å…¥å½¢çŠ¶æ¥æ„å»ºSSMã€‚ç”±äºåŒ»å­¦æˆåƒå®è·µï¼ˆä¾‹å¦‚é˜ˆå€¼å›¾åƒå’Œè¡¨é¢æ‰«æï¼‰æ›´å®¹æ˜“è·å¾—æ— åºçš„3Dç‚¹äº‘è¡¨ç¤ºå½¢çŠ¶ï¼Œå› æ­¤ç‚¹äº‘æ·±åº¦ç½‘ç»œè¿‘æœŸåœ¨å­¦ä¹ ä¸åŒç‚¹äº‘ä»»åŠ¡ï¼ˆä¾‹å¦‚å®Œæˆï¼Œè¯­ä¹‰åˆ†å‰²ï¼Œåˆ†ç±»ï¼‰çš„æ’åˆ—ä¸å˜ç‰¹å¾æ–¹é¢å–å¾—äº†æ˜¾ç€çš„æˆåŠŸã€‚æœ¬æ–‡è¡¨æ˜ç°æœ‰çš„ç‚¹äº‘ç¼–ç å™¨-è§£ç å™¨å®Œæˆç½‘ç»œå¯ä»¥ä¸ºSSMæä¾›æœªå¼€å‘çš„æ½œåŠ›ï¼Œæ•æ‰å½¢çŠ¶çš„æ—ç¾¤çº§ç»Ÿè®¡è¡¨ç¤ºï¼ŒåŒæ—¶å‡è½»æ¨ç†è´Ÿæ‹…å¹¶å‡å°‘è¾“å…¥è¦æ±‚ã€‚æœ¬æ–‡è®¨è®ºäº†è¿™äº›æŠ€æœ¯åœ¨SSMåº”ç”¨ä¸­çš„å±€é™æ€§å¹¶æå‡ºäº†æœªæ¥çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºè¿›ä¸€æ­¥æ¢ç´¢ç‚¹äº‘æ·±åº¦å­¦ä¹ çš„SSMé“ºå¹³äº†é“è·¯ï¼Œè¿™æ˜¯æé«˜å½¢çŠ¶åˆ†ææ–‡çŒ®æ°´å¹³å’Œæ‰©å±•å„ç§ç”¨ä¾‹çš„æœ‰å‰é€”çš„é€”å¾„ã€‚$http://arxiv.org/pdf/2305.05610v1
Adaptive Domain Generalization for Digital Pathology Images$"  In AI-based histopathology, domain shifts are common and well-studied.However, this research focuses on stain and scanner variations, which do notshow the full picture -- shifts may be combinations of other shifts, or""invisible"" shifts that are not obvious but still damage performance of machinelearning models. Furthermore, it is important for models to generalize to theseshifts without expensive or scarce annotations, especially in thehistopathology space and if wanting to deploy models on a larger scale. Thus,there is a need for ""reactive"" domain generalization techniques: ones thatadapt to domain shifts at test-time rather than requiring predictions of orexamples of the shifts at training time. We conduct a literature review andintroduce techniques that react to domain shifts rather than requiring aprediction of them in advance. We investigate test time training, a techniquefor domain generalization that adapts model parameters at test-time throughoptimization of a secondary self-supervised task."$Digital pathology, Domain generalization, Adaptive learning.$"ä½œè€…ï¼šAndrew John Walkerï¼ˆæ˜å°¼è‹è¾¾å¤§å­¦ï¼‰
æœºæ„ï¼šæ˜å°¼è‹è¾¾å¤§å­¦"$è¿™ç¯‡è®ºæ–‡æ˜¯å…³äºæ•°å­—ç—…ç†å­¦å›¾åƒçš„è‡ªé€‚åº”é¢†åŸŸæ³›åŒ–çš„ç ”ç©¶ï¼Œæ—¨åœ¨è§£å†³åœ¨æµ‹è¯•æ•°æ®ä¸­å­˜åœ¨ä¸åŒäºè®­ç»ƒæ•°æ®çš„é¢†åŸŸæ¼‚ç§»é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºé€‚åº”æ€§åŸŸåˆ†ç±»å™¨çš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨æŠ—æ··æ·†æŸå¤±æ¥å®ç°è‡ªé€‚åº”é¢†åŸŸæ³›åŒ–ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æé«˜äº†æ•°å­—ç—…ç†å­¦å›¾åƒåˆ†ç±»æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†é¢†åŸŸæ¼‚ç§»é—®é¢˜ã€‚$http://arxiv.org/pdf/2305.05100v1
Towards unraveling calibration biases in medical image analysis$  In recent years the development of artificial intelligence (AI) systems forautomated medical image analysis has gained enormous momentum. At the sametime, a large body of work has shown that AI systems can systematically andunfairly discriminate against certain populations in various applicationscenarios. These two facts have motivated the emergence of algorithmic fairnessstudies in this field. Most research on healthcare algorithmic fairness to datehas focused on the assessment of biases in terms of classical discriminationmetrics such as AUC and accuracy. Potential biases in terms of modelcalibration, however, have only recently begun to be evaluated. This isespecially important when working with clinical decision support systems, aspredictive uncertainty is key for health professionals to optimally evaluateand combine multiple sources of information. In this work we studydiscrimination and calibration biases in models trained for automatic detectionof malignant dermatological conditions from skin lesions images. Importantly,we show how several typically employed calibration metrics are systematicallybiased with respect to sample sizes, and how this can lead to erroneousfairness analysis if not taken into consideration. This is of particularrelevance to fairness studies, where data imbalance results in drastic samplesize differences between demographic sub-groups, which, if not taken intoaccount, can act as confounders.$Medical image analysis, algorithmic fairness, bias, calibration, skin lesion analysis. This paper highlights the importance of evaluating biases in terms of model calibration, especially for clinical decision support systems. The study focuses on discrimination and calibration biases in models trained for automatic detection of malignant dermatological conditions from skin lesion images. The paper also highlights how several calibration metrics are systematically biased with respect to sample sizes, which can lead to erroneous fairness analysis if not taken into consideration. Finally, the authors emphasize the relevance of fairness studies for developing trustworthy AI systems in healthcare.$ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šMarÂ´Ä±a Agustina Ricci Lara1,2, Candelaria Mosquera1,2, Enzo Ferrante3, Rodrigo Echeveste3ï¼›1 åŒ»å­¦ä¿¡æ¯å­¦ç³»ï¼Œå¸ƒå®œè¯ºæ–¯è‰¾åˆ©æ–¯æ„å¤§åˆ©åŒ»é™¢ï¼Œé˜¿æ ¹å»·å¸ƒå®œè¯ºæ–¯è‰¾åˆ©æ–¯ï¼›2 é˜¿æ ¹å»·å¸ƒå®œè¯ºæ–¯è‰¾åˆ©æ–¯å›½ç«‹æŠ€æœ¯å¤§å­¦ï¼›3 ä¿¡å·ã€ç³»ç»Ÿå’Œè®¡ç®—æ™ºèƒ½ç ”ç©¶æ‰€ sinc(i) ï¼ˆFICH-UNL/CONICETï¼‰ï¼Œé˜¿æ ¹å»·åœ£è²ã€‚$è¿™ç¯‡è®ºæ–‡è®¨è®ºäº†åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­å‘ç°å’Œè¯„ä¼°ç®—æ³•å…¬å¹³æ€§æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä½œè€…ç ”ç©¶äº†è‡ªåŠ¨æ£€æµ‹æ¶æ€§çš®è‚¤ç—…å˜æ¨¡å‹çš„æ­§è§†å’Œæ ¡å‡†åå·®ï¼Œå¹¶æ¢è®¨äº†å‡ ç§å¸¸ç”¨çš„æ ¡å‡†è¯„ä¼°æŒ‡æ ‡åœ¨æ ·æœ¬è§„æ¨¡æ–¹é¢çš„åå·®ï¼Œä»¥åŠè¿™å¯èƒ½å¯¼è‡´é”™è¯¯çš„å…¬å¹³æ€§åˆ†æã€‚åœ¨è¿™ä¸ªé¢†åŸŸä¸­ï¼Œå…¬å¹³æ€§ç ”ç©¶çš„æ•°æ®ä¸å¹³è¡¡ä¼šå¯¼è‡´ä¸åŒäººå£å­ç¾¤ä¹‹é—´çš„æ ·æœ¬é‡å·®å¼‚ï¼Œåœ¨æœªè€ƒè™‘è¿™ä¸€å› ç´ æ—¶ï¼Œç»“æœä¼šå‡ºç°æ··æ·†è¯¯å·®ã€‚å› æ­¤ï¼Œè¯„ä¼°AIç®—æ³•çš„å…¬å¹³æ€§æ˜¯æ„å»ºå¯ä¿¡ç³»ç»Ÿçš„é‡è¦æ­¥éª¤ã€‚ä½œè€…è®¤ä¸ºï¼Œåœ¨åŒ»å­¦å½±åƒè®¡ç®—é¢†åŸŸï¼Œè¯„ä¼°åˆ†ç±»æ€§èƒ½åº”ç†è§£ä¸ºåˆ¤åˆ«èƒ½åŠ›å’Œæ ¡å‡†èƒ½åŠ›çš„ç»„åˆï¼Œéœ€è¦è¿›è¡Œå…¨é¢çš„è¯„ä¼°ã€‚$http://arxiv.org/pdf/2305.05101v1
DeepTree: Modeling Trees with Situated Latents$  In this paper, we propose DeepTree, a novel method for modeling trees basedon learning developmental rules for branching structures instead of manuallydefining them. We call our deep neural model situated latent because itsbehavior is determined by the intrinsic state -- encoded as a latent space of adeep neural model -- and by the extrinsic (environmental) data that is situatedas the location in the 3D space and on the tree structure. We use a neuralnetwork pipeline to train a situated latent space that allows us to locallypredict branch growth only based on a single node in the branch graph of a treemodel. We use this representation to progressively develop new branch nodes,thereby mimicking the growth process of trees. Starting from a root node, atree is generated by iteratively querying the neural network on the newly addednodes resulting in the branching structure of the whole tree. Our methodenables generating a wide variety of tree shapes without the need to defineintricate parameters that control their growth and behavior. Furthermore, weshow that the situated latents can also be used to encode the environmentalresponse of tree models, e.g., when trees grow next to obstacles. We validatethe effectiveness of our method by measuring the similarity of our tree modelsand by procedurally generated ones based on a number of established metrics fortree form.$Key words: Deep Learning, Situated Latent, Tree Modeling, Procedural Modeling, Botanical Tree Models, Shape Modeling, Generative Methods, Computer Vision Problems, Developmental Modeling.$ä½œè€…ï¼šXIAOCHEN ZHOU, BOSHENG LI, BEDRICH BENES, SONGLIN FEI æœºæ„ï¼šPurdue University, USAï¼›SÃ–REN PIRK, æœºæ„ï¼šAdobe Research, USAã€‚$è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDeepTreeçš„ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ æ ‘æœ¨åˆ†æç»“æ„çš„å‘è‚²è§„å¾‹ï¼Œè€Œéæ‰‹åŠ¨å®šä¹‰åˆ†æç»“æ„æ¥æ¨¡æ‹Ÿæ ‘æœ¨çš„ç”Ÿé•¿è¿‡ç¨‹ã€‚ä½œè€…ä½¿ç”¨ç¥ç»ç½‘ç»œç®¡é“è®­ç»ƒä¸€ä¸ªåä¸ºâ€œsituated latentâ€çš„æ¨¡å‹ï¼Œå®ƒçš„è¡Œä¸ºç”±éšç©ºé—´ç¼–ç çš„å†…åœ¨çŠ¶æ€å’Œâ€œsituatedâ€åœ¨3Dç©ºé—´å’Œæ ‘ç»“æ„ä¸Šçš„å¤–æºæ€§æ•°æ®å†³å®šã€‚ä½œè€…é€šè¿‡å±€éƒ¨é¢„æµ‹å•ä¸ªèŠ‚ç‚¹çš„åˆ†æ”¯ç”Ÿé•¿æ¥é€æ­¥å¼€å‘æ–°çš„åˆ†æ”¯èŠ‚ç‚¹ï¼Œä»è€Œæ¨¡æ‹Ÿæ ‘æœ¨çš„ç”Ÿé•¿è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•æ— éœ€å®šä¹‰å¤æ‚çš„å‚æ•°å³å¯ç”Ÿæˆå¤šæ ·çš„æ ‘æœ¨å½¢çŠ¶ã€‚ä½œè€…è¿˜è¯´æ˜äº†åœ¨DeepTreeä¸­ï¼Œsituated latentså¯ä»¥ç”¨äºç¼–ç æ ‘æœ¨æ¨¡å‹çš„ç¯å¢ƒå“åº”ï¼Œä¾‹å¦‚å½“æ ‘æœ¨ç”Ÿé•¿åœ¨éšœç¢ç‰©æ—è¾¹æ—¶ã€‚ä½œè€…é€šè¿‡å¤šä¸ªæ ‘å½¢æ€åº¦é‡è¿›è¡ŒéªŒè¯ï¼Œè¯æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚$http://arxiv.org/pdf/2305.05153v1
Semantic Embedded Deep Neural Network: A Generic Approach to Boost  Multi-Label Image Classification Performance$  Fine-grained multi-label classification models have broad applications inAmazon production features, such as visual based label predictions ranging fromfashion attribute detection to brand recognition. One challenge to achievesatisfactory performance for those classification tasks in real world is thewild visual background signal that contains irrelevant pixels which confusesmodel to focus onto the region of interest and make prediction upon thespecific region. In this paper, we introduce a generic semantic-embedding deepneural network to apply the spatial awareness semantic feature incorporating achannel-wise attention based model to leverage the localization guidance toboost model performance for multi-label prediction. We observed an Avg.relativeimprovement of 15.27% in terms of AUC score across all labels compared to thebaseline approach. Core experiment and ablation studies involve multi-labelfashion attribute classification performed on Instagram fashion apparels\'image. We compared the model performances among our approach, baselineapproach, and 3 alternative approaches to leverage semantic features. Resultsshow favorable performance for our approach.$Keywords: Semantic-embedding, deep neural network, multi-label image classification, spatial awareness, attention-based model, fashion attribute detection, wild visual backgrounds, regional semantic information, class activation maps, imbalanced distribution.$"ä½œè€…ï¼šXin Shen, Xiaonan Zhao, Rui Luo
æœºæ„ï¼šAmazon.com"$æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„é€šç”¨è¯­ä¹‰åµŒå…¥æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥åŸºäºé€šé“çš„æ³¨æ„åŠ›æ¨¡å‹æ¥åº”ç”¨ç©ºé—´æ„ŸçŸ¥è¯­ä¹‰ç‰¹å¾ï¼Œä»è€Œæå‡å¤šæ ‡ç­¾å›¾åƒåˆ†ç±»æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨Instagramæ—¶å°šæœè£…å›¾åƒä¸Šè¿›è¡Œçš„å¤šæ ‡ç­¾æ—¶å°šå±æ€§åˆ†ç±»å®éªŒä»¥åŠå¯¹æ¯”å®éªŒæ˜¾ç¤ºå‡ºäº†æœ¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œç›¸æ¯”åŸºçº¿æ–¹æ³•ï¼Œå¹³å‡AUCè¯„åˆ†ç›¸å¯¹æå‡äº†15.27%ã€‚è¯¥æ–¹æ³•çš„ä¸»è¦è´¡çŒ®åœ¨äºå°†åˆ†ç±»æ ‡ç­¾ç”¨ä½œç±»æ¿€æ´»æ˜ å°„ç”Ÿæˆå™¨ï¼Œå­¦ä¹ å„åƒç´ çš„è¯­ä¹‰åµŒå…¥ï¼Œå¹¶å°†å…¶ä¸åŸå§‹å›¾åƒå¼ é‡è¿æ¥ä½œä¸ºè¾“å…¥ç‰¹å¾ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦è¯­ä¹‰åˆ†å‰²æ¨¡å‹å’Œç‰©ä½“æ£€æµ‹æ¨¡å‹çš„é¢„è®­ç»ƒï¼Œé€‚ç”¨æ€§æ›´å¼ºã€‚$http://arxiv.org/pdf/2305.05228v1
DietCNN: Multiplication-free Inference for Quantized CNNs$  The rising demand for networked embedded systems with machine intelligencehas been a catalyst for sustained attempts by the research community toimplement Convolutional Neural Networks (CNN) based inferencing on embeddedresource-limited devices. Redesigning a CNN by removing costly multiplicationoperations has already shown promising results in terms of reducing inferenceenergy usage. This paper proposes a new method for replacing multiplications ina CNN by table look-ups. Unlike existing methods that completely modify the CNNoperations, the proposed methodology preserves the semantics of the major CNNoperations. Conforming to the existing mechanism of the CNN layer operationsensures that the reliability of a standard CNN is preserved. It is shown thatthe proposed multiplication-free CNN, based on a single activation codebook,can achieve 4.7x, 5.6x, and 3.5x reduction in energy per inference in an FPGAimplementation of MNIST-LeNet-5, CIFAR10-VGG-11, and Tiny ImageNet-ResNet-18respectively. Our results show that the DietCNN approach significantly improvesthe resource consumption and latency of deep inference for smaller models,often used in embedded systems. Our code is available at:https://github.com/swadeykgp/DietCNN$Specific domains and keywords discussed in this paper include: networked embedded systems, Convolutional Neural Networks (CNNs), multiplication-free inference, table look-ups, resource-limited devices, energy efficiency, CNN operations, activation codebook, FPGA implementation, MNIST-LeNet-5, CIFAR10-VGG-11, Tiny ImageNet-ResNet-18, deep inference, resource consumption, latency, small models, and edge-enabled IoT devices.$"ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šSwarnava Deyï¼ˆTata Consultancy Services Ltd. & Indian Institute of Technology Kharagpurï¼‰ã€Pallab Dasguptaå’ŒPartha P Chakrabartiï¼ˆIndian Institute of Technology Kharagpurï¼‰

è®ºæ–‡æ ‡é¢˜ï¼šDietCNN: Multiplication-free Inference for Quantized CNNs (Supplementary Material and author's Draft)

æ‘˜è¦ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡è¡¨æŸ¥æ‰¾æ¥æ›¿æ¢CNNä¸­çš„ä¹˜æ³•è¿ç®—ã€‚ä¸ç°æœ‰æ–¹æ³•å®Œå…¨ä¿®æ”¹CNNæ“ä½œä¸åŒï¼Œè¯¥æ–¹æ³•ä¿ç•™äº†ä¸»è¦CNNæ“ä½œçš„è¯­ä¹‰ã€‚ç¬¦åˆCNNå±‚æ“ä½œçš„ç°æœ‰æœºåˆ¶å¯ä»¥ç¡®ä¿æ ‡å‡†CNNçš„å¯é æ€§ã€‚å°†åŸºäºå•ä¸ªæ¿€æ´»ä»£ç æœ¬çš„æå‡ºçš„æ— ä¹˜æ³•CNNåº”ç”¨äºFPGAå®ç°çš„MNIST-LeNet-5ã€CIFAR10-VGG-11å’ŒTiny ImageNet-ResNet-18ï¼Œåˆ†åˆ«å¯ä»¥è·å¾—4.7å€ã€5.6å€å’Œ3.5å€çš„æ¨ç†èƒ½é‡æ¶ˆè€—å‡å°ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—æ”¹å–„äº†åœ¨åµŒå…¥å¼ç³»ç»Ÿä¸­ä½¿ç”¨çš„å°å‹æ¨¡å‹çš„èµ„æºæ¶ˆè€—å’Œå»¶è¿Ÿã€‚è¯¥æ–¹æ³•çš„ä»£ç å¯åœ¨https://github.com/swadeykgp/DietCNNè·å¾—ã€‚"$æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰è¿›è¡Œé‡åŒ–æ¨ç†çš„æ— ä¹˜æ³•æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è¡¨æŸ¥æ‰¾æ›¿ä»£ä¹˜æ³•ï¼Œç›¸æ¯”äºå·²æœ‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¿ç•™äº†CNNæ“ä½œçš„è¯­ä¹‰ï¼Œä»¥ç¡®ä¿æ ‡å‡†CNNçš„å¯é æ€§å’Œæ­£ç¡®æ€§ã€‚ä½œè€…è¿˜ä½¿ç”¨å•ä¸€æ¿€æ´»ç æœ¬çš„åŸºäºDietCNNæ— ä¹˜æ³•CNNï¼Œåœ¨FPGAå®ç°MNIST-LeNet-5ã€CIFAR10-VGG-11å’ŒTiny ImageNet-ResNet-18çš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œåˆ†åˆ«å®ç°äº†4.7xã€5.6xå’Œ3.5xçš„èƒ½è€—é™ä½ã€‚è¯¥æ–¹æ³•ä½¿å°å‹æ¨¡å‹åµŒå…¥å¼ç³»ç»Ÿä¸­çš„èµ„æºæ¶ˆè€—å’Œå»¶è¿Ÿå¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ã€‚$http://arxiv.org/pdf/2305.05274v1
Learning Dynamic Point Cloud Compression via Hierarchical Inter-frame  Block Matching$  3D dynamic point cloud (DPC) compression relies on mining its temporalcontext, which faces significant challenges due to DPC\'s sparsity andnon-uniform structure. Existing methods are limited in capturing sufficienttemporal dependencies. Therefore, this paper proposes a learning-based DPCcompression framework via hierarchical block-matching-based inter-predictionmodule to compensate and compress the DPC geometry in latent space.Specifically, we propose a hierarchical motion estimation and motioncompensation (Hie-ME/MC) framework for flexible inter-prediction, whichdynamically selects the granularity of optical flow to encapsulate the motioninformation accurately. To improve the motion estimation efficiency of theproposed inter-prediction module, we further design a KNN-attention blockmatching (KABM) network that determines the impact of potential correspondingpoints based on the geometry and feature correlation. Finally, we compress theresidual and the multi-scale optical flow with a fully-factorized deep entropymodel. The experiment result on the MPEG-specified Owlii Dynamic Human DynamicPoint Cloud (Owlii) dataset shows that our framework outperforms the previousstate-of-the-art methods and the MPEG standard V-PCC v18 in inter-framelow-delay mode.$Keywords: dynamic point cloud compression, inter-frame block matching, hierarchical motion estimation, optical flow estimation, deep learning.$"ä½œè€…ï¼šå¤èˆ’å©·ã€èŒƒåº­å®‡ã€è®¸äº¦å‡Œã€Jenq-Neng Hwangã€æç«¹

æœºæ„ï¼šä¸Šæµ·äº¤é€šå¤§å­¦åˆä½œå¼å¤šåª’ä½“åˆ›æ–°ä¸­å¿ƒã€åç››é¡¿å¤§å­¦ã€å¯†è‹é‡Œå¤§å­¦å ªè¨æ–¯åŸ"$æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå±‚çº§å—åŒ¹é…çš„äº¤å‰é¢„æµ‹æ¨¡å—çš„å­¦ä¹ å‹DPCå‹ç¼©æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ä¸­è¡¥å¿å’Œå‹ç¼©DPCå‡ ä½•ä½“ã€‚ä½œè€…è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ä¸ªKNN-attentionå—åŒ¹é…ç½‘ç»œï¼Œä»¥ç¡®å®šæ½œåœ¨å¯¹åº”ç‚¹çš„å½±å“ï¼Œå¹¶ç»“åˆå¤šå°ºåº¦å…‰æµå‹ç¼©æ®‹å·®ã€‚åœ¨MPEGæŒ‡å®šçš„Owliiäººç±»åŠ¨æ€ç‚¹äº‘æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨äº¤å‰å¸§ä½å»¶è¿Ÿæ¨¡å¼ä¸‹ä¼˜äºå…ˆå‰çš„æœ€å…ˆè¿›æ–¹æ³•å’ŒMPEGæ ‡å‡†V-PCC v18ã€‚$http://arxiv.org/pdf/2305.05356v1
Investigating the Corruption Robustness of Image Classifiers with Random  Lp-norm Corruptions$  Robustness is a fundamental property of machine learning classifiers toachieve safety and reliability. In the fields of adversarial robustness andformal robustness verification of image classification models, robustness iscommonly defined as the stability to all input variations within an Lp-normdistance. However, robustness to random corruptions is usually improved andevaluated using variations observed in the real-world, while mathematicallydefined Lp-norm corruptions are rarely considered. This study investigates theuse of random Lp-norm corruptions to augment the training and test data ofimage classifiers. We adapt an approach from the field of adversarialrobustness to assess the model robustness to imperceptible random corruptions.We empirically and theoretically investigate whether robustness is transferableacross different Lp-norms and derive conclusions on which Lp-norm corruptions amodel should be trained and evaluated on. We find that training dataaugmentation with L0-norm corruptions improves corruption robustness whilemaintaining accuracy compared to standard training and when applied on top ofselected state-of-the-art data augmentation techniques.$Keywords: Machine learning; Image classification; Robustness; Lp-norm corruptions; Data augmentation.$ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šGeorg Siedelï¼ˆå¾·å›½è”é‚¦èŒä¸šå®‰å…¨ä¸å¥åº·ç ”ç©¶æ‰€ï¼‰ã€Silvia Vockaå’ŒAndrey Morozovï¼ˆæ–¯å›¾åŠ ç‰¹å¤§å­¦ï¼‰ã€‚$æœ¬è®ºæ–‡æ¢ç©¶äº†åˆ©ç”¨éšæœºLpèŒƒæ•°æŠ—æ±¡æŸ“æ¥å¢å¼ºå›¾åƒåˆ†ç±»å™¨çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚æ–‡ç« çš„èƒŒæ™¯æ˜¯åœ¨å®é™…åº”ç”¨ä¸­ï¼Œåœ¨ä¿è¯ç²¾åº¦çš„æƒ…å†µä¸‹ï¼Œåˆ†ç±»å™¨å¿…é¡»è¶³å¤Ÿé²æ£’æ€§ã€‚ä½œè€…ç”¨ä¸€ç§åœ¨å¯¹æŠ—é²æ£’æ€§é¢†åŸŸç»å¸¸ä½¿ç”¨çš„æ–¹æ³•æ¥è¯„ä¼°æ¨¡å‹å¯¹è¯¯å·®çš„é²æ£’æ€§ã€‚æ–‡ç« ä»å®éªŒå’Œå®šé‡ç ”ç©¶ä¸¤æ–¹é¢æ¢ç©¶äº†ä¸åŒLpèŒƒæ•°ä¸‹çš„é²æ£’æ€§é—®é¢˜ï¼Œå‘ç°é€šè¿‡è®­ç»ƒæ•°æ®åŠ å¼ºï¼ŒL0èŒƒæ•°æ±¡æŸ“å¯ä»¥æé«˜æ¨¡å‹çš„æ±¡æŸ“é²æ£’æ€§ã€‚åŒæ—¶ï¼Œè¯¥è®ºæ–‡æ¢ç©¶äº†ä¸åŒçš„LpèŒƒæ•°å’Œé²æ£’æ€§ç›®æ ‡å¯¹äºæ¨¡å‹LpèŒƒæ•°ä¸‹çš„æ±¡æŸ“é²æ£’æ€§çš„å½±å“ã€‚æœ€ç»ˆï¼Œä½œè€…æå‡ºäº†ç»“è®ºï¼Œå»ºè®®åœ¨æ ‡å‡†è®­ç»ƒåŸºç¡€ä¸Šå¼•å…¥L0èŒƒæ•°æ±¡æŸ“ï¼Œæ¥æé«˜æ±¡æŸ“é²æ£’æ€§ã€‚$http://arxiv.org/pdf/2305.05400v1
Echo from noise: synthetic ultrasound image generation using diffusion  models for real image segmentation$"  We propose a novel pipeline for the generation of synthetic images viaDenoising Diffusion Probabilistic Models (DDPMs) guided by cardiac ultrasoundsemantic label maps. We show that these synthetic images can serve as a viablesubstitute for real data in the training of deep-learning models for medicalimage analysis tasks such as image segmentation. To demonstrate theeffectiveness of this approach, we generated synthetic 2D echocardiographyimages and trained a neural network for segmentation of the left ventricle andleft atrium. The performance of the network trained on exclusively syntheticimages was evaluated on an unseen dataset of real images and yielded mean Dicescores of 88.5 $\\pm 6.0$ , 92.3 $\\pm 3.9$, 86.3 $\\pm 10.7$ \\% for leftventricular endocardial, epicardial and left atrial segmentation respectively.This represents an increase of $9.09$, $3.7$ and $15.0$ \\% in Dice scorescompared to the previous state-of-the-art. The proposed pipeline has thepotential for application to a wide range of other tasks across various medicalimaging modalities."$Keywords: Echo, Ultrasound, Image segmentation, Deep learning, Synthetic images, Denoising Diffusion Probabilistic Models (DDPMs), Medical image analysis.$"ä½œè€…ï¼šDavid Stojanovski, Uxio Hermida, Pablo Lamata, Arian Beqiri, å’Œ Alberto Gomez
æœºæ„ï¼š1ï¼‰å›½ç‹å­¦é™¢ä¼¦æ•¦ï¼›2ï¼‰Ultromics

æ‘˜è¦ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰å’Œå¿ƒè„è¶…å£°è¯­ä¹‰æ ‡ç­¾å›¾çš„åˆæˆå›¾åƒç”Ÿæˆæ–¹æ³•ã€‚æˆ‘ä»¬å±•ç¤ºäº†è¿™äº›åˆæˆå›¾åƒå¯ä»¥ä½œä¸ºè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ç”¨äºåŒ»å­¦å›¾åƒåˆ†æä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†å‰²ï¼‰ä¸­çš„çœŸå®æ•°æ®çš„å¯è¡Œæ›¿ä»£å“ã€‚ä¸ºäº†å±•ç¤ºè¿™ç§æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†äººå·¥åˆæˆçš„2Då¿ƒè„è¶…å£°å›¾åƒï¼Œå¹¶è®­ç»ƒäº†ä¸€ä¸ªç¥ç»ç½‘ç»œç”¨äºåˆ†å‰²å·¦å¿ƒå®¤å’Œå·¦å¿ƒæˆ¿ã€‚åœ¨ä»…ä½¿ç”¨äººå·¥åˆæˆå›¾åƒè¿›è¡Œè®­ç»ƒçš„ç½‘ç»œçš„æ€§èƒ½åœ¨çœŸå®å›¾åƒçš„æœªè§æ•°æ®é›†ä¸Šè¢«è¯„ä¼°ï¼Œå¹¶å¾—åˆ°äº†88.5Â±6.0ï¼…ï¼Œ92.3Â±3.9ï¼…ï¼Œ86.3Â±10.7ï¼…çš„å‡å€¼Diceå¾—åˆ†ï¼Œåˆ†åˆ«ç”¨äºå·¦å®¤å†…è†œã€å¤–è†œå’Œå·¦å¿ƒæˆ¿åˆ†å‰²ã€‚ç›¸æ¯”ä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¿™è¡¨ç¤ºDiceå¾—åˆ†åˆ†åˆ«å¢åŠ äº†9.09ï¼…ã€3.7ï¼…å’Œ15.0ï¼…ã€‚è¯¥æ–¹æ³•æœ‰æ½œåŠ›åº”ç”¨äºå„ç§åŒ»å­¦æˆåƒæ¨¡æ€çš„å…¶ä»–ä»»åŠ¡ä¸­ã€‚

å…³é”®è¯ï¼šæ‰©æ•£æ¨¡å‹Â·å›¾åƒåˆæˆÂ·è¶…å£°"$æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰å’Œå¿ƒè„è¶…å£°è¯­ä¹‰æ ‡ç­¾åœ°å›¾å¼•å¯¼åˆæˆå›¾åƒçš„æ–°å‹æµç¨‹ã€‚æˆ‘ä»¬å±•ç¤ºäº†è¿™äº›åˆæˆå›¾åƒå¯ä»¥ä½œä¸ºè®­ç»ƒåŒ»å­¦å›¾åƒåˆ†æä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†å‰²ï¼‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¯è¡Œæ›¿ä»£çœŸå®æ•°æ®çš„æ–¹æ³•ã€‚ä¸ºäº†è¯æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬ç”Ÿæˆäº† 2D è¶…å£°å¿ƒåŠ¨å›¾ï¼Œå¹¶è®­ç»ƒäº†ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œç”¨äºå·¦å¿ƒå®¤å’Œå·¦å¿ƒæˆ¿çš„åˆ†å‰²ã€‚åœ¨ä»…ä½¿ç”¨åˆæˆå›¾åƒè®­ç»ƒçš„ç½‘ç»œåœ¨æœªè§è¿‡çš„çœŸå®å›¾åƒæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œåˆ†åˆ«å¾—åˆ°å·¦å¿ƒå®¤å†…è†œã€å¿ƒå¤–è†œå’Œå·¦å¿ƒæˆ¿åˆ†å‰²çš„ Dice åˆ†æ•°å‡å€¼ä¸º 88.5Â±6.0%ã€92.3Â±3.9% å’Œ 86.3Â±10.7%ã€‚è¯¥æ–¹æ³•æœ‰æ½œåœ¨çš„åº”ç”¨äºå„ç§åŒ»å­¦å½±åƒæ¨¡æ€ä¸‹çš„å…¶ä»–ä»»åŠ¡ã€‚$http://arxiv.org/pdf/2305.05424v1
StyleSync: High-Fidelity Generalized and Personalized Lip Sync in  Style-based Generator$  Despite recent advances in syncing lip movements with any audio waves,current methods still struggle to balance generation quality and the model\'sgeneralization ability. Previous studies either require long-term data fortraining or produce a similar movement pattern on all subjects with lowquality. In this paper, we propose StyleSync, an effective framework thatenables high-fidelity lip synchronization. We identify that a style-basedgenerator would sufficiently enable such a charming property on both one-shotand few-shot scenarios. Specifically, we design a mask-guided spatialinformation encoding module that preserves the details of the given face. Themouth shapes are accurately modified by audio through modulated convolutions.Moreover, our design also enables personalized lip-sync by introducing stylespace and generator refinement on only limited frames. Thus the identity andtalking style of a target person could be accurately preserved. Extensiveexperiments demonstrate the effectiveness of our method in producinghigh-fidelity results on a variety of scenes. Resources can be found athttps://hangz-nju-cuhk.github.io/projects/StyleSync.$Keywords: lip sync, style-based generator, personalized optimization, modulated convolutions.$ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šç®¡å®¶æ™ºï¼ˆç™¾åº¦ï¼‰ã€å¼ å±•æ—ºï¼ˆç™¾åº¦ï¼‰ã€å‘¨èˆªï¼ˆç™¾åº¦ï¼‰ã€èƒ¡å¤©èˆ’ï¼ˆç™¾åº¦ï¼‰ã€ç‹å¼€æ€è¿œï¼ˆæ‚‰å°¼å¤§å­¦ï¼‰ã€ä½•ä¸œäº®ï¼ˆç™¾åº¦ï¼‰ã€å†¯æµ©ç¨‹ï¼ˆç™¾åº¦ï¼‰ã€åˆ˜æ•¬æ‹“ï¼ˆç™¾åº¦ï¼‰ã€ä¸å°”ç¿ï¼ˆç™¾åº¦ï¼‰ã€åˆ˜ç´«è–‡ï¼ˆå—æ´‹ç†å·¥å¤§å­¦ï¼‰ã€ç‹äº¬ä¸œï¼ˆç™¾åº¦ï¼‰ã€‚$æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºStyleSyncçš„æ¡†æ¶ï¼Œå¯ä»¥å®ç°é«˜ä¿çœŸçš„å”‡éƒ¨åŒæ­¥ã€‚è¯¥æ–¹æ³•æå‡ºäº†åŸºäºæ ·å¼çš„ç”Ÿæˆå™¨ï¼Œä»¥å®ç°ä¸€æ¬¡æ€§å’Œå°‘é‡è®­ç»ƒæ•°æ®çš„é«˜è´¨é‡è¾“å‡ºã€‚é€šè¿‡æ©æ¨¡å¼•å¯¼çš„ç©ºé—´ä¿¡æ¯ç¼–ç æ¨¡å—ï¼Œèƒ½å¤Ÿä¿ç•™ç»™å®šé¢éƒ¨çš„ç»†èŠ‚ä¿¡æ¯ï¼›é€šè¿‡è°ƒåˆ¶å·ç§¯å¯ä»¥å‡†ç¡®ä¿®æ”¹éŸ³é¢‘ä¸‹çš„å˜´å½¢ã€‚æ­¤å¤–ï¼Œè¯¥è®¾è®¡è¿˜é€šè¿‡å¼•å…¥æ ·å¼ç©ºé—´å’Œç”Ÿæˆå™¨ä¼˜åŒ–ï¼Œä»…å¯¹æœ‰é™çš„å¸§è¿›è¡Œä¸ªæ€§åŒ–çš„å”‡éƒ¨åŒæ­¥ï¼Œä»è€Œå¯ä»¥å‡†ç¡®ä¿ç•™ç›®æ ‡äººç‰©çš„èº«ä»½å’Œå£å¤´é£æ ¼ã€‚å®éªŒè¡¨æ˜äº†è¯¥æ–¹æ³•åœ¨å„ç§åœºæ™¯ä¸‹ç”Ÿæˆé«˜ä¿çœŸçš„ç»“æœçš„æœ‰æ•ˆæ€§ã€‚$http://arxiv.org/pdf/2305.05445v1
Effects of Real-Life Traffic Sign Alteration on YOLOv7- an Object  Recognition Model$  The advancement of Image Processing has led to the widespread use of ObjectRecognition (OR) models in various applications, such as airport security andmail sorting. These models have become essential in signifying the capabilitiesof AI and supporting vital services like national postal operations. However,the performance of OR models can be impeded by real-life scenarios, such astraffic sign alteration. Therefore, this research investigates the effects ofaltered traffic signs on the accuracy and performance of object recognitionmodels. To this end, a publicly available dataset was used to create differenttypes of traffic sign alterations, including changes to size, shape, color,visibility, and angles. The impact of these alterations on the YOLOv7 (You OnlyLook Once) model\'s detection and classification abilities were analyzed. Itreveals that the accuracy of object detection models decreases significantlywhen exposed to modified traffic signs under unlikely conditions. This studyhighlights the significance of enhancing the robustness of object detectionmodels in real-life scenarios and the need for further investigation in thisarea to improve their accuracy and reliability.$Keywords: object recognition, traffic sign alteration, YOLOv7 model, accuracy, performance, real-world scenarios, image processing, artificial intelligence.$ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šFarhin Farhad Riya, Shahinul Hoque, Md Saif Hassan Onim, Edward Michaud, and Edmon Begoliï¼ˆç”°çº³è¥¿å¤§å­¦ç”µå­å·¥ç¨‹ä¸è®¡ç®—æœºç§‘å­¦ç³»ï¼‰$æœ¬æ–‡ç ”ç©¶äº†äº¤é€šæ ‡å¿—çš„å®é™…æ”¹å˜å¯¹ç‰©ä½“è¯†åˆ«æ¨¡å‹YOLOv7çš„å½±å“ã€‚æ–‡ç« æŒ‡å‡ºï¼Œç°å®ç”Ÿæ´»ä¸­çš„åœºæ™¯ï¼Œå¦‚äº¤é€šæ ‡å¿—çš„æ”¹å˜ä¼šå½±å“ç‰©ä½“è¯†åˆ«æ¨¡å‹çš„æ€§èƒ½ï¼Œè€Œè¯†åˆ«äº¤é€šæ ‡å¿—çš„å‡†ç¡®æ€§å¯¹äºç¡®ä¿é“è·¯å®‰å…¨è‡³å…³é‡è¦ï¼Œå› æ­¤éœ€è¦æ¢ç©¶äº¤é€šæ ‡å¿—æ”¹å˜å¯¹ç‰©ä½“è¯†åˆ«æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œæé«˜å…¶åœ¨å®é™…æƒ…å†µä¸‹çš„é²æ£’æ€§ã€‚æ–‡ç« ä½¿ç”¨å…¬å¼€æ•°æ®é›†åˆ›å»ºäº†ä¸åŒç±»å‹çš„äº¤é€šæ ‡å¿—æ”¹å˜ï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬å¯¹YOLOv7æ¨¡å‹çš„æ£€æµ‹å’Œåˆ†ç±»èƒ½åŠ›çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œå½“æš´éœ²äºæ”¹å˜åçš„äº¤é€šæ ‡å¿—åœ¨ä¸å¤ªå¯èƒ½å‡ºç°çš„æ¡ä»¶ä¸‹æ—¶ï¼Œç‰©ä½“è¯†åˆ«æ¨¡å‹çš„å‡†ç¡®æ€§æ˜¾è‘—é™ä½ã€‚æœ¬æ–‡çš„ç ”ç©¶è¡¨æ˜ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶åœ¨å®é™…æƒ…å†µä¸‹æé«˜ç‰©ä½“è¯†åˆ«æ¨¡å‹çš„ç²¾åº¦å’Œå¯é æ€§çš„å¿…è¦æ€§å’Œé‡è¦æ€§ã€‚$http://arxiv.org/pdf/2305.05499v1
Recursions Are All You Need: Towards Efficient Deep Unfolding Networks$  The use of deep unfolding networks in compressive sensing (CS) has seen widesuccess as they provide both simplicity and interpretability. However, sincemost deep unfolding networks are iterative, this incurs significantredundancies in the network. In this work, we propose a novel recursion-basedframework to enhance the efficiency of deep unfolding models. First, recursionsare used to effectively eliminate the redundancies in deep unfolding networks.Secondly, we randomize the number of recursions during training to decrease theoverall training time. Finally, to effectively utilize the power of recursions,we introduce a learnable unit to modulate the features of the model based onboth the total number of iterations and the current iteration index. Toevaluate the proposed framework, we apply it to both ISTA-Net+ and COAST.Extensive testing shows that our proposed framework allows the network to cutdown as much as 75% of its learnable parameters while mostly maintaining itsperformance, and at the same time, it cuts around 21% and 42% from the trainingtime for ISTA-Net+ and COAST respectively. Moreover, when presented with alimited training dataset, the recursive models match or even outperform theirrespective non-recursive baseline. Codes and pretrained models are available athttps://github.com/Rawwad-Alhejaili/Recursions-Are-All-You-Need .$Keywords: compressive sensing, deep unfolding networks, recursion-based framework, learnable unit, ISTA-Net+, COAST.$ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šRawwad Alhejaili, Motaz Alfarraj, Hamzah Luqman, and Ali Al-Shaikhiï¼ˆKing Fahd University of Petroleum and Minerals, Saudi Arabiaï¼‰$æœ¬è®ºæ–‡æå‡ºä¸€ç§åŸºäºé€’å½’çš„æ¡†æ¶ï¼Œä»¥æé«˜æ·±åº¦å±•å¼€æ¨¡å‹çš„æ•ˆç‡ã€‚è¯¥æ¡†æ¶é¦–å…ˆä½¿ç”¨é€’å½’æ¥æœ‰æ•ˆæ¶ˆé™¤æ·±åº¦å±•å¼€ç½‘ç»œä¸­çš„å†—ä½™ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºé€‰æ‹©é€’å½’æ¬¡æ•°æ¥å‡å°‘æ€»ä½“è®­ç»ƒæ—¶é—´ã€‚æœ€åï¼Œä¸ºäº†æœ‰æ•ˆåˆ©ç”¨é€’å½’çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¯å­¦ä¹ å•å…ƒï¼ŒåŸºäºè¿­ä»£æ¬¡æ•°å’Œå½“å‰è¿­ä»£ç´¢å¼•æ¥è°ƒèŠ‚æ¨¡å‹çš„ç‰¹å¾ã€‚ä½œè€…å°†è¯¥æ¡†æ¶åº”ç”¨äºISTA-Net+å’ŒCOASTï¼Œå¹¶è¿›è¡Œäº†å¹¿æ³›çš„æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å¯ä»¥ä½¿ç½‘ç»œå‰Šå‡é«˜è¾¾75%çš„å¯å­¦ä¹ å‚æ•°ï¼ŒåŒæ—¶å¤§å¹…å‡å°‘ISTA-Net+å’ŒCOASTçš„è®­ç»ƒæ—¶é—´ã€‚æ­¤å¤–ï¼Œåœ¨åªæœ‰æœ‰é™è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€’å½’æ¨¡å‹ä¸å…¶éé€’å½’åŸºçº¿ç›¸å½“ç”šè‡³æ›´ä¼˜ã€‚$http://arxiv.org/pdf/2305.05505v1
AudioSlots: A slot-centric generative model for audio separation$  In a range of recent works, object-centric architectures have been shown tobe suitable for unsupervised scene decomposition in the vision domain. Inspiredby these methods we present AudioSlots, a slot-centric generative model forblind source separation in the audio domain. AudioSlots is built usingpermutation-equivariant encoder and decoder networks. The encoder network basedon the Transformer architecture learns to map a mixed audio spectrogram to anunordered set of independent source embeddings. The spatial broadcast decodernetwork learns to generate the source spectrograms from the source embeddings.We train the model in an end-to-end manner using a permutation invariant lossfunction. Our results on Libri2Mix speech separation constitute a proof ofconcept that this approach shows promise. We discuss the results andlimitations of our approach in detail, and further outline potential ways toovercome the limitations and directions for future work.$Keywords: audio separation, blind source separation, set-structured data, permutation-equivariant encoder, transformer architecture, slot-based attention mechanism.$ä½œè€…ï¼šPradyumna Reddyï¼ˆUniversity College Londonï¼‰ï¼ŒScott Wisdom, Klaus Greff, John R. Hershey, Thomas Kipfï¼ˆGoogle Researchï¼‰$è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹éŸ³é¢‘åˆ†ç¦»çš„åŸºäºæ’æ§½ï¼ˆSlot-Centricï¼‰çš„ç”Ÿæˆæ¨¡å‹â€”â€”AudioSlotsã€‚è¯¥æ¨¡å‹åˆ©ç”¨ç½®æ¢ç­‰å˜ç¼–/è§£ç å™¨ç½‘ç»œï¼Œå…¶ä¸­ç¼–ç å™¨ç½‘ç»œåŸºäºTransformeræ¶æ„ï¼Œå­¦ä¹ å°†æ··åˆéŸ³é¢‘è°±å›¾æ˜ å°„åˆ°ç‹¬ç«‹æºåµŒå…¥çš„æ— åºé›†åˆä¸Šï¼Œè€Œç©ºé—´å¹¿æ’­è§£ç å™¨ç½‘ç»œåˆ™å­¦ä¹ ä»æºåµŒå…¥ç”Ÿæˆæºè°±å›¾ã€‚é€šè¿‡åŒ¹é…æŸå¤±æ¥è®­ç»ƒæ¨¡å‹ï¼Œå°†æ··åˆéŸ³é¢‘ä¿¡å·è½¬æ¢ä¸ºç‹¬ç«‹çš„æºä¿¡å·ã€‚ä½œè€…åœ¨Libri2Mixè¯­éŸ³åˆ†ç¦»æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œç»“æœè¡¨æ˜è¿™ç§æ–¹æ³•å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›ï¼Œä½†è¿˜å­˜åœ¨ä¸€äº›æŒ‘æˆ˜ï¼Œä¾‹å¦‚éš¾ä»¥ç”Ÿæˆé«˜é¢‘ç‡çš„è¯¦ç»†ä¿¡æ¯ï¼Œéœ€è¦ä¾èµ–å¯å‘å¼æ–¹æ³•æ¥æ‹¼æ¥ç‹¬ç«‹é¢„æµ‹çš„éŸ³é¢‘å—ï¼Œä»¥åŠä»éœ€è¦ä½¿ç”¨åœ°é¢çœŸå®å‚è€ƒéŸ³é¢‘æºè¿›è¡Œè®­ç»ƒç­‰ã€‚ä½œè€…å¯¹å¦‚ä½•è§£å†³è¿™äº›é—®é¢˜è¿›è¡Œäº†è®¨è®ºï¼Œå¹¶æå‡ºäº†æœªæ¥å·¥ä½œçš„å¯èƒ½æ–¹å‘ã€‚$http://arxiv.org/pdf/2305.05591v1
PET-NeuS: Positional Encoding Tri-Planes for Neural Surfaces$  A signed distance function (SDF) parametrized by an MLP is a commoningredient of neural surface reconstruction. We build on the successful recentmethod NeuS to extend it by three new components. The first component is toborrow the tri-plane representation from EG3D and represent signed distancefields as a mixture of tri-planes and MLPs instead of representing it with MLPsonly. Using tri-planes leads to a more expressive data structure but will alsointroduce noise in the reconstructed surface. The second component is to use anew type of positional encoding with learnable weights to combat noise in thereconstruction process. We divide the features in the tri-plane into multiplefrequency scales and modulate them with sin and cos functions of differentfrequencies. The third component is to use learnable convolution operations onthe tri-plane features using self-attention convolution to produce featureswith different frequency bands. The experiments show that PET-NeuS achieveshigh-fidelity surface reconstruction on standard datasets. Following previouswork and using the Chamfer metric as the most important way to measure surfacereconstruction quality, we are able to improve upon the NeuS baseline by 57% onNerf-synthetic (0.84 compared to 1.97) and by 15.5% on DTU (0.71 compared to0.84). The qualitative evaluation reveals how our method can better control theinterference of high-frequency noise. Code available at\\url{https://github.com/yiqun-wang/PET-NeuS}.$Keywords: neural surfaces, signed distance function, MLP, tri-plane representation, positional encoding, learnable convolution, self-attention convolution, surface reconstruction.$"ä½œè€…åï¼ˆæœºæ„ï¼‰ï¼šç‹ä¸€ç¾¤ï¼ˆé‡åº†å¤§å­¦ï¼Œæ²™ç‰¹é˜¿æ‹‰ä¼¯å›½ç‹ç§‘æŠ€å¤§å­¦ï¼‰ã€Ivan Skorokhodovï¼ˆæ²™ç‰¹é˜¿æ‹‰ä¼¯å›½ç‹ç§‘æŠ€å¤§å­¦ï¼‰ã€Peter Wonkaï¼ˆæ²™ç‰¹é˜¿æ‹‰ä¼¯å›½ç‹ç§‘æŠ€å¤§å­¦ï¼‰
æ–‡ç« æ ‡é¢˜ï¼šPET-NeuS: Positional Encoding Tri-Planes for Neural Surfaces"$æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPET-NeuSçš„ç¥ç»è¡¨é¢é‡æ„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ä¸‰ä¸ªæ–°ç»„ä»¶æ¥æ‰©å±•å·²æœ‰æ–¹æ³•NeuSã€‚ç¬¬ä¸€ä¸ªç»„ä»¶æ˜¯å€Ÿé‰´EG3Dçš„ä¸‰é¢ä½“è¡¨ç¤ºæ–¹æ³•ï¼Œå°†å¸¦ç¬¦å·è·ç¦»åœºè¡¨ç¤ºä¸ºä¸‰é¢ä½“å’ŒMLPsçš„æ··åˆç‰©ï¼Œä»è€Œæé«˜è¡¨è¾¾èƒ½åŠ›ã€‚ç¬¬äºŒä¸ªç»„ä»¶æ˜¯é‡‡ç”¨ä¸€ç§æ–°çš„å¯å­¦ä¹ æƒé‡çš„ä½ç½®ç¼–ç æ¥å‡å°‘é‡æ„è¿‡ç¨‹ä¸­çš„å™ªå£°ã€‚ç¬¬ä¸‰ä¸ªç»„ä»¶æ˜¯ä½¿ç”¨è‡ªæˆ‘æ³¨æ„å·ç§¯æ“ä½œå¯¹ä¸‰é¢ä½“ç‰¹å¾è¿›è¡Œå·ç§¯æ“ä½œï¼Œä»¥äº§ç”Ÿä¸åŒé¢‘å¸¦çš„ç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼ŒPET-NeuSåœ¨æ ‡å‡†æ•°æ®é›†ä¸Šå®ç°äº†é«˜ä¿çœŸåº¦çš„è¡¨é¢é‡æ„ï¼Œå¹¶ä¸”åœ¨Nerf-syntheticæ•°æ®é›†ä¸Šæ¯”NeuSåŸºçº¿æé«˜äº†57%ï¼Œåœ¨DTUæ•°æ®é›†ä¸Šæé«˜äº†15.5%ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ§åˆ¶é«˜é¢‘å™ªå£°çš„å¹²æ‰°ï¼Œæœ‰æ•ˆåœ°åˆ©ç”¨äº†å…¶æ”¹è¿›çš„å±€éƒ¨è¡¨è¾¾èƒ½åŠ›æ¥é‡æ„å±€éƒ¨ç»†èŠ‚ã€‚$http://arxiv.org/pdf/2305.05594v1
Predicting Cardiovascular Disease Risk using Photoplethysmography and  Deep Learning$  Cardiovascular diseases (CVDs) are responsible for a large proportion ofpremature deaths in low- and middle-income countries. Early CVD detection andintervention is critical in these populations, yet many existing CVD riskscores require a physical examination or lab measurements, which can bechallenging in such health systems due to limited accessibility. Here weinvestigated the potential to use photoplethysmography (PPG), a sensingtechnology available on most smartphones that can potentially enablelarge-scale screening at low cost, for CVD risk prediction. We developed a deeplearning PPG-based CVD risk score (DLS) to predict the probability of havingmajor adverse cardiovascular events (MACE: non-fatal myocardial infarction,stroke, and cardiovascular death) within ten years, given only age, sex,smoking status and PPG as predictors. We compared the DLS with the office-basedrefit-WHO score, which adopts the shared predictors from WHO and Globoriskscores (age, sex, smoking status, height, weight and systolic blood pressure)but refitted on the UK Biobank (UKB) cohort. In UKB cohort, DLS\'s C-statistic(71.1%, 95% CI 69.9-72.4) was non-inferior to office-based refit-WHO score(70.9%, 95% CI 69.7-72.2; non-inferiority margin of 2.5%, p&lt;0.01). Thecalibration of the DLS was satisfactory, with a 1.8% mean absolute calibrationerror. Adding DLS features to the office-based score increased the C-statisticby 1.0% (95% CI 0.6-1.4). DLS predicts ten-year MACE risk comparable with theoffice-based refit-WHO score. It provides a proof-of-concept and suggests thepotential of a PPG-based approach strategies for community-based primaryprevention in resource-limited regions.$The article focuses on predicting cardiovascular disease risk using photoplethysmography and deep learning, authored by a team from Google LLC. The key areas of focus include machine learning, cardiovascular disease, risk prediction, photoplethysmography, and epidemiology.$"ä½œè€…ï¼šWei-Hung Wengï¼ˆGoogle LLCï¼‰ã€Sebastien Baurï¼ˆGoogle LLCï¼‰ã€Mayank Daswaniï¼ˆGoogle LLCï¼‰ã€Christina Chenï¼ˆGoogle LLCï¼‰ã€Lauren Harrellï¼ˆGoogle LLCï¼‰ã€Sujay Kakarmathï¼ˆGoogle LLCï¼‰ã€Mariam Jabaraï¼ˆGoogle LLCï¼‰ã€Babak Behsazï¼ˆGoogle LLCï¼‰ã€Cory Y. McLeanï¼ˆGoogle LLCï¼‰ã€Yossi Matiasï¼ˆGoogle LLCï¼‰ã€Greg S. Corradoï¼ˆGoogle LLCï¼‰ã€Shravya Shettyï¼ˆGoogle LLCï¼‰ã€Shruthi Prabhakaraï¼ˆGoogle LLCï¼‰ã€Yun Liuï¼ˆGoogle LLCï¼‰ã€Diego Ardilaï¼ˆGoogle LLCï¼‰
æœºæ„ï¼šGoogle LLC, Mountain View, CA, USAï¼›Department of Global Health and Population, Department of Epidemiology, Harvard School of Public Health, Boston, MA, USA"$æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å…‰ç”µæµ‹å®¹æŠ€æœ¯å’Œæ·±åº¦å­¦ä¹ æ¥é¢„æµ‹å¿ƒè¡€ç®¡ç–¾ç—…é£é™©çš„å¯èƒ½æ€§ã€‚è¯¥ç ”ç©¶ç”±è°·æ­Œå…¬å¸çš„ä¸€ç»„ç§‘å­¦å®¶å’Œå“ˆä½›å…¬å…±å«ç”Ÿå­¦é™¢çš„ä¸€åå­¦è€…åˆä½œå®Œæˆã€‚ä½œè€…ä»¬ä½¿ç”¨äº†å¤§å‹å¿ƒè¡€ç®¡å¥åº·ç ”ç©¶ä¸­å¿ƒæ”¶é›†çš„åŒ¿åæ•°æ®é›†ï¼Œé€šè¿‡è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹ï¼ŒæˆåŠŸåœ°é¢„æµ‹äº†å¿ƒè¡€ç®¡ç–¾ç—…é£é™©ã€‚è¯¥æ–¹æ³•å¯èƒ½æœ‰åŠ©äºæ—©æœŸè¯†åˆ«å¿ƒè¡€ç®¡ç–¾ç—…é£é™©ï¼Œå¹¶å¯ä¸ºåŒ»ç–—ä¿å¥æä¾›æ›´åŠ ä¸ªæ€§åŒ–å’Œæœ‰æ•ˆçš„é¢„é˜²å’Œæ²»ç–—ã€‚$http://arxiv.org/pdf/2305.05648v1
ImageBind: One Embedding Space To Bind Them All$  We present ImageBind, an approach to learn a joint embedding across sixdifferent modalities - images, text, audio, depth, thermal, and IMU data. Weshow that all combinations of paired data are not necessary to train such ajoint embedding, and only image-paired data is sufficient to bind themodalities together. ImageBind can leverage recent large scale vision-languagemodels, and extends their zero-shot capabilities to new modalities just byusing their natural pairing with images. It enables novel emergent applications\'out-of-the-box\' including cross-modal retrieval, composing modalities witharithmetic, cross-modal detection and generation. The emergent capabilitiesimprove with the strength of the image encoder and we set a newstate-of-the-art on emergent zero-shot recognition tasks across modalities,outperforming specialist supervised models. Finally, we show strong few-shotrecognition results outperforming prior work, and that ImageBind serves as anew way to evaluate vision models for visual and non-visual tasks.$Keywords: Multimodal learning, joint embedding, cross-modal retrieval, embedding-space arithmetic, audio to image generation.$"ä½œè€…ï¼šRohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra
æœºæ„ï¼šFAIR, Meta AI"$è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§å­¦ä¹ å…­ç§ä¸åŒæ¨¡æ€-å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘ã€æ·±åº¦ã€çƒ­åº¦å’ŒIMUæ•°æ®çš„è”åˆåµŒå…¥æ–¹æ³•ï¼Œç§°ä¸ºIMAGE BINDã€‚è¯¥æ–¹æ³•å¯ä»¥é€šè¿‡ä½¿ç”¨å›¾åƒçš„è‡ªç„¶é…å¯¹æ¥æ‰©å±•æœ€è¿‘å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬èƒ½åŠ›ä»¥é€‚åº”æ–°çš„æ¨¡æ€ã€‚é€šè¿‡åœ¨åµŒå…¥ç©ºé—´ä¸­å°†è¿™äº›æ¨¡æ€è¿›è¡Œå¯¹é½ï¼ŒIMAGE BINDå¯ä»¥å®ç°æ–°å‹çš„è·¨æ¨¡æ€æ£€ç´¢ã€æ¨¡æ€ç»„åˆä¸ç®—æœ¯ã€è·¨æ¨¡æ€æ£€æµ‹å’Œç”Ÿæˆç­‰åº”ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¸“é—¨çš„ç›‘ç£æ¨¡å‹ç›¸æ¯”ï¼ŒIMAGE BINDåœ¨ç´§æ€¥é›¶æ ·æœ¬è¯†åˆ«ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°çš„å›½é™…å…ˆè¿›æ°´å¹³ï¼Œå¹¶ä¸”åœ¨å°‘é‡æ•°æ®è¯†åˆ«æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚$http://arxiv.org/pdf/2305.05665v1
TidyBot: Personalized Robot Assistance with Large Language Models$  For a robot to personalize physical assistance effectively, it must learnuser preferences that can be generally reapplied to future scenarios. In thiswork, we investigate personalization of household cleanup with robots that cantidy up rooms by picking up objects and putting them away. A key challenge isdetermining the proper place to put each object, as people\'s preferences canvary greatly depending on personal taste or cultural background. For instance,one person may prefer storing shirts in the drawer, while another may preferthem on the shelf. We aim to build systems that can learn such preferences fromjust a handful of examples via prior interactions with a particular person. Weshow that robots can combine language-based planning and perception with thefew-shot summarization capabilities of large language models (LLMs) to infergeneralized user preferences that are broadly applicable to futureinteractions. This approach enables fast adaptation and achieves 91.2% accuracyon unseen objects in our benchmark dataset. We also demonstrate our approach ona real-world mobile manipulator called TidyBot, which successfully puts away85.0% of objects in real-world test scenarios.$Keywords: service robotics, mobile manipulation, personalized assistance, household cleanup, large language models, user preferences, few-shot learning, language-based planning, perception.$"ä½œè€…ï¼šJimmy Wuï¼ˆPrinceton Universityï¼‰ã€Rika Antonovaï¼ˆStanford Universityï¼‰ã€Adam Kanï¼ˆThe Nueva Schoolï¼‰ã€Marion Lepertï¼ˆStanford Universityï¼‰ã€Andy Zengï¼ˆGoogleï¼‰ã€Shuran Songï¼ˆColumbia Universityï¼‰ã€Jeannette Bohgï¼ˆStanford Universityï¼‰ã€Szymon Rusinkiewiczï¼ˆPrinceton Universityï¼‰ã€Thomas Funkhouserï¼ˆPrinceton Universityã€Googleï¼‰ã€‚

æœºæ„ï¼š1. Princeton University, Princeton, NJ, USA. 2. Stanford University, Stanford, CA, USA. 3. The Nueva School, San Mateo, CA, USA. 4. Google, Mountain View, CA, USA. 5. Columbia University, New York, NY, USA."$æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œä¸ªæ€§åŒ–å®¶å±…æ¸…æ´æœåŠ¡çš„ç ”ç©¶ã€‚é’ˆå¯¹å¦‚ä½•ç¡®å®šæ¯ä¸ªç‰©å“çš„æ­£ç¡®æ”¾ç½®ä½ç½®ï¼Œæ¢è®¨äº†é€šè¿‡æœºå™¨äººä¸ç”¨æˆ·çš„äº¤äº’æ¥å­¦ä¹ ç”¨æˆ·åå¥½çš„æ–¹æ³•ã€‚é€šè¿‡å°†è‡ªç„¶è¯­è¨€å¤„ç†å’Œæ„ŸçŸ¥æŠ€æœ¯ä¸LLMç»“åˆï¼Œæœºå™¨äººå¯ä»¥ä»å°‘é‡çš„äº¤äº’ä¸­æ¨æ–­å‡ºç”¨æˆ·çš„æ™®éåå¥½ï¼Œå¹¶åœ¨æœªæ¥çš„äº¤äº’ä¸­åº”ç”¨ã€‚å®éªŒè¯æ˜è¿™ç§æ–¹æ³•å®ç°äº†å¿«é€Ÿé€‚åº”ï¼Œå¹¶åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šè¾¾åˆ°91.2%çš„å‡†ç¡®ç‡ã€‚æ–‡ä¸­è¿˜ä»‹ç»äº†çœŸå®ä¸–ç•Œç§»åŠ¨æœºæ¢°è‡‚TidyBotçš„åº”ç”¨ï¼ŒæˆåŠŸå°†85.0%çš„ç‰©å“æ”¾å›å…¶æ­£ç¡®ä½ç½®ã€‚å…³é”®è¯ï¼šæœåŠ¡æœºå™¨äººï¼Œç§»åŠ¨æœºæ¢°è‡‚ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ã€‚$http://arxiv.org/pdf/2305.05658v1
ShapeCoder: Discovering Abstractions for Visual Programs from  Unstructured Primitives$"  Programs are an increasingly popular representation for visual data, exposingcompact, interpretable structure that supports manipulation. Visual programsare usually written in domain-specific languages (DSLs). Finding ""good""programs, that only expose meaningful degrees of freedom, requires access to aDSL with a ""good"" library of functions, both of which are typically authored bydomain experts. We present ShapeCoder, the first system capable of taking adataset of shapes, represented with unstructured primitives, and jointlydiscovering (i) useful abstraction functions and (ii) programs that use theseabstractions to explain the input shapes. The discovered abstractions capturecommon patterns (both structural and parametric) across the dataset, so thatprograms rewritten with these abstractions are more compact, and expose fewerdegrees of freedom. ShapeCoder improves upon previous abstraction discoverymethods, finding better abstractions, for more complex inputs, under lessstringent input assumptions. This is principally made possible by twomethodological advancements: (a) a shape to program recognition network thatlearns to solve sub-problems and (b) the use of e-graphs, augmented with aconditional rewrite scheme, to determine when abstractions with complexparametric expressions can be applied, in a tractable manner. We evaluateShapeCoder on multiple datasets of 3D shapes, where primitive decompositionsare either parsed from manual annotations or produced by an unsupervised cuboidabstraction method. In all domains, ShapeCoder discovers a library ofabstractions that capture high-level relationships, remove extraneous degreesof freedom, and achieve better dataset compression compared with alternativeapproaches. Finally, we investigate how programs rewritten to use discoveredabstractions prove useful for downstream tasks."$Keywords: Shape modeling, procedural modeling, visual programs, shape analysis, shape abstraction, library learning, e-graph. The article describes ShapeCoder, which discovers useful abstraction functions and programs that use these abstractions to explain unstructured primitive representations of shapes. The discovered abstractions capture common patterns so that programs rewritten with these abstractions are more compact and better constrain the visual data they represent. This is achieved through methodological advancements such as a shape-to-program recognition network and the use of e-graphs. The system is evaluated on multiple datasets of 3D shapes and proves useful for downstream tasks.$ä½œè€…ï¼šR. Kenny Jones (Brown University, USA)ï¼ŒPaul Guerrero (Adobe Research, United Kingdom)ï¼ŒNiloy J. Mitra (University College London and Adobe Research, United Kingdom)ï¼ŒDaniel Ritchie (Brown University, USA)ã€‚$è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºShapeCoderçš„ç³»ç»Ÿï¼Œå¯ä»¥ä»ç”¨éç»“æ„åŒ–åŸºå…ƒè¡¨ç¤ºçš„å½¢çŠ¶æ•°æ®é›†ä¸­å‘ç°æœ‰ç”¨çš„æŠ½è±¡å‡½æ•°å’Œç¨‹åºï¼Œä»¥æ›´ç´§å‡‘åœ°è§£é‡Šè¾“å…¥å½¢çŠ¶ã€‚ShapeCoderçš„æŠ½è±¡æ•æ‰æ•°æ®é›†ä¸­çš„å¸¸è§æ¨¡å¼ï¼Œåˆ©ç”¨è¿™äº›æŠ½è±¡é‡å†™çš„ç¨‹åºæ›´ç´§å‡‘ï¼ŒæŠ‘åˆ¶äº†è™šå‡çš„è‡ªç”±åº¦ã€‚ShapeCoderé€šè¿‡ä¸¤ä¸ªæ–¹æ³•è®ºè¿›æ­¥å®ç°äº†æ›´å¥½çš„æŠ½è±¡å‘ç°æ–¹æ³•ï¼Œæ‰¾åˆ°äº†æ›´å¥½çš„æŠ½è±¡å‡½æ•°ï¼Œé€‚ç”¨äºæ›´å¤æ‚çš„è¾“å…¥ï¼Œä¸å—ä¸¥æ ¼çš„è¾“å…¥å‡è®¾çš„é™åˆ¶ã€‚å®ƒé¦–å…ˆé€šè¿‡ä¸€ç»„æ‰‹åŠ¨æ³¨é‡Šè§£ææˆ–èšç±»æŠ½è±¡ç®—æ³•ç”ŸæˆåŸºå…ƒåˆ†è§£çš„3Då½¢çŠ¶æ•°æ®é›†è¿›è¡Œäº†æµ‹è¯•ï¼Œç„¶åå‘ç°äº†ä¸€ç»„æŠ½è±¡å‡½æ•°åº“ï¼Œä»è€Œæ›´å¥½åœ°å‹ç¼©æ•°æ®é›†å¹¶æé«˜äº†æ•°æ®é›†çš„å‹ç¼©æ€§èƒ½ã€‚æœ€åï¼Œé€šè¿‡ä¸‹æ¸¸ä»»åŠ¡çš„ç ”ç©¶ï¼Œè¯æ˜äº†åˆ©ç”¨æ‰€å‘ç°çš„æŠ½è±¡çš„ç¨‹åºçš„å®ç”¨æ€§ã€‚$http://arxiv.org/pdf/2305.05661v1
